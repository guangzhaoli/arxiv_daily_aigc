[
    {
        "title": "Dynamic-TreeRPO: Breaking the Independent Trajectory Bottleneck with Structured Sampling",
        "summary": "The integration of Reinforcement Learning (RL) into flow matching models for\ntext-to-image (T2I) generation has driven substantial advances in generation\nquality. However, these gains often come at the cost of exhaustive exploration\nand inefficient sampling strategies due to slight variation in the sampling\ngroup. Building on this insight, we propose Dynamic-TreeRPO, which implements\nthe sliding-window sampling strategy as a tree-structured search with dynamic\nnoise intensities along depth. We perform GRPO-guided optimization and\nconstrained Stochastic Differential Equation (SDE) sampling within this tree\nstructure. By sharing prefix paths of the tree, our design effectively\namortizes the computational overhead of trajectory search. With well-designed\nnoise intensities for each tree layer, Dynamic-TreeRPO can enhance the\nvariation of exploration without any extra computational cost. Furthermore, we\nseamlessly integrate Supervised Fine-Tuning (SFT) and RL paradigm within\nDynamic-TreeRPO to construct our proposed LayerTuning-RL, reformulating the\nloss function of SFT as a dynamically weighted Progress Reward Model (PRM)\nrather than a separate pretraining method. By associating this weighted PRM\nwith dynamic-adaptive clipping bounds, the disruption of exploration process in\nDynamic-TreeRPO is avoided. Benefiting from the tree-structured sampling and\nthe LayerTuning-RL paradigm, our model dynamically explores a diverse search\nspace along effective directions. Compared to existing baselines, our approach\ndemonstrates significant superiority in terms of semantic consistency, visual\nfidelity, and human preference alignment on established benchmarks, including\nHPS-v2.1, PickScore, and ImageReward. In particular, our model outperforms SoTA\nby $4.9\\%$, $5.91\\%$, and $8.66\\%$ on those benchmarks, respectively, while\nimproving the training efficiency by nearly $50\\%$.",
        "url": "http://arxiv.org/abs/2509.23352v1",
        "published_date": "2025-09-27T14:59:31+00:00",
        "updated_date": "2025-09-27T14:59:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xiaolong Fu",
            "Lichen Ma",
            "Zipeng Guo",
            "Gaojing Zhou",
            "Chongxiao Wang",
            "ShiPing Dong",
            "Shizhe Zhou",
            "Shizhe Zhou",
            "Ximan Liu",
            "Jingling Fu",
            "Tan Lit Sin",
            "Yu Shi",
            "Zhen Chen",
            "Junshi Huang",
            "Jason Li"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Dataset"
        ],
        "tldr": "Dynamic-TreeRPO proposes a tree-structured search strategy with dynamic noise intensities to improve exploration efficiency in Reinforcement Learning models for text-to-image generation.",
        "tldr_zh": "Dynamic-TreeRPO 提出了一种以动态噪声强度为特点的树结构搜索策略，以提高强化学习模型在文本到图像生成中的探索效率。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "Self-Consistency as a Free Lunch: Reducing Hallucinations in Vision-Language Models via Self-Reflection",
        "summary": "Vision-language models often hallucinate details, generating non-existent\nobjects or inaccurate attributes that compromise output reliability. Existing\nmethods typically address these issues via extensive human annotations or\nexternal supervision from more powerful models. In this work, we present a\nnovel framework that leverages the model's self-consistency between long\nresponses and short answers to generate preference pairs for training. We\nobserve that short binary questions tend to yield highly reliable responses,\nwhich can be used to query the target model to evaluate and rank its generated\nresponses. Specifically, we design a self-reflection pipeline where detailed\nmodel responses are compared against concise binary answers, and inconsistency\nsignals are utilized to automatically curate high-quality training data without\nhuman annotations or external model-based supervision. By relying solely on\nself-consistency rather than external supervision, our method offers a scalable\nand efficient solution that effectively reduces hallucinations using unlabeled\ndata. Extensive experiments on multiple benchmarks, i.e., AMBER,\nMultiObject-Hal (ROPE), Object HalBench, and MMHal-Bench, demonstrate\nsignificant improvements in factual grounding and reliability. Moreover, our\napproach maintains robust instruction-following ability, as evidenced by\nenhanced performance on LLaVA-Bench and MMBench.",
        "url": "http://arxiv.org/abs/2509.23236v1",
        "published_date": "2025-09-27T10:37:11+00:00",
        "updated_date": "2025-09-27T10:37:11+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mingfei Han",
            "Haihong Hao",
            "Jinxing Zhou",
            "Zhihui Li",
            "Yuhui Zheng",
            "Xueqing Deng",
            "Linjie Yang",
            "Xiaojun Chang"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper introduces a self-consistency framework to reduce hallucinations in vision-language models without external supervision, showing significant improvements in factual grounding and reliability.",
        "tldr_zh": "本文介绍了一种自一致性框架，用于减少视觉-语言模型中的幻觉，无需外部监督，显示出在事实基础和可靠性方面的显著改善。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Sparse2Dense: A Keypoint-driven Generative Framework for Human Video Compression and Vertex Prediction",
        "summary": "For bandwidth-constrained multimedia applications, simultaneously achieving\nultra-low bitrate human video compression and accurate vertex prediction\nremains a critical challenge, as it demands the harmonization of dynamic motion\nmodeling, detailed appearance synthesis, and geometric consistency. To address\nthis challenge, we propose Sparse2Dense, a keypoint-driven generative framework\nthat leverages extremely sparse 3D keypoints as compact transmitted symbols to\nenable ultra-low bitrate human video compression and precise human vertex\nprediction. The key innovation is the multi-task learning-based and\nkeypoint-aware deep generative model, which could encode complex human motion\nvia compact 3D keypoints and leverage these sparse keypoints to estimate dense\nmotion for video synthesis with temporal coherence and realistic textures.\nAdditionally, a vertex predictor is integrated to learn human vertex geometry\nthrough joint optimization with video generation, ensuring alignment between\nvisual content and geometric structure. Extensive experiments demonstrate that\nthe proposed Sparse2Dense framework achieves competitive compression\nperformance for human video over traditional/generative video codecs, whilst\nenabling precise human vertex prediction for downstream geometry applications.\nAs such, Sparse2Dense is expected to facilitate bandwidth-efficient\nhuman-centric media transmission, such as real-time motion analysis, virtual\nhuman animation, and immersive entertainment.",
        "url": "http://arxiv.org/abs/2509.23169v1",
        "published_date": "2025-09-27T07:54:56+00:00",
        "updated_date": "2025-09-27T07:54:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bolin Chen",
            "Ru-Ling Liao",
            "Yan Ye",
            "Jie Chen",
            "Shanzhi Yin",
            "Xinrui Ju",
            "Shiqi Wang",
            "Yibo Fan"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Transformer"
        ],
        "tldr": "Sparse2Dense proposes a keypoint-driven framework for human video compression and vertex prediction, achieving low bitrate compression and accurate vertex prediction.",
        "tldr_zh": "Sparse2Dense提出了一个基于关键点的框架，用于人类视频压缩和顶点预测，实现了低比特率压缩和准确的顶点预测。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Stochastic Interpolants via Conditional Dependent Coupling",
        "summary": "Existing image generation models face critical challenges regarding the\ntrade-off between computation and fidelity. Specifically, models relying on a\npretrained Variational Autoencoder (VAE) suffer from information loss, limited\ndetail, and the inability to support end-to-end training. In contrast, models\noperating directly in the pixel space incur prohibitive computational cost.\nAlthough cascade models can mitigate computational cost, stage-wise separation\nprevents effective end-to-end optimization, hampers knowledge sharing, and\noften results in inaccurate distribution learning within each stage. To address\nthese challenges, we introduce a unified multistage generative framework based\non our proposed Conditional Dependent Coupling strategy. It decomposes the\ngenerative process into interpolant trajectories at multiple stages, ensuring\naccurate distribution learning while enabling end-to-end optimization.\nImportantly, the entire process is modeled as a single unified Diffusion\nTransformer, eliminating the need for disjoint modules and also enabling\nknowledge sharing. Extensive experiments demonstrate that our method achieves\nboth high fidelity and efficiency across multiple resolutions.",
        "url": "http://arxiv.org/abs/2509.23122v1",
        "published_date": "2025-09-27T05:03:08+00:00",
        "updated_date": "2025-09-27T05:03:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chenrui Ma",
            "Xi Xiao",
            "Tianyang Wang",
            "Xiao Wang",
            "Yanning Shen"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "GAN"
        ],
        "tldr": "The paper introduces a multistage generative framework to improve image generation models by ensuring accurate distribution learning and enabling end-to-end optimization, achieving high fidelity and efficiency across multiple resolutions.",
        "tldr_zh": "本文介绍了一种多阶段生成框架，通过确保准确的分布学习和启用端对端优化，改进图像生成模型，在多个分辨率上实现高保真度和效率。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "ARSS: Taming Decoder-only Autoregressive Visual Generation for View Synthesis From Single View",
        "summary": "Despite their exceptional generative quality, diffusion models have limited\napplicability to world modeling tasks, such as novel view generation from\nsparse inputs. This limitation arises because diffusion models generate outputs\nin a non-causal manner, often leading to distortions or inconsistencies across\nviews, and making it difficult to incrementally adapt accumulated knowledge to\nnew queries. In contrast, autoregressive (AR) models operate in a causal\nfashion, generating each token based on all previously generated tokens. In\nthis work, we introduce \\textbf{ARSS}, a novel framework that leverages a\nGPT-style decoder-only AR model to generate novel views from a single image,\nconditioned on a predefined camera trajectory. We employ a video tokenizer to\nmap continuous image sequences into discrete tokens and propose a camera\nencoder that converts camera trajectories into 3D positional guidance. Then to\nenhance generation quality while preserving the autoregressive structure, we\npropose a autoregressive transformer module that randomly permutes the spatial\norder of tokens while maintaining their temporal order. Extensive qualitative\nand quantitative experiments on public datasets demonstrate that our method\nperforms comparably to, or better than, state-of-the-art view synthesis\napproaches based on diffusion models. Our code will be released upon paper\nacceptance.",
        "url": "http://arxiv.org/abs/2509.23008v1",
        "published_date": "2025-09-27T00:03:09+00:00",
        "updated_date": "2025-09-27T00:03:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenbin Teng",
            "Gonglin Chen",
            "Haiwei Chen",
            "Yajie Zhao"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "ARSS introduces a decoder-only autoregressive model for view synthesis from a single image with a predefined camera trajectory, showing promising results compared to diffusion models.",
        "tldr_zh": "ARSS 提出了一种仅使用解码器的自回归模型，用于根据预定义的相机轨迹从单个图像进行视图合成，与扩散模型相比表现出有希望的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Robot Learning from Any Images",
        "summary": "We introduce RoLA, a framework that transforms any in-the-wild image into an\ninteractive, physics-enabled robotic environment. Unlike previous methods, RoLA\noperates directly on a single image without requiring additional hardware or\ndigital assets. Our framework democratizes robotic data generation by producing\nmassive visuomotor robotic demonstrations within minutes from a wide range of\nimage sources, including camera captures, robotic datasets, and Internet\nimages. At its core, our approach combines a novel method for single-view\nphysical scene recovery with an efficient visual blending strategy for\nphotorealistic data collection. We demonstrate RoLA's versatility across\napplications like scalable robotic data generation and augmentation, robot\nlearning from Internet images, and single-image real-to-sim-to-real systems for\nmanipulators and humanoids. Video results are available at\nhttps://sihengz02.github.io/RoLA .",
        "url": "http://arxiv.org/abs/2509.22970v1",
        "published_date": "2025-09-26T22:10:34+00:00",
        "updated_date": "2025-09-26T22:10:34+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Siheng Zhao",
            "Jiageng Mao",
            "Wei Chow",
            "Zeyu Shangguan",
            "Tianheng Shi",
            "Rong Xue",
            "Yuxi Zheng",
            "Yijia Weng",
            "Yang You",
            "Daniel Seita",
            "Leonidas Guibas",
            "Sergey Zakharov",
            "Vitor Guizilini",
            "Yue Wang"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "RoLA is a framework that turns any image into an interactive robotic environment for data generation and learning, democratizing robotic data generation.",
        "tldr_zh": "RoLA 是一个框架，将任何图像转化为互动机器人环境，用于数据生成和学习，使机器人数据生成更加民主化。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Soft-Di[M]O: Improving One-Step Discrete Image Generation with Soft Embeddings",
        "summary": "One-step generators distilled from Masked Diffusion Models (MDMs) compress\nmultiple sampling steps into a single forward pass, enabling efficient text and\nimage synthesis. However, they suffer two key limitations: they inherit\nmodeling bias from the teacher, and their discrete token outputs block gradient\nflow, preventing post-distillation refinements such as adversarial training,\nreward-based fine-tuning, and Test-Time Embedding Optimization (TTEO). In this\nwork, we introduce soft embeddings, a simple relaxation that replaces discrete\ntokens with the expected embeddings under the generator's output distribution.\nSoft embeddings preserve representation fidelity for one-step discrete\ngenerator while providing a fully differentiable continuous surrogate that is\ncompatible with teacher backbones and tokenizer decoders. Integrating soft\nembeddings into the Di[M]O distillation framework (denoted Soft-Di[M]O) makes\none-step generators end-to-end trainable and enables straightforward\napplication of GAN-based refinement, differentiable reward fine-tuning, and\nTTEO. Empirically, across multiple MDM teachers (e.g., MaskBit, MaskGen),\nSoft-Di[M]O achieves state-of-the-art one-step results: improved class-to-image\nperformance, a one-step FID of 1.56 on ImageNet-256 with GAN-based refinement,\nalong with higher GenEval and HPS scores on text-to-image with reward\nfine-tuning, and further gains from TTEO.",
        "url": "http://arxiv.org/abs/2509.22925v1",
        "published_date": "2025-09-26T20:51:20+00:00",
        "updated_date": "2025-09-26T20:51:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Yuanzhi Zhu",
            "Xi Wang",
            "Stéphane Lathuilière",
            "Vicky Kalogeiton"
        ],
        "ai_categories": [
            "GAN",
            "Transformer"
        ],
        "tldr": "Soft embeddings are introduced to improve one-step discrete image generation by providing a continuous surrogate, enabling end-to-end training and various refinements such as GAN-based refinement and reward fine-tuning.",
        "tldr_zh": "引入软嵌入来改善一步离散图像生成，通过提供连续替代，实现端到端训练以及各种优化，如基于GAN的改进和奖励微调。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "Convolutional Set Transformer",
        "summary": "We introduce the Convolutional Set Transformer (CST), a novel neural\narchitecture designed to process image sets of arbitrary cardinality that are\nvisually heterogeneous yet share high-level semantics - such as a common\ncategory, scene, or concept. Existing set-input networks, e.g., Deep Sets and\nSet Transformer, are limited to vector inputs and cannot directly handle 3D\nimage tensors. As a result, they must be cascaded with a feature extractor,\ntypically a CNN, which encodes images into embeddings before the set-input\nnetwork can model inter-image relationships. In contrast, CST operates directly\non 3D image tensors, performing feature extraction and contextual modeling\nsimultaneously, thereby enabling synergies between the two processes. This\ndesign yields superior performance in tasks such as Set Classification and Set\nAnomaly Detection and further provides native compatibility with CNN\nexplainability methods such as Grad-CAM, unlike competing approaches that\nremain opaque. Finally, we show that CSTs can be pre-trained on large-scale\ndatasets and subsequently adapted to new domains and tasks through standard\nTransfer Learning schemes. To support further research, we release CST-15, a\nCST backbone pre-trained on ImageNet\n(https://github.com/chinefed/convolutional-set-transformer).",
        "url": "http://arxiv.org/abs/2509.22889v1",
        "published_date": "2025-09-26T20:13:00+00:00",
        "updated_date": "2025-09-26T20:13:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Federico Chinello",
            "Giacomo Boracchi"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The Convolutional Set Transformer (CST) is a new neural architecture that processes image sets directly, showing superior performance in tasks like Set Classification and Set Anomaly Detection. It can be pre-trained on large-scale datasets and adapted to new domains through Transfer Learning.",
        "tldr_zh": "卷积集变压器（CST）是一种新的神经架构，直接处理图像集，在集合分类和异常检测等任务中表现出卓越性能。它可以在大规模数据集上进行预训练，并通过迁移学习适应新领域。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "Introducing Multimodal Paradigm for Learning Sleep Staging PSG via General-Purpose Model",
        "summary": "Sleep staging is essential for diagnosing sleep disorders and assessing\nneurological health. Existing automatic methods typically extract features from\ncomplex polysomnography (PSG) signals and train domain-specific models, which\noften lack intuitiveness and require large, specialized datasets. To overcome\nthese limitations, we introduce a new paradigm for sleep staging that leverages\nlarge multimodal general-purpose models to emulate clinical diagnostic\npractices. Specifically, we convert raw one-dimensional PSG time-series into\nintuitive two-dimensional waveform images and then fine-tune a multimodal large\nmodel to learn from these representations. Experiments on three public datasets\n(ISRUC, MASS, SHHS) demonstrate that our approach enables general-purpose\nmodels, without prior exposure to sleep data, to acquire robust staging\ncapabilities. Moreover, explanation analysis reveals our model learned to mimic\nthe visual diagnostic workflow of human experts for sleep staging by PSG\nimages. The proposed method consistently outperforms state-of-the-art baselines\nin accuracy and robustness, highlighting its efficiency and practical value for\nmedical applications. The code for the signal-to-image pipeline and the PSG\nimage dataset will be released.",
        "url": "http://arxiv.org/abs/2509.22810v1",
        "published_date": "2025-09-26T18:14:43+00:00",
        "updated_date": "2025-09-26T18:14:43+00:00",
        "categories": [
            "eess.SP",
            "cs.CV"
        ],
        "authors": [
            "Jianheng Zhou",
            "Chenyu Liu",
            "Jinan Zhou",
            "Yi Ding",
            "Yang Liu",
            "Haoran Luo",
            "Ziyu Jia",
            "Xinliang Zhou"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel approach for sleep staging using multimodal general-purpose models, outperforming existing methods in accuracy and robustness.",
        "tldr_zh": "本文介绍了一种利用多模态通用模型进行睡眠分期的新方法，优于现有方法在准确性和稳健性方面。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "AttAnchor: Guiding Cross-Modal Token Alignment in VLMs with Attention Anchors",
        "summary": "A fundamental reason for the dominance of attention over RNNs and LSTMs in\nLLMs is its ability to capture long-range dependencies by modeling direct\ninteractions between all tokens, overcoming the sequential limitations of\nrecurrent architectures. Similarly, a key reason why today's vision language\nmodels (VLMs) hallucinate and underperform pure language models is that they\nrely on direct concatenation of image and text tokens with a modality-blinded\npositional encoding, which conveniently adopts the pretrained LLM backbone but\nforces unnecessary long-distance attention between semantically related tokens\nacross modalities. This underscores the urgent need for mechanisms that\nefficiently enhance token locality and cross-modal alignment. In response, we\npropose Attention Anchor, a parameter-free framework that efficiently groups\nsemantically similar tokens across modalities, improving cross-modal locality.\nBy inserting text tokens near relevant visual patches, we create semantic\nsignposts that reveal true content-based cross-modal attention scores, guiding\nthe model to focus on the correct image regions for tasks such as VQA, MMBench\nand POPE. This improves answer accuracy and reduces hallucinations without\ndisrupting the prompt's semantic flow. AttAnchor achieves improvements across\n13 out of 15 different metrics and benchmarks, including up to 32% gains on\nreasoning tasks and up to 15% improvements on hallucination benchmarks.\nAttAnchor enables TinyLLaVA 1B to outperform much larger models like LLaVA 7B\nand QwenVL 3B on POPE with only 0.1% inference time overhead. To the best of\nour knowledge, this work is among the first to investigate mixed-modal token\ngrouping, where text and image tokens are clustered jointly into shared groups\nrather than being grouped within a single modality or merely aligned post-hoc\nwith additional alignment losses.",
        "url": "http://arxiv.org/abs/2509.23109v1",
        "published_date": "2025-09-27T04:37:26+00:00",
        "updated_date": "2025-09-27T04:37:26+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Junyang Zhang",
            "Tianyi Zhu",
            "Thierry Tambe"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality"
        ],
        "tldr": "The paper introduces Attention Anchor, a framework that enhances token locality and cross-modal alignment in vision language models by grouping semantically similar tokens across modalities. It improves answer accuracy, reduces hallucinations, and achieves significant gains on various benchmarks.",
        "tldr_zh": "这篇论文介绍了Attention Anchor框架，通过在视觉语言模型中将语义相似的标记跨模态进行分组，提高了答案准确性，减少了幻觉，并在各种基准测试中取得了显著进展。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "FishAI 2.0: Marine Fish Image Classification with Multi-modal Few-shot Learning",
        "summary": "Traditional marine biological image recognition faces challenges of\nincomplete datasets and unsatisfactory model accuracy, particularly for\nfew-shot conditions of rare species where data scarcity significantly hampers\nthe performance. To address these issues, this study proposes an intelligent\nmarine fish recognition framework, FishAI 2.0, integrating multimodal few-shot\ndeep learning techniques with image generation for data augmentation. First, a\nhierarchical marine fish benchmark dataset, which provides a comprehensive data\nfoundation for subsequent model training, is utilized to train the FishAI 2.0\nmodel. To address the data scarcity of rare classes, the large language model\nDeepSeek was employed to generate high-quality textual descriptions, which are\ninput into Stable Diffusion 2 for image augmentation through a hierarchical\ndiffusion strategy that extracts latent encoding to construct a multimodal\nfeature space. The enhanced visual-textual datasets were then fed into a\nContrastive Language-Image Pre-Training (CLIP) based model, enabling robust\nfew-shot image recognition. Experimental results demonstrate that FishAI 2.0\nachieves a Top-1 accuracy of 91.67 percent and Top-5 accuracy of 97.97 percent\nat the family level, outperforming baseline CLIP and ViT models with a\nsubstantial margin for the minority classes with fewer than 10 training\nsamples. To better apply FishAI 2.0 to real-world scenarios, at the genus and\nspecies level, FishAI 2.0 respectively achieves a Top-1 accuracy of 87.58\npercent and 85.42 percent, demonstrating practical utility. In summary, FishAI\n2.0 improves the efficiency and accuracy of marine fish identification and\nprovides a scalable technical solution for marine ecological monitoring and\nconservation, highlighting its scientific value and practical applicability.",
        "url": "http://arxiv.org/abs/2509.22930v1",
        "published_date": "2025-09-26T20:54:35+00:00",
        "updated_date": "2025-09-26T20:54:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chenghan Yang",
            "Peng Zhou",
            "Dong-Sheng Zhang",
            "Yueyun Wang",
            "Hong-Bin Shen",
            "Xiaoyong Pan"
        ],
        "ai_categories": [
            "Multimodality",
            "LoRA",
            "Diffusion",
            "Dataset"
        ],
        "tldr": "FishAI 2.0 uses multimodal few-shot learning and image generation to improve marine fish image classification accuracy, achieving high performance even for rare species with limited data.",
        "tldr_zh": "FishAI 2.0 使用多模态少样本学习和图像生成技术来提高海洋鱼类图像分类精度，即使对于数据有限的稀有物种也能取得很高的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "VideoScore2: Think before You Score in Generative Video Evaluation",
        "summary": "Recent advances in text-to-video generation have produced increasingly\nrealistic and diverse content, yet evaluating such videos remains a fundamental\nchallenge due to their multi-faceted nature encompassing visual quality,\nsemantic alignment, and physical consistency. Existing evaluators and reward\nmodels are limited to single opaque scores, lack interpretability, or provide\nonly coarse analysis, making them insufficient for capturing the comprehensive\nnature of video quality assessment. We present VideoScore2, a\nmulti-dimensional, interpretable, and human-aligned framework that explicitly\nevaluates visual quality, text-to-video alignment, and physical/common-sense\nconsistency while producing detailed chain-of-thought rationales. Our model is\ntrained on a large-scale dataset VideoFeedback2 containing 27,168\nhuman-annotated videos with both scores and reasoning traces across three\ndimensions, using a two-stage pipeline of supervised fine-tuning followed by\nreinforcement learning with Group Relative Policy Optimization (GRPO) to\nenhance analytical robustness. Extensive experiments demonstrate that\nVideoScore2 achieves superior performance with 44.35 (+5.94) accuracy on our\nin-domain benchmark VideoScore-Bench-v2 and 50.37 (+4.32) average performance\nacross four out-of-domain benchmarks (VideoGenReward-Bench, VideoPhy2, etc),\nwhile providing interpretable assessments that bridge the gap between\nevaluation and controllable generation through effective reward modeling for\nBest-of-N sampling. Project Page: https://tiger-ai-lab.github.io/VideoScore2/",
        "url": "http://arxiv.org/abs/2509.22799v1",
        "published_date": "2025-09-26T18:09:03+00:00",
        "updated_date": "2025-09-26T18:09:03+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Xuan He",
            "Dongfu Jiang",
            "Ping Nie",
            "Minghao Liu",
            "Zhengxuan Jiang",
            "Mingyi Su",
            "Wentao Ma",
            "Junru Lin",
            "Chun Ye",
            "Yi Lu",
            "Keming Wu",
            "Benjamin Schneider",
            "Quy Duc Do",
            "Zhuofeng Li",
            "Yiming Jia",
            "Yuxuan Zhang",
            "Guo Cheng",
            "Haozhe Wang",
            "Wangchunshu Zhou",
            "Qunshu Lin",
            "Yuanxing Zhang",
            "Ge Zhang",
            "Wenhao Huang",
            "Wenhu Chen"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Transformer"
        ],
        "tldr": "VideoScore2 is a multi-dimensional framework for evaluating text-to-video generation, providing detailed rationale and improved performance in video quality assessment.",
        "tldr_zh": "VideoScore2是一个多维框架，用于评估文本到视频生成，在视频质量评估方面提供了详细的解释和改进性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "DEFT: Decompositional Efficient Fine-Tuning for Text-to-Image Models",
        "summary": "Efficient fine-tuning of pre-trained Text-to-Image (T2I) models involves\nadjusting the model to suit a particular task or dataset while minimizing\ncomputational resources and limiting the number of trainable parameters.\nHowever, it often faces challenges in striking a trade-off between aligning\nwith the target distribution: learning a novel concept from a limited image for\npersonalization and retaining the instruction ability needed for unifying\nmultiple tasks, all while maintaining editability (aligning with a variety of\nprompts or in-context generation). In this work, we introduce DEFT,\nDecompositional Efficient Fine-Tuning, an efficient fine-tuning framework that\nadapts a pre-trained weight matrix by decomposing its update into two\ncomponents with two trainable matrices: (1) a projection onto the complement of\na low-rank subspace spanned by a low-rank matrix, and (2) a low-rank update.\nThe single trainable low-rank matrix defines the subspace, while the other\ntrainable low-rank matrix enables flexible parameter adaptation within that\nsubspace. We conducted extensive experiments on the Dreambooth and Dreambench\nPlus datasets for personalization, the InsDet dataset for object and scene\nadaptation, and the VisualCloze dataset for a universal image generation\nframework through visual in-context learning with both Stable Diffusion and a\nunified model. Our results demonstrated state-of-the-art performance,\nhighlighting the emergent properties of efficient fine-tuning. Our code is\navailable on \\href{https://github.com/MAXNORM8650/DEFT}{DEFTBase}.",
        "url": "http://arxiv.org/abs/2509.22793v1",
        "published_date": "2025-09-26T18:01:15+00:00",
        "updated_date": "2025-09-26T18:01:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Komal Kumar",
            "Rao Muhammad Anwer",
            "Fahad Shahbaz Khan",
            "Salman Khan",
            "Ivan Laptev",
            "Hisham Cholakkal"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Diffusion"
        ],
        "tldr": "DEFT is an efficient fine-tuning framework for Text-to-Image models that introduces a decomposition approach, demonstrating state-of-the-art performance in personalization, object and scene adaptation, and universal image generation.",
        "tldr_zh": "DEFT是一种高效的微调框架，针对文本到图像模型，引入了一种分解方法，在个性化、对象和场景适应以及通用图像生成方面表现出色。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Enhanced Fracture Diagnosis Based on Critical Regional and Scale Aware in YOLO",
        "summary": "Fracture detection plays a critical role in medical imaging analysis,\ntraditional fracture diagnosis relies on visual assessment by experienced\nphysicians, however the speed and accuracy of this approach are constrained by\nthe expertise. With the rapid advancements in artificial intelligence, deep\nlearning models based on the YOLO framework have been widely employed for\nfracture detection, demonstrating significant potential in improving diagnostic\nefficiency and accuracy. This study proposes an improved YOLO-based model,\ntermed Fracture-YOLO, which integrates novel Critical-Region-Selector Attention\n(CRSelector) and Scale-Aware (ScA) heads to further enhance detection\nperformance. Specifically, the CRSelector module utilizes global texture\ninformation to focus on critical features of fracture regions. Meanwhile, the\nScA module dynamically adjusts the weights of features at different scales,\nenhancing the model's capacity to identify fracture targets at multiple scales.\nExperimental results demonstrate that, compared to the baseline model,\nFracture-YOLO achieves a significant improvement in detection precision, with\nmAP50 and mAP50-95 increasing by 4 and 3, surpassing the baseline model and\nachieving state-of-the-art (SOTA) performance.",
        "url": "http://arxiv.org/abs/2509.23408v1",
        "published_date": "2025-09-27T16:53:15+00:00",
        "updated_date": "2025-09-27T16:53:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuyang Sun",
            "Junchuan Yu",
            "Cuiming Zou"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces an improved YOLO-based model for fracture detection in medical imaging, achieving significant improvements in detection precision.",
        "tldr_zh": "该论文介绍了一种改进的基于YOLO的模型，用于医学图像中的骨折检测，实现了检测精度的显著提高。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "WorldSplat: Gaussian-Centric Feed-Forward 4D Scene Generation for Autonomous Driving",
        "summary": "Recent advances in driving-scene generation and reconstruction have\ndemonstrated significant potential for enhancing autonomous driving systems by\nproducing scalable and controllable training data. Existing generation methods\nprimarily focus on synthesizing diverse and high-fidelity driving videos;\nhowever, due to limited 3D consistency and sparse viewpoint coverage, they\nstruggle to support convenient and high-quality novel-view synthesis (NVS).\nConversely, recent 3D/4D reconstruction approaches have significantly improved\nNVS for real-world driving scenes, yet inherently lack generative capabilities.\nTo overcome this dilemma between scene generation and reconstruction, we\npropose \\textbf{WorldSplat}, a novel feed-forward framework for 4D\ndriving-scene generation. Our approach effectively generates consistent\nmulti-track videos through two key steps: ((i)) We introduce a 4D-aware latent\ndiffusion model integrating multi-modal information to produce pixel-aligned 4D\nGaussians in a feed-forward manner. ((ii)) Subsequently, we refine the novel\nview videos rendered from these Gaussians using a enhanced video diffusion\nmodel. Extensive experiments conducted on benchmark datasets demonstrate that\n\\textbf{WorldSplat} effectively generates high-fidelity, temporally and\nspatially consistent multi-track novel view driving videos.",
        "url": "http://arxiv.org/abs/2509.23402v1",
        "published_date": "2025-09-27T16:47:44+00:00",
        "updated_date": "2025-09-27T16:47:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziyue Zhu",
            "Zhanqian Wu",
            "Zhenxin Zhu",
            "Lijun Zhou",
            "Haiyang Sun",
            "Bing Wan",
            "Kun Ma",
            "Guang Chen",
            "Hangjun Ye",
            "Jin Xie",
            "jian Yang"
        ],
        "ai_categories": [
            "GAN",
            "Diffusion",
            "Dataset"
        ],
        "tldr": "WorldSplat proposes a Gaussian-centric feed-forward 4D scene generation framework for autonomous driving, aiming to improve novel-view synthesis and generate consistent multi-track driving videos.",
        "tldr_zh": "WorldSplat提出了一个以高斯为中心的前馈4D场景生成框架，旨在改善新视图合成并生成一致的多轨道驾驶视频。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Generative Modeling of Shape-Dependent Self-Contact Human Poses",
        "summary": "One can hardly model self-contact of human poses without considering\nunderlying body shapes. For example, the pose of rubbing a belly for a person\nwith a low BMI leads to penetration of the hand into the belly for a person\nwith a high BMI. Despite its relevance, existing self-contact datasets lack the\nvariety of self-contact poses and precise body shapes, limiting conclusive\nanalysis between self-contact poses and shapes. To address this, we begin by\nintroducing the first extensive self-contact dataset with precise body shape\nregistration, Goliath-SC, consisting of 383K self-contact poses across 130\nsubjects. Using this dataset, we propose generative modeling of self-contact\nprior conditioned by body shape parameters, based on a body-part-wise latent\ndiffusion with self-attention. We further incorporate this prior into\nsingle-view human pose estimation while refining estimated poses to be in\ncontact. Our experiments suggest that shape conditioning is vital to the\nsuccessful modeling of self-contact pose distribution, hence improving\nsingle-view pose estimation in self-contact.",
        "url": "http://arxiv.org/abs/2509.23393v1",
        "published_date": "2025-09-27T16:26:38+00:00",
        "updated_date": "2025-09-27T16:26:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Takehiko Ohkawa",
            "Jihyun Lee",
            "Shunsuke Saito",
            "Jason Saragih",
            "Fabian Prado",
            "Yichen Xu",
            "Shoou-I Yu",
            "Ryosuke Furuta",
            "Yoichi Sato",
            "Takaaki Shiratori"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Diffusion"
        ],
        "tldr": "The paper introduces a dataset on self-contact human poses combined with body shapes and proposes a generative model for self-contact poses conditioned by body shape parameters to improve pose estimation.",
        "tldr_zh": "该论文介绍了一个关于自身接触人体姿势的数据集，结合了身体形状，并提出了一种以身体形状参数为条件的自身接触姿势生成模型，以改善姿势估计。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical Contrastive Decoding",
        "summary": "Multimodal large language models (MLLMs) have recently achieved remarkable\nprogress in radiology by integrating visual perception with natural language\nunderstanding. However, they often generate clinically unsupported\ndescriptions, known as medical hallucinations, which pose serious risks in\nmedical applications that demand accuracy and image-grounded outputs. Through\nempirical analysis, we find that prompt-induced hallucinations remain prevalent\nin radiology MLLMs, largely due to over-sensitivity to clinical sections. To\naddress this, we introduce Clinical Contrastive Cecoding (CCD), a training-free\nand retrieval-free inference framework that integrates structured clinical\nsignals from task-specific radiology expert models. CCD introduces a dual-stage\ncontrastive mechanism to refine token-level logits during generation, thereby\nenhancing clinical fidelity without modifying the base MLLM. Experiments on\nthree datasets and multiple models demonstrate that CCD consistently improves\noverall performance on radiology report generation (RRG). On the MIMIC-CXR\ndataset, it yields up to a 17% improvement in RadGraph-F1 when applied to\nstate-of-the-art RRG models. Our approach provides a lightweight and\ngeneralisable solution for mitigating medical hallucinations, effectively\nbridging expert models and MLLMs in radiology.",
        "url": "http://arxiv.org/abs/2509.23379v1",
        "published_date": "2025-09-27T16:01:09+00:00",
        "updated_date": "2025-09-27T16:01:09+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "I.2.10; J.3; I.5.4"
        ],
        "authors": [
            "Xi Zhang",
            "Zaiqiao Meng",
            "Jake Lever",
            "Edmond S. L. Ho"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces Clinical Contrastive Decoding (CCD) to mitigate medical hallucinations in radiology MLLMs, improving overall performance on radiology report generation.",
        "tldr_zh": "该论文引入临床对比解码（CCD）以减轻放射学MLLM中的医学幻觉，在放射学报告生成上提高了整体性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniPose: Unified Cross-modality Pose Prior Propagation towards RGB-D data for Weakly Supervised 3D Human Pose Estimation",
        "summary": "In this paper, we present UniPose, a unified cross-modality pose prior\npropagation method for weakly supervised 3D human pose estimation (HPE) using\nunannotated single-view RGB-D sequences (RGB, depth, and point cloud data).\nUniPose transfers 2D HPE annotations from large-scale RGB datasets (e.g., MS\nCOCO) to the 3D domain via self-supervised learning on easily acquired RGB-D\nsequences, eliminating the need for labor-intensive 3D keypoint annotations.\nThis approach bridges the gap between 2D and 3D domains without suffering from\nissues related to multi-view camera calibration or synthetic-to-real data\nshifts. During training, UniPose leverages off-the-shelf 2D pose estimations as\nweak supervision for point cloud networks, incorporating spatial-temporal\nconstraints like body symmetry and joint motion. The 2D-to-3D back-projection\nloss and cross-modality interaction further enhance this process. By treating\nthe point cloud network's 3D HPE results as pseudo ground truth, our\nanchor-to-joint prediction method performs 3D lifting on RGB and depth\nnetworks, making it more robust against inaccuracies in 2D HPE results compared\nto state-of-the-art methods. Experiments on CMU Panoptic and ITOP datasets show\nthat UniPose achieves comparable performance to fully supervised methods.\nIncorporating large-scale unlabeled data (e.g., NTU RGB+D 60) enhances its\nperformance under challenging conditions, demonstrating its potential for\npractical applications. Our proposed 3D lifting method also achieves\nstate-of-the-art results.",
        "url": "http://arxiv.org/abs/2509.23376v1",
        "published_date": "2025-09-27T15:49:30+00:00",
        "updated_date": "2025-09-27T15:49:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinghong Zheng",
            "Changlong Jiang",
            "Jiaqi Li",
            "Haohong Kuang",
            "Hang Xu",
            "Tingbing Yan"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "UniPose is a novel method for weakly supervised 3D human pose estimation that transfers 2D annotations to 3D domain, achieving comparable performance to fully supervised methods.",
        "tldr_zh": "UniPose是一种新颖的弱监督三维人体姿势估计方法，将2D标注转移到3D领域，实现与完全监督方法相当的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CasPoinTr: Point Cloud Completion with Cascaded Networks and Knowledge Distillation",
        "summary": "Point clouds collected from real-world environments are often incomplete due\nto factors such as limited sensor resolution, single viewpoints, occlusions,\nand noise. These challenges make point cloud completion essential for various\napplications. A key difficulty in this task is predicting the overall shape and\nreconstructing missing regions from highly incomplete point clouds. To address\nthis, we introduce CasPoinTr, a novel point cloud completion framework using\ncascaded networks and knowledge distillation. CasPoinTr decomposes the\ncompletion task into two synergistic stages: Shape Reconstruction, which\ngenerates auxiliary information, and Fused Completion, which leverages this\ninformation alongside knowledge distillation to generate the final output.\nThrough knowledge distillation, a teacher model trained on denser point clouds\ntransfers incomplete-complete associative knowledge to the student model,\nenhancing its ability to estimate the overall shape and predict missing\nregions. Together, the cascaded networks and knowledge distillation enhance the\nmodel's ability to capture global shape context while refining local details,\neffectively bridging the gap between incomplete inputs and complete targets.\nExperiments on ShapeNet-55 under different difficulty settings demonstrate that\nCasPoinTr outperforms existing methods in shape recovery and detail\npreservation, highlighting the effectiveness of our cascaded structure and\ndistillation strategy.",
        "url": "http://arxiv.org/abs/2509.23375v1",
        "published_date": "2025-09-27T15:49:24+00:00",
        "updated_date": "2025-09-27T15:49:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yifan Yang",
            "Yuxiang Yan",
            "Boda Liu",
            "Jian Pu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "CasPoinTr introduces a novel framework for point cloud completion using cascaded networks and knowledge distillation, outperforming existing methods in shape recovery and detail preservation.",
        "tldr_zh": "CasPoinTr 提出了一种使用级联网络和知识蒸馏的点云完成框架，优于现有方法在形状恢复和细节保留方面。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DentVLM: A Multimodal Vision-Language Model for Comprehensive Dental Diagnosis and Enhanced Clinical Practice",
        "summary": "Diagnosing and managing oral diseases necessitate advanced visual\ninterpretation across diverse imaging modalities and integrated information\nsynthesis. While current AI models excel at isolated tasks, they often fall\nshort in addressing the complex, multimodal requirements of comprehensive\nclinical dental practice. Here we introduce DentVLM, a multimodal\nvision-language model engineered for expert-level oral disease diagnosis.\nDentVLM was developed using a comprehensive, large-scale, bilingual dataset of\n110,447 images and 2.46 million visual question-answering (VQA) pairs. The\nmodel is capable of interpreting seven 2D oral imaging modalities across 36\ndiagnostic tasks, significantly outperforming leading proprietary and\nopen-source models by 19.6% higher accuracy for oral diseases and 27.9% for\nmalocclusions. In a clinical study involving 25 dentists, evaluating 1,946\npatients and encompassing 3,105 QA pairs, DentVLM surpassed the diagnostic\nperformance of 13 junior dentists on 21 of 36 tasks and exceeded that of 12\nsenior dentists on 12 of 36 tasks. When integrated into a collaborative\nworkflow, DentVLM elevated junior dentists' performance to senior levels and\nreduced diagnostic time for all practitioners by 15-22%. Furthermore, DentVLM\nexhibited promising performance across three practical utility scenarios,\nincluding home-based dental health management, hospital-based intelligent\ndiagnosis and multi-agent collaborative interaction. These findings establish\nDentVLM as a robust clinical decision support tool, poised to enhance primary\ndental care, mitigate provider-patient imbalances, and democratize access to\nspecialized medical expertise within the field of dentistry.",
        "url": "http://arxiv.org/abs/2509.23344v1",
        "published_date": "2025-09-27T14:47:37+00:00",
        "updated_date": "2025-09-27T14:47:37+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zijie Meng",
            "Jin Hao",
            "Xiwei Dai",
            "Yang Feng",
            "Jiaxiang Liu",
            "Bin Feng",
            "Huikai Wu",
            "Xiaotang Gai",
            "Hengchuan Zhu",
            "Tianxiang Hu",
            "Yangyang Wu",
            "Hongxia Xu",
            "Jin Li",
            "Jun Xiao",
            "Xiaoqiang Liu",
            "Joey Tianyi Zhou",
            "Fudong Zhu",
            "Zhihe Zhao",
            "Lunguo Xia",
            "Bing Fang",
            "Jimeng Sun",
            "Jian Wu",
            "Zuozhu Liu"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "AIGC"
        ],
        "tldr": "DentVLM is a multimodal vision-language model designed for expert-level oral disease diagnosis, outperforming other models and improving diagnostic accuracy and efficiency.",
        "tldr_zh": "DentVLM是一个为专家级口腔疾病诊断设计的多模态视觉语言模型，优于其他模型，提高了诊断准确性和效率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "LRPO: Enhancing Blind Face Restoration through Online Reinforcement Learning",
        "summary": "Blind Face Restoration (BFR) encounters inherent challenges in exploring its\nlarge solution space, leading to common artifacts like missing details and\nidentity ambiguity in the restored images. To tackle these challenges, we\npropose a Likelihood-Regularized Policy Optimization (LRPO) framework, the\nfirst to apply online reinforcement learning (RL) to the BFR task. LRPO\nleverages rewards from sampled candidates to refine the policy network,\nincreasing the likelihood of high-quality outputs while improving restoration\nperformance on low-quality inputs. However, directly applying RL to BFR creates\nincompatibility issues, producing restoration results that deviate\nsignificantly from the ground truth. To balance perceptual quality and\nfidelity, we propose three key strategies: 1) a composite reward function\ntailored for face restoration assessment, 2) ground-truth guided likelihood\nregularization, and 3) noise-level advantage assignment. Extensive experiments\ndemonstrate that our proposed LRPO significantly improves the face restoration\nquality over baseline methods and achieves state-of-the-art performance.",
        "url": "http://arxiv.org/abs/2509.23339v1",
        "published_date": "2025-09-27T14:42:29+00:00",
        "updated_date": "2025-09-27T14:42:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bin Wu",
            "Yahui Liu",
            "Chi Zhang",
            "Yao Zhao",
            "Wei Wang"
        ],
        "ai_categories": [
            "GAN"
        ],
        "tldr": "The paper introduces LRPO, an online reinforcement learning framework for enhancing blind face restoration quality, outperforming current methods.",
        "tldr_zh": "本文介绍了LRPO，一种在线强化学习框架，用于提升盲目人脸修复的质量，优于当前方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Decoupling Reasoning and Perception: An LLM-LMM Framework for Faithful Visual Reasoning",
        "summary": "Significant advancements in the reasoning capabilities of Large Language\nModels (LLMs) are now driven by test-time scaling laws, particularly those\nleveraging extended Chain-of-Thought (CoT) reasoning. Inspired by these\nbreakthroughs, researchers have extended these paradigms to Large Multimodal\nModels (LMMs). However, a critical limitation emerges: as their reasoning\nchains extend, LMMs increasingly rely on textual logic, progressively losing\ngrounding in the underlying visual information. This leads to reasoning paths\nthat diverge from the image content, culminating in erroneous conclusions. To\naddress this, we introduce a strikingly simple yet effective training-free\nvisual-reasoning pipeline. The core concept is to decouple the reasoning and\nperception processes. A powerful LLM orchestrates the high-level reasoning,\nstrategically interrogating a LMM to extract specific visual information\nrequired for its logical chain. The LMM, in turn, functions exclusively as a\nvisual question-answering engine, supplying the necessary perceptual details on\ndemand. This lightweight, plug-and-play approach requires no additional\ntraining or architectural changes. Comprehensive evaluations validate that our\nframework effectively governs the visual reasoning process, leading to a\nsignificant reduction in visually-unfounded reasoning steps and a substantial\nimprovement in reasoning fidelity.",
        "url": "http://arxiv.org/abs/2509.23322v1",
        "published_date": "2025-09-27T14:13:41+00:00",
        "updated_date": "2025-09-27T14:13:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hongrui Jia",
            "Chaoya Jiang",
            "Shikun Zhang",
            "Wei Ye"
        ],
        "ai_categories": [
            "Multimodality"
        ],
        "tldr": "The paper proposes a framework to improve visual reasoning by decoupling reasoning and perception processes in Large Language Models and Large Multimodal Models.",
        "tldr_zh": "本文提出了一种框架，通过在大型语言模型和大型多模态模型中解耦推理和感知过程来改善视觉推理。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "C3-OWD: A Curriculum Cross-modal Contrastive Learning Framework for Open-World Detection",
        "summary": "Object detection has advanced significantly in the closed-set setting, but\nreal-world deployment remains limited by two challenges: poor generalization to\nunseen categories and insufficient robustness under adverse conditions. Prior\nresearch has explored these issues separately: visible-infrared detection\nimproves robustness but lacks generalization, while open-world detection\nleverages vision-language alignment strategy for category diversity but\nstruggles under extreme environments. This trade-off leaves robustness and\ndiversity difficult to achieve simultaneously. To mitigate these issues, we\npropose \\textbf{C3-OWD}, a curriculum cross-modal contrastive learning\nframework that unifies both strengths. Stage~1 enhances robustness by\npretraining with RGBT data, while Stage~2 improves generalization via\nvision-language alignment. To prevent catastrophic forgetting between two\nstages, we introduce an Exponential Moving Average (EMA) mechanism that\ntheoretically guarantees preservation of pre-stage performance with bounded\nparameter lag and function consistency. Experiments on FLIR, OV-COCO, and\nOV-LVIS demonstrate the effectiveness of our approach: C3-OWD achieves $80.1$\nAP$^{50}$ on FLIR, $48.6$ AP$^{50}_{\\text{Novel}}$ on OV-COCO, and $35.7$\nmAP$_r$ on OV-LVIS, establishing competitive performance across both robustness\nand diversity evaluations. Code available at:\nhttps://github.com/justin-herry/C3-OWD.git.",
        "url": "http://arxiv.org/abs/2509.23316v1",
        "published_date": "2025-09-27T14:04:15+00:00",
        "updated_date": "2025-09-27T14:04:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Siheng Wang",
            "Zhengdao Li",
            "Yanshu Li",
            "Canran Xiao",
            "Haibo Zhan",
            "Zhengtao Yao",
            "Xuzhi Zhang",
            "Jiale Kang",
            "Linshan Li",
            "Weiming Liu",
            "Zhikang Dong",
            "Jifeng Shen",
            "Junhao Dong",
            "Qiang Sun",
            "Piotr Koniusz"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a framework, C3-OWD, that combines robustness and generalization in object detection by pretraining with RGBT data and leveraging vision-language alignment.",
        "tldr_zh": "本文介绍了一个框架C3-OWD，通过使用RGBT数据预训练和利用视觉-语言对齐来结合鲁棒性和泛化性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Balanced Diffusion-Guided Fusion for Multimodal Remote Sensing Classification",
        "summary": "Deep learning-based techniques for the analysis of multimodal remote sensing\ndata have become popular due to their ability to effectively integrate\ncomplementary spatial, spectral, and structural information from different\nsensors. Recently, denoising diffusion probabilistic models (DDPMs) have\nattracted attention in the remote sensing community due to their powerful\nability to capture robust and complex spatial-spectral distributions. However,\npre-training multimodal DDPMs may result in modality imbalance, and effectively\nleveraging diffusion features to guide complementary diversity feature\nextraction remains an open question. To address these issues, this paper\nproposes a balanced diffusion-guided fusion (BDGF) framework that leverages\nmultimodal diffusion features to guide a multi-branch network for land-cover\nclassification. Specifically, we propose an adaptive modality masking strategy\nto encourage the DDPMs to obtain a modality-balanced rather than spectral\nimage-dominated data distribution. Subsequently, these diffusion features\nhierarchically guide feature extraction among CNN, Mamba, and transformer\nnetworks by integrating feature fusion, group channel attention, and\ncross-attention mechanisms. Finally, a mutual learning strategy is developed to\nenhance inter-branch collaboration by aligning the probability entropy and\nfeature similarity of individual subnetworks. Extensive experiments on four\nmultimodal remote sensing datasets demonstrate that the proposed method\nachieves superior classification performance. The code is available at\nhttps://github.com/HaoLiu-XDU/BDGF.",
        "url": "http://arxiv.org/abs/2509.23310v1",
        "published_date": "2025-09-27T13:55:32+00:00",
        "updated_date": "2025-09-27T13:55:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hao Liu",
            "Yongjie Zheng",
            "Yuhan Kang",
            "Mingyang Zhang",
            "Maoguo Gong",
            "Lorenzo Bruzzone"
        ],
        "ai_categories": [
            "Multimodality",
            "Diffusion",
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes a balanced diffusion-guided fusion framework for multimodal remote sensing classification, achieving superior performance by integrating diffusion features and a multi-branch network.",
        "tldr_zh": "该论文提出了一种平衡扩散引导融合框架，用于多模态遥感分类，通过整合扩散特征和多分支网络实现了卓越性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Seeing the Unseen in Low-light Spike Streams",
        "summary": "Spike camera, a type of neuromorphic sensor with high-temporal resolution,\nshows great promise for high-speed visual tasks. Unlike traditional cameras,\nspike camera continuously accumulates photons and fires asynchronous spike\nstreams. Due to unique data modality, spike streams require reconstruction\nmethods to become perceptible to the human eye.\n  However, lots of methods struggle to handle spike streams in low-light\nhigh-speed scenarios due to severe noise and sparse information. In this work,\nwe propose Diff-SPK, the first diffusion-based reconstruction method for spike\ncamera. Diff-SPK effectively leverages generative priors to supplement texture\ninformation in low-light conditions. Specifically, it first employs an\n\\textbf{E}nhanced \\textbf{T}exture \\textbf{f}rom Inter-spike \\textbf{I}nterval\n(ETFI) to aggregate sparse information from low-light spike streams. Then, ETFI\nserves as a conditioning input for ControlNet to generate the high-speed\nscenes. To improve the quality of results, we introduce an ETFI-based feature\nfusion module during the generation process.\n  Moreover, we establish the first bona fide benchmark for the low-light spike\nstream reconstruction task. It significantly surpasses existing reconstruction\ndatasets in scale and provides quantitative illumination information. The\nperformance on real low-light spike streams demonstrates the superiority of\nDiff-SPK.",
        "url": "http://arxiv.org/abs/2509.23304v1",
        "published_date": "2025-09-27T13:33:03+00:00",
        "updated_date": "2025-09-27T13:33:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Liwen Hu",
            "Yang Li",
            "Mianzhi Liu",
            "Yijia Guo",
            "Shenghao Xie",
            "Ziluo Ding",
            "Tiejun Huang",
            "Lei Ma"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper introduces a diffusion-based reconstruction method called Diff-SPK for low-light spike camera streams and establishes a benchmark for this task, showing superior performance.",
        "tldr_zh": "本文介绍了一种名为Diff-SPK的扩散重建方法，用于低光spike相机流，并为该任务建立了一个基准，展示了卓越的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Seeing Through the Blur: Unlocking Defocus Maps for Deepfake Detection",
        "summary": "The rapid advancement of generative AI has enabled the mass production of\nphotorealistic synthetic images, blurring the boundary between authentic and\nfabricated visual content. This challenge is particularly evident in deepfake\nscenarios involving facial manipulation, but also extends to broader\nAI-generated content (AIGC) cases involving fully synthesized scenes. As such\ncontent becomes increasingly difficult to distinguish from reality, the\nintegrity of visual media is under threat. To address this issue, we propose a\nphysically interpretable deepfake detection framework and demonstrate that\ndefocus blur can serve as an effective forensic signal. Defocus blur is a\ndepth-dependent optical phenomenon that naturally occurs in camera-captured\nimages due to lens focus and scene geometry. In contrast, synthetic images\noften lack realistic depth-of-field (DoF) characteristics. To capture these\ndiscrepancies, we construct a defocus blur map and use it as a discriminative\nfeature for detecting manipulated content. Unlike RGB textures or\nfrequency-domain signals, defocus blur arises universally from optical imaging\nprinciples and encodes physical scene structure. This makes it a robust and\ngeneralizable forensic cue. Our approach is supported by three in-depth feature\nanalyses, and experimental results confirm that defocus blur provides a\nreliable and interpretable cue for identifying synthetic images. We aim for our\ndefocus-based detection pipeline and interpretability tools to contribute\nmeaningfully to ongoing research in media forensics. The implementation is\npublicly available at:\nhttps://github.com/irissun9602/Defocus-Deepfake-Detection",
        "url": "http://arxiv.org/abs/2509.23289v1",
        "published_date": "2025-09-27T13:02:53+00:00",
        "updated_date": "2025-09-27T13:02:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Minsun Jeon",
            "Simon S. Woo"
        ],
        "ai_categories": [
            "AIGC",
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes a deepfake detection framework using defocus blur as a forensic signal, demonstrating its effectiveness in identifying manipulated content.",
        "tldr_zh": "本文提出了一种使用焦外模糊作为取证信号的深度伪造检测框架，并证明其在识别篡改内容方面的有效性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Vid-Freeze: Protecting Images from Malicious Image-to-Video Generation via Temporal Freezing",
        "summary": "The rapid progress of image-to-video (I2V) generation models has introduced\nsignificant risks, enabling video synthesis from static images and facilitating\ndeceptive or malicious content creation. While prior defenses such as I2VGuard\nattempt to immunize images, effective and principled protection to block motion\nremains underexplored. In this work, we introduce Vid-Freeze - a novel\nattention-suppressing adversarial attack that adds carefully crafted\nadversarial perturbations to images. Our method explicitly targets the\nattention mechanism of I2V models, completely disrupting motion synthesis while\npreserving semantic fidelity of the input image. The resulting immunized images\ngenerate stand-still or near-static videos, effectively blocking malicious\ncontent creation. Our experiments demonstrate the impressive protection\nprovided by the proposed approach, highlighting the importance of attention\nattacks as a promising direction for robust and proactive defenses against\nmisuse of I2V generation models.",
        "url": "http://arxiv.org/abs/2509.23279v1",
        "published_date": "2025-09-27T12:26:34+00:00",
        "updated_date": "2025-09-27T12:26:34+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Rohit Chowdhury",
            "Aniruddha Bala",
            "Rohan Jaiswal",
            "Siddharth Roheda"
        ],
        "ai_categories": [
            "GAN",
            "AIGC",
            "Transformer"
        ],
        "tldr": "Vid-Freeze proposes a novel approach to protect images from being used to generate deceptive videos by targeting the attention mechanism of image-to-video models.",
        "tldr_zh": "Vid-Freeze提出了一种新颖的方法，通过针对图像到视频模型的注意机制来保护图像，防止生成欺诈性视频。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OracleGS: Grounding Generative Priors for Sparse-View Gaussian Splatting",
        "summary": "Sparse-view novel view synthesis is fundamentally ill-posed due to severe\ngeometric ambiguity. Current methods are caught in a trade-off: regressive\nmodels are geometrically faithful but incomplete, whereas generative models can\ncomplete scenes but often introduce structural inconsistencies. We propose\nOracleGS, a novel framework that reconciles generative completeness with\nregressive fidelity for sparse view Gaussian Splatting. Instead of using\ngenerative models to patch incomplete reconstructions, our\n\"propose-and-validate\" framework first leverages a pre-trained 3D-aware\ndiffusion model to synthesize novel views to propose a complete scene. We then\nrepurpose a multi-view stereo (MVS) model as a 3D-aware oracle to validate the\n3D uncertainties of generated views, using its attention maps to reveal regions\nwhere the generated views are well-supported by multi-view evidence versus\nwhere they fall into regions of high uncertainty due to occlusion, lack of\ntexture, or direct inconsistency. This uncertainty signal directly guides the\noptimization of a 3D Gaussian Splatting model via an uncertainty-weighted loss.\nOur approach conditions the powerful generative prior on multi-view geometric\nevidence, filtering hallucinatory artifacts while preserving plausible\ncompletions in under-constrained regions, outperforming state-of-the-art\nmethods on datasets including Mip-NeRF 360 and NeRF Synthetic.",
        "url": "http://arxiv.org/abs/2509.23258v1",
        "published_date": "2025-09-27T11:19:32+00:00",
        "updated_date": "2025-09-27T11:19:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Atakan Topaloglu",
            "Kunyi Li",
            "Michael Niemeyer",
            "Nassir Navab",
            "A. Murat Tekalp",
            "Federico Tombari"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "OracleGS proposes a framework that combines generative completeness with regressive fidelity for sparse-view Gaussian splatting, outperforming state-of-the-art methods on various datasets.",
        "tldr_zh": "OracleGS提出了一个框架，将生成完整性与回归保真度相结合，在各种数据集上优于最先进的方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Training Vision-Language Process Reward Models for Test-Time Scaling in Multimodal Reasoning: Key Insights and Lessons Learned",
        "summary": "Process Reward Models (PRMs) provide step-level supervision that improves the\nreliability of reasoning in large language models. While PRMs have been\nextensively studied in text-based domains, their extension to Vision Language\nModels (VLMs) remains limited. Existing Vision-Language PRMs (VL-PRMs) rely on\nMonte Carlo Tree Search (MCTS) for data construction, which can often produce\nnoisy supervision signals and limit generalization across tasks. In this work,\nwe aim to elucidate the design space of VL-PRMs by exploring diverse strategies\nfor dataset construction, training, and test-time scaling. First, we introduce\na hybrid data synthesis framework that combines MCTS with judgments from a\nstrong VLM, producing more accurate step-level labels. Second, we propose\nperception-focused supervision, enabling our PRM to explicitly detect errors at\nthe visual grounding stage of reasoning. Third, we systematically evaluate\nmultiple test-time scaling strategies, showing that our PRMs can reliably guide\nVLMs toward more accurate solutions. Our experiments covering five diverse\nmultimodal benchmarks (MMMU, PuzzleVQA, AlgoPuzzleVQA, MathVista, and\nMathVision) reveal several key insights: (i) VL-PRMs when used as Outcome\nReward Models (ORMs) during test-time scaling (TTS) can outperform VL-PRM\nguided process step selection, (ii) smaller VL-PRMs can match or even surpass\nlarger ones in detecting process errors, (iii) VL-PRMs uncover latent reasoning\nabilities in stronger VLM backbones, (iv) perception-level supervision leads to\nsignificant gains in test-time scaling, and (v) TTS performance of different\npolicies improve on advanced math reasoning datasets despite not training\nVL-PRMs on such datasets. We hope our work will motivate further research and\nsupport the advancement of VLMs.",
        "url": "http://arxiv.org/abs/2509.23250v1",
        "published_date": "2025-09-27T10:56:58+00:00",
        "updated_date": "2025-09-27T10:56:58+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Brandon Ong",
            "Tej Deep Pala",
            "Vernon Toh",
            "William Chandra Tjhi",
            "Soujanya Poria"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper explores the design space of Vision-Language Process Reward Models (VL-PRMs) for improving multimodal reasoning by combining MCTS with judgments from a strong Vision-Language Model (VLM). It introduces perception-focused supervision and evaluates multiple test-time scaling strategies across diverse benchmarks.",
        "tldr_zh": "这篇论文探究了视觉-语言处理奖励模型（VL-PRMs）的设计空间，通过将MCTS与来自强大的视觉-语言模型（VLM）的判断相结合，以提高多模态推理能力。介绍了感知焦点监督，并在多个基准测试上评估了多种测试时间缩放策略。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Increasing the Diversity in RGB-to-Thermal Image Translation for Automotive Applications",
        "summary": "Thermal imaging in Advanced Driver Assistance Systems (ADAS) improves road\nsafety with superior perception in low-light and harsh weather conditions\ncompared to traditional RGB cameras. However, research in this area faces\nchallenges due to limited dataset availability and poor representation in\ndriving simulators. RGB-to-thermal image translation offers a potential\nsolution, but existing methods focus on one-to-one mappings. We propose a\none-to-many mapping using a multi-modal translation framework enhanced with our\nComponent-aware Adaptive Instance Normalization (CoAdaIN). Unlike the original\nAdaIN, which applies styles globally, CoAdaIN adapts styles to different image\ncomponents individually. The result, as we show, is more realistic and diverse\nthermal image translations. This is the accepted author manuscript of the paper\npublished in IEEE Sensors Conference 2024. The final published version is\navailable at 10.1109/SENSORS60989.2024.10785056.",
        "url": "http://arxiv.org/abs/2509.23243v1",
        "published_date": "2025-09-27T10:49:56+00:00",
        "updated_date": "2025-09-27T10:49:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaili Wang",
            "Leonardo Ravaglia",
            "Roberto Longo",
            "Lore Goetschalckx",
            "David Van Hamme",
            "Julie Moeyersoms",
            "Ben Stoffelen",
            "Tom De Schepper"
        ],
        "ai_categories": [
            "Multimodality",
            "GaN",
            "Dataset"
        ],
        "tldr": "The paper proposes a method for translating RGB images to thermal images in a diverse and realistic manner using a one-to-many mapping approach with Component-aware Adaptive Instance Normalization (CoAdaIN).",
        "tldr_zh": "本文提出了一种使用一对多映射方法和Component-aware自适应实例归一化（CoAdaIN）来将RGB图像转换为热像的方法，实现多样化和逼真的效果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Patch Rebirth: Toward Fast and Transferable Model Inversion of Vision Transformers",
        "summary": "Model inversion is a widely adopted technique in data-free learning that\nreconstructs synthetic inputs from a pretrained model through iterative\noptimization, without access to original training data. Unfortunately, its\napplication to state-of-the-art Vision Transformers (ViTs) poses a major\ncomputational challenge, due to their expensive self-attention mechanisms. To\naddress this, Sparse Model Inversion (SMI) was proposed to improve efficiency\nby pruning and discarding seemingly unimportant patches, which were even\nclaimed to be obstacles to knowledge transfer. However, our empirical findings\nsuggest the opposite: even randomly selected patches can eventually acquire\ntransferable knowledge through continued inversion. This reveals that\ndiscarding any prematurely inverted patches is inefficient, as it suppresses\nthe extraction of class-agnostic features essential for knowledge transfer,\nalong with class-specific features. In this paper, we propose Patch Rebirth\nInversion (PRI), a novel approach that incrementally detaches the most\nimportant patches during the inversion process to construct sparse synthetic\nimages, while allowing the remaining patches to continue evolving for future\nselection. This progressive strategy not only improves efficiency, but also\nencourages initially less informative patches to gradually accumulate more\nclass-relevant knowledge, a phenomenon we refer to as the Re-Birth effect,\nthereby effectively balancing class-agnostic and class-specific knowledge.\nExperimental results show that PRI achieves up to 10x faster inversion than\nstandard Dense Model Inversion (DMI) and 2x faster than SMI, while consistently\noutperforming SMI in accuracy and matching the performance of DMI.",
        "url": "http://arxiv.org/abs/2509.23235v1",
        "published_date": "2025-09-27T10:35:44+00:00",
        "updated_date": "2025-09-27T10:35:44+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Seongsoo Heo",
            "Dong-Wan Choi"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper introduces Patch Rebirth Inversion (PRI) for fast and transferable model inversion of Vision Transformers, improving efficiency and knowledge transfer.",
        "tldr_zh": "该论文介绍了Patch Rebirth Inversion (PRI)，用于快速和可转移的视觉变换器模型反演，提高效率和知识转移。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UltraUNet: Real-Time Ultrasound Tongue Segmentation for Diverse Linguistic and Imaging Conditions",
        "summary": "Ultrasound tongue imaging (UTI) is a non-invasive and cost-effective tool for\nstudying speech articulation, motor control, and related disorders. However,\nreal-time tongue contour segmentation remains challenging due to low\nsignal-to-noise ratios, imaging variability, and computational demands. We\npropose UltraUNet, a lightweight encoder-decoder architecture optimized for\nreal-time segmentation of tongue contours in ultrasound images. UltraUNet\nincorporates domain-specific innovations such as lightweight\nSqueeze-and-Excitation blocks, Group Normalization for small-batch stability,\nand summation-based skip connections to reduce memory and computational\noverhead. It achieves 250 frames per second and integrates ultrasound-specific\naugmentations like denoising and blur simulation. Evaluations on 8 datasets\ndemonstrate high accuracy and robustness, with single-dataset Dice = 0.855 and\nMSD = 0.993px, and cross-dataset Dice averaging 0.734 and 0.761. UltraUNet\nprovides a fast, accurate solution for speech research, clinical diagnostics,\nand analysis of speech motor disorders.",
        "url": "http://arxiv.org/abs/2509.23225v1",
        "published_date": "2025-09-27T10:11:33+00:00",
        "updated_date": "2025-09-27T10:11:33+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Alisher Myrgyyassov",
            "Zhen Song",
            "Yu Sun",
            "Bruce Xiao Wang",
            "Min Ney Wong",
            "Yongping Zheng"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "UltraUNet is a real-time segmentation model for ultrasound tongue images, achieving high accuracy and speed for speech research and clinical diagnostics.",
        "tldr_zh": "UltraUNet是一种实时分割模型，用于超声舌头图像，在语音研究和临床诊断方面取得了高准确性和速度。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Confidence-Calibrating Regularization for Robust Brain MRI Segmentation Under Domain Shift",
        "summary": "The Segment Anything Model (SAM) exhibits strong zero-shot performance on\nnatural images but suffers from domain shift and overconfidence when applied to\nmedical volumes. We propose \\textbf{CalSAM}, a lightweight adaptation framework\nthat (i) reduces encoder sensitivity to domain shift via a \\emph{Feature Fisher\nInformation Penalty} (FIP) computed on 3D feature maps and (ii) penalizes\noverconfident voxel-wise errors through a \\emph{Confidence Misalignment\nPenalty} (CMP). The combined loss, \\(\\mathcal{L}_{\\mathrm{CalSAM}}\\) fine-tunes\nonly the mask decoder while keeping SAM's encoders frozen. On cross-center and\nscanner-shift evaluations, CalSAM substantially improves accuracy and\ncalibration: e.g., on the BraTS scanner split (Siemens$\\to$GE) CalSAM shows a\n$+7.4\\%$ relative improvement in $\\mathrm{DSC}$ (80.1\\% vs.\\ 74.6\\%), a\n$-26.9\\%$ reduction in $\\mathrm{HD95}$ (4.6 mm vs.\\ 6.3 mm), and a $-39.5\\%$\nreduction in $\\mathrm{ECE}$ (5.2\\% vs.\\ 8.6\\%). On ATLAS-C (motion\ncorruptions), CalSAM achieves a $+5.3\\%$ relative improvement in $\\mathrm{DSC}$\n(75.9\\%) and a $-32.6\\%$ reduction in $\\mathrm{ECE}$ (5.8\\%). Ablations show\nFIP and CMP contribute complementary gains ($p<0.01$), and the Fisher penalty\nincurs a modest $\\sim$15\\% training-time overhead. CalSAM therefore delivers\nimproved domain generalization and better-calibrated uncertainty estimates for\nbrain MRI segmentation, while retaining the computational benefits of freezing\nSAM's encoder.",
        "url": "http://arxiv.org/abs/2509.23176v1",
        "published_date": "2025-09-27T08:12:12+00:00",
        "updated_date": "2025-09-27T08:12:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Behraj Khan",
            "Tahir Qasim Syed"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes CalSAM, a framework for improving brain MRI segmentation by addressing domain shift and overconfidence issues. It shows significant improvements in accuracy and calibration on cross-center and scanner-shift evaluations.",
        "tldr_zh": "本文提出了CalSAM框架，通过解决领域偏移和过度自信问题，改善了脑部MRI分割。在跨中心和扫描仪偏移评估中显示出显著的准确性和校准改进。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "WeatherCycle: Unpaired Multi-Weather Restoration via Color Space Decoupled Cycle Learning",
        "summary": "Unsupervised image restoration under multi-weather conditions remains a\nfundamental yet underexplored challenge. While existing methods often rely on\ntask-specific physical priors, their narrow focus limits scalability and\ngeneralization to diverse real-world weather scenarios. In this work, we\npropose \\textbf{WeatherCycle}, a unified unpaired framework that reformulates\nweather restoration as a bidirectional degradation-content translation cycle,\nguided by degradation-aware curriculum regularization. At its core,\nWeatherCycle employs a \\textit{lumina-chroma decomposition} strategy to\ndecouple degradation from content without modeling complex weather, enabling\ndomain conversion between degraded and clean images. To model diverse and\ncomplex degradations, we propose a \\textit{Lumina Degradation Guidance Module}\n(LDGM), which learns luminance degradation priors from a degraded image pool\nand injects them into clean images via frequency-domain amplitude modulation,\nenabling controllable and realistic degradation modeling. Additionally, we\nincorporate a \\textit{Difficulty-Aware Contrastive Regularization (DACR)}\nmodule that identifies hard samples via a CLIP-based classifier and enforces\ncontrastive alignment between hard samples and restored features to enhance\nsemantic consistency and robustness. Extensive experiments across serve\nmulti-weather datasets, demonstrate that our method achieves state-of-the-art\nperformance among unsupervised approaches, with strong generalization to\ncomplex weather degradations.",
        "url": "http://arxiv.org/abs/2509.23150v1",
        "published_date": "2025-09-27T06:44:27+00:00",
        "updated_date": "2025-09-27T06:44:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenxuan Fang",
            "Jiangwei Weng",
            "Jianjun Qian",
            "Jian Yang",
            "Jun Li"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a unified framework called WeatherCycle for unsupervised image restoration under various weather conditions using degradation-content translation cycles and degradation-aware curriculum regularization.",
        "tldr_zh": "本文提出了一个名为WeatherCycle的统一框架，通过退化-内容翻译循环和退化感知课程规范实现在各种天气条件下的无监督图像恢复。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Benchmarking DINOv3 for Multi-Task Stroke Analysis on Non-Contrast CT",
        "summary": "Non-contrast computed tomography (NCCT) is essential for rapid stroke\ndiagnosis but is limited by low image contrast and signal to noise ratio. We\naddress this challenge by leveraging DINOv3, a state-of-the-art self-supervised\nvision transformer, to generate powerful feature representations for a\ncomprehensive set of stroke analysis tasks. Our evaluation encompasses infarct\nand hemorrhage segmentation, anomaly classification (normal vs. stroke and\nnormal vs. infarct vs. hemorrhage), hemorrhage subtype classification (EDH,\nSDH, SAH, IPH, IVH), and dichotomized ASPECTS classification (<=6 vs. >6) on\nmultiple public and private datasets. This study establishes strong benchmarks\nfor these tasks and demonstrates the potential of advanced self-supervised\nmodels to improve automated stroke diagnosis from NCCT, providing a clear\nanalysis of both the advantages and current constraints of the approach. The\ncode is available at https://github.com/Zzz0251/DINOv3-stroke.",
        "url": "http://arxiv.org/abs/2509.23132v1",
        "published_date": "2025-09-27T05:33:46+00:00",
        "updated_date": "2025-09-27T05:33:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Donghao Zhang",
            "Yimin Chen",
            "Kauê TN Duarte",
            "Taha Aslan",
            "Mohamed AlShamrani",
            "Brij Karmur",
            "Yan Wan",
            "Shengcai Chen",
            "Bo Hu",
            "Bijoy K Menon",
            "Wu Qiu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper benchmarks DINOv3 for multi-task stroke analysis on non-contrast CT, showing strong results in various stroke analysis tasks.",
        "tldr_zh": "本文在非对比 CT 上对 DINOv3 进行了多任务卒中分析基准测试，显示出在各种卒中分析任务中强大的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CoPatch: Zero-Shot Referring Image Segmentation by Leveraging Untapped Spatial Knowledge in CLIP",
        "summary": "Spatial grounding is crucial for referring image segmentation (RIS), where\nthe goal of the task is to localize an object described by language. Current\nfoundational vision-language models (VLMs), such as CLIP, excel at aligning\nimages and text but struggle with understanding spatial relationships. Within\nthe language stream, most existing methods often focus on the primary noun\nphrase when extracting local text features, undermining contextual tokens.\nWithin the vision stream, CLIP generates similar features for images with\ndifferent spatial layouts, resulting in limited sensitivity to spatial\nstructure. To address these limitations, we propose \\textsc{CoPatch}, a\nzero-shot RIS framework that leverages internal model components to enhance\nspatial representations in both text and image modalities. For language,\n\\textsc{CoPatch} constructs hybrid text features by incorporating context\ntokens carrying spatial cues. For vision, it extracts patch-level image\nfeatures using our novel path discovered from intermediate layers, where\nspatial structure is better preserved. These enhanced features are fused into a\nclustered image-text similarity map, \\texttt{CoMap}, enabling precise mask\nselection. As a result, \\textsc{CoPatch} significantly improves spatial\ngrounding in zero-shot RIS across RefCOCO, RefCOCO+, RefCOCOg, and PhraseCut (+\n2--7 mIoU) without requiring any additional training. Our findings underscore\nthe importance of recovering and leveraging the untapped spatial knowledge\ninherently embedded in VLMs, thereby paving the way for opportunities in\nzero-shot RIS.",
        "url": "http://arxiv.org/abs/2509.23098v1",
        "published_date": "2025-09-27T04:12:10+00:00",
        "updated_date": "2025-09-27T04:12:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Na Min An",
            "Inha Kang",
            "Minhyun Lee",
            "Hyunjung Shim"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "CoPatch proposes a zero-shot referring image segmentation framework to improve spatial grounding in VLMs, achieving significant performance improvements without additional training.",
        "tldr_zh": "CoPatch提出了一种零-shot参考图像分割框架，以提高VLM中的空间定位，实现了显著的性能改进，无需额外训练。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Follow-Your-Preference: Towards Preference-Aligned Image Inpainting",
        "summary": "This paper investigates image inpainting with preference alignment. Instead\nof introducing a novel method, we go back to basics and revisit fundamental\nproblems in achieving such alignment. We leverage the prominent direct\npreference optimization approach for alignment training and employ public\nreward models to construct preference training datasets. Experiments are\nconducted across nine reward models, two benchmarks, and two baseline models\nwith varying structures and generative algorithms. Our key findings are as\nfollows: (1) Most reward models deliver valid reward scores for constructing\npreference data, even if some of them are not reliable evaluators. (2)\nPreference data demonstrates robust trends in both candidate scaling and sample\nscaling across models and benchmarks. (3) Observable biases in reward models,\nparticularly in brightness, composition, and color scheme, render them\nsusceptible to cause reward hacking. (4) A simple ensemble of these models\nyields robust and generalizable results by mitigating such biases. Built upon\nthese observations, our alignment models significantly outperform prior models\nacross standard metrics, GPT-4 assessments, and human evaluations, without any\nchanges to model structures or the use of new datasets. We hope our work can\nset a simple yet solid baseline, pushing this promising frontier. Our code is\nopen-sourced at: https://github.com/shenytzzz/Follow-Your-Preference.",
        "url": "http://arxiv.org/abs/2509.23082v1",
        "published_date": "2025-09-27T03:32:30+00:00",
        "updated_date": "2025-09-27T03:32:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yutao Shen",
            "Junkun Yuan",
            "Toru Aonishi",
            "Hideki Nakayama",
            "Yue Ma"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper explores image inpainting by aligning with preferences, using existing reward models to construct training datasets. Through experiments, they show improved results without changing model structures or datasets.",
        "tldr_zh": "本文通过与偏好对齐的方式探索图像修复，利用现有奖励模型构建训练数据集。通过实验，他们展示了在不改变模型结构或数据集的情况下取得了改进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Mask What Matters: Controllable Text-Guided Masking for Self-Supervised Medical Image Analysis",
        "summary": "The scarcity of annotated data in specialized domains such as medical imaging\npresents significant challenges to training robust vision models. While\nself-supervised masked image modeling (MIM) offers a promising solution,\nexisting approaches largely rely on random high-ratio masking, leading to\ninefficiency and poor semantic alignment. Moreover, region-aware variants\ntypically depend on reconstruction heuristics or supervised signals, limiting\ntheir adaptability across tasks and modalities. We propose Mask What Matters, a\ncontrollable text-guided masking framework for self-supervised medical image\nanalysis. By leveraging vision-language models for prompt-based region\nlocalization, our method flexibly applies differentiated masking to emphasize\ndiagnostically relevant regions while reducing redundancy in background areas.\nThis controllable design enables better semantic alignment, improved\nrepresentation learning, and stronger cross-task generalizability.\nComprehensive evaluation across multiple medical imaging modalities, including\nbrain MRI, chest CT, and lung X-ray, shows that Mask What Matters consistently\noutperforms existing MIM methods (e.g., SparK), achieving gains of up to +3.1\npercentage points in classification accuracy, +1.3 in box average precision\n(BoxAP), and +1.1 in mask average precision (MaskAP) for detection. Notably, it\nachieves these improvements with substantially lower overall masking ratios\n(e.g., 40\\% vs. 70\\%). This work demonstrates that controllable, text-driven\nmasking can enable semantically aligned self-supervised learning, advancing the\ndevelopment of robust vision models for medical image analysis.",
        "url": "http://arxiv.org/abs/2509.23054v1",
        "published_date": "2025-09-27T02:26:56+00:00",
        "updated_date": "2025-09-27T02:26:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruilang Wang",
            "Shuotong Xu",
            "Bowen Liu",
            "Runlin Huang",
            "Donglong Chen",
            "Weifeng Su"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a controllable text-guided masking framework for self-supervised medical image analysis, outperforming existing methods by emphasizing diagnostically relevant regions and achieving better semantic alignment.",
        "tldr_zh": "该论文介绍了一种可控的文本引导遮罩框架，用于自监督医学图像分析，通过强调诊断相关区域，实现了更好的语义对齐，胜过现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Activation Matching for Explanation Generation",
        "summary": "In this paper we introduce an activation-matching--based approach to generate\nminimal, faithful explanations for the decision-making of a pretrained\nclassifier on any given image. Given an input image \\(x\\) and a frozen model\n\\(f\\), we train a lightweight autoencoder to output a binary mask \\(m\\) such\nthat the explanation \\(e = m \\odot x\\) preserves both the model's prediction\nand the intermediate activations of \\(x\\). Our objective combines: (i)\nmulti-layer activation matching with KL divergence to align distributions and\ncross-entropy to retain the top-1 label for both the image and the explanation;\n(ii) mask priors -- L1 area for minimality, a binarization penalty for crisp\n0/1 masks, and total variation for compactness; and (iii) abductive constraints\nfor faithfulness and necessity. Together, these objectives yield small,\nhuman-interpretable masks that retain classifier behavior while discarding\nirrelevant input regions, providing practical and faithful minimalist\nexplanations for the decision making of the underlying model.",
        "url": "http://arxiv.org/abs/2509.23051v1",
        "published_date": "2025-09-27T02:12:09+00:00",
        "updated_date": "2025-09-27T02:12:09+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Pirzada Suhail",
            "Aditya Anand",
            "Amit Sethi"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces an activation-matching-based approach to generate minimal, faithful explanations for the decision-making of a pretrained classifier on images.",
        "tldr_zh": "本文介绍了一种基于激活匹配的方法，用于生成有关预训练分类器在图像上做出决策的简洁且忠实的解释。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MMeViT: Multi-Modal ensemble ViT for Post-Stroke Rehabilitation Action Recognition",
        "summary": "Rehabilitation therapy for stroke patients faces a supply shortage despite\nthe increasing demand. To address this issue, remote monitoring systems that\nreduce the burden on medical staff are emerging as a viable alternative. A key\ncomponent of these remote monitoring systems is Human Action Recognition (HAR)\ntechnology, which classifies actions. However, existing HAR studies have\nprimarily focused on non-disable individuals, making them unsuitable for\nrecognizing the actions of stroke patients. HAR research for stroke has largely\nconcentrated on classifying relatively simple actions using machine learning\nrather than deep learning. In this study, we designed a system to monitor the\nactions of stroke patients, focusing on domiciliary upper limb Activities of\nDaily Living (ADL). Our system utilizes IMU (Inertial Measurement Unit) sensors\nand an RGB-D camera, which are the most common modalities in HAR. We directly\ncollected a dataset through this system, investigated an appropriate preprocess\nand proposed a deep learning model suitable for processing multimodal data. We\nanalyzed the collected dataset and found that the action data of stroke\npatients is less clustering than that of non-disabled individuals.\nSimultaneously, we found that the proposed model learns similar tendencies for\neach label in data with features that are difficult to clustering. This study\nsuggests the possibility of expanding the deep learning model, which has\nlearned the action features of stroke patients, to not only simple action\nrecognition but also feedback such as assessment contributing to domiciliary\nrehabilitation in future research. The code presented in this study is\navailable at https://github.com/ye-Kim/MMeViT.",
        "url": "http://arxiv.org/abs/2509.23044v1",
        "published_date": "2025-09-27T01:46:26+00:00",
        "updated_date": "2025-09-27T01:46:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ye-eun Kim",
            "Suhyeon Lim",
            "Andrew J. Choi"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper presents a system using deep learning to monitor the actions of stroke patients in their daily activities, potentially aiding in domiciliary rehabilitation.",
        "tldr_zh": "该论文提出了使用深度学习监测中风患者在日常活动中的行为的系统，可能有助于家庭康复。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Sensor-Adaptive Flood Mapping with Pre-trained Multi-Modal Transformers across SAR and Multispectral Modalities",
        "summary": "Floods are increasingly frequent natural disasters causing extensive human\nand economic damage, highlighting the critical need for rapid and accurate\nflood inundation mapping. While remote sensing technologies have advanced flood\nmonitoring capabilities, operational challenges persist: single-sensor\napproaches face weather-dependent data availability and limited revisit\nperiods, while multi-sensor fusion methods require substantial computational\nresources and large-scale labeled datasets. To address these limitations, this\nstudy introduces a novel sensor-flexible flood detection methodology by\nfine-tuning Presto, a lightweight ($\\sim$0.4M parameters) multi-modal\npre-trained transformer that processes both Synthetic Aperture Radar (SAR) and\nmultispectral (MS) data at the pixel level. Our approach uniquely enables flood\nmapping using SAR-only, MS-only, or combined SAR+MS inputs through a single\nmodel architecture, addressing the critical operational need for rapid response\nwith whatever sensor data becomes available first during disasters. We\nevaluated our method on the Sen1Floods11 dataset against the large-scale\nPrithvi-100M baseline ($\\sim$100M parameters) across three realistic data\navailability scenarios. The proposed model achieved superior performance with\nan F1 score of 0.896 and mIoU of 0.886 in the optimal sensor-fusion scenario,\noutperforming the established baseline. Crucially, the model demonstrated\nrobustness by maintaining effective performance in MS-only scenarios (F1:\n0.893) and functional capabilities in challenging SAR-only conditions (F1:\n0.718), confirming the advantage of multi-modal pre-training for operational\nflood mapping. Our parameter-efficient, sensor-flexible approach offers an\naccessible and robust solution for real-world disaster scenarios requiring\nimmediate flood extent assessment regardless of sensor availability\nconstraints.",
        "url": "http://arxiv.org/abs/2509.23035v1",
        "published_date": "2025-09-27T01:09:30+00:00",
        "updated_date": "2025-09-27T01:09:30+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Tomohiro Tanaka",
            "Narumasa Tsutsumida"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a sensor-flexible flood mapping approach using a pre-trained multi-modal transformer for SAR and multispectral data, achieving superior performance in flood detection.",
        "tldr_zh": "本文介绍了一种使用预训练的多模态变换器进行SAR和多光谱数据的传感器灵活洪水映射方法，实现了在洪水检测方面的优越性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Perceptual Influence: Improving the Perceptual Loss Design for Low-Dose CT Enhancement",
        "summary": "Perceptual losses have emerged as powerful tools for training networks to\nenhance Low-Dose Computed Tomography (LDCT) images, offering an alternative to\ntraditional pixel-wise losses such as Mean Squared Error, which often lead to\nover-smoothed reconstructions and loss of clinically relevant details in LDCT\nimages. The perceptual losses operate in a latent feature space defined by a\npretrained encoder and aim to preserve semantic content by comparing high-level\nfeatures rather than raw pixel values. However, the design of perceptual losses\ninvolves critical yet underexplored decisions, including the feature\nrepresentation level, the dataset used to pretrain the encoder, and the\nrelative importance assigned to the perceptual component during optimization.\nIn this work, we introduce the concept of perceptual influence (a metric that\nquantifies the relative contribution of the perceptual loss term to the total\nloss) and propose a principled framework to assess the impact of the loss\ndesign choices on the model training performance. Through systematic\nexperimentation, we show that the widely used configurations in the literature\nto set up a perceptual loss underperform compared to better-designed\nalternatives. Our findings show that better perceptual loss designs lead to\nsignificant improvements in noise reduction and structural fidelity of\nreconstructed CT images, without requiring any changes to the network\narchitecture. We also provide objective guidelines, supported by statistical\nanalysis, to inform the effective use of perceptual losses in LDCT denoising.\nOur source code is available at\nhttps://github.com/vngabriel/perceptual-influence.",
        "url": "http://arxiv.org/abs/2509.23025v1",
        "published_date": "2025-09-27T00:48:43+00:00",
        "updated_date": "2025-09-27T00:48:43+00:00",
        "categories": [
            "cs.CV",
            "I.5.1; I.5.4; I.4.3; J.3"
        ],
        "authors": [
            "Gabriel A. Viana",
            "Luis F. Alves Pereira",
            "Tsang Ing Ren",
            "George D. C. Cavalcanti",
            "Jan Sijbers"
        ],
        "ai_categories": [
            "Dataset",
            "GAN",
            "Other"
        ],
        "tldr": "The paper introduces the concept of perceptual influence to improve the design of perceptual losses for enhancing low-dose CT images, leading to better noise reduction and structural fidelity.",
        "tldr_zh": "该论文引入了感知影响的概念，以改进感知损失设计，提高低剂量CT图像的噪声降低和结构保真度。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Copyright Infringement Detection in Text-to-Image Diffusion Models via Differential Privacy",
        "summary": "The widespread deployment of large vision models such as Stable Diffusion\nraises significant legal and ethical concerns, as these models can memorize and\nreproduce copyrighted content without authorization. Existing detection\napproaches often lack robustness and fail to provide rigorous theoretical\nunderpinnings. To address these gaps, we formalize the concept of copyright\ninfringement and its detection from the perspective of Differential Privacy\n(DP), and introduce the conditional sensitivity metric, a concept analogous to\nsensitivity in DP, that quantifies the deviation in a diffusion model's output\ncaused by the inclusion or exclusion of a specific training data point. To\noperationalize this metric, we propose D-Plus-Minus (DPM), a novel post-hoc\ndetection framework that identifies copyright infringement in text-to-image\ndiffusion models. Specifically, DPM simulates inclusion and exclusion processes\nby fine-tuning models in two opposing directions: learning or unlearning.\nBesides, to disentangle concept-specific influence from the global parameter\nshifts induced by fine-tuning, DPM computes confidence scores over orthogonal\nprompt distributions using statistical metrics. Moreover, to facilitate\nstandardized benchmarking, we also construct the Copyright Infringement\nDetection Dataset (CIDD), a comprehensive resource for evaluating detection\nacross diverse categories. Our results demonstrate that DPM reliably detects\ninfringement content without requiring access to the original training dataset\nor text prompts, offering an interpretable and practical solution for\nsafeguarding intellectual property in the era of generative AI.",
        "url": "http://arxiv.org/abs/2509.23022v1",
        "published_date": "2025-09-27T00:38:12+00:00",
        "updated_date": "2025-09-27T00:38:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiafeng Man",
            "Zhipeng Wei",
            "Jingjing Chen"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes a method for detecting copyright infringement in text-to-image diffusion models using a concept called Differential Privacy and introduces a new detection framework called D-Plus-Minus (DPM). It also presents a dataset for evaluating detection performance.",
        "tldr_zh": "本文提出一种使用差分隐私概念在文本到图像扩散模型中检测版权侵权的方法，并引入了名为D-Plus-Minus (DPM) 的新检测框架。同时还提出了一个用于评估检测性能的数据集。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Planning with Unified Multimodal Models",
        "summary": "With the powerful reasoning capabilities of large language models (LLMs) and\nvision-language models (VLMs), many recent works have explored using them for\ndecision-making. However, most of these approaches rely solely on\nlanguage-based reasoning, which limits their ability to reason and make\ninformed decisions. Recently, a promising new direction has emerged with\nunified multimodal models (UMMs), which support both multimodal inputs and\noutputs. We believe such models have greater potential for decision-making by\nenabling reasoning through generated visual content. To this end, we propose\nUni-Plan, a planning framework built on UMMs. Within this framework, a single\nmodel simultaneously serves as the policy, dynamics model, and value function.\nIn addition, to avoid hallucinations in dynamics predictions, we present a\nnovel approach self-discriminated filtering, where the generative model serves\nas a self-discriminator to filter out invalid dynamics predictions. Experiments\non long-horizon planning tasks show that Uni-Plan substantially improves\nsuccess rates compared to VLM-based methods, while also showing strong data\nscalability, requiring no expert demonstrations and achieving better\nperformance under the same training-data size. This work lays a foundation for\nfuture research in reasoning and decision-making with UMMs.",
        "url": "http://arxiv.org/abs/2509.23014v1",
        "published_date": "2025-09-27T00:13:13+00:00",
        "updated_date": "2025-09-27T00:13:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yihao Sun",
            "Zhilong Zhang",
            "Yang Yu",
            "Pierre-Luc Bacon"
        ],
        "ai_categories": [
            "Multimodality"
        ],
        "tldr": "The paper introduces Uni-Plan, a planning framework using unified multimodal models (UMMs) for decision-making. It improves success rates compared to language-based methods and shows strong data scalability without requiring expert demonstrations.",
        "tldr_zh": "本文介绍了Uni-Plan，一种利用统一多模态模型（UMMs）进行决策的规划框架。它提高了成功率，同时显示出很强的数据可扩展性，无需专家示范。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Geometry-Aware Losses for Structure-Preserving Text-to-Sign Language Generation",
        "summary": "Sign language translation from text to video plays a crucial role in enabling\neffective communication for Deaf and hard--of--hearing individuals. A major\nchallenge lies in generating accurate and natural body poses and movements that\nfaithfully convey intended meanings. Prior methods often neglect the anatomical\nconstraints and coordination patterns of human skeletal motion, resulting in\nrigid or biomechanically implausible outputs. To address this, we propose a\nnovel approach that explicitly models the relationships among skeletal\njoints--including shoulders, arms, and hands--by incorporating geometric\nconstraints on joint positions, bone lengths, and movement dynamics. During\ntraining, we introduce a parent-relative reweighting mechanism to enhance\nfinger flexibility and reduce motion stiffness. Additionally, bone-pose losses\nand bone-length constraints enforce anatomically consistent structures. Our\nmethod narrows the performance gap between the previous best and the\nground-truth oracle by 56.51%, and further reduces discrepancies in bone length\nand movement variance by 18.76% and 5.48%, respectively, demonstrating\nsignificant gains in anatomical realism and motion naturalness.",
        "url": "http://arxiv.org/abs/2509.23011v1",
        "published_date": "2025-09-27T00:06:17+00:00",
        "updated_date": "2025-09-27T00:06:17+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Zetian Wu",
            "Tianshuo Zhou",
            "Stefan Lee",
            "Liang Huang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel approach to generate sign language from text to video by considering geometric constraints on joint positions, bone lengths, and movement dynamics.",
        "tldr_zh": "本文提出了一种新的方法，通过考虑关节位置、骨长和运动动力学的几何约束来生成从文本到视频的手语。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Disentangling Static and Dynamic Information for Reducing Static Bias in Action Recognition",
        "summary": "Action recognition models rely excessively on static cues rather than dynamic\nhuman motion, which is known as static bias. This bias leads to poor\nperformance in real-world applications and zero-shot action recognition. In\nthis paper, we propose a method to reduce static bias by separating temporal\ndynamic information from static scene information. Our approach uses a\nstatistical independence loss between biased and unbiased streams, combined\nwith a scene prediction loss. Our experiments demonstrate that this method\neffectively reduces static bias and confirm the importance of scene prediction\nloss.",
        "url": "http://arxiv.org/abs/2509.23009v1",
        "published_date": "2025-09-27T00:03:41+00:00",
        "updated_date": "2025-09-27T00:03:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Masato Kobayashi",
            "Ning Ding",
            "Toru Tamaki"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a method to reduce static bias in action recognition by separating dynamic and static information, which improves performance in real-world applications.",
        "tldr_zh": "该论文提出了一种方法，通过分离动态和静态信息来减少行动识别中的静态偏差，从而提高在实际应用中的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LLMs Behind the Scenes: Enabling Narrative Scene Illustration",
        "summary": "Generative AI has established the opportunity to readily transform content\nfrom one medium to another. This capability is especially powerful for\nstorytelling, where visual illustrations can illuminate a story originally\nexpressed in text. In this paper, we focus on the task of narrative scene\nillustration, which involves automatically generating an image depicting a\nscene in a story. Motivated by recent progress on text-to-image models, we\nconsider a pipeline that uses LLMs as an interface for prompting text-to-image\nmodels to generate scene illustrations given raw story text. We apply\nvariations of this pipeline to a prominent story corpus in order to synthesize\nillustrations for scenes in these stories. We conduct a human annotation task\nto obtain pairwise quality judgments for these illustrations. The outcome of\nthis process is the SceneIllustrations dataset, which we release as a new\nresource for future work on cross-modal narrative transformation. Through our\nanalysis of this dataset and experiments modeling illustration quality, we\ndemonstrate that LLMs can effectively verbalize scene knowledge implicitly\nevoked by story text. Moreover, this capability is impactful for generating and\nevaluating illustrations.",
        "url": "http://arxiv.org/abs/2509.22940v1",
        "published_date": "2025-09-26T21:15:18+00:00",
        "updated_date": "2025-09-26T21:15:18+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Melissa Roemmele",
            "John Joon Young Chung",
            "Taewook Kim",
            "Yuqian Sun",
            "Alex Calderwood",
            "Max Kreminski"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper explores using Language Models to prompt text-to-image models for generating visual illustrations of scenes from stories, creating a new dataset for cross-modal narrative transformation.",
        "tldr_zh": "本文探讨了使用语言模型来触发文本到图像模型，生成故事场景的视觉插图，为跨模态叙述转换创造了新的数据集。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "MonoCon: A general framework for learning ultra-compact high-fidelity representations using monotonicity constraints",
        "summary": "Learning high-quality, robust, efficient, and disentangled representations is\na central challenge in artificial intelligence (AI). Deep metric learning\nframeworks tackle this challenge primarily using architectural and optimization\nconstraints. Here, we introduce a third approach that instead relies on\n$\\textit{functional}$ constraints. Specifically, we present MonoCon, a simple\nframework that uses a small monotonic multi-layer perceptron (MLP) head\nattached to any pre-trained encoder. Due to co-adaptation between encoder and\nhead guided by contrastive loss and monotonicity constraints, MonoCon learns\nrobust, disentangled, and highly compact embeddings at a practically negligible\nperformance cost. On the CIFAR-100 image classification task, MonoCon yields\nrepresentations that are nearly 9x more compact and 1.5x more robust than the\nfine-tuned encoder baseline, while retaining 99\\% of the baseline's 5-NN\nclassification accuracy. We also report a 3.4x more compact and 1.4x more\nrobust representation on an SNLI sentence similarity task for a marginal\nreduction in the STSb score, establishing MonoCon as a general domain-agnostic\nframework. Crucially, these robust, ultra-compact representations learned via\nfunctional constraints offer a unified solution to critical challenges in\ndisparate contexts ranging from edge computing to cloud-scale retrieval.",
        "url": "http://arxiv.org/abs/2509.22931v1",
        "published_date": "2025-09-26T20:54:40+00:00",
        "updated_date": "2025-09-26T20:54:40+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Shreyas Gokhale"
        ],
        "ai_categories": [
            "Transformer",
            "AIGC"
        ],
        "tldr": "MonoCon is a framework that uses functional constraints to learn ultra-compact and high-fidelity representations, showing promising results on image and text tasks.",
        "tldr_zh": "MonoCon 是一个框架，利用功能约束来学习超紧凑的高保真度表示，在图像和文本任务上表现出色。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "ControlEvents: Controllable Synthesis of Event Camera Datawith Foundational Prior from Image Diffusion Models",
        "summary": "In recent years, event cameras have gained significant attention due to their\nbio-inspired properties, such as high temporal resolution and high dynamic\nrange. However, obtaining large-scale labeled ground-truth data for event-based\nvision tasks remains challenging and costly. In this paper, we present\nControlEvents, a diffusion-based generative model designed to synthesize\nhigh-quality event data guided by diverse control signals such as class text\nlabels, 2D skeletons, and 3D body poses. Our key insight is to leverage the\ndiffusion prior from foundation models, such as Stable Diffusion, enabling\nhigh-quality event data generation with minimal fine-tuning and limited labeled\ndata. Our method streamlines the data generation process and significantly\nreduces the cost of producing labeled event datasets. We demonstrate the\neffectiveness of our approach by synthesizing event data for visual\nrecognition, 2D skeleton estimation, and 3D body pose estimation. Our\nexperiments show that the synthesized labeled event data enhances model\nperformance in all tasks. Additionally, our approach can generate events based\non unseen text labels during training, illustrating the powerful text-based\ngeneration capabilities inherited from foundation models.",
        "url": "http://arxiv.org/abs/2509.22864v1",
        "published_date": "2025-09-26T19:22:07+00:00",
        "updated_date": "2025-09-26T19:22:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yixuan Hu",
            "Yuxuan Xue",
            "Simon Klenk",
            "Daniel Cremers",
            "Gerard Pons-Moll"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper introduces ControlEvents, a model for synthesizing event camera data with control signals, using diffusion models to reduce the need for labeled data.",
        "tldr_zh": "本文介绍了ControlEvents，这是一个用于合成事件相机数据的模型，利用扩散模型减少对标注数据的需求。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multimodal Slice Interaction Network Enhanced by Transfer Learning for Precise Segmentation of Internal Gross Tumor Volume in Lung Cancer PET/CT Imaging",
        "summary": "Lung cancer remains the leading cause of cancerrelated deaths globally.\nAccurate delineation of internal gross tumor volume (IGTV) in PET/CT imaging is\npivotal for optimal radiation therapy in mobile tumors such as lung cancer to\naccount for tumor motion, yet is hindered by the limited availability of\nannotated IGTV datasets and attenuated PET signal intensity at tumor\nboundaries. In this study, we present a transfer learningbased methodology\nutilizing a multimodal interactive perception network with MAMBA, pre-trained\non extensive gross tumor volume (GTV) datasets and subsequently fine-tuned on a\nprivate IGTV cohort. This cohort constitutes the PET/CT subset of the\nLung-cancer Unified Cross-modal Imaging Dataset (LUCID). To further address the\nchallenge of weak PET intensities in IGTV peripheral slices, we introduce a\nslice interaction module (SIM) within a 2.5D segmentation framework to\neffectively model inter-slice relationships. Our proposed module integrates\nchannel and spatial attention branches with depthwise convolutions, enabling\nmore robust learning of slice-to-slice dependencies and thereby improving\noverall segmentation performance. A comprehensive experimental evaluation\ndemonstrates that our approach achieves a Dice of 0.609 on the private IGTV\ndataset, substantially surpassing the conventional baseline score of 0.385.\nThis work highlights the potential of transfer learning, coupled with advanced\nmultimodal techniques and a SIM to enhance the reliability and clinical\nrelevance of IGTV segmentation for lung cancer radiation therapy planning.",
        "url": "http://arxiv.org/abs/2509.22841v1",
        "published_date": "2025-09-26T18:48:08+00:00",
        "updated_date": "2025-09-26T18:48:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yi Luo",
            "Yike Guo",
            "Hamed Hooshangnejad",
            "Rui Zhang",
            "Xue Feng",
            "Quan Chen",
            "Wil Ngwa",
            "Kai Ding"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper presents a transfer learning-based method using a multimodal network and a slice interaction module to improve the segmentation of internal gross tumor volume in lung cancer PET/CT imaging.",
        "tldr_zh": "该论文提出了一种基于迁移学习的方法，利用多模态网络和切片交互模块来改进肺癌PET/CT影像中内部肿瘤体积的分割。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Seeing Isn't Believing: Context-Aware Adversarial Patch Synthesis via Conditional GAN",
        "summary": "Adversarial patch attacks pose a severe threat to deep neural networks, yet\nmost existing approaches rely on unrealistic white-box assumptions, untargeted\nobjectives, or produce visually conspicuous patches that limit real-world\napplicability. In this work, we introduce a novel framework for fully\ncontrollable adversarial patch generation, where the attacker can freely choose\nboth the input image x and the target class y target, thereby dictating the\nexact misclassification outcome. Our method combines a generative U-Net design\nwith Grad-CAM-guided patch placement, enabling semantic-aware localization that\nmaximizes attack effectiveness while preserving visual realism. Extensive\nexperiments across convolutional networks (DenseNet-121, ResNet-50) and vision\ntransformers (ViT-B/16, Swin-B/16, among others) demonstrate that our approach\nachieves state-of-the-art performance across all settings, with attack success\nrates (ASR) and target-class success (TCS) consistently exceeding 99%.\n  Importantly, we show that our method not only outperforms prior white-box\nattacks and untargeted baselines, but also surpasses existing non-realistic\napproaches that produce detectable artifacts. By simultaneously ensuring\nrealism, targeted control, and black-box applicability-the three most\nchallenging dimensions of patch-based attacks-our framework establishes a new\nbenchmark for adversarial robustness research, bridging the gap between\ntheoretical attack strength and practical stealthiness.",
        "url": "http://arxiv.org/abs/2509.22836v1",
        "published_date": "2025-09-26T18:39:21+00:00",
        "updated_date": "2025-09-26T18:39:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Roie Kazoom",
            "Alon Goldberg",
            "Hodaya Cohen",
            "Ofer Hadar"
        ],
        "ai_categories": [
            "GAN",
            "Transformer"
        ],
        "tldr": "The paper introduces a framework for generating adversarial patches that can trick deep neural networks into misclassifying images, achieving high success rates while appearing realistic and controllable.",
        "tldr_zh": "本文介绍了一种生成对抗性贴片的框架，可以欺骗深度神经网络对图像进行错误分类，成功率很高，同时看起来又逼真又可控。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "MMPB: It's Time for Multi-Modal Personalization",
        "summary": "Visual personalization is essential in user-facing AI systems such as smart\nhomes and healthcare, where aligning model behavior with user-centric concepts\nis critical. However, recent large Vision-Language Models (VLMs), despite their\nbroad applicability, remain underexplored in their ability to adapt to\nindividual users. In this paper, we introduce MMPB, the first extensive\nbenchmark for evaluating VLMs on personalization. MMPB comprises 10k\nimage-query pairs and includes 111 personalizable concepts across four\ncategories: humans, animals, objects, and characters, with the human category\nenriched with preference-grounded queries. We structure personalization into\nthree main task types, each highlighting a different key property of VLMs.\nUsing 23 widely used VLMs including both open- and closed-source models, we\nevaluate personalization performance via a three-stage protocol: concept\ninjection, multi-turn dialogue, and personalized querying. Our findings\nindicate that most VLMs (including some closed-source models) struggle with\npersonalization, particularly in maintaining consistency over dialogue,\nhandling user preferences, and adapting to visual cues. Our analysis reveals\nthat the challenges in VLM personalization (such as refusal behaviors and\nlong-context forgetting) highlight substantial room for improvement. By\nidentifying these limitations and offering a scalable benchmark, MMPB offers\nvaluable insights and a solid foundation for future research toward truly\npersonalized multi-modal AI. Project Page: aidaslab.github.io/MMPB",
        "url": "http://arxiv.org/abs/2509.22820v1",
        "published_date": "2025-09-26T18:24:48+00:00",
        "updated_date": "2025-09-26T18:24:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jaeik Kim",
            "Woojin Kim",
            "Woohyeon Park",
            "Jaeyoung Do"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a benchmark for evaluating Vision-Language Models (VLMs) on personalization, highlighting challenges and room for improvement in maintaining consistency, handling user preferences, and adapting to visual cues.",
        "tldr_zh": "本文介绍了一个基准测试，用于评估视觉语言模型（VLMs）在个性化方面的表现，突出了在保持一致性、处理用户偏好和适应视觉线索方面的挑战和改进空间。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TRUST: Test-Time Refinement using Uncertainty-Guided SSM Traverses",
        "summary": "State Space Models (SSMs) have emerged as efficient alternatives to Vision\nTransformers (ViTs), with VMamba standing out as a pioneering architecture\ndesigned for vision tasks. However, their generalization performance degrades\nsignificantly under distribution shifts. To address this limitation, we propose\nTRUST (Test-Time Refinement using Uncertainty-Guided SSM Traverses), a novel\ntest-time adaptation (TTA) method that leverages diverse traversal permutations\nto generate multiple causal perspectives of the input image. Model predictions\nserve as pseudo-labels to guide updates of the Mamba-specific parameters, and\nthe adapted weights are averaged to integrate the learned information across\ntraversal scans. Altogether, TRUST is the first approach that explicitly\nleverages the unique architectural properties of SSMs for adaptation.\nExperiments on seven benchmarks show that TRUST consistently improves\nrobustness and outperforms existing TTA methods.",
        "url": "http://arxiv.org/abs/2509.22813v1",
        "published_date": "2025-09-26T18:19:25+00:00",
        "updated_date": "2025-09-26T18:19:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sahar Dastani",
            "Ali Bahri",
            "Gustavo Adolfo Vargas Hakim",
            "Moslem Yazdanpanah",
            "Mehrdad Noori",
            "David Osowiechi",
            "Samuel Barbeau",
            "Ismail Ben Ayed",
            "Herve Lombaert",
            "Christian Desrosiers"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "TRUST proposes a test-time adaptation method for State Space Models in vision tasks, improving robustness and outperforming existing methods on benchmarks.",
        "tldr_zh": "TRUST提出了一种针对视觉任务的状态空间模型的测试时适应方法，提高了稳健性，并在基准测试中优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks",
        "summary": "To improve efficiency and temporal coherence, Vision-Language-Action (VLA)\nmodels often predict action chunks; however, this action chunking harms\nreactivity under inference delay and long horizons. We introduce Asynchronous\nAction Chunk Correction (A2C2), which is a lightweight real-time chunk\ncorrection head that runs every control step and adds a time-aware correction\nto any off-the-shelf VLA's action chunk. The module combines the latest\nobservation, the predicted action from VLA (base action), a positional feature\nthat encodes the index of the base action within the chunk, and some features\nfrom the base policy, then outputs a per-step correction. This preserves the\nbase model's competence while restoring closed-loop responsiveness. The\napproach requires no retraining of the base policy and is orthogonal to\nasynchronous execution schemes such as Real Time Chunking (RTC). On the dynamic\nKinetix task suite (12 tasks) and LIBERO Spatial, our method yields consistent\nsuccess rate improvements across increasing delays and execution horizons (+23%\npoint and +7% point respectively, compared to RTC), and also improves\nrobustness for long horizons even with zero injected delay. Since the\ncorrection head is small and fast, there is minimal overhead compared to the\ninference of large VLA models. These results indicate that A2C2 is an\neffective, plug-in mechanism for deploying high-capacity chunking policies in\nreal-time control.",
        "url": "http://arxiv.org/abs/2509.23224v1",
        "published_date": "2025-09-27T10:07:49+00:00",
        "updated_date": "2025-09-27T10:07:49+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.SY",
            "eess.SY"
        ],
        "authors": [
            "Kohei Sendai",
            "Maxime Alvarez",
            "Tatsuya Matsushima",
            "Yutaka Matsuo",
            "Yusuke Iwasawa"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces Asynchronous Action Chunk Correction (A2C2) to improve VLA models by adding real-time corrections to action chunks, enhancing reactivity and performance across different tasks.",
        "tldr_zh": "本文引入了A2C2机制，通过实时纠正行为块，提高了VLA模型的性能，增强了在不同任务中的反应速度。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.75
    },
    {
        "title": "Seeing Symbols, Missing Cultures: Probing Vision-Language Models' Reasoning on Fire Imagery and Cultural Meaning",
        "summary": "Vision-Language Models (VLMs) often appear culturally competent but rely on\nsuperficial pattern matching rather than genuine cultural understanding. We\nintroduce a diagnostic framework to probe VLM reasoning on fire-themed cultural\nimagery through both classification and explanation analysis. Testing multiple\nmodels on Western festivals, non-Western traditions, and emergency scenes\nreveals systematic biases: models correctly identify prominent Western\nfestivals but struggle with underrepresented cultural events, frequently\noffering vague labels or dangerously misclassifying emergencies as\ncelebrations. These failures expose the risks of symbolic shortcuts and\nhighlight the need for cultural evaluation beyond accuracy metrics to ensure\ninterpretable and fair multimodal systems.",
        "url": "http://arxiv.org/abs/2509.23311v1",
        "published_date": "2025-09-27T13:56:12+00:00",
        "updated_date": "2025-09-27T13:56:12+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Haorui Yu",
            "Qiufeng Yi",
            "Yijia Chu",
            "Yang Zhao"
        ],
        "ai_categories": [
            "Multimodality",
            "Other"
        ],
        "tldr": "The paper investigates how Vision-Language Models reason on fire-themed cultural imagery and highlights their biases in understanding underrepresented cultural events, emphasizing the need for cultural evaluation in AI systems.",
        "tldr_zh": "本文研究视觉语言模型在火焰主题文化图像上的推理，突出它们对欠代表文化事件的偏见，强调AI系统中文化评估的必要性。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "SynDoc: A Hybrid Discriminative-Generative Framework for Enhancing Synthetic Domain-Adaptive Document Key Information Extraction",
        "summary": "Domain-specific Visually Rich Document Understanding (VRDU) presents\nsignificant challenges due to the complexity and sensitivity of documents in\nfields such as medicine, finance, and material science. Existing Large\n(Multimodal) Language Models (LLMs/MLLMs) achieve promising results but face\nlimitations such as hallucinations, inadequate domain adaptation, and reliance\non extensive fine-tuning datasets. This paper introduces SynDoc, a novel\nframework that combines discriminative and generative models to address these\nchallenges. SynDoc employs a robust synthetic data generation workflow, using\nstructural information extraction and domain-specific query generation to\nproduce high-quality annotations. Through adaptive instruction tuning, SynDoc\nimproves the discriminative model's ability to extract domain-specific\nknowledge. At the same time, a recursive inferencing mechanism iteratively\nrefines the output of both models for stable and accurate predictions. This\nframework demonstrates scalable, efficient, and precise document understanding\nand bridges the gap between domain-specific adaptation and general world\nknowledge for document key information extraction tasks.",
        "url": "http://arxiv.org/abs/2509.23273v1",
        "published_date": "2025-09-27T12:01:52+00:00",
        "updated_date": "2025-09-27T12:01:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yihao Ding",
            "Soyeon Caren Han",
            "Yanbei Jiang",
            "Yan Li",
            "Zechuan Li",
            "Yifan Peng"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "SynDoc is a hybrid discriminative-generative framework for improving synthetic domain-adaptive document key information extraction, aiming to bridge the gap between domain-specific adaptation and general world knowledge.",
        "tldr_zh": "SynDoc是一个混合辨别-生成框架，旨在改善合成领域自适应文档关键信息提取，旨在弥合领域特定适应和一般世界知识之间的差距。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "FMC-DETR: Frequency-Decoupled Multi-Domain Coordination for Aerial-View Object Detection",
        "summary": "Aerial-view object detection is a critical technology for real-world\napplications such as natural resource monitoring, traffic management, and\nUAV-based search and rescue. Detecting tiny objects in high-resolution aerial\nimagery presents a long-standing challenge due to their limited visual cues and\nthe difficulty of modeling global context in complex scenes. Existing methods\nare often hampered by delayed contextual fusion and inadequate non-linear\nmodeling, failing to effectively use global information to refine shallow\nfeatures and thus encountering a performance bottleneck. To address these\nchallenges, we propose FMC-DETR, a novel framework with frequency-decoupled\nfusion for aerial-view object detection. First, we introduce the Wavelet\nKolmogorov-Arnold Transformer (WeKat) backbone, which applies cascaded wavelet\ntransforms to enhance global low-frequency context perception in shallow\nfeatures while preserving fine-grained details, and employs Kolmogorov-Arnold\nnetworks to achieve adaptive non-linear modeling of multi-scale dependencies.\nNext, a lightweight Cross-stage Partial Fusion (CPF) module reduces redundancy\nand improves multi-scale feature interaction. Finally, we introduce the\nMulti-Domain Feature Coordination (MDFC) module, which unifies spatial,\nfrequency, and structural priors to to balance detail preservation and global\nenhancement. Extensive experiments on benchmark aerial-view datasets\ndemonstrate that FMC-DETR achieves state-of-the-art performance with fewer\nparameters. On the challenging VisDrone dataset, our model achieves\nimprovements of 6.5% AP and 8.2% AP50 over the baseline, highlighting its\neffectiveness in tiny object detection. The code can be accessed at\nhttps://github.com/bloomingvision/FMC-DETR.",
        "url": "http://arxiv.org/abs/2509.23056v1",
        "published_date": "2025-09-27T02:28:22+00:00",
        "updated_date": "2025-09-27T02:28:22+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Ben Liang",
            "Yuan Liu",
            "Bingwen Qiu",
            "Yihong Wang",
            "Xiubao Sui",
            "Qian Chen"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces FMC-DETR for aerial-view object detection, achieving state-of-the-art performance with fewer parameters.",
        "tldr_zh": "本文介绍了用于航拍物体检测的FMC-DETR，在更少参数的情况下实现了最先进性能。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "GeLoc3r: Enhancing Relative Camera Pose Regression with Geometric Consistency Regularization",
        "summary": "Prior ReLoc3R achieves breakthrough performance with fast 25ms inference and\nstate-of-the-art regression accuracy, yet our analysis reveals subtle geometric\ninconsistencies in its internal representations that prevent reaching the\nprecision ceiling of correspondence-based methods like MASt3R (which require\n300ms per pair). In this work, we present GeLoc3r, a novel approach to relative\ncamera pose estimation that enhances pose regression methods through Geometric\nConsistency Regularization (GCR). GeLoc3r overcomes the speed-accuracy dilemma\nby training regression networks to produce geometrically consistent poses\nwithout inference-time geometric computation. During training, GeLoc3r\nleverages ground-truth depth to generate dense 3D-2D correspondences, weights\nthem using a FusionTransformer that learns correspondence importance, and\ncomputes geometrically-consistent poses via weighted RANSAC. This creates a\nconsistency loss that transfers geometric knowledge into the regression\nnetwork. Unlike FAR method which requires both regression and geometric solving\nat inference, GeLoc3r only uses the enhanced regression head at test time,\nmaintaining ReLoc3R's fast speed and approaching MASt3R's high accuracy. On\nchallenging benchmarks, GeLoc3r consistently outperforms ReLoc3R, achieving\nsignificant improvements including 40.45% vs. 34.85% AUC@5{\\deg} on the CO3Dv2\ndataset (16% relative improvement), 68.66% vs. 66.70% AUC@5{\\deg} on\nRealEstate10K, and 50.45% vs. 49.60% on MegaDepth1500. By teaching geometric\nconsistency during training rather than enforcing it at inference, GeLoc3r\nrepresents a paradigm shift in how neural networks learn camera geometry,\nachieving both the speed of regression and the geometric understanding of\ncorrespondence methods.",
        "url": "http://arxiv.org/abs/2509.23038v1",
        "published_date": "2025-09-27T01:21:38+00:00",
        "updated_date": "2025-09-27T01:21:38+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jingxing Li",
            "Yongjae Lee",
            "Deliang Fan"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "GeLoc3r introduces a novel approach to camera pose estimation, enhancing regression methods with geometric consistency regularization to achieve fast and accurate results.",
        "tldr_zh": "GeLoc3r引入了一种新颖的摄像机姿态估计方法，通过几何一致性正则化增强回归方法，以实现快速和准确的结果。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7.5
    },
    {
        "title": "Brain Tumor Classification from MRI Scans via Transfer Learning and Enhanced Feature Representation",
        "summary": "Brain tumors are abnormal cell growths in the central nervous system (CNS),\nand their timely detection is critical for improving patient outcomes. This\npaper proposes an automatic and efficient deep-learning framework for brain\ntumor detection from magnetic resonance imaging (MRI) scans. The framework\nemploys a pre-trained ResNet50 model for feature extraction, followed by Global\nAverage Pooling (GAP) and linear projection to obtain compact, high-level image\nrepresentations. These features are then processed by a novel Dense-Dropout\nsequence, a core contribution of this work, which enhances non-linear feature\nlearning, reduces overfitting, and improves robustness through diverse feature\ntransformations. Another major contribution is the creation of the Mymensingh\nMedical College Brain Tumor (MMCBT) dataset, designed to address the lack of\nreliable brain tumor MRI resources. The dataset comprises MRI scans from 209\nsubjects (ages 9 to 65), including 3671 tumor and 13273 non-tumor images, all\nclinically verified under expert supervision. To overcome class imbalance, the\ntumor class was augmented, resulting in a balanced dataset well-suited for deep\nlearning research.",
        "url": "http://arxiv.org/abs/2509.22956v1",
        "published_date": "2025-09-26T21:41:30+00:00",
        "updated_date": "2025-09-26T21:41:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ahta-Shamul Hoque Emran",
            "Hafija Akter",
            "Abdullah Al Shiam",
            "Abu Saleh Musa Miah",
            "Anichur Rahman",
            "Fahmid Al Farid",
            "Hezerul Abdul Karim"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a deep learning framework using transfer learning and novel feature representation for brain tumor classification from MRI scans, along with a new dataset addressing the lack of reliable resources.",
        "tldr_zh": "该论文介绍了一种使用迁移学习和新特征表示的深度学习框架，用于从MRI扫描中对脑肿瘤进行分类，同时提供了一种解决可靠数据资源缺乏的新数据集。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Real-World Transferable Adversarial Attack on Face-Recognition Systems",
        "summary": "Adversarial attacks on face recognition (FR) systems pose a significant\nsecurity threat, yet most are confined to the digital domain or require\nwhite-box access. We introduce GaP (Gaussian Patch), a novel method to generate\na universal, physically transferable adversarial patch under a strict black-box\nsetting. Our approach uses a query-efficient, zero-order greedy algorithm to\niteratively construct a symmetric, grayscale pattern for the forehead. The\npatch is optimized by successively adding Gaussian blobs, guided only by the\ncosine similarity scores from a surrogate FR model to maximally degrade\nidentity recognition. We demonstrate that with approximately 10,000 queries to\na black-box ArcFace model, the resulting GaP achieves a high attack success\nrate in both digital and real-world physical tests. Critically, the attack\nshows strong transferability, successfully deceiving an entirely unseen FaceNet\nmodel. Our work highlights a practical and severe vulnerability, proving that\nrobust, transferable attacks can be crafted with limited knowledge of the\ntarget system.",
        "url": "http://arxiv.org/abs/2509.23198v1",
        "published_date": "2025-09-27T09:09:06+00:00",
        "updated_date": "2025-09-27T09:09:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Andrey Kaznacheev",
            "Matvey Mikhalchuk",
            "Andrey Kuznetsov",
            "Aleksandr Petiushko",
            "Anton Razzhigaev"
        ],
        "ai_categories": [
            "GAN",
            "AIGC",
            "Other"
        ],
        "tldr": "The paper introduces a novel method to generate a physically transferable adversarial patch for face-recognition systems, showing high attack success rates in both digital and real-world tests.",
        "tldr_zh": "该论文介绍了一种新的方法，用于生成可在现实世界中传递的面部识别对抗性补丁，在数字和真实环境下都展现出高攻击成功率。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Earth-Agent: Unlocking the Full Landscape of Earth Observation with Agents",
        "summary": "Earth observation (EO) is essential for understanding the evolving states of\nthe Earth system. Although recent MLLMs have advanced EO research, they still\nlack the capability to tackle complex tasks that require multi-step reasoning\nand the use of domain-specific tools. Agent-based methods offer a promising\ndirection, but current attempts remain in their infancy, confined to RGB\nperception, shallow reasoning, and lacking systematic evaluation protocols. To\novercome these limitations, we introduce Earth-Agent, the first agentic\nframework that unifies RGB and spectral EO data within an MCP-based tool\necosystem, enabling cross-modal, multi-step, and quantitative spatiotemporal\nreasoning beyond pretrained MLLMs. Earth-Agent supports complex scientific\ntasks such as geophysical parameter retrieval and quantitative spatiotemporal\nanalysis by dynamically invoking expert tools and models across modalities. To\nsupport comprehensive evaluation, we further propose Earth-Bench, a benchmark\nof 248 expert-curated tasks with 13,729 images, spanning spectrum, products and\nRGB modalities, and equipped with a dual-level evaluation protocol that\nassesses both reasoning trajectories and final outcomes. We conduct\ncomprehensive experiments varying different LLM backbones, comparisons with\ngeneral agent frameworks, and comparisons with MLLMs on remote sensing\nbenchmarks, demonstrating both the effectiveness and potential of Earth-Agent.\nEarth-Agent establishes a new paradigm for EO analysis, moving the field toward\nscientifically grounded, next-generation applications of LLMs in Earth\nobservation. Our code and dataset will be publicly released.",
        "url": "http://arxiv.org/abs/2509.23141v1",
        "published_date": "2025-09-27T06:04:28+00:00",
        "updated_date": "2025-09-27T06:04:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Peilin Feng",
            "Zhutao Lv",
            "Junyan Ye",
            "Xiaolei Wang",
            "Xinjie Huo",
            "Jinhua Yu",
            "Wanghan Xu",
            "Wenlong Zhang",
            "Lei Bai",
            "Conghui He",
            "Weijia Li"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "Earth-Agent introduces a new framework for Earth observation that combines RGB and spectral data to enable complex reasoning and analysis tasks beyond pretrained models.",
        "tldr_zh": "Earth-Agent提出了一个新的框架，将RGB和光谱数据结合起来，以实现超越预训练模型的复杂推理和分析任务。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "HTMA-Net: Towards Multiplication-Avoiding Neural Networks via Hadamard Transform and In-Memory Computing",
        "summary": "Reducing the cost of multiplications is critical for efficient deep neural\nnetwork deployment, especially in energy-constrained edge devices. In this\nwork, we introduce HTMA-Net, a novel framework that integrates the Hadamard\nTransform (HT) with multiplication-avoiding (MA) SRAM-based in-memory computing\nto reduce arithmetic complexity while maintaining accuracy. Unlike prior\nmethods that only target multiplications in convolutional layers or focus\nsolely on in-memory acceleration, HTMA-Net selectively replaces intermediate\nconvolutions with Hybrid Hadamard-based transform layers whose internal\nconvolutions are implemented via multiplication-avoiding in-memory operations.\nWe evaluate HTMA-Net on ResNet-18 using CIFAR-10, CIFAR-100, and Tiny ImageNet,\nand provide a detailed comparison against regular, MF-only, and HT-only\nvariants. Results show that HTMA-Net eliminates up to 52\\% of multiplications\ncompared to baseline ResNet-18, ResNet-20, and ResNet-50 models, while\nachieving comparable accuracy in evaluation and significantly reducing\ncomputational complexity and the number of parameters. Our results demonstrate\nthat combining structured Hadamard transform layers with SRAM-based in-memory\ncomputing multiplication-avoiding operators is a promising path towards\nefficient deep learning architectures.",
        "url": "http://arxiv.org/abs/2509.23103v1",
        "published_date": "2025-09-27T04:26:02+00:00",
        "updated_date": "2025-09-27T04:26:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Emadeldeen Hamdan",
            "Ahmet Enis Cetin"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "HTMA-Net introduces a new framework combining Hadamard Transform and in-memory computing to reduce multiplication costs in deep neural networks, showing promising results on ResNet models.",
        "tldr_zh": "HTMA-Net引入了一个新的框架，结合Hadamard变换和内存计算，以减少深度神经网络中的乘法成本，在ResNet模型上取得了有希望的结果。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Deep Learning for Oral Health: Benchmarking ViT, DeiT, BEiT, ConvNeXt, and Swin Transformer",
        "summary": "Objective: The aim of this study was to systematically evaluate and compare\nthe performance of five state-of-the-art transformer-based architectures -\nVision Transformer (ViT), Data-efficient Image Transformer (DeiT), ConvNeXt,\nSwin Transformer, and Bidirectional Encoder Representation from Image\nTransformers (BEiT) - for multi-class dental disease classification. The study\nspecifically focused on addressing real-world challenges such as data\nimbalance, which is often overlooked in existing literature.\n  Study Design: The Oral Diseases dataset was used to train and validate the\nselected models. Performance metrics, including validation accuracy, precision,\nrecall, and F1-score, were measured, with special emphasis on how well each\narchitecture managed imbalanced classes.\n  Results: ConvNeXt achieved the highest validation accuracy at 81.06, followed\nby BEiT at 80.00 and Swin Transformer at 79.73, all demonstrating strong\nF1-scores. ViT and DeiT achieved accuracies of 79.37 and 78.79, respectively,\nbut both struggled particularly with Caries-related classes.\n  Conclusions: ConvNeXt, Swin Transformer, and BEiT showed reliable diagnostic\nperformance, making them promising candidates for clinical application in\ndental imaging. These findings provide guidance for model selection in future\nAI-driven oral disease diagnostic tools and highlight the importance of\naddressing data imbalance in real-world scenarios",
        "url": "http://arxiv.org/abs/2509.23100v1",
        "published_date": "2025-09-27T04:17:04+00:00",
        "updated_date": "2025-09-27T04:17:04+00:00",
        "categories": [
            "cs.CV",
            "68U10: Image processing"
        ],
        "authors": [
            "Ajo Babu George",
            "Sadhvik Bathini",
            "Niranjana S R"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "This paper compares the performance of five transformer-based architectures for dental disease classification, highlighting the importance of addressing data imbalance in real-world scenarios.",
        "tldr_zh": "本文比较了五种基于transformer的架构对于口腔疾病分类的性能，并强调了在真实场景中解决数据不平衡的重要性。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7.25
    },
    {
        "title": "ADAM: A Diverse Archive of Mankind for Evaluating and Enhancing LLMs in Biographical Reasoning",
        "summary": "We introduce ADAM (A Diverse Archive of Mankind), a framework for evaluating\nand improving multimodal large language models (MLLMs) in biographical\nreasoning. To the best of our knowledge, this is the first work to\nsystematically examine LLM capabilities in biography, a critical yet\nunderexplored dimension of factual knowledge. At its core, AdamDB is a\nmultilingual and multimodal dataset covering over 4 million individuals across\ngeography, time, and profession, while AdamBench provides cognitively\nstructured evaluations based on Bloom's taxonomy, spanning six reasoning levels\nin both English and native languages. To address hallucinations, particularly\nfor lesser-known individuals, we propose AdamRAG, a retrieval-augmented\ngeneration system tailored to biographical contexts. Experiments show that\nAdamRAG substantially improves open-source models and modestly benefits\nclosed-source ones, with the largest gains on lower-order reasoning. Popularity\nstrongly mediates accuracy, and multimodal input via face images offers\nsmaller, less consistent improvements than retrieval. ADAM establishes the\nfirst benchmark and framework for cognitively, culturally, and multimodally\ngrounded biographical evaluation, advancing the development of multilingual,\naccurate, and hallucination-resistant MLLMs.",
        "url": "http://arxiv.org/abs/2509.22991v1",
        "published_date": "2025-09-26T23:04:28+00:00",
        "updated_date": "2025-09-26T23:04:28+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.IR",
            "cs.LG"
        ],
        "authors": [
            "Jasin Cekinmez",
            "Omid Ghahroodi",
            "Saad Fowad Chandle",
            "Dhiman Gupta",
            "Ehsaneddin Asgari"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces ADAM, a framework for evaluating and enhancing large language models in biographical reasoning, with a focus on improving accuracy and reducing hallucinations.",
        "tldr_zh": "该论文介绍了ADAM框架，用于评估和增强大型语言模型在传记推理中的应用，重点在于提高准确性和减少幻觉。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 7.25
    },
    {
        "title": "Learning KAN-based Implicit Neural Representations for Deformable Image Registration",
        "summary": "Deformable image registration (DIR) is a cornerstone of medical image\nanalysis, enabling spatial alignment for tasks like comparative studies and\nmulti-modal fusion. While learning-based methods (e.g., CNNs, transformers)\noffer fast inference, they often require large training datasets and struggle\nto match the precision of classical iterative approaches on some organ types\nand imaging modalities. Implicit neural representations (INRs) have emerged as\na promising alternative, parameterizing deformations as continuous mappings\nfrom coordinates to displacement vectors. However, this comes at the cost of\nrequiring instance-specific optimization, making computational efficiency and\nseed-dependent learning stability critical factors for these methods. In this\nwork, we propose KAN-IDIR and RandKAN-IDIR, the first integration of\nKolmogorov-Arnold Networks (KANs) into deformable image registration with\nimplicit neural representations (INRs). Our proposed randomized basis sampling\nstrategy reduces the required number of basis functions in KAN while\nmaintaining registration quality, thereby significantly lowering computational\ncosts. We evaluated our approach on three diverse datasets (lung CT, brain MRI,\ncardiac MRI) and compared it with competing instance-specific learning-based\napproaches, dataset-trained deep learning models, and classical registration\napproaches. KAN-IDIR and RandKAN-IDIR achieved the highest accuracy among\nINR-based methods across all evaluated modalities and anatomies, with minimal\ncomputational overhead and superior learning stability across multiple random\nseeds. Additionally, we discovered that our RandKAN-IDIR model with randomized\nbasis sampling slightly outperforms the model with learnable basis function\nindices, while eliminating its additional training-time complexity.",
        "url": "http://arxiv.org/abs/2509.22874v1",
        "published_date": "2025-09-26T19:37:46+00:00",
        "updated_date": "2025-09-26T19:37:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nikita Drozdov",
            "Marat Zinovev",
            "Dmitry Sorokin"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces KAN-based implicit neural representations for deformable image registration, achieving high accuracy across different modalities and anatomies while reducing computational costs.",
        "tldr_zh": "该论文介绍了基于KAN的隐式神经表示用于可变形图像配准，在不同模态和解剖结构下实现高准确性，并降低计算成本。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Learning Temporal Saliency for Time Series Forecasting with Cross-Scale Attention",
        "summary": "Explainability in time series forecasting is essential for improving model\ntransparency and supporting informed decision-making. In this work, we present\nCrossScaleNet, an innovative architecture that combines a patch-based\ncross-attention mechanism with multi-scale processing to achieve both high\nperformance and enhanced temporal explainability. By embedding attention\nmechanisms into the training process, our model provides intrinsic\nexplainability for temporal saliency, making its decision-making process more\ntransparent. Traditional post-hoc methods for temporal saliency detection are\ncomputationally expensive, particularly when compared to feature importance\ndetection. While ablation techniques may suffice for datasets with fewer\nfeatures, identifying temporal saliency poses greater challenges due to its\ncomplexity. We validate CrossScaleNet on synthetic datasets with known saliency\nground truth and on established public benchmarks, demonstrating the robustness\nof our method in identifying temporal saliency. Experiments on real-world\ndatasets for forecasting task show that our approach consistently outperforms\nmost transformer-based models, offering better explainability without\nsacrificing predictive accuracy. Our evaluations demonstrate superior\nperformance in both temporal saliency detection and forecasting accuracy.\nMoreover, we highlight that existing models claiming explainability often fail\nto maintain strong performance on standard benchmarks. CrossScaleNet addresses\nthis gap, offering a balanced approach that captures temporal saliency\neffectively while delivering state-of-the-art forecasting performance across\ndatasets of varying complexity.",
        "url": "http://arxiv.org/abs/2509.22839v1",
        "published_date": "2025-09-26T18:43:51+00:00",
        "updated_date": "2025-09-26T18:43:51+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Ibrahim Delibasoglu",
            "Fredrik Heintz"
        ],
        "ai_categories": [
            "Transformer",
            "AIGC",
            "Other"
        ],
        "tldr": "CrossScaleNet is a novel architecture for time series forecasting that combines cross-attention with multi-scale processing to achieve high performance and temporal explainability.",
        "tldr_zh": "CrossScaleNet是一种新颖的架构，用于时间序列预测，结合了交叉注意力和多尺度处理，实现了高性能和时间可解释性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "FracDetNet: Advanced Fracture Detection via Dual-Focus Attention and Multi-scale Calibration in Medical X-ray Imaging",
        "summary": "In this paper, an advanced fracture detection framework, FracDetNet, is\nproposed to address challenges in medical imaging, as accurate fracture\ndetection is essential for enhancing diagnostic efficiency in clinical\npractice. Despite recent advancements, existing methods still struggle with\ndetecting subtle and morphologically diverse fractures due to variable imaging\nangles and suboptimal image quality. To overcome these limitations, FracDetNet\nintegrates Dual-Focus Attention (DFA) and Multi-scale Calibration (MC).\nSpecifically, the DFA module effectively captures detailed local features and\ncomprehensive global context through combined global and local attention\nmechanisms. Additionally, the MC adaptively refines feature representations to\nenhance detection performance. Experimental evaluations on the publicly\navailable GRAZPEDWRI-DX dataset demonstrate state-of-the-art performance, with\nFracDetNet achieving a mAP$_{50-95}$ of 40.0\\%, reflecting a \\textbf{7.5\\%}\nimprovement over the baseline model. Furthermore, the mAP$_{50}$ reaches\n63.9\\%, representing an increase of \\textbf{4.2\\%}, with fracture-specific\ndetection accuracy also enhanced by \\textbf{2.9\\%}.",
        "url": "http://arxiv.org/abs/2509.23416v1",
        "published_date": "2025-09-27T17:15:55+00:00",
        "updated_date": "2025-09-27T17:15:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuyang Sun",
            "Cuiming Zou"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "FracDetNet is a framework for advanced fracture detection in medical imaging that integrates Dual-Focus Attention and Multi-scale Calibration to improve accuracy, achieving state-of-the-art performance on a dataset.",
        "tldr_zh": "FracDetNet 是一个用于医学成像中的先进骨折检测框架，它集成了双重关注和多尺度校准以提高准确性，在数据集上取得了最先进的性能。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Graph Your Own Prompt",
        "summary": "We propose Graph Consistency Regularization (GCR), a novel framework that\ninjects relational graph structures, derived from model predictions, into the\nlearning process to promote class-aware, semantically meaningful feature\nrepresentations. Functioning as a form of self-prompting, GCR enables the model\nto refine its internal structure using its own outputs. While deep networks\nlearn rich representations, these often capture noisy inter-class similarities\nthat contradict the model's predicted semantics. GCR addresses this issue by\nintroducing parameter-free Graph Consistency Layers (GCLs) at arbitrary depths.\nEach GCL builds a batch-level feature similarity graph and aligns it with a\nglobal, class-aware masked prediction graph, derived by modulating softmax\nprediction similarities with intra-class indicators. This alignment enforces\nthat feature-level relationships reflect class-consistent prediction behavior,\nacting as a semantic regularizer throughout the network. Unlike prior work, GCR\nintroduces a multi-layer, cross-space graph alignment mechanism with adaptive\nweighting, where layer importance is learned from graph discrepancy magnitudes.\nThis allows the model to prioritize semantically reliable layers and suppress\nnoisy ones, enhancing feature quality without modifying the architecture or\ntraining procedure. GCR is model-agnostic, lightweight, and improves semantic\nstructure across various networks and datasets. Experiments show that GCR\npromotes cleaner feature structure, stronger intra-class cohesion, and improved\ngeneralization, offering a new perspective on learning from prediction\nstructure. [Project website](https://darcyddx.github.io/gcr/)\n[Code](https://github.com/Darcyddx/graph-prompt)",
        "url": "http://arxiv.org/abs/2509.23373v1",
        "published_date": "2025-09-27T15:45:07+00:00",
        "updated_date": "2025-09-27T15:45:07+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Xi Ding",
            "Lei Wang",
            "Piotr Koniusz",
            "Yongsheng Gao"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes Graph Consistency Regularization (GCR), a framework that uses relational graph structures to improve feature representations in deep networks by aligning them with class-aware predictions, enhancing feature quality and generalization.",
        "tldr_zh": "本文提出了图一致性正则化（GCR），通过将图结构与深度网络的特征表示对齐，从而提高了特征质量和泛化能力。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "GRAPE: Let GPRO Supervise Query Rewriting by Ranking for Retrieval",
        "summary": "The CLIP model has become a cornerstone of large-scale retrieval systems by\naligning text and image data in a unified embedding space. Despite its\nsimplicity and efficiency, CLIP struggles when applied to tasks whose input\ndistributions diverge from its training corpus, such as queries with\nmultilingual, long-form, or multimodal differences. To avoid costly retraining,\nexisting methods mainly adopt query-rewriting strategies with large language\nmodels (LLMs), aiming to mitigate distribution gaps at the query level.\nHowever, due to the lack of supervision signals, LLMs fail to generate the\noptimal one that fits the training distribution. We address this challenge with\nGRAPE (Grouped Ranking-Aware Policy Optimization Enhancement), a plug-and-play\nenhancement approach that incorporates ranking signals into retrieval-guided\nquery rewriting with LLMs. Intuitively, GRAPE proposes to leverage GRPO to\nbridge distributional differences -- including length, multilingual, and\nmodality shifts -- by transforming queries into forms better aligned with the\nretriever's training distribution. However, our preliminary experiment finds\nthat naively finetuning LLM with similarity scores can lead to score inflation,\nwhere nearly all candidates are assigned unexpectedly high scores regardless of\ntheir true relevance. To address score inflation, we propose a corpus-relative\nranking-based reward, which explicitly aligns optimization with ranking metrics\nwhile suppressing spurious score inflation. Extensive experiments demonstrate\nthat GRAPE consistently improves retrieval performance under distributional\nshifts -- including multilingual differences (Flickr30k-CN, CVLUE, XM3600),\nlength differences (Wikipedia), and multimodal differences (CIRR) -- achieving\nan average improvement of 4.9\\% in Recall\\@10. The code is available at\nhttps://github.com/Chinese0123456/GRAPE.git",
        "url": "http://arxiv.org/abs/2509.23370v1",
        "published_date": "2025-09-27T15:36:59+00:00",
        "updated_date": "2025-09-27T15:36:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhaohua Zhang",
            "Jianhuan Zhuo",
            "Muxi Chen",
            "Chenchen Zhao",
            "Wenyu Jiang",
            "Tianwen Jiang",
            "Mingyang Chen",
            "Yu Tang",
            "Qiuyong Xiao",
            "Jihong Zhang",
            "Zhixun Su"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces GRAPE, a method to improve retrieval performance by incorporating ranking signals into query rewriting with large language models.",
        "tldr_zh": "本文介绍了GRAPE，一种通过将排名信号纳入大语言模型中进行查询重写以提高检索性能的方法。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Test-time Uncertainty Estimation for Medical Image Registration via Transformation Equivariance",
        "summary": "Accurate image registration is essential for downstream applications, yet\ncurrent deep registration networks provide limited indications of whether and\nwhen their predictions are reliable. Existing uncertainty estimation\nstrategies, such as Bayesian methods, ensembles, or MC dropout, require\narchitectural changes or retraining, limiting their applicability to pretrained\nregistration networks. Instead, we propose a test-time uncertainty estimation\nframework that is compatible with any pretrained networks. Our framework is\ngrounded in the transformation equivariance property of registration, which\nstates that the true mapping between two images should remain consistent under\nspatial perturbations of the input. By analyzing the variance of network\npredictions under such perturbations, we derive a theoretical decomposition of\nperturbation-based uncertainty in registration. This decomposition separates\ninto two terms: (i) an intrinsic spread, reflecting epistemic noise, and (ii) a\nbias jitter, capturing how systematic error drifts under perturbations. Across\nfour anatomical structures (brain, cardiac, abdominal, and lung) and multiple\nregistration models (uniGradICON, SynthMorph), the uncertainty maps correlate\nconsistently with registration errors and highlight regions requiring caution.\nOur framework turns any pretrained registration network into a risk-aware tool\nat test time, placing medical image registration one step closer to safe\ndeployment in clinical and large-scale research settings.",
        "url": "http://arxiv.org/abs/2509.23355v1",
        "published_date": "2025-09-27T15:03:06+00:00",
        "updated_date": "2025-09-27T15:03:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lin Tian",
            "Xiaoling Hu",
            "Juan Eugenio Iglesias"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper proposes a test-time uncertainty estimation framework for medical image registration using pretrained networks, grounded in transformation equivariance.",
        "tldr_zh": "该论文提出了一种利用预训练网络的医学图像配准的测试时间不确定性估计框架，基于变换等变性。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "DiffTex: Differentiable Texturing for Architectural Proxy Models",
        "summary": "Simplified proxy models are commonly used to represent architectural\nstructures, reducing storage requirements and enabling real-time rendering.\nHowever, the geometric simplifications inherent in proxies result in a loss of\nfine color and geometric details, making it essential for textures to\ncompensate for the loss. Preserving the rich texture information from the\noriginal dense architectural reconstructions remains a daunting task,\nparticularly when working with unordered RGB photographs. We propose an\nautomated method for generating realistic texture maps for architectural proxy\nmodels at the texel level from an unordered collection of registered\nphotographs. Our approach establishes correspondences between texels on a UV\nmap and pixels in the input images, with each texel's color computed as a\nweighted blend of associated pixel values. Using differentiable rendering, we\noptimize blending parameters to ensure photometric and perspective consistency,\nwhile maintaining seamless texture coherence. Experimental results demonstrate\nthe effectiveness and robustness of our method across diverse architectural\nmodels and varying photographic conditions, enabling the creation of\nhigh-quality textures that preserve visual fidelity and structural detail.",
        "url": "http://arxiv.org/abs/2509.23336v1",
        "published_date": "2025-09-27T14:39:53+00:00",
        "updated_date": "2025-09-27T14:39:53+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Weidan Xiong",
            "Yongli Wu",
            "Bochuan Zeng",
            "Jianwei Guo",
            "Dani Lischinski",
            "Daniel Cohen-Or",
            "Hui Huang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a method, DiffTex, for generating realistic texture maps for architectural proxy models from unordered RGB photographs using differentiable rendering.",
        "tldr_zh": "本文介绍了一种使用可微分渲染从无序RGB照片中为建筑代理模型生成逼真纹理地图的方法，名为DiffTex。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "DDP: Dual-Decoupled Prompting for Multi-Label Class-Incremental Learning",
        "summary": "Prompt-based methods have shown strong effectiveness in single-label\nclass-incremental learning, but their direct extension to multi-label\nclass-incremental learning (MLCIL) performs poorly due to two intrinsic\nchallenges: semantic confusion from co-occurring categories and\ntrue-negative-false-positive confusion caused by partial labeling. We propose\nDual-Decoupled Prompting (DDP), a replay-free and parameter-efficient framework\nthat explicitly addresses both issues. DDP assigns class-specific\npositive-negative prompts to disentangle semantics and introduces Progressive\nConfidence Decoupling (PCD), a curriculum-inspired decoupling strategy that\nsuppresses false positives. Past prompts are frozen as knowledge anchors, and\ninterlayer prompting enhances efficiency. On MS-COCO and PASCAL VOC, DDP\nconsistently outperforms prior methods and is the first replay-free MLCIL\napproach to exceed 80% mAP and 70% F1 under the standard MS-COCO B40-C10\nbenchmark.",
        "url": "http://arxiv.org/abs/2509.23335v1",
        "published_date": "2025-09-27T14:39:43+00:00",
        "updated_date": "2025-09-27T14:39:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaile Du",
            "Zihan Ye",
            "Junzhou Xie",
            "Fan Lyu",
            "Yixi Shen",
            "Yuyang Li",
            "Miaoxuan Zhu",
            "Fuyuan Hu",
            "Ling Shao",
            "Guangcan Liu"
        ],
        "ai_categories": [
            "AIGC"
        ],
        "tldr": "The paper introduces a framework called DDP to address challenges in multi-label class-incremental learning by using class-specific prompts and a decoupling strategy, outperforming prior methods on benchmark datasets.",
        "tldr_zh": "该论文提出了一个名为DDP的框架，通过使用类特定提示和解耦策略来解决多标签类增量学习中的挑战，从而在基准数据集上优于先前的方法。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Targeted perturbations reveal brain-like local coding axes in robustified, but not standard, ANN-based brain models",
        "summary": "Artificial neural networks (ANNs) have become the de facto standard for\nmodeling the human visual system, primarily due to their success in predicting\nneural responses. However, with many models now achieving similar predictive\naccuracy, we need a stronger criterion. Here, we use small-scale adversarial\nprobes to characterize the local representational geometry of many highly\npredictive ANN-based brain models. We report four key findings. First, we show\nthat most contemporary ANN-based brain models are unexpectedly fragile. Despite\nhigh prediction scores, their response predictions are highly sensitive to\nsmall, imperceptible perturbations, revealing unreliable local coding\ndirections. Second, we demonstrate that a model's sensitivity to adversarial\nprobes can better discriminate between candidate neural encoding models than\nprediction accuracy alone. Third, we find that standard models rely on distinct\nlocal coding directions that do not transfer across model architectures.\nFinally, we show that adversarial probes from robustified models produce\ngeneralizable and semantically meaningful changes, suggesting that they capture\nthe local coding dimensions of the visual system. Together, our work shows that\nlocal representational geometry provides a stronger criterion for brain model\nevaluation. We also provide empirical grounds for favoring robust models, whose\nmore stable coding axes not only align better with neural selectivity but also\ngenerate concrete, testable predictions for future experiments.",
        "url": "http://arxiv.org/abs/2509.23333v1",
        "published_date": "2025-09-27T14:39:36+00:00",
        "updated_date": "2025-09-27T14:39:36+00:00",
        "categories": [
            "q-bio.NC",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Nikolas McNeal",
            "N. Apurva Ratan Murty"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper investigates the local coding axes in artificial neural network-based brain models, highlighting the importance of robust models with stable coding directions.",
        "tldr_zh": "本文研究了基于人工神经网络的脑模型中的局部编码轴，强调了具有稳定编码方向的健壮模型的重要性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Learning Regional Monsoon Patterns with a Multimodal Attention U-Net",
        "summary": "Accurate monsoon rainfall prediction is vital for India's agriculture, water\nmanagement, and climate risk planning, yet remains challenging due to sparse\nground observations and complex regional variability. We present a multimodal\ndeep learning framework for high-resolution precipitation classification that\nleverages satellite and Earth observation data. Unlike previous rainfall\nprediction models based on coarse 5-50 km grids, we curate a new 1 km\nresolution dataset for five Indian states, integrating seven key geospatial\nmodalities: land surface temperature, vegetation (NDVI), soil moisture,\nrelative humidity, wind speed, elevation, and land use, covering the\nJune-September 2024 monsoon season. Our approach uses an attention-guided U-Net\narchitecture to capture spatial patterns and temporal dependencies across\nmodalities, combined with focal and dice loss functions to handle rainfall\nclass imbalance defined by the India Meteorological Department (IMD).\nExperiments demonstrate that our multimodal framework consistently outperforms\nunimodal baselines and existing deep learning methods, especially in extreme\nrainfall categories. This work contributes a scalable framework, benchmark\ndataset, and state-of-the-art results for regional monsoon forecasting, climate\nresilience, and geospatial AI applications in India.",
        "url": "http://arxiv.org/abs/2509.23267v1",
        "published_date": "2025-09-27T11:48:30+00:00",
        "updated_date": "2025-09-27T11:48:30+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Swaib Ilias Mazumder",
            "Manish Kumar",
            "Aparajita Khan"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper presents a deep learning framework for monsoon rainfall prediction in India using satellite and Earth observation data with high resolution. The framework outperforms existing methods especially in extreme rainfall categories.",
        "tldr_zh": "本文提出了一个利用高分辨率卫星和地球观测数据进行印度季风降雨预测的深度学习框架。该框架在特定降雨情况下优于现有方法。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "LiDAR-based Human Activity Recognition through Laplacian Spectral Analysis",
        "summary": "Human Activity Recognition supports applications in healthcare,\nmanufacturing, and human-machine interaction. LiDAR point clouds offer a\nprivacy-preserving alternative to cameras and are robust to illumination. We\npropose a HAR method based on graph spectral analysis. Each LiDAR frame is\nmapped to a proximity graph (epsilon-graph) and the Laplacian spectrum is\ncomputed. Eigenvalues and statistics of eigenvectors form pose descriptors, and\ntemporal statistics over sliding windows yield fixed vectors for classification\nwith support vector machines and random forests. On the MM-Fi dataset with 40\nsubjects and 27 activities, under a strict subject-independent protocol, the\nmethod reaches 94.4% accuracy on a 13-class rehabilitation set and 90.3% on all\n27 activities. It also surpasses the skeleton-based baselines reported for\nMM-Fi. The contribution is a compact and interpretable feature set derived\ndirectly from point cloud geometry that provides an accurate and efficient\nalternative to end-to-end deep learning.",
        "url": "http://arxiv.org/abs/2509.23255v1",
        "published_date": "2025-09-27T11:16:53+00:00",
        "updated_date": "2025-09-27T11:16:53+00:00",
        "categories": [
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Sasan Sharifipour",
            "Constantino Álvarez Casado",
            "Le Nguyen",
            "Tharindu Ekanayake",
            "Manuel Lage Cañellas",
            "Nhi Nguyen",
            "Miguel Bordallo López"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper proposes a method for Human Activity Recognition using LiDAR point clouds and graph spectral analysis, achieving high accuracy and outperforming existing methods.",
        "tldr_zh": "该论文提出了一种利用LiDAR点云和图谱谱分析进行人体活动识别的方法，实现了高准确性，并优于现有方法。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "TATTOO: Training-free AesTheTic-aware Outfit recOmmendation",
        "summary": "The global fashion e-commerce market relies significantly on intelligent and\naesthetic-aware outfit-completion tools to promote sales. While previous\nstudies have approached the problem of fashion outfit-completion and\ncompatible-item retrieval, most of them require expensive, task-specific\ntraining on large-scale labeled data, and no effort is made to guide outfit\nrecommendation with explicit human aesthetics. In the era of Multimodal Large\nLanguage Models (MLLMs), we show that the conventional training-based pipeline\ncould be streamlined to a training-free paradigm, with better recommendation\nscores and enhanced aesthetic awareness. We achieve this with TATTOO, a\nTraining-free AesTheTic-aware Outfit recommendation approach. It first\ngenerates a target-item description using MLLMs, followed by an aesthetic\nchain-of-thought used to distill the images into a structured aesthetic profile\nincluding color, style, occasion, season, material, and balance. By fusing the\nvisual summary of the outfit with the textual description and aesthetics\nvectors using a dynamic entropy-gated mechanism, candidate items can be\nrepresented in a shared embedding space and be ranked accordingly. Experiments\non a real-world evaluation set Aesthetic-100 show that TATTOO achieves\nstate-of-the-art performance compared with existing training-based methods.\nAnother standard Polyvore dataset is also used to measure the advanced\nzero-shot retrieval capability of our training-free method.",
        "url": "http://arxiv.org/abs/2509.23242v1",
        "published_date": "2025-09-27T10:46:55+00:00",
        "updated_date": "2025-09-27T10:46:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuntian Wu",
            "Xiaonan Hu",
            "Ziqi Zhou",
            "Hao Lu"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces TATTOO, a training-free aesthetic-aware outfit recommendation approach using Multimodal Large Language Models. It achieves state-of-the-art performance in outfit recommendation without specific training.",
        "tldr_zh": "本文介绍了TATTOO，一种无需训练的美学感知服装推荐方法，利用多模态大型语言模型。它在服装推荐领域取得了业界领先的表现，无需特定训练。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Unsupervised Online 3D Instance Segmentation with Synthetic Sequences and Dynamic Loss",
        "summary": "Unsupervised online 3D instance segmentation is a fundamental yet challenging\ntask, as it requires maintaining consistent object identities across LiDAR\nscans without relying on annotated training data. Existing methods, such as\nUNIT, have made progress in this direction but remain constrained by limited\ntraining diversity, rigid temporal sampling, and heavy dependence on noisy\npseudo-labels. We propose a new framework that enriches the training\ndistribution through synthetic point cloud sequence generation, enabling\ngreater diversity without relying on manual labels or simulation engines. To\nbetter capture temporal dynamics, our method incorporates a flexible sampling\nstrategy that leverages both adjacent and non-adjacent frames, allowing the\nmodel to learn from long-range dependencies as well as short-term variations.\nIn addition, a dynamic-weighting loss emphasizes confident and informative\nsamples, guiding the network toward more robust representations. Through\nextensive experiments on SemanticKITTI, nuScenes, and PandaSet, our method\nconsistently outperforms UNIT and other unsupervised baselines, achieving\nhigher segmentation accuracy and more robust temporal associations. The code\nwill be publicly available at github.com/Eaphan/SFT3D.",
        "url": "http://arxiv.org/abs/2509.23194v1",
        "published_date": "2025-09-27T08:53:27+00:00",
        "updated_date": "2025-09-27T08:53:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yifan Zhang",
            "Wei Zhang",
            "Chuangxin He",
            "Zhonghua Miao",
            "Junhui Hou"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "A new framework for unsupervised online 3D instance segmentation using synthetic point cloud sequences and dynamic loss function, outperforming existing methods.",
        "tldr_zh": "使用合成点云序列和动态损失函数的新框架进行无监督在线3D实例分割，优于现有方法。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "TRAX: TRacking Axles for Accurate Axle Count Estimation",
        "summary": "Accurate counting of vehicle axles is essential for traffic control, toll\ncollection, and infrastructure development. We present an end-to-end,\nvideo-based pipeline for axle counting that tackles limitations of previous\nworks in dense environments. Our system leverages a combination of YOLO-OBB to\ndetect and categorize vehicles, and YOLO to detect tires. Detected tires are\nintelligently associated to their respective parent vehicles, enabling accurate\naxle prediction even in complex scenarios. However, there are a few challenges\nin detection when it comes to scenarios with longer and occluded vehicles. We\nmitigate vehicular occlusions and partial detections for longer vehicles by\nproposing a novel TRAX (Tire and Axle Tracking) Algorithm to successfully track\naxle-related features between frames. Our method stands out by significantly\nreducing false positives and improving the accuracy of axle-counting for long\nvehicles, demonstrating strong robustness in real-world traffic videos. This\nwork represents a significant step toward scalable, AI-driven axle counting\nsystems, paving the way for machine vision to replace legacy roadside\ninfrastructure.",
        "url": "http://arxiv.org/abs/2509.23171v1",
        "published_date": "2025-09-27T08:04:06+00:00",
        "updated_date": "2025-09-27T08:04:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Avinash Rai",
            "Sandeep Jana",
            "Vishal Vijay"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper presents a video-based pipeline called TRAX for accurate axle counting in dense environments by tracking tires and axles of vehicles, overcoming challenges with long and occluded vehicles.",
        "tldr_zh": "本文提出了一种名为TRAX的基于视频的管道，用于在密集环境中通过跟踪车辆的轮胎和轴来实现精确的轴计数，克服了长车辆和遮挡车辆的挑战。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Towards Comprehensive Interactive Change Understanding in Remote Sensing: A Large-scale Dataset and Dual-granularity Enhanced VLM",
        "summary": "Remote sensing change understanding (RSCU) is essential for analyzing remote\nsensing images and understanding how human activities affect the environment.\nHowever, existing datasets lack deep understanding and interactions in the\ndiverse change captioning, counting, and localization tasks. To tackle these\ngaps, we construct ChangeIMTI, a new large-scale interactive multi-task\ninstruction dataset that encompasses four complementary tasks including change\ncaptioning, binary change classification, change counting, and change\nlocalization. Building upon this new dataset, we further design a novel\nvision-guided vision-language model (ChangeVG) with dual-granularity awareness\nfor bi-temporal remote sensing images (i.e., two remote sensing images of the\nsame area at different times). The introduced vision-guided module is a\ndual-branch architecture that synergistically combines fine-grained spatial\nfeature extraction with high-level semantic summarization. These enriched\nrepresentations further serve as the auxiliary prompts to guide large\nvision-language models (VLMs) (e.g., Qwen2.5-VL-7B) during instruction tuning,\nthereby facilitating the hierarchical cross-modal learning. We extensively\nconduct experiments across four tasks to demonstrate the superiority of our\napproach. Remarkably, on the change captioning task, our method outperforms the\nstrongest method Semantic-CC by 1.39 points on the comprehensive S*m metric,\nwhich integrates the semantic similarity and descriptive accuracy to provide an\noverall evaluation of change caption. Moreover, we also perform a series of\nablation studies to examine the critical components of our method.",
        "url": "http://arxiv.org/abs/2509.23105v1",
        "published_date": "2025-09-27T04:28:42+00:00",
        "updated_date": "2025-09-27T04:28:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junxiao Xue",
            "Quan Deng",
            "Xuecheng Wu",
            "Kelu Yao",
            "Xinyi Yin",
            "Fei Yu",
            "Wei Zhou",
            "Yanfei Zhong",
            "Yang Liu",
            "Dingkang Yang"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a new dataset and model for comprehensive change understanding in remote sensing using a vision-guided vision-language model.",
        "tldr_zh": "该论文介绍了一个新的数据集和模型，利用视觉引导的视觉语言模型来进行遥感中的全面变化理解。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Streamline pathology foundation model by cross-magnification distillation",
        "summary": "Foundation models (FM) have transformed computational pathology but remain\ncomputationally prohibitive for clinical deployment due to their massive\nparameter counts and high-magnification processing requirements. Here, we\nintroduce XMAG, a lightweight FM developed through corss-magnification\ndistillation that transfers knowledge from state-of-the-art 20x magnification\nteacher to an efficient 5x magnification student architecture. XMAG employs a\ncompact backbone and operates entirely at 5x, requiring 11.3 times fewer\npatches per whole slide image (WSI) compared to existing approaches. Our Novel\ndistillation framework incorporates dual-level knowledge transfer, aligning\nboth global image representations and local spatial token mapping. We trained\nXMAG on 3.49 million images curated from publicly available datasets and\nevaluated performance across six clinically relevant histopathology analysis\ntasks spanning multiple cancer types. XMAG achieved diagnostic accuracy within\n1% of substantially larger foundation models while delivering 30-fold\nprocessing acceleration, reaching 8.8 WSIs per minute processing speed. Our\ncross-institutional validation confirmed robust generalization. Further, we\ndeveloped an end-to-end training strategy to further boost our model's\nperformance to approach the larger FMs' performance. These results establish\ncross-magnification distillation as a viable approach for deploying FM\ncapabilities in resource-constrained clinical environments, potentially\nenabling real-time pathology AI integration.",
        "url": "http://arxiv.org/abs/2509.23097v1",
        "published_date": "2025-09-27T04:11:53+00:00",
        "updated_date": "2025-09-27T04:11:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziyu Su",
            "Abdul Rehman Akbar",
            "Usama Sajjad",
            "Anil V. Parwani",
            "Muhammad Khalid Khan Niazi"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces XMAG, a lightweight pathology model trained through cross-magnification distillation, achieving high diagnostic accuracy and processing acceleration for real-time pathology AI integration.",
        "tldr_zh": "本文介绍了XMAG，一种轻量级病理模型，通过跨放大蒸馏训练的，实现了高诊断准确性和处理加速，可用于实时病理AI集成。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7.0
    },
    {
        "title": "UniPrototype: Humn-Robot Skill Learning with Uniform Prototypes",
        "summary": "Data scarcity remains a fundamental challenge in robot learning. While human\ndemonstrations benefit from abundant motion capture data and vast internet\nresources, robotic manipulation suffers from limited training examples. To\nbridge this gap between human and robot manipulation capabilities, we propose\nUniPrototype, a novel framework that enables effective knowledge transfer from\nhuman to robot domains via shared motion primitives. ur approach makes three\nkey contributions: (1) We introduce a compositional prototype discovery\nmechanism with soft assignments, enabling multiple primitives to co-activate\nand thus capture blended and hierarchical skills; (2) We propose an adaptive\nprototype selection strategy that automatically adjusts the number of\nprototypes to match task complexity, ensuring scalable and efficient\nrepresentation; (3) We demonstrate the effectiveness of our method through\nextensive experiments in both simulation environments and real-world robotic\nsystems. Our results show that UniPrototype successfully transfers human\nmanipulation knowledge to robots, significantly improving learning efficiency\nand task performance compared to existing approaches.The code and dataset will\nbe released upon acceptance at an anonymous repository.",
        "url": "http://arxiv.org/abs/2509.23021v1",
        "published_date": "2025-09-27T00:33:39+00:00",
        "updated_date": "2025-09-27T00:33:39+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Xiao Hu",
            "Qi Yin",
            "Yangming Shi",
            "Yang Ye"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "UniPrototype is a framework for transferring human manipulation knowledge to robots through shared motion primitives, improving learning efficiency and task performance.",
        "tldr_zh": "UniPrototype是一个通过共享运动原语将人类操纵知识转移到机器人的框架，提高了学习效率和任务性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Hemorica: A Comprehensive CT Scan Dataset for Automated Brain Hemorrhage Classification, Segmentation, and Detection",
        "summary": "Timely diagnosis of Intracranial hemorrhage (ICH) on Computed Tomography (CT)\nscans remains a clinical priority, yet the development of robust Artificial\nIntelligence (AI) solutions is still hindered by fragmented public data. To\nclose this gap, we introduce Hemorica, a publicly available collection of 372\nhead CT examinations acquired between 2012 and 2024. Each scan has been\nexhaustively annotated for five ICH subtypes-epidural (EPH), subdural (SDH),\nsubarachnoid (SAH), intraparenchymal (IPH), and intraventricular (IVH)-yielding\npatient-wise and slice-wise classification labels, subtype-specific bounding\nboxes, two-dimensional pixel masks and three-dimensional voxel masks. A\ndouble-reading workflow, preceded by a pilot consensus phase and supported by\nneurosurgeon adjudication, maintained low inter-rater variability.\nComprehensive statistical analysis confirms the clinical realism of the\ndataset. To establish reference baselines, standard convolutional and\ntransformer architectures were fine-tuned for binary slice classification and\nhemorrhage segmentation. With only minimal fine-tuning, lightweight models such\nas MobileViT-XS achieved an F1 score of 87.8% in binary classification, whereas\na U-Net with a DenseNet161 encoder reached a Dice score of 85.5% for binary\nlesion segmentation that validate both the quality of the annotations and the\nsufficiency of the sample size. Hemorica therefore offers a unified,\nfine-grained benchmark that supports multi-task and curriculum learning,\nfacilitates transfer to larger but weakly labelled cohorts, and facilitates the\nprocess of designing an AI-based assistant for ICH detection and quantification\nsystems.",
        "url": "http://arxiv.org/abs/2509.22993v1",
        "published_date": "2025-09-26T23:09:41+00:00",
        "updated_date": "2025-09-26T23:09:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kasra Davoodi",
            "Mohammad Hoseyni",
            "Javad Khoramdel",
            "Reza Barati",
            "Reihaneh Mortazavi",
            "Amirhossein Nikoofard",
            "Mahdi Aliyari-Shoorehdeli",
            "Jaber Hatam Parikhan"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces Hemorica, a CT scan dataset for brain hemorrhage classification, segmentation, and detection. It includes annotations for different types of hemorrhages and achieved high performance with AI models.",
        "tldr_zh": "该论文介绍了Hemorica，一个用于大脑出血分类、分割和检测的 CT 扫描数据集。它包括不同类型出血的标注，并且通过 AI 模型取得了较高性能。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.0
    },
    {
        "title": "TY-RIST: Tactical YOLO Tricks for Real-time Infrared Small Target Detection",
        "summary": "Infrared small target detection (IRSTD) is critical for defense and\nsurveillance but remains challenging due to (1) target loss from minimal\nfeatures, (2) false alarms in cluttered environments, (3) missed detections\nfrom low saliency, and (4) high computational costs. To address these issues,\nwe propose TY-RIST, an optimized YOLOv12n architecture that integrates (1) a\nstride-aware backbone with fine-grained receptive fields, (2) a high-resolution\ndetection head, (3) cascaded coordinate attention blocks, and (4) a branch\npruning strategy that reduces computational cost by about 25.5% while\nmarginally improving accuracy and enabling real-time inference. We also\nincorporate the Normalized Gaussian Wasserstein Distance (NWD) to enhance\nregression stability. Extensive experiments on four benchmarks and across 20\ndifferent models demonstrate state-of-the-art performance, improving mAP at 0.5\nIoU by +7.9%, Precision by +3%, and Recall by +10.2%, while achieving up to 123\nFPS on a single GPU. Cross-dataset validation on a fifth dataset further\nconfirms strong generalization capability. Additional results and resources are\navailable at https://www.github.com/moured/TY-RIST",
        "url": "http://arxiv.org/abs/2509.22909v1",
        "published_date": "2025-09-26T20:36:57+00:00",
        "updated_date": "2025-09-26T20:36:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Abdulkarim Atrash",
            "Omar Moured",
            "Yufan Chen",
            "Jiaming Zhang",
            "Seyda Ertekin",
            "Omur Ugur"
        ],
        "ai_categories": [
            "Dataset",
            "GAN"
        ],
        "tldr": "The paper introduces TY-RIST, an optimized YOLOv12n architecture, for real-time infrared small target detection with improved performance and reduced computational costs.",
        "tldr_zh": "本文介绍了TY-RIST，一种优化的YOLOv12n架构，用于实时红外小目标检测，性能提高且降低计算成本。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Spatial-Spectral Binarized Neural Network for Panchromatic and Multi-spectral Images Fusion",
        "summary": "Remote sensing pansharpening aims to reconstruct spatial-spectral properties\nduring the fusion of panchromatic (PAN) images and low-resolution\nmulti-spectral (LR-MS) images, finally generating the high-resolution\nmulti-spectral (HR-MS) images. Although deep learning-based models have\nachieved excellent performance, they often come with high computational\ncomplexity, which hinder their applications on resource-limited devices. In\nthis paper, we explore the feasibility of applying the binary neural network\n(BNN) to pan-sharpening. Nevertheless, there are two main issues with\nbinarizing pan-sharpening models: (i) the binarization will cause serious\nspectral distortion due to the inconsistent spectral distribution of the\nPAN/LR-MS images; (ii) the common binary convolution kernel is difficult to\nadapt to the multi-scale and anisotropic spatial features of remote sensing\nobjects, resulting in serious degradation of contours. To address the above\nissues, we design the customized spatial-spectral binarized convolution\n(S2B-Conv), which is composed of the Spectral-Redistribution Mechanism (SRM)\nand Gabor Spatial Feature Amplifier (GSFA). Specifically, SRM employs an affine\ntransformation, generating its scaling and bias parameters through a dynamic\nlearning process. GSFA, which randomly selects different frequencies and angles\nwithin a preset range, enables to better handle multi-scale and-directional\nspatial features. A series of S2B-Conv form a brand-new binary network for\npan-sharpening, dubbed as S2BNet. Extensive quantitative and qualitative\nexperiments have shown our high-efficiency binarized pan-sharpening method can\nattain a promising performance.",
        "url": "http://arxiv.org/abs/2509.23321v1",
        "published_date": "2025-09-27T14:10:51+00:00",
        "updated_date": "2025-09-27T14:10:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yizhen Jiang",
            "Mengting Ma",
            "Anqi Zhu",
            "Xiaowen Ma",
            "Jiaxin Li",
            "Wei Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Other"
        ],
        "tldr": "The paper introduces a Spatial-Spectral Binarized Neural Network for enhancing image quality by fusing panchromatic and multi-spectral images with reduced computational complexity.",
        "tldr_zh": "该论文介绍了一种空间光谱二值神经网络，通过融合全色和多光谱图像来提高图像质量，同时降低计算复杂度。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Learning Unified Representation of 3D Gaussian Splatting",
        "summary": "A well-designed vectorized representation is crucial for the learning systems\nnatively based on 3D Gaussian Splatting. While 3DGS enables efficient and\nexplicit 3D reconstruction, its parameter-based representation remains hard to\nlearn as features, especially for neural-network-based models. Directly feeding\nraw Gaussian parameters into learning frameworks fails to address the\nnon-unique and heterogeneous nature of the Gaussian parameterization, yielding\nhighly data-dependent models. This challenge motivates us to explore a more\nprincipled approach to represent 3D Gaussian Splatting in neural networks that\npreserves the underlying color and geometric structure while enforcing unique\nmapping and channel homogeneity. In this paper, we propose an embedding\nrepresentation of 3DGS based on continuous submanifold fields that encapsulate\nthe intrinsic information of Gaussian primitives, thereby benefiting the\nlearning of 3DGS.",
        "url": "http://arxiv.org/abs/2509.22917v1",
        "published_date": "2025-09-26T20:44:59+00:00",
        "updated_date": "2025-09-26T20:44:59+00:00",
        "categories": [
            "cs.CV",
            "I.4.10"
        ],
        "authors": [
            "Yuelin Xin",
            "Yuheng Liu",
            "Xiaohui Xie",
            "Xinke Li"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper proposes an embedding representation of 3D Gaussian Splatting based on continuous submanifold fields to improve learning efficiency.",
        "tldr_zh": "本文提出了一种基于连续子流形场的三维高斯分片嵌入表示方法，以提升学习效率。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Robust Fine-Tuning from Non-Robust Pretrained Models: Mitigating Suboptimal Transfer With Adversarial Scheduling",
        "summary": "Fine-tuning pretrained models is a standard and effective workflow in modern\nmachine learning. However, robust fine-tuning (RFT), which aims to\nsimultaneously achieve adaptation to a downstream task and robustness to\nadversarial examples, remains challenging. Despite the abundance of non-robust\npretrained models in open-source repositories, their potential for RFT is less\nunderstood. We address this knowledge gap by systematically examining RFT from\nsuch non-robust models. Our experiments reveal that fine-tuning non-robust\nmodels with a robust objective, even under small perturbations, can lead to\npoor performance, a phenomenon that we dub \\emph{suboptimal transfer}. In\nchallenging scenarios (eg, difficult tasks, high perturbation), the resulting\nperformance can be so low that it may be considered a transfer failure. We find\nthat fine-tuning using a robust objective impedes task adaptation at the\nbeginning of training and eventually prevents optimal transfer. However, we\npropose a novel heuristic, \\emph{Epsilon-Scheduling}, a schedule over\nperturbation strength used during training that promotes optimal transfer.\nAdditionally, we introduce \\emph{expected robustness}, a metric that captures\nperformance across a range of perturbations, providing a more comprehensive\nevaluation of the accuracy-robustness trade-off for diverse models at test\ntime. Extensive experiments on a wide range of configurations (six pretrained\nmodels and five datasets) show that \\emph{Epsilon-Scheduling} successfully\nprevents \\emph{suboptimal transfer} and consistently improves expected\nrobustness.",
        "url": "http://arxiv.org/abs/2509.23325v1",
        "published_date": "2025-09-27T14:20:57+00:00",
        "updated_date": "2025-09-27T14:20:57+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Jonas Ngnawé",
            "Maxime Heuillet",
            "Sabyasachi Sahoo",
            "Yann Pequignot",
            "Ola Ahmad",
            "Audrey Durand",
            "Frédéric Precioso",
            "Christian Gagné"
        ],
        "ai_categories": [
            "GAN",
            "Other"
        ],
        "tldr": "The paper explores the challenge of fine-tuning non-robust pretrained models for robustness to adversarial attacks, introducing a novel method called Epsilon-Scheduling to improve transfer performance.",
        "tldr_zh": "本文探讨了将非鲁棒性预训练模型进行微调以提高对抗攻击鲁棒性的挑战，并引入了一种称为Epsilon-Scheduling的新方法来改善转移性能。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.25
    },
    {
        "title": "Desensitizing for Improving Corruption Robustness in Point Cloud Classification through Adversarial Training",
        "summary": "Due to scene complexity, sensor inaccuracies, and processing imprecision,\npoint cloud corruption is inevitable. Over-reliance on input features is the\nroot cause of DNN vulnerabilities. It remains unclear whether this issue exists\nin 3D tasks involving point clouds and whether reducing dependence on these\nfeatures can enhance the model's robustness to corrupted point clouds. This\nstudy attempts to answer these questions. Specifically, we quantified the\nsensitivity of the DNN to point cloud features using Shapley values and found\nthat models trained using traditional methods exhibited high sensitivity values\nfor certain features. Furthermore, under an equal pruning ratio, prioritizing\nthe pruning of highly sensitive features causes more severe damage to model\nperformance than random pruning. We propose `Desensitized Adversarial Training'\n(DesenAT), generating adversarial samples using feature desensitization and\nconducting training within a self-distillation framework, which aims to\nalleviate DNN's over-reliance on point clouds features by smoothing\nsensitivity. First, data points with high contribution components are\neliminated, and spatial transformation is used to simulate corruption scenes,\ngenerate adversarial samples, and conduct adversarial training on the model.\nNext, to compensate for information loss in adversarial samples, we use the\nself-distillation method to transfer knowledge from clean samples to\nadversarial samples, and perform adversarial training in a distillation\nmanner.Extensive experiments on ModelNet-C and PointCloud-C demonstrate show\nthat the propose method can effectively improve the robustness of the model\nwithout reducing the performance of clean data sets. This code is publicly\navailable at\n\\href{https://github.com/JerkyT/DesenAT/tree/master}{https://github.com/JerkyT/DesenAT}.",
        "url": "http://arxiv.org/abs/2509.23010v1",
        "published_date": "2025-09-27T00:04:00+00:00",
        "updated_date": "2025-09-27T00:04:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhiqiang Tian",
            "Weigang Li",
            "Chunhua Deng",
            "Junwei Hu",
            "Yongqiang Wang",
            "Wenping Liu"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes a method called Desensitized Adversarial Training (DesenAT) to improve the corruption robustness in point cloud classification by reducing the model's reliance on specific features through feature desensitization and adversarial training.",
        "tldr_zh": "本文提出了一种名为DesenAT的方法，通过特征去敏感化和对抗训练来减少模型对特定特征的依赖，从而提高点云分类中的冲突鲁棒性。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.0
    }
]