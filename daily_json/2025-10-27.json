[
    {
        "title": "Cross-view Localization and Synthesis - Datasets, Challenges and Opportunities",
        "summary": "Cross-view localization and synthesis are two fundamental tasks in cross-view\nvisual understanding, which deals with cross-view datasets: overhead (satellite\nor aerial) and ground-level imagery. These tasks have gained increasing\nattention due to their broad applications in autonomous navigation, urban\nplanning, and augmented reality. Cross-view localization aims to estimate the\ngeographic position of ground-level images based on information provided by\noverhead imagery while cross-view synthesis seeks to generate ground-level\nimages based on information from the overhead imagery. Both tasks remain\nchallenging due to significant differences in viewing perspective, resolution,\nand occlusion, which are widely embedded in cross-view datasets. Recent years\nhave witnessed rapid progress driven by the availability of large-scale\ndatasets and novel approaches. Typically, cross-view localization is formulated\nas an image retrieval problem where ground-level features are matched with\ntiled overhead images feature, extracted by convolutional neural networks\n(CNNs) or vision transformers (ViTs) for cross-view feature embedding.\nCross-view synthesis, on the other hand, seeks to generate ground-level views\nbased on information from overhead imagery, generally using generative\nadversarial networks (GANs) or diffusion models. This paper presents a\ncomprehensive survey of advances in cross-view localization and synthesis,\nreviewing widely used datasets, highlighting key challenges, and providing an\norganized overview of state-of-the-art techniques. Furthermore, it discusses\ncurrent limitations, offers comparative analyses, and outlines promising\ndirections for future research. We also include the project page via\nhttps://github.com/GDAOSU/Awesome-Cross-View-Methods.",
        "url": "http://arxiv.org/abs/2510.22736v1",
        "published_date": "2025-10-26T16:09:53+00:00",
        "updated_date": "2025-10-26T16:09:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ningli Xu",
            "Rongjun Qin"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper provides a comprehensive survey of advances in cross-view localization and synthesis, reviewing datasets, challenges, and techniques for autonomous navigation and urban planning.",
        "tldr_zh": "本文对交叉视图定位和合成进行了全面调查，审查了用于自主导航和城市规划的数据集、挑战和技术。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "Top-Down Semantic Refinement for Image Captioning",
        "summary": "Large Vision-Language Models (VLMs) face an inherent contradiction in image\ncaptioning: their powerful single-step generation capabilities often lead to a\nmyopic decision-making process. This makes it difficult to maintain global\nnarrative coherence while capturing rich details, a limitation that is\nparticularly pronounced in tasks that require multi-step and complex scene\ndescription. To overcome this fundamental challenge, we redefine image\ncaptioning as a goal-oriented hierarchical refinement planning problem, and\nfurther propose a novel framework, named Top-Down Semantic Refinement (TDSR),\nwhich models the generation process as a Markov Decision Process (MDP).\nHowever, planning within the vast state space of a VLM presents a significant\ncomputational hurdle. Our core contribution, therefore, is the design of a\nhighly efficient Monte Carlo Tree Search (MCTS) algorithm tailored for VLMs. By\nincorporating a visual-guided parallel expansion and a lightweight value\nnetwork, our TDSR reduces the call frequency to the expensive VLM by an order\nof magnitude without sacrificing planning quality. Furthermore, an adaptive\nearly stopping mechanism dynamically matches computational overhead to the\nimage's complexity. Extensive experiments on multiple benchmarks, including\nDetailCaps, COMPOSITIONCAP, and POPE, demonstrate that our TDSR, as a\nplug-and-play module, can significantly enhance the performance of existing\nVLMs (e.g., LLaVA-1.5, Qwen2.5-VL) by achieving state-of-the-art or highly\ncompetitive results in fine-grained description, compositional generalization,\nand hallucination suppression.",
        "url": "http://arxiv.org/abs/2510.22391v1",
        "published_date": "2025-10-25T18:27:00+00:00",
        "updated_date": "2025-10-25T18:27:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jusheng Zhang",
            "Kaitong Cai",
            "Jing Yang",
            "Jian Wang",
            "Chengpei Tang",
            "Keze Wang"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper introduces a framework called Top-Down Semantic Refinement for image captioning, addressing challenges in maintaining narrative coherence and capturing rich details in vision-language models.",
        "tldr_zh": "本文介绍了一种名为顶 - 下语义细化的框架，用于图像字幕，解决了视觉语言模型中维护叙事连贯性和捕捉丰富细节的挑战。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "DeepfakeBench-MM: A Comprehensive Benchmark for Multimodal Deepfake Detection",
        "summary": "The misuse of advanced generative AI models has resulted in the widespread\nproliferation of falsified data, particularly forged human-centric audiovisual\ncontent, which poses substantial societal risks (e.g., financial fraud and\nsocial instability). In response to this growing threat, several works have\npreliminarily explored countermeasures. However, the lack of sufficient and\ndiverse training data, along with the absence of a standardized benchmark,\nhinder deeper exploration. To address this challenge, we first build Mega-MMDF,\na large-scale, diverse, and high-quality dataset for multimodal deepfake\ndetection. Specifically, we employ 21 forgery pipelines through the combination\nof 10 audio forgery methods, 12 visual forgery methods, and 6 audio-driven face\nreenactment methods. Mega-MMDF currently contains 0.1 million real samples and\n1.1 million forged samples, making it one of the largest and most diverse\nmultimodal deepfake datasets, with plans for continuous expansion. Building on\nit, we present DeepfakeBench-MM, the first unified benchmark for multimodal\ndeepfake detection. It establishes standardized protocols across the entire\ndetection pipeline and serves as a versatile platform for evaluating existing\nmethods as well as exploring novel approaches. DeepfakeBench-MM currently\nsupports 5 datasets and 11 multimodal deepfake detectors. Furthermore, our\ncomprehensive evaluations and in-depth analyses uncover several key findings\nfrom multiple perspectives (e.g., augmentation, stacked forgery). We believe\nthat DeepfakeBench-MM, together with our large-scale Mega-MMDF, will serve as\nfoundational infrastructures for advancing multimodal deepfake detection.",
        "url": "http://arxiv.org/abs/2510.22622v1",
        "published_date": "2025-10-26T10:40:52+00:00",
        "updated_date": "2025-10-26T10:40:52+00:00",
        "categories": [
            "cs.CR",
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Kangran Zhao",
            "Yupeng Chen",
            "Xiaoyu Zhang",
            "Yize Chen",
            "Weinan Guan",
            "Baicheng Chen",
            "Chengzhe Sun",
            "Soumyya Kanti Datta",
            "Qingshan Liu",
            "Siwei Lyu",
            "Baoyuan Wu"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a new benchmark, DeepfakeBench-MM, and a large dataset, Mega-MMDF, for multimodal deepfake detection, aiming to standardize testing protocols and improve detection methods.",
        "tldr_zh": "该论文介绍了一个新的基准DeepfakeBench-MM和一个大型数据集Mega-MMDF，用于多模态深度伪造检测，旨在规范测试协议并提高检测方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Edge Collaborative Gaussian Splatting with Integrated Rendering and Communication",
        "summary": "Gaussian splatting (GS) struggles with degraded rendering quality on low-cost\ndevices. To address this issue, we present edge collaborative GS (ECO-GS),\nwhere each user can switch between a local small GS model to guarantee\ntimeliness and a remote large GS model to guarantee fidelity. However, deciding\nhow to engage the large GS model is nontrivial, due to the interdependency\nbetween rendering requirements and resource conditions. To this end, we propose\nintegrated rendering and communication (IRAC), which jointly optimizes\ncollaboration status (i.e., deciding whether to engage large GS) and edge power\nallocation (i.e., enabling remote rendering) under communication constraints\nacross different users by minimizing a newly-derived GS switching function.\nDespite the nonconvexity of the problem, we propose an efficient penalty\nmajorization minimization (PMM) algorithm to obtain the critical point\nsolution. Furthermore, we develop an imitation learning optimization (ILO)\nalgorithm, which reduces the computational time by over 100x compared to PMM.\nExperiments demonstrate the superiority of PMM and the real-time execution\ncapability of ILO.",
        "url": "http://arxiv.org/abs/2510.22718v1",
        "published_date": "2025-10-26T15:33:29+00:00",
        "updated_date": "2025-10-26T15:33:29+00:00",
        "categories": [
            "cs.IT",
            "cs.CV",
            "math.IT"
        ],
        "authors": [
            "Yujie Wan",
            "Chenxuan Liu",
            "Shuai Wang",
            "Tong Zhang",
            "James Jianqiao Yu",
            "Kejiang Ye",
            "Dusit Niyato",
            "Chengzhong Xu"
        ],
        "ai_categories": [
            "Multimodality",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces edge collaborative Gaussian splatting to improve rendering quality on low-cost devices by optimizing collaboration status and power allocation. Two algorithms, PMM and ILO, are proposed with PMM showing superiority and ILO reducing computational time significantly.",
        "tldr_zh": "本文引入边缘协同高斯飞溅以改善在成本低廉设备上的渲染质量，通过优化协作状态和功率分配。提出两种算法，PMM表现卓越，ILO大幅减少计算时间。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction",
        "summary": "Humans naturally perceive the geometric structure and semantic content of a\n3D world as intertwined dimensions, enabling coherent and accurate\nunderstanding of complex scenes. However, most prior approaches prioritize\ntraining large geometry models for low-level 3D reconstruction and treat\nhigh-level spatial understanding in isolation, overlooking the crucial\ninterplay between these two fundamental aspects of 3D-scene analysis, thereby\nlimiting generalization and leading to poor performance in downstream 3D\nunderstanding tasks. Recent attempts have mitigated this issue by simply\naligning 3D models with specific language models, thus restricting perception\nto the aligned model's capacity and limiting adaptability to downstream tasks.\nIn this paper, we propose InstanceGrounded Geometry Transformer (IGGT), an\nend-to-end large unified transformer to unify the knowledge for both spatial\nreconstruction and instance-level contextual understanding. Specifically, we\ndesign a 3D-Consistent Contrastive Learning strategy that guides IGGT to encode\na unified representation with geometric structures and instance-grounded\nclustering through only 2D visual inputs. This representation supports\nconsistent lifting of 2D visual inputs into a coherent 3D scene with explicitly\ndistinct object instances. To facilitate this task, we further construct\nInsScene-15K, a large-scale dataset with high-quality RGB images, poses, depth\nmaps, and 3D-consistent instance-level mask annotations with a novel data\ncuration pipeline.",
        "url": "http://arxiv.org/abs/2510.22706v1",
        "published_date": "2025-10-26T14:57:44+00:00",
        "updated_date": "2025-10-26T14:57:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hao Li",
            "Zhengyu Zou",
            "Fangfu Liu",
            "Xuanyang Zhang",
            "Fangzhou Hong",
            "Yukang Cao",
            "Yushi Lan",
            "Manyuan Zhang",
            "Gang Yu",
            "Dingwen Zhang",
            "Ziwei Liu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a new model, IGGT, that combines geometric reconstruction and semantic understanding for 3D scenes using a large unified transformer.",
        "tldr_zh": "本文提出了一种新模型IGGT，它利用大型统一变换器结合几何重建和语义理解来处理三维场景。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Atlas Urban Index: A VLM-Based Approach for Spatially and Temporally Calibrated Urban Development Monitoring",
        "summary": "We introduce the {\\em Atlas Urban Index} (AUI), a metric for measuring urban\ndevelopment computed using Sentinel-2 \\citep{spoto2012sentinel2} satellite\nimagery. Existing approaches, such as the {\\em Normalized Difference Built-up\nIndex} (NDBI), often struggle to accurately capture urban development due to\nfactors like atmospheric noise, seasonal variation, and cloud cover. These\nlimitations hinder large-scale monitoring of human development and\nurbanization. To address these challenges, we propose an approach that\nleverages {\\em Vision-Language Models }(VLMs) to provide a development score\nfor regions. Specifically, we collect a time series of Sentinel-2 images for\neach region. Then, we further process the images within fixed time windows to\nget an image with minimal cloud cover, which serves as the representative image\nfor that time window. To ensure consistent scoring, we adopt two strategies:\n(i) providing the VLM with a curated set of reference images representing\ndifferent levels of urbanization, and (ii) supplying the most recent past image\nto both anchor temporal consistency and mitigate cloud-related noise in the\ncurrent image. Together, these components enable AUI to overcome the challenges\nof traditional urbanization indices and produce more reliable and stable\ndevelopment scores. Our qualitative experiments on Bangalore suggest that AUI\noutperforms standard indices such as NDBI.",
        "url": "http://arxiv.org/abs/2510.22702v1",
        "published_date": "2025-10-26T14:53:36+00:00",
        "updated_date": "2025-10-26T14:53:36+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.ET",
            "eess.IV"
        ],
        "authors": [
            "Mithul Chander",
            "Sai Pragnya Ranga",
            "Prathamesh Mayekar"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces the Atlas Urban Index (AUI) using Vision-Language Models to monitor urban development with Sentinel-2 satellite imagery, outperforming traditional indices like NDBI.",
        "tldr_zh": "该论文介绍了利用视觉语言模型和Sentinel-2卫星图像监测城市发展的Atlas Urban Index（AUI），表现优于传统的NDBI指数。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "WaveMAE: Wavelet decomposition Masked Auto-Encoder for Remote Sensing",
        "summary": "Self-supervised learning (SSL) has recently emerged as a key strategy for\nbuilding foundation models in remote sensing, where the scarcity of annotated\ndata limits the applicability of fully supervised approaches. In this work, we\nintroduce WaveMAE, a masked autoencoding framework tailored for multispectral\nsatellite imagery. Unlike conventional pixel-based reconstruction, WaveMAE\nleverages a multi-level Discrete Wavelet Transform (DWT) to disentangle\nfrequency components and guide the encoder toward learning scale-aware\nhigh-frequency representations. We further propose a Geo-conditioned Positional\nEncoding (GPE), which incorporates geographical priors via Spherical Harmonics,\nencouraging embeddings that respect both semantic and geospatial structure. To\nensure fairness in evaluation, all methods are pretrained on the same dataset\n(fMoW-S2) and systematically evaluated on the diverse downstream tasks of the\nPANGAEA benchmark, spanning semantic segmentation, regression, change\ndetection, and multilabel classification. Extensive experiments demonstrate\nthat WaveMAE achieves consistent improvements over prior state-of-the-art\napproaches, with substantial gains on segmentation and regression benchmarks.\nThe effectiveness of WaveMAE pretraining is further demonstrated by showing\nthat even a lightweight variant, containing only 26.4% of the parameters,\nachieves state-of-the-art performance. Our results establish WaveMAE as a\nstrong and geographically informed foundation model for multispectral remote\nsensing imagery.",
        "url": "http://arxiv.org/abs/2510.22697v1",
        "published_date": "2025-10-26T14:45:30+00:00",
        "updated_date": "2025-10-26T14:45:30+00:00",
        "categories": [
            "cs.CV",
            "68T07",
            "I.2.6; I.4.10; J.2"
        ],
        "authors": [
            "Vittorio Bernuzzi",
            "Leonardo Rossi",
            "Tomaso Fontanini",
            "Massimo Bertozzi",
            "Andrea Prati"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "WaveMAE is a masked autoencoder framework designed for multispectral satellite imagery in remote sensing, which outperforms prior state-of-the-art methods in various downstream tasks.",
        "tldr_zh": "WaveMAE是一种为遥感领域的多光谱卫星图像设计的掩码自编码器框架，该框架在各种下游任务中优于先前的最先进方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Windsock is Dancing: Adaptive Multimodal Retrieval-Augmented Generation",
        "summary": "Multimodal Retrieval-Augmented Generation (MRAG) has emerged as a promising\nmethod to generate factual and up-to-date responses of Multimodal Large\nLanguage Models (MLLMs) by incorporating non-parametric knowledge from external\nknowledge bases. However, existing MRAG approaches suffer from static retrieval\nstrategies, inflexible modality selection, and suboptimal utilization of\nretrieved information, leading to three critical challenges: determining when\nto retrieve, what modality to incorporate, and how to utilize retrieved\ninformation effectively. To address these challenges, we introduce Windsock, a\nquery-dependent module making decisions on retrieval necessity and modality\nselection, effectively reducing computational overhead and improving response\nquality. Additionally, we propose Dynamic Noise-Resistance (DANCE) Instruction\nTuning, an adaptive training strategy that enhances MLLMs' ability to utilize\nretrieved information while maintaining robustness against noise. Moreover, we\nadopt a self-assessment approach leveraging knowledge within MLLMs to convert\nquestion-answering datasets to MRAG training datasets. Extensive experiments\ndemonstrate that our proposed method significantly improves the generation\nquality by 17.07% while reducing 8.95% retrieval times.",
        "url": "http://arxiv.org/abs/2510.22694v1",
        "published_date": "2025-10-26T14:36:16+00:00",
        "updated_date": "2025-10-26T14:36:16+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.IR"
        ],
        "authors": [
            "Shu Zhao",
            "Tianyi Shen",
            "Nilesh Ahuja",
            "Omesh Tickoo",
            "Vijaykrishnan Narayanan"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer"
        ],
        "tldr": "The paper introduces Windsock and DANCE Instruction Tuning to improve Multimodal Retrieval-Augmented Generation by addressing challenges in retrieval necessity, modality selection, and information utilization.",
        "tldr_zh": "本文介绍了Windsock和DANCE指导调节，通过解决检索必要性、模态选择和信息利用方面的挑战来改进多模式检索增强生成。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RoboSVG: A Unified Framework for Interactive SVG Generation with Multi-modal Guidance",
        "summary": "Scalable Vector Graphics (SVGs) are fundamental to digital design and robot\ncontrol, encoding not only visual structure but also motion paths in\ninteractive drawings. In this work, we introduce RoboSVG, a unified multimodal\nframework for generating interactive SVGs guided by textual, visual, and\nnumerical signals. Given an input query, the RoboSVG model first produces\nmultimodal guidance, then synthesizes candidate SVGs through dedicated\ngeneration modules, and finally refines them under numerical guidance to yield\nhigh-quality outputs. To support this framework, we construct RoboDraw, a\nlarge-scale dataset of one million examples, each pairing an SVG generation\ncondition (e.g., text, image, and partial SVG) with its corresponding\nground-truth SVG code. RoboDraw dataset enables systematic study of four tasks,\nincluding basic generation (Text-to-SVG, Image-to-SVG) and interactive\ngeneration (PartialSVG-to-SVG, PartialImage-to-SVG). Extensive experiments\ndemonstrate that RoboSVG achieves superior query compliance and visual fidelity\nacross tasks, establishing a new state of the art in versatile SVG generation.\nThe dataset and source code of this project will be publicly available soon.",
        "url": "http://arxiv.org/abs/2510.22684v1",
        "published_date": "2025-10-26T13:57:08+00:00",
        "updated_date": "2025-10-26T13:57:08+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Jiuniu Wang",
            "Gongjie Zhang",
            "Quanhao Qian",
            "Junlong Gao",
            "Deli Zhao",
            "Ran Xu"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "RoboSVG is a framework for generating interactive SVGs using textual, visual, and numerical guidance, with a focus on achieving high-quality outputs. The paper introduces a large-scale dataset and demonstrates superior performance in SVG generation tasks.",
        "tldr_zh": "RoboSVG是一个利用文本、视觉和数字指导生成交互式SVG的框架，致力于实现高质量输出。该论文介绍了一个大规模数据集，并在SVG生成任务中表现出卓越性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Alias-Free ViT: Fractional Shift Invariance via Linear Attention",
        "summary": "Transformers have emerged as a competitive alternative to convnets in vision\ntasks, yet they lack the architectural inductive bias of convnets, which may\nhinder their potential performance. Specifically, Vision Transformers (ViTs)\nare not translation-invariant and are more sensitive to minor image\ntranslations than standard convnets. Previous studies have shown, however, that\nconvnets are also not perfectly shift-invariant, due to aliasing in\ndownsampling and nonlinear layers. Consequently, anti-aliasing approaches have\nbeen proposed to certify convnets' translation robustness. Building on this\nline of work, we propose an Alias-Free ViT, which combines two main components.\nFirst, it uses alias-free downsampling and nonlinearities. Second, it uses\nlinear cross-covariance attention that is shift-equivariant to both integer and\nfractional translations, enabling a shift-invariant global representation. Our\nmodel maintains competitive performance in image classification and outperforms\nsimilar-sized models in terms of robustness to adversarial translations.",
        "url": "http://arxiv.org/abs/2510.22673v1",
        "published_date": "2025-10-26T13:28:28+00:00",
        "updated_date": "2025-10-26T13:28:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hagay Michaeli",
            "Daniel Soudry"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion"
        ],
        "tldr": "The paper introduces an Alias-Free Vision Transformer that achieves shift-invariance via linear cross-covariance attention, improving robustness to image translations and maintaining competitive performance in image classification tasks.",
        "tldr_zh": "本文介绍了一种Alias-Free Vision Transformer，通过线性交叉协方差注意力实现了位移不变性，提高了对图像平移的鲁棒性，并在图像分类任务中保持了竞争性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Look and Tell: A Dataset for Multimodal Grounding Across Egocentric and Exocentric Views",
        "summary": "We introduce Look and Tell, a multimodal dataset for studying referential\ncommunication across egocentric and exocentric perspectives. Using Meta Project\nAria smart glasses and stationary cameras, we recorded synchronized gaze,\nspeech, and video as 25 participants instructed a partner to identify\ningredients in a kitchen. Combined with 3D scene reconstructions, this setup\nprovides a benchmark for evaluating how different spatial representations (2D\nvs. 3D; ego vs. exo) affect multimodal grounding. The dataset contains 3.67\nhours of recordings, including 2,707 richly annotated referential expressions,\nand is designed to advance the development of embodied agents that can\nunderstand and engage in situated dialogue.",
        "url": "http://arxiv.org/abs/2510.22672v1",
        "published_date": "2025-10-26T13:27:59+00:00",
        "updated_date": "2025-10-26T13:27:59+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.RO",
            "I.2.10; I.2.9; I.2.7; H.5.2"
        ],
        "authors": [
            "Anna Deichler",
            "Jonas Beskow"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a dataset called Look and Tell for studying referential communication using smart glasses and cameras. It aims to advance the development of agents that can engage in dialogue.",
        "tldr_zh": "本文介绍了一个名为Look and Tell的数据集，用于使用智能眼镜和摄像机研究指称性交流。其旨在推动能够参与对话的代理的发展。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Self-Attention Decomposition For Training Free Diffusion Editing",
        "summary": "Diffusion models achieve remarkable fidelity in image synthesis, yet precise\ncontrol over their outputs for targeted editing remains challenging. A key step\ntoward controllability is to identify interpretable directions in the model's\nlatent representations that correspond to semantic attributes. Existing\napproaches for finding interpretable directions typically rely on sampling\nlarge sets of images or training auxiliary networks, which limits efficiency.\nWe propose an analytical method that derives semantic editing directions\ndirectly from the pretrained parameters of diffusion models, requiring neither\nadditional data nor fine-tuning. Our insight is that self-attention weight\nmatrices encode rich structural information about the data distribution learned\nduring training. By computing the eigenvectors of these weight matrices, we\nobtain robust and interpretable editing directions. Experiments demonstrate\nthat our method produces high-quality edits across multiple datasets while\nreducing editing time significantly by 60% over current benchmarks.",
        "url": "http://arxiv.org/abs/2510.22650v1",
        "published_date": "2025-10-26T12:22:56+00:00",
        "updated_date": "2025-10-26T12:22:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tharun Anand",
            "Mohammad Hassan Vali",
            "Arno Solin"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion"
        ],
        "tldr": "The paper presents a method for deriving semantic editing directions in diffusion models directly from pretrained parameters, improving efficiency and producing high-quality edits in image synthesis.",
        "tldr_zh": "该论文提出了一种方法，通过从预训练参数中直接推导扩散模型的语义编辑方向，在图像合成中提高效率并产生高质量的编辑。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SWAN: Self-supervised Wavelet Neural Network for Hyperspectral Image Unmixing",
        "summary": "In this article, we present SWAN: a three-stage, self-supervised wavelet\nneural network for joint estimation of endmembers and abundances from\nhyperspectral imagery. The contiguous and overlapping hyperspectral band images\nare first expanded to Biorthogonal wavelet basis space that provides sparse,\ndistributed, and multi-scale representations. The idea is to exploit latent\nsymmetries from thus obtained invariant and covariant features using a\nself-supervised learning paradigm. The first stage, SWANencoder maps the input\nwavelet coefficients to a compact lower-dimensional latent space. The second\nstage, SWANdecoder uses the derived latent representation to reconstruct the\ninput wavelet coefficients. Interestingly, the third stage SWANforward learns\nthe underlying physics of the hyperspectral image. A three-stage combined loss\nfunction is formulated in the image acquisition domain that eliminates the need\nfor ground truth and enables self-supervised training. Adam is employed for\noptimizing the proposed loss function, while Sigmoid with a dropout of 0.3 is\nincorporated to avoid possible overfitting. Kernel regularizers bound the\nmagnitudes and preserve spatial variations in the estimated endmember\ncoefficients. The output of SWANencoder represents estimated abundance maps\nduring inference, while weights of SWANdecoder are retrieved to extract\nendmembers. Experiments are conducted on two benchmark synthetic data sets with\ndifferent signal-to-noise ratios as well as on three real benchmark\nhyperspectral data sets while comparing the results with several\nstate-of-the-art neural network-based unmixing methods. The qualitative,\nquantitative, and ablation results show performance enhancement by learning a\nresilient unmixing function as well as promoting self-supervision and compact\nnetwork parameters for practical applications.",
        "url": "http://arxiv.org/abs/2510.22607v1",
        "published_date": "2025-10-26T10:05:48+00:00",
        "updated_date": "2025-10-26T10:05:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yassh Ramchandani",
            "Vijayashekhar S S",
            "Jignesh S. Bhatt"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "SWAN is a self-supervised wavelet neural network for hyperspectral image unmixing, showing performance enhancement and promoting self-supervision for practical applications.",
        "tldr_zh": "SWAN是一种用于高光谱图像解混的自监督小波神经网络，展示了性能提升并促进自我监督以实际应用。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Projection Embedded Diffusion Bridge for CT Reconstruction from Incomplete Data",
        "summary": "Reconstructing CT images from incomplete projection data remains challenging\ndue to the ill-posed nature of the problem. Diffusion bridge models have\nrecently shown promise in restoring clean images from their corresponding\nFiltered Back Projection (FBP) reconstructions, but incorporating data\nconsistency into these models remains largely underexplored. Incorporating data\nconsistency can improve reconstruction fidelity by aligning the reconstructed\nimage with the observed projection data, and can enhance detail recovery by\nintegrating structural information contained in the projections. In this work,\nwe propose the Projection Embedded Diffusion Bridge (PEDB). PEDB introduces a\nnovel reverse stochastic differential equation (SDE) to sample from the\ndistribution of clean images conditioned on both the FBP reconstruction and the\nincomplete projection data. By explicitly conditioning on the projection data\nin sampling the clean images, PEDB naturally incorporates data consistency. We\nembed the projection data into the score function of the reverse SDE. Under\ncertain assumptions, we derive a tractable expression for the posterior score.\nIn addition, we introduce a free parameter to control the level of\nstochasticity in the reverse process. We also design a discretization scheme\nfor the reverse SDE to mitigate discretization error. Extensive experiments\ndemonstrate that PEDB achieves strong performance in CT reconstruction from\nthree types of incomplete data, including sparse-view, limited-angle, and\ntruncated projections. For each of these types, PEDB outperforms evaluated\nstate-of-the-art diffusion bridge models across standard, noisy, and\ndomain-shift evaluations.",
        "url": "http://arxiv.org/abs/2510.22605v1",
        "published_date": "2025-10-26T10:00:27+00:00",
        "updated_date": "2025-10-26T10:00:27+00:00",
        "categories": [
            "cs.CV",
            "physics.med-ph"
        ],
        "authors": [
            "Yuang Wang",
            "Pengfei Jin",
            "Siyeop Yoon",
            "Matthew Tivnan",
            "Shaoyang Zhang",
            "Li Zhang",
            "Quanzheng Li",
            "Zhiqiang Chen",
            "Dufan Wu"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN"
        ],
        "tldr": "The paper proposes a Projection Embedded Diffusion Bridge (PEDB) model for CT image reconstruction from incomplete data, outperforming state-of-the-art models in various scenarios.",
        "tldr_zh": "该论文提出了一种投影嵌入扩散桥（PEDB）模型，用于从不完整数据中重建CT图像，在各种情况下优于最先进的模型。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMS",
        "summary": "Large language models (LLMs) have recently advanced auditory speech\nrecognition (ASR), visual speech recognition (VSR), and audio-visual speech\nrecognition (AVSR). However, understanding of their internal dynamics under\nfine-tuning remains limited. In natural language processing, recent work has\nrevealed attention sinks, tokens that attract disproportionately high\nattention, and associated massive activations in which some features of sink\ntokens exhibit huge activation in LLMs. In this work, we are the first to study\nthese phenomena in multimodal speech recognition. Through a detailed analysis\nof audio-visual LLMs, we identify attention sinks and massive activations not\nonly at the BOS token but also at intermediate low-semantic tokens across ASR,\nVSR, and AVSR. We show that massive activations originate in the MLP layers and\ncorrespond to fixed feature indices across all sink tokens. We further show\nthat intermediate sink tokens exhibit high cosine similarity to the BOS token,\nthereby amplifying attention and activation. Building on these insights, we\nintroduce a simple decorrelation loss that reduces cosine similarity between\nBOS and other tokens, effectively mitigating intermediate sinks and massive\nactivations. Furthermore, our method improves word error rate (WER) under high\naudio-visual feature downsampling while remaining stable at lower downsampling\nrates.",
        "url": "http://arxiv.org/abs/2510.22603v1",
        "published_date": "2025-10-26T09:44:20+00:00",
        "updated_date": "2025-10-26T09:44:20+00:00",
        "categories": [
            "eess.AS",
            "cs.CV",
            "cs.SD"
        ],
        "authors": [
            "Anand",
            "Umberto Cappellazzo",
            "Stavros Petridis",
            "Maja Pantic"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer"
        ],
        "tldr": "The paper addresses attention sinks and massive activations in multimodal speech recognition using large language models and proposes a decorrelation loss to mitigate these issues.",
        "tldr_zh": "本文针对大语言模型在多模式语音识别中的注意力槽和大量激活问题提出解决方案。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From Pixels to Views: Learning Angular-Aware and Physics-Consistent Representations for Light Field Microscopy",
        "summary": "Light field microscopy (LFM) has become an emerging tool in neuroscience for\nlarge-scale neural imaging in vivo, notable for its single-exposure volumetric\nimaging, broad field of view, and high temporal resolution. However,\nlearning-based 3D reconstruction in XLFM remains underdeveloped due to two core\nchallenges: the absence of standardized datasets and the lack of methods that\ncan efficiently model its angular-spatial structure while remaining physically\ngrounded. We address these challenges by introducing three key contributions.\nFirst, we construct the XLFM-Zebrafish benchmark, a large-scale dataset and\nevaluation suite for XLFM reconstruction. Second, we propose Masked View\nModeling for Light Fields (MVN-LF), a self-supervised task that learns angular\npriors by predicting occluded views, improving data efficiency. Third, we\nformulate the Optical Rendering Consistency Loss (ORC Loss), a differentiable\nrendering constraint that enforces alignment between predicted volumes and\ntheir PSF-based forward projections. On the XLFM-Zebrafish benchmark, our\nmethod improves PSNR by 7.7% over state-of-the-art baselines.",
        "url": "http://arxiv.org/abs/2510.22577v1",
        "published_date": "2025-10-26T08:28:05+00:00",
        "updated_date": "2025-10-26T08:28:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Feng He",
            "Guodong Tan",
            "Qiankun Li",
            "Jun Yu",
            "Quan Wen"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper introduces a method to improve 3D reconstruction in light field microscopy using a new dataset, self-supervised learning, and a rendering consistency loss, showing a 7.7% improvement over baselines.",
        "tldr_zh": "本文提出了一种方法，通过新数据集、自监督学习和渲染稳定性损失来改进光场显微镜的3D重建，相比基线方法提高了7.7%。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MELDAE: A Framework for Micro-Expression Spotting, Detection, and Automatic Evaluation in In-the-Wild Conversational Scenes",
        "summary": "Accurately analyzing spontaneous, unconscious micro-expressions is crucial\nfor revealing true human emotions, but this task remains challenging in wild\nscenarios, such as natural conversation. Existing research largely relies on\ndatasets from controlled laboratory environments, and their performance\ndegrades dramatically in the real world. To address this issue, we propose\nthree contributions: the first micro-expression dataset focused on\nconversational-in-the-wild scenarios; an end-to-end localization and detection\nframework, MELDAE; and a novel boundary-aware loss function that improves\ntemporal accuracy by penalizing onset and offset errors. Extensive experiments\ndemonstrate that our framework achieves state-of-the-art results on the WDMD\ndataset, improving the key F1_{DR} localization metric by 17.72% over the\nstrongest baseline, while also demonstrating excellent generalization\ncapabilities on existing benchmarks.",
        "url": "http://arxiv.org/abs/2510.22575v1",
        "published_date": "2025-10-26T08:18:16+00:00",
        "updated_date": "2025-10-26T08:18:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yigui Feng",
            "Qinglin Wang",
            "Yang Liu",
            "Ke Liu",
            "Haotian Mo",
            "Enhao Huang",
            "Gencheng Liu",
            "Mingzhe Liu",
            "Jie Liu"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a framework for spotting and evaluating micro-expressions in conversational settings, achieving state-of-the-art results in wild scenarios.",
        "tldr_zh": "本文介绍了一个用于在对话情景中发现和评估微表情的框架，在野外场景中取得了最新成果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "STATUS Bench: A Rigorous Benchmark for Evaluating Object State Understanding in Vision-Language Models",
        "summary": "Object state recognition aims to identify the specific condition of objects,\nsuch as their positional states (e.g., open or closed) and functional states\n(e.g., on or off). While recent Vision-Language Models (VLMs) are capable of\nperforming a variety of multimodal tasks, it remains unclear how precisely they\ncan identify object states. To alleviate this issue, we introduce the STAte and\nTransition UnderStanding Benchmark (STATUS Bench), the first benchmark for\nrigorously evaluating the ability of VLMs to understand subtle variations in\nobject states in diverse situations. Specifically, STATUS Bench introduces a\nnovel evaluation scheme that requires VLMs to perform three tasks\nsimultaneously: object state identification (OSI), image retrieval (IR), and\nstate change identification (SCI). These tasks are defined over our fully\nhand-crafted dataset involving image pairs, their corresponding object state\ndescriptions and state change descriptions. Furthermore, we introduce a\nlarge-scale training dataset, namely STATUS Train, which consists of 13 million\nsemi-automatically created descriptions. This dataset serves as the largest\nresource to facilitate further research in this area. In our experiments, we\ndemonstrate that STATUS Bench enables rigorous consistency evaluation and\nreveal that current state-of-the-art VLMs still significantly struggle to\ncapture subtle object state distinctions. Surprisingly, under the proposed\nrigorous evaluation scheme, most open-weight VLMs exhibited chance-level\nzero-shot performance. After fine-tuning on STATUS Train, Qwen2.5-VL achieved\nperformance comparable to Gemini 2.0 Flash. These findings underscore the\nnecessity of STATUS Bench and Train for advancing object state recognition in\nVLM research.",
        "url": "http://arxiv.org/abs/2510.22571v1",
        "published_date": "2025-10-26T08:04:28+00:00",
        "updated_date": "2025-10-26T08:04:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "authors": [
            "Mahiro Ukai",
            "Shuhei Kurita",
            "Nakamasa Inoue"
        ],
        "ai_categories": [],
        "tldr": "The paper introduces STATUS Bench, a benchmark for evaluating object state understanding in Vision-Language Models (VLMs) with a focus on subtle variations in object states. Current VLMs struggle to capture these distinctions.",
        "tldr_zh": "该论文介绍了STATUS Bench，这是一个针对细微对象状态理解的视觉-语言模型（VLMs）评估基准。当前的VLMs难以捕捉这些细微差别。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Learning Event-guided Exposure-agnostic Video Frame Interpolation via Adaptive Feature Blending",
        "summary": "Exposure-agnostic video frame interpolation (VFI) is a challenging task that\naims to recover sharp, high-frame-rate videos from blurry, low-frame-rate\ninputs captured under unknown and dynamic exposure conditions. Event cameras\nare sensors with high temporal resolution, making them especially advantageous\nfor this task. However, existing event-guided methods struggle to produce\nsatisfactory results on severely low-frame-rate blurry videos due to the lack\nof temporal constraints. In this paper, we introduce a novel event-guided\nframework for exposure-agnostic VFI, addressing this limitation through two key\ncomponents: a Target-adaptive Event Sampling (TES) and a Target-adaptive\nImportance Mapping (TIM). Specifically, TES samples events around the target\ntimestamp and the unknown exposure time to better align them with the\ncorresponding blurry frames. TIM then generates an importance map that\nconsiders the temporal proximity and spatial relevance of consecutive features\nto the target. Guided by this map, our framework adaptively blends consecutive\nfeatures, allowing temporally aligned features to serve as the primary cues\nwhile spatially relevant ones offer complementary support. Extensive\nexperiments on both synthetic and real-world datasets demonstrate the\neffectiveness of our approach in exposure-agnostic VFI scenarios.",
        "url": "http://arxiv.org/abs/2510.22565v1",
        "published_date": "2025-10-26T07:44:26+00:00",
        "updated_date": "2025-10-26T07:44:26+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Junsik Jung",
            "Yoonki Cho",
            "Woo Jae Kim",
            "Lin Wang",
            "Sune-eui Yoon"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "This paper introduces a novel framework for exposure-agnostic video frame interpolation using event cameras, addressing the limitations of existing methods through adaptive feature blending.",
        "tldr_zh": "本文介绍了一种利用事件摄像头进行曝光无关视频帧插值的新框架，通过自适应特征融合解决了现有方法的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SRSR: Enhancing Semantic Accuracy in Real-World Image Super-Resolution with Spatially Re-Focused Text-Conditioning",
        "summary": "Existing diffusion-based super-resolution approaches often exhibit semantic\nambiguities due to inaccuracies and incompleteness in their text conditioning,\ncoupled with the inherent tendency for cross-attention to divert towards\nirrelevant pixels. These limitations can lead to semantic misalignment and\nhallucinated details in the generated high-resolution outputs. To address\nthese, we propose a novel, plug-and-play spatially re-focused super-resolution\n(SRSR) framework that consists of two core components: first, we introduce\nSpatially Re-focused Cross-Attention (SRCA), which refines text conditioning at\ninference time by applying visually-grounded segmentation masks to guide\ncross-attention. Second, we introduce a Spatially Targeted Classifier-Free\nGuidance (STCFG) mechanism that selectively bypasses text influences on\nungrounded pixels to prevent hallucinations. Extensive experiments on both\nsynthetic and real-world datasets demonstrate that SRSR consistently\noutperforms seven state-of-the-art baselines in standard fidelity metrics (PSNR\nand SSIM) across all datasets, and in perceptual quality measures (LPIPS and\nDISTS) on two real-world benchmarks, underscoring its effectiveness in\nachieving both high semantic fidelity and perceptual quality in\nsuper-resolution.",
        "url": "http://arxiv.org/abs/2510.22534v1",
        "published_date": "2025-10-26T05:03:55+00:00",
        "updated_date": "2025-10-26T05:03:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chen Chen",
            "Majid Abdolshah",
            "Violetta Shevchenko",
            "Hongdong Li",
            "Chang Xu",
            "Pulak Purkait"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel super-resolution framework called SRSR to enhance semantic accuracy in generated high-resolution images by refining text conditioning and preventing hallucinations.",
        "tldr_zh": "本文提出了一种新颖的超分辨率框架SRSR，通过改进文本条件和防止幻觉，提高生成高分辨率图像的语义准确性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "AesCrop: Aesthetic-driven Cropping Guided by Composition",
        "summary": "Aesthetic-driven image cropping is crucial for applications like view\nrecommendation and thumbnail generation, where visual appeal significantly\nimpacts user engagement. A key factor in visual appeal is composition--the\ndeliberate arrangement of elements within an image. Some methods have\nsuccessfully incorporated compositional knowledge through evaluation-based and\nregression-based paradigms. However, evaluation-based methods lack globality\nwhile regression-based methods lack diversity. Recently, hybrid approaches that\nintegrate both paradigms have emerged, bridging the gap between these two to\nachieve better diversity and globality. Notably, existing hybrid methods do not\nincorporate photographic composition guidance, a key attribute that defines\nphotographic aesthetics. In this work, we introduce AesCrop, a\ncomposition-aware hybrid image-cropping model that integrates a VMamba image\nencoder, augmented with a novel Mamba Composition Attention Bias (MCAB) and a\ntransformer decoder to perform end-to-end rank-based image cropping, generating\nmultiple crops along with the corresponding quality scores. By explicitly\nencoding compositional cues into the attention mechanism, MCAB directs AesCrop\nto focus on the most compositionally salient regions. Extensive experiments\ndemonstrate that AesCrop outperforms current state-of-the-art methods,\ndelivering superior quantitative metrics and qualitatively more pleasing crops.",
        "url": "http://arxiv.org/abs/2510.22528v1",
        "published_date": "2025-10-26T04:30:02+00:00",
        "updated_date": "2025-10-26T04:30:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yen-Hong Wong",
            "Lai-Kuan Wong"
        ],
        "ai_categories": [
            "Transformer",
            "AIGC"
        ],
        "tldr": "AesCrop is a composition-aware hybrid image-cropping model that outperforms existing methods in generating visually appealing image crops.",
        "tldr_zh": "AesCrop是一种考虑构图的混合图像裁剪模型，优于现有方法在生成视觉上吸引人的图像裁剪方面。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Open Multimodal Retrieval-Augmented Factual Image Generation",
        "summary": "Large Multimodal Models (LMMs) have achieved remarkable progress in\ngenerating photorealistic and prompt-aligned images, but they often produce\noutputs that contradict verifiable knowledge, especially when prompts involve\nfine-grained attributes or time-sensitive events. Conventional\nretrieval-augmented approaches attempt to address this issue by introducing\nexternal information, yet they are fundamentally incapable of grounding\ngeneration in accurate and evolving knowledge due to their reliance on static\nsources and shallow evidence integration. To bridge this gap, we introduce\nORIG, an agentic open multimodal retrieval-augmented framework for Factual\nImage Generation (FIG), a new task that requires both visual realism and\nfactual grounding. ORIG iteratively retrieves and filters multimodal evidence\nfrom the web and incrementally integrates the refined knowledge into enriched\nprompts to guide generation. To support systematic evaluation, we build\nFIG-Eval, a benchmark spanning ten categories across perceptual, compositional,\nand temporal dimensions. Experiments demonstrate that ORIG substantially\nimproves factual consistency and overall image quality over strong baselines,\nhighlighting the potential of open multimodal retrieval for factual image\ngeneration.",
        "url": "http://arxiv.org/abs/2510.22521v1",
        "published_date": "2025-10-26T04:13:31+00:00",
        "updated_date": "2025-10-26T04:13:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.IR",
            "cs.LG"
        ],
        "authors": [
            "Yang Tian",
            "Fan Liu",
            "Jingyuan Zhang",
            "Wei Bi",
            "Yupeng Hu",
            "Liqiang Nie"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces ORIG, a framework for factual image generation that improves factual consistency and overall image quality through open multimodal retrieval.",
        "tldr_zh": "本文介绍了ORIG，这是一个通过开放式多模态检索改善事实一致性和整体图像质量的框架。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GateFuseNet: An Adaptive 3D Multimodal Neuroimaging Fusion Network for Parkinson's Disease Diagnosis",
        "summary": "Accurate diagnosis of Parkinson's disease (PD) from MRI remains challenging\ndue to symptom variability and pathological heterogeneity. Most existing\nmethods rely on conventional magnitude-based MRI modalities, such as\nT1-weighted images (T1w), which are less sensitive to PD pathology than\nQuantitative Susceptibility Mapping (QSM), a phase-based MRI technique that\nquantifies iron deposition in deep gray matter nuclei. In this study, we\npropose GateFuseNet, an adaptive 3D multimodal fusion network that integrates\nQSM and T1w images for PD diagnosis. The core innovation lies in a gated fusion\nmodule that learns modality-specific attention weights and channel-wise gating\nvectors for selective feature modulation. This hierarchical gating mechanism\nenhances ROI-aware features while suppressing irrelevant signals. Experimental\nresults show that our method outperforms three existing state-of-the-art\napproaches, achieving 85.00% accuracy and 92.06% AUC. Ablation studies further\nvalidate the contributions of ROI guidance, multimodal integration, and fusion\npositioning. Grad-CAM visualizations confirm the model's focus on clinically\nrelevant pathological regions. The source codes and pretrained models can be\nfound at https://github.com/YangGaoUQ/GateFuseNet",
        "url": "http://arxiv.org/abs/2510.22507v1",
        "published_date": "2025-10-26T03:11:26+00:00",
        "updated_date": "2025-10-26T03:11:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Rui Jin",
            "Chen Chen",
            "Yin Liu",
            "Hongfu Sun",
            "Min Zeng",
            "Min Li",
            "Yang Gao"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Other"
        ],
        "tldr": "GateFuseNet is a novel 3D multimodal fusion network for Parkinson's disease diagnosis using QSM and T1w images, outperforming existing approaches.",
        "tldr_zh": "GateFuseNet是一个新颖的3D多模态融合网络，用于帕金森病的诊断，使用QSM和T1w图像，优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "LAMP: Data-Efficient Linear Affine Weight-Space Models for Parameter-Controlled 3D Shape Generation and Extrapolation",
        "summary": "Generating high-fidelity 3D geometries that satisfy specific parameter\nconstraints has broad applications in design and engineering. However, current\nmethods typically rely on large training datasets and struggle with\ncontrollability and generalization beyond the training distributions. To\novercome these limitations, we introduce LAMP (Linear Affine Mixing of\nParametric shapes), a data-efficient framework for controllable and\ninterpretable 3D generation. LAMP first aligns signed distance function (SDF)\ndecoders by overfitting each exemplar from a shared initialization, then\nsynthesizes new geometries by solving a parameter-constrained mixing problem in\nthe aligned weight space. To ensure robustness, we further propose a safety\nmetric that detects geometry validity via linearity mismatch. We evaluate LAMP\non two 3D parametric benchmarks: DrivAerNet++ and BlendedNet. We found that\nLAMP enables (i) controlled interpolation within bounds with as few as 100\nsamples, (ii) safe extrapolation by up to 100% parameter difference beyond\ntraining ranges, (iii) physics performance-guided optimization under fixed\nparameters. LAMP significantly outperforms conditional autoencoder and Deep\nNetwork Interpolation (DNI) baselines in both extrapolation and data\nefficiency. Our results demonstrate that LAMP advances controllable,\ndata-efficient, and safe 3D generation for design exploration, dataset\ngeneration, and performance-driven optimization.",
        "url": "http://arxiv.org/abs/2510.22491v1",
        "published_date": "2025-10-26T02:12:20+00:00",
        "updated_date": "2025-10-26T02:12:20+00:00",
        "categories": [
            "cs.LG",
            "cs.CE",
            "cs.CV"
        ],
        "authors": [
            "Ghadi Nehme",
            "Yanxia Zhang",
            "Dule Shu",
            "Matt Klenk",
            "Faez Ahmed"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "LAMP is a data-efficient framework for controllable and interpretable 3D shape generation, surpassing baseline methods in interpolation, extrapolation, and data efficiency.",
        "tldr_zh": "LAMP 是一个数据高效的框架，用于可控且可解释的三维形状生成，在插值、外推和数据效率方面超越基准方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DynaPose4D: High-Quality 4D Dynamic Content Generation via Pose Alignment Loss",
        "summary": "Recent advancements in 2D and 3D generative models have expanded the\ncapabilities of computer vision. However, generating high-quality 4D dynamic\ncontent from a single static image remains a significant challenge. Traditional\nmethods have limitations in modeling temporal dependencies and accurately\ncapturing dynamic geometry changes, especially when considering variations in\ncamera perspective. To address this issue, we propose DynaPose4D, an innovative\nsolution that integrates 4D Gaussian Splatting (4DGS) techniques with\nCategory-Agnostic Pose Estimation (CAPE) technology. This framework uses 3D\nGaussian Splatting to construct a 3D model from single images, then predicts\nmulti-view pose keypoints based on one-shot support from a chosen view,\nleveraging supervisory signals to enhance motion consistency. Experimental\nresults show that DynaPose4D achieves excellent coherence, consistency, and\nfluidity in dynamic motion generation. These findings not only validate the\nefficacy of the DynaPose4D framework but also indicate its potential\napplications in the domains of computer vision and animation production.",
        "url": "http://arxiv.org/abs/2510.22473v1",
        "published_date": "2025-10-26T01:11:13+00:00",
        "updated_date": "2025-10-26T01:11:13+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jing Yang",
            "Yufeng Yang"
        ],
        "ai_categories": [
            "GAN",
            "Transformer"
        ],
        "tldr": "The paper proposes DynaPose4D, a framework for generating high-quality 4D dynamic content from single static images, achieving excellent coherence and fluidity in dynamic motion generation.",
        "tldr_zh": "本文提出了DynaPose4D框架，用于从单个静态图像生成高质量的4D动态内容，实现了在动态运动生成中优秀的连贯性和流畅性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Benchmarking Egocentric Multimodal Goal Inference for Assistive Wearable Agents",
        "summary": "There has been a surge of interest in assistive wearable agents: agents\nembodied in wearable form factors (e.g., smart glasses) who take assistive\nactions toward a user's goal/query (e.g. \"Where did I leave my keys?\"). In this\nwork, we consider the important complementary problem of inferring that goal\nfrom multi-modal contextual observations. Solving this \"goal inference\" problem\nholds the promise of eliminating the effort needed to interact with such an\nagent. This work focuses on creating WAGIBench, a strong benchmark to measure\nprogress in solving this problem using vision-language models (VLMs). Given the\nlimited prior work in this area, we collected a novel dataset comprising 29\nhours of multimodal data from 348 participants across 3,477 recordings,\nfeaturing ground-truth goals alongside accompanying visual, audio, digital, and\nlongitudinal contextual observations. We validate that human performance\nexceeds model performance, achieving 93% multiple-choice accuracy compared with\n84% for the best-performing VLM. Generative benchmark results that evaluate\nseveral families of modern vision-language models show that larger models\nperform significantly better on the task, yet remain far from practical\nusefulness, as they produce relevant goals only 55% of the time. Through a\nmodality ablation, we show that models benefit from extra information in\nrelevant modalities with minimal performance degradation from irrelevant\nmodalities.",
        "url": "http://arxiv.org/abs/2510.22443v1",
        "published_date": "2025-10-25T21:54:01+00:00",
        "updated_date": "2025-10-25T21:54:01+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Vijay Veerabadran",
            "Fanyi Xiao",
            "Nitin Kamra",
            "Pedro Matias",
            "Joy Chen",
            "Caley Drooff",
            "Brett D Roads",
            "Riley Williams",
            "Ethan Henderson",
            "Xuanyi Zhao",
            "Kevin Carlberg",
            "Joseph Tighe",
            "Karl Ridgeway"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces WAGIBench, a benchmark for multimodal goal inference for assistive wearable agents. It evaluates vision-language models and shows that human performance exceeds model performance.",
        "tldr_zh": "本文介绍了WAGIBench，一个用于辅助可穿戴代理的多模式目标推断的基准。它评估了视觉语言模型，并表明人类的性能优于模型的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Hollywood Town: Long-Video Generation via Cross-Modal Multi-Agent Orchestration",
        "summary": "Recent advancements in multi-agent systems have demonstrated significant\npotential for enhancing creative task performance, such as long video\ngeneration. This study introduces three innovations to improve multi-agent\ncollaboration. First, we propose OmniAgent, a hierarchical, graph-based\nmulti-agent framework for long video generation that leverages a\nfilm-production-inspired architecture to enable modular specialization and\nscalable inter-agent collaboration. Second, inspired by context engineering, we\npropose hypergraph nodes that enable temporary group discussions among agents\nlacking sufficient context, reducing individual memory requirements while\nensuring adequate contextual information. Third, we transition from directed\nacyclic graphs (DAGs) to directed cyclic graphs with limited retries, allowing\nagents to reflect and refine outputs iteratively, thereby improving earlier\nstages through feedback from subsequent nodes. These contributions lay the\ngroundwork for developing more robust multi-agent systems in creative tasks.",
        "url": "http://arxiv.org/abs/2510.22431v1",
        "published_date": "2025-10-25T20:34:18+00:00",
        "updated_date": "2025-10-25T20:34:18+00:00",
        "categories": [
            "cs.MA",
            "cs.CV"
        ],
        "authors": [
            "Zheng Wei",
            "Mingchen Li",
            "Zeqian Zhang",
            "Ruibin Yuan",
            "Pan Hui",
            "Huamin Qu",
            "James Evans",
            "Maneesh Agrawala",
            "Anyi Rao"
        ],
        "ai_categories": [
            "GAN",
            "Transformer"
        ],
        "tldr": "The paper introduces a hierarchical multi-agent framework for long video generation, incorporating group discussions and iterative refinement to improve output quality.",
        "tldr_zh": "该论文介绍了一种用于长视频生成的分层多智能体框架，包括群体讨论和迭代改进以提高输出质量。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Cross-Species Transfer Learning in Agricultural AI: Evaluating ZebraPose Adaptation for Dairy Cattle Pose Estimation",
        "summary": "Pose estimation serves as a cornerstone of computer vision for understanding\nanimal posture, behavior, and welfare. Yet, agricultural applications remain\nconstrained by the scarcity of large, annotated datasets for livestock,\nespecially dairy cattle. This study evaluates the potential and limitations of\ncross-species transfer learning by adapting ZebraPose - a vision\ntransformer-based model trained on synthetic zebra imagery - for 27-keypoint\ndetection in dairy cows under real barn conditions. Using three configurations\n- a custom on-farm dataset (375 images, Sussex, New Brunswick, Canada), a\nsubset of the APT-36K benchmark dataset, and their combination, we\nsystematically assessed model accuracy and generalization across environments.\nWhile the combined model achieved promising performance (AP = 0.86, AR = 0.87,\nPCK 0.5 = 0.869) on in-distribution data, substantial generalization failures\noccurred when applied to unseen barns and cow populations. These findings\nexpose the synthetic-to-real domain gap as a major obstacle to agricultural AI\ndeployment and emphasize that morphological similarity between species is\ninsufficient for cross-domain transfer. The study provides practical insights\ninto dataset diversity, environmental variability, and computational\nconstraints that influence real-world deployment of livestock monitoring\nsystems. We conclude with a call for agriculture-first AI design, prioritizing\nfarm-level realism, cross-environment robustness, and open benchmark datasets\nto advance trustworthy and scalable animal-centric technologies.",
        "url": "http://arxiv.org/abs/2510.22618v1",
        "published_date": "2025-10-26T10:31:22+00:00",
        "updated_date": "2025-10-26T10:31:22+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mackenzie Tapp",
            "Sibi Chakravarthy Parivendan",
            "Kashfia Sailunaz",
            "Suresh Neethirajan"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper evaluates the use of a vision transformer model trained on synthetic zebra imagery to estimate key points in dairy cattle pose under real conditions, highlighting the challenges of cross-species transfer learning in agriculture AI.",
        "tldr_zh": "本文评估了在真实条件下使用基于合成斑马图像训练的视觉转换器模型来估计奶牛姿势的关键点，突出了在农业AI中跨物种迁移学习的挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.75
    },
    {
        "title": "Neural-HAR: A Dimension-Gated CNN Accelerator for Real-Time Radar Human Activity Recognition",
        "summary": "Radar-based human activity recognition (HAR) is attractive for unobtrusive\nand privacy-preserving monitoring, yet many CNN/RNN solutions remain too heavy\nfor edge deployment, and even lightweight ViT/SSM variants often exceed\npractical compute and memory budgets. We introduce Neural-HAR, a\ndimension-gated CNN accelerator tailored for real-time radar HAR on\nresource-constrained platforms. At its core is GateCNN, a parameter-efficient\nDoppler-temporal network that (i) embeds Doppler vectors to emphasize frequency\nevolution over time and (ii) applies dual-path gated convolutions that modulate\nDoppler-aware content features with temporal gates, complemented by a residual\npath for stable training. On the University of Glasgow UoG2020 continuous radar\ndataset, GateCNN attains 86.4% accuracy with only 2.7k parameters and 0.28M\nFLOPs per inference, comparable to CNN-BiGRU at a fraction of the complexity.\nOur FPGA prototype on Xilinx Zynq-7000 Z-7007S reaches 107.5 $\\mu$s latency and\n15 mW dynamic power using LUT-based ROM and distributed RAM only (zero\nDSP/BRAM), demonstrating real-time, energy-efficient edge inference. Code and\nHLS conversion scripts are available at https://github.com/lab-emi/AIRHAR.",
        "url": "http://arxiv.org/abs/2510.22772v1",
        "published_date": "2025-10-26T17:42:28+00:00",
        "updated_date": "2025-10-26T17:42:28+00:00",
        "categories": [
            "eess.SP",
            "cs.CV"
        ],
        "authors": [
            "Yizhuo Wu",
            "Francesco Fioranelli",
            "Chang Gao"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "Neural-HAR introduces a dimension-gated CNN accelerator for real-time radar human activity recognition, achieving high accuracy with low complexity and energy-efficient edge inference.",
        "tldr_zh": "Neural-HAR引入了一种维度门控CNN加速器，用于实时雷达人类活动识别，在低复杂度和高效能边缘推断下取得高准确性。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "S-Chain: Structured Visual Chain-of-Thought For Medicine",
        "summary": "Faithful reasoning in medical vision-language models (VLMs) requires not only\naccurate predictions but also transparent alignment between textual rationales\nand visual evidence. While Chain-of-Thought (CoT) prompting has shown promise\nin medical visual question answering (VQA), no large-scale expert-level dataset\nhas captured stepwise reasoning with precise visual grounding. We introduce\nS-Chain, the first large-scale dataset of 12,000 expert-annotated medical\nimages with bounding boxes and structured visual CoT (SV-CoT), explicitly\nlinking visual regions to reasoning steps. The dataset further supports 16\nlanguages, totaling over 700k VQA pairs for broad multilingual applicability.\nUsing S-Chain, we benchmark state-of-the-art medical VLMs (ExGra-Med,\nLLaVA-Med) and general-purpose VLMs (Qwen2.5-VL, InternVL2.5), showing that\nSV-CoT supervision significantly improves interpretability, grounding fidelity,\nand robustness. Beyond benchmarking, we study its synergy with\nretrieval-augmented generation, revealing how domain knowledge and visual\ngrounding interact during autoregressive reasoning. Finally, we propose a new\nmechanism that strengthens the alignment between visual evidence and reasoning,\nimproving both reliability and efficiency. S-Chain establishes a new benchmark\nfor grounded medical reasoning and paves the way toward more trustworthy and\nexplainable medical VLMs.",
        "url": "http://arxiv.org/abs/2510.22728v1",
        "published_date": "2025-10-26T15:57:14+00:00",
        "updated_date": "2025-10-26T15:57:14+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Khai Le-Duc",
            "Duy M. H. Nguyen",
            "Phuong T. H. Trinh",
            "Tien-Phat Nguyen",
            "Nghiem T. Diep",
            "An Ngo",
            "Tung Vu",
            "Trinh Vuong",
            "Anh-Tien Nguyen",
            "Mau Nguyen",
            "Van Trung Hoang",
            "Khai-Nguyen Nguyen",
            "Hy Nguyen",
            "Chris Ngo",
            "Anji Liu",
            "Nhat Ho",
            "Anne-Christin Hauschild",
            "Khanh Xuan Nguyen",
            "Thanh Nguyen-Tang",
            "Pengtao Xie",
            "Daniel Sonntag",
            "James Zou",
            "Mathias Niepert",
            "Anh Totti Nguyen"
        ],
        "ai_categories": [
            "Dataset",
            "Multimodality"
        ],
        "tldr": "The paper introduces S-Chain, a dataset of expert-annotated medical images with structured visual reasoning steps, improving interpretability and grounding fidelity in medical visual question answering models.",
        "tldr_zh": "本文介绍了S-Chain，这是一个包含结构化视觉推理步骤的专家注释的医学图像数据集，可以提高医学视觉问答模型的解释性和对齐准确性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "VADTree: Explainable Training-Free Video Anomaly Detection via Hierarchical Granularity-Aware Tree",
        "summary": "Video anomaly detection (VAD) focuses on identifying anomalies in videos.\nSupervised methods demand substantial in-domain training data and fail to\ndeliver clear explanations for anomalies. In contrast, training-free methods\nleverage the knowledge reserves and language interactivity of large pre-trained\nmodels to detect anomalies. However, the current fixed-length temporal window\nsampling approaches struggle to accurately capture anomalies with varying\ntemporal spans. Therefore, we propose VADTree that utilizes a Hierarchical\nGranularityaware Tree (HGTree) structure for flexible sampling in VAD. VADTree\nleverages the knowledge embedded in a pre-trained Generic Event Boundary\nDetection (GEBD) model to characterize potential anomaly event boundaries.\nSpecifically, VADTree decomposes the video into generic event nodes based on\nboundary confidence, and performs adaptive coarse-fine hierarchical structuring\nand redundancy removal to construct the HGTree. Then, the multi-dimensional\npriors are injected into the visual language models (VLMs) to enhance the\nnode-wise anomaly perception, and anomaly reasoning for generic event nodes is\nachieved via large language models (LLMs). Finally, an inter-cluster node\ncorrelation method is used to integrate the multi-granularity anomaly scores.\nExtensive experiments on three challenging datasets demonstrate that VADTree\nachieves state-of-the-art performance in training-free settings while\ndrastically reducing the number of sampled video segments. The code will be\navailable at https://github.com/wenlongli10/VADTree.",
        "url": "http://arxiv.org/abs/2510.22693v1",
        "published_date": "2025-10-26T14:36:15+00:00",
        "updated_date": "2025-10-26T14:36:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenlong Li",
            "Yifei Xu",
            "Yuan Rao",
            "Zhenhua Wang",
            "Shuiguang Deng"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "VADTree is a training-free method for video anomaly detection using a Hierarchical Granularity-aware Tree structure, achieving state-of-the-art performance with reduced sampled video segments.",
        "tldr_zh": "VADTree是一种无需训练的视频异常检测方法，利用分层粒度感知树结构，实现了最先进的性能，并减少了采样视频段数。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "LVD-GS: Gaussian Splatting SLAM for Dynamic Scenes via Hierarchical Explicit-Implicit Representation Collaboration Rendering",
        "summary": "3D Gaussian Splatting SLAM has emerged as a widely used technique for\nhigh-fidelity mapping in spatial intelligence. However, existing methods often\nrely on a single representation scheme, which limits their performance in\nlarge-scale dynamic outdoor scenes and leads to cumulative pose errors and\nscale ambiguity. To address these challenges, we propose \\textbf{LVD-GS}, a\nnovel LiDAR-Visual 3D Gaussian Splatting SLAM system. Motivated by the human\nchain-of-thought process for information seeking, we introduce a hierarchical\ncollaborative representation module that facilitates mutual reinforcement for\nmapping optimization, effectively mitigating scale drift and enhancing\nreconstruction robustness. Furthermore, to effectively eliminate the influence\nof dynamic objects, we propose a joint dynamic modeling module that generates\nfine-grained dynamic masks by fusing open-world segmentation with implicit\nresidual constraints, guided by uncertainty estimates from DINO-Depth features.\nExtensive evaluations on KITTI, nuScenes, and self-collected datasets\ndemonstrate that our approach achieves state-of-the-art performance compared to\nexisting methods.",
        "url": "http://arxiv.org/abs/2510.22669v1",
        "published_date": "2025-10-26T13:16:39+00:00",
        "updated_date": "2025-10-26T13:16:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Wenkai Zhu",
            "Xu Li",
            "Qimin Xu",
            "Benwu Wang",
            "Kun Wei",
            "Yiming Peng",
            "Zihang Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces LVD-GS, a novel LiDAR-Visual 3D Gaussian Splatting SLAM system for mapping dynamic scenes. It utilizes hierarchical collaboration and dynamic modeling to improve mapping accuracy and handle dynamic objects.",
        "tldr_zh": "该论文引入了LVD-GS，一种新颖的LiDAR-Visual 3D高斯点分布SLAM系统，用于映射动态场景。它利用分层协作和动态建模来提高映射精度和处理动态物体。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Robust Atypical Mitosis Classification with DenseNet121: Stain-Aware Augmentation and Hybrid Loss for Domain Generalization",
        "summary": "Atypical mitotic figures are important biomarkers of tumor aggressiveness in\nhistopathology, yet reliable recognition remains challenging due to severe\nclass imbalance and variability across imaging domains. We present a\nDenseNet-121-based framework tailored for atypical mitosis classification in\nthe MIDOG 2025 (Track 2) setting. Our method integrates stain-aware\naugmentation (Macenko), geometric and intensity transformations, and\nimbalance-aware learning via weighted sampling with a hybrid objective\ncombining class-weighted binary cross-entropy and focal loss. Trained\nend-to-end with AdamW and evaluated across multiple independent domains, the\nmodel demonstrates strong generalization under scanner and staining shifts,\nachieving balanced accuracy 85.0%, AUROC 0.927, sensitivity 89.2%, and\nspecificity 80.9% on the official test set. These results indicate that\ncombining DenseNet-121 with stain-aware augmentation and imbalance-adaptive\nobjectives yields a robust, domain-generalizable framework for atypical mitosis\nclassification suitable for real-world computational pathology workflows.",
        "url": "http://arxiv.org/abs/2510.22630v1",
        "published_date": "2025-10-26T11:24:55+00:00",
        "updated_date": "2025-10-26T11:24:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Adinath Dukre",
            "Ankan Deria",
            "Yutong Xie",
            "Imran Razzak"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper presents a DenseNet-121-based framework for robust atypical mitosis classification using stain-aware augmentation and hybrid loss, achieving strong generalization across multiple imaging domains.",
        "tldr_zh": "本文提出了一种基于DenseNet-121的框架，用于使用染色感知增强和混合损失进行强健的非典型有丝分裂分类，在多个成像领域实现强大的泛化能力。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Bag-of-Word-Groups (BoWG): A Robust and Efficient Loop Closure Detection Method Under Perceptual Aliasing",
        "summary": "Loop closure is critical in Simultaneous Localization and Mapping (SLAM)\nsystems to reduce accumulative drift and ensure global mapping consistency.\nHowever, conventional methods struggle in perceptually aliased environments,\nsuch as narrow pipes, due to vector quantization, feature sparsity, and\nrepetitive textures, while existing solutions often incur high computational\ncosts. This paper presents Bag-of-Word-Groups (BoWG), a novel loop closure\ndetection method that achieves superior precision-recall, robustness, and\ncomputational efficiency. The core innovation lies in the introduction of word\ngroups, which captures the spatial co-occurrence and proximity of visual words\nto construct an online dictionary. Additionally, drawing inspiration from\nprobabilistic transition models, we incorporate temporal consistency directly\ninto similarity computation with an adaptive scheme, substantially improving\nprecision-recall performance. The method is further strengthened by a feature\ndistribution analysis module and dedicated post-verification mechanisms. To\nevaluate the effectiveness of our method, we conduct experiments on both public\ndatasets and a confined-pipe dataset we constructed. Results demonstrate that\nBoWG surpasses state-of-the-art methods, including both traditional and\nlearning-based approaches, in terms of precision-recall and computational\nefficiency. Our approach also exhibits excellent scalability, achieving an\naverage processing time of 16 ms per image across 17,565 images in the\nBicocca25b dataset.",
        "url": "http://arxiv.org/abs/2510.22529v1",
        "published_date": "2025-10-26T04:31:01+00:00",
        "updated_date": "2025-10-26T04:31:01+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Xiang Fei",
            "Tina Tian",
            "Howie Choset",
            "Lu Li"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces Bag-of-Word-Groups (BoWG), a method for loop closure detection in SLAM systems that is robust, efficient, and outperforms existing techniques.",
        "tldr_zh": "本文介绍了Bag-of-Word-Groups（BoWG），一种用于SLAM系统中的闭环检测的方法，具有鲁棒性、高效性，并且胜过现有技术。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Single-Teacher View Augmentation: Boosting Knowledge Distillation via Angular Diversity",
        "summary": "Knowledge Distillation (KD) aims to train a lightweight student model by\ntransferring knowledge from a large, high-capacity teacher. Recent studies have\nshown that leveraging diverse teacher perspectives can significantly improve\ndistillation performance; however, achieving such diversity typically requires\nmultiple teacher networks, leading to high computational costs. In this work,\nwe propose a novel cost-efficient knowledge augmentation method for KD that\ngenerates diverse multi-views by attaching multiple branches to a single\nteacher. To ensure meaningful semantic variation across multi-views, we\nintroduce two angular diversity objectives: 1) constrained inter-angle\ndiversify loss, which maximizes angles between augmented views while preserving\nproximity to the original teacher output, and 2) intra-angle diversify loss,\nwhich encourages an even distribution of views around the original output. The\nensembled knowledge from these angularly diverse views, along with the original\nteacher, is distilled into the student. We further theoretically demonstrate\nthat our objectives increase the diversity among ensemble members and thereby\nreduce the upper bound of the ensemble's expected loss, leading to more\neffective distillation. Experimental results show that our method surpasses an\nexisting knowledge augmentation method across diverse configurations. Moreover,\nthe proposed method is compatible with other KD frameworks in a plug-and-play\nfashion, providing consistent improvements in generalization performance.",
        "url": "http://arxiv.org/abs/2510.22480v1",
        "published_date": "2025-10-26T01:41:08+00:00",
        "updated_date": "2025-10-26T01:41:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Seonghoon Yu",
            "Dongjun Nam",
            "Dina Katabi",
            "Jeany Son"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer"
        ],
        "tldr": "The paper proposes a cost-efficient method for knowledge distillation that generates diverse views by attaching multiple branches to a single teacher model, improving distillation performance.",
        "tldr_zh": "本文提出了一种节约成本的知识蒸馏方法，通过将多个分支连接到单个教师模型，生成多样化视角，提高蒸馏性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "SemiETPicker: Fast and Label-Efficient Particle Picking for CryoET Tomography Using Semi-Supervised Learning",
        "summary": "Cryogenic Electron Tomography (CryoET) combined with sub-volume averaging\n(SVA) is the only imaging modality capable of resolving protein structures\ninside cells at molecular resolution. Particle picking, the task of localizing\nand classifying target proteins in 3D CryoET volumes, remains the main\nbottleneck. Due to the reliance on time-consuming manual labels, the vast\nreserve of unlabeled tomograms remains underutilized. In this work, we present\na fast, label-efficient semi-supervised framework that exploits this untapped\ndata. Our framework consists of two components: (i) an end-to-end\nheatmap-supervised detection model inspired by keypoint detection, and (ii) a\nteacher-student co-training mechanism that enhances performance under sparse\nlabeling conditions. Furthermore, we introduce multi-view pseudo-labeling and a\nCryoET-specific DropBlock augmentation strategy to further boost performance.\nExtensive evaluations on the large-scale CZII dataset show that our approach\nimproves F1 by 10% over supervised baselines, underscoring the promise of\nsemi-supervised learning for leveraging unlabeled CryoET data.",
        "url": "http://arxiv.org/abs/2510.22454v1",
        "published_date": "2025-10-25T23:09:22+00:00",
        "updated_date": "2025-10-25T23:09:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Linhan Wang",
            "Jianwen Dou",
            "Wang Li",
            "Shengkun Wang",
            "Zhiwu Xie",
            "Chang-Tien Lu",
            "Yinlin Chen"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a semi-supervised framework for efficient particle picking in CryoET tomography using unlabeled data, achieving a 10% improvement in F1 score over supervised baselines.",
        "tldr_zh": "该论文介绍了一种半监督框架，利用未标记的数据在CryoET层析中实现有效的颗粒挑选，相较于监督基线取得了10%的F1分数提升。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "ConMatFormer: A Multi-attention and Transformer Integrated ConvNext based Deep Learning Model for Enhanced Diabetic Foot Ulcer Classification",
        "summary": "Diabetic foot ulcer (DFU) detection is a clinically significant yet\nchallenging task due to the scarcity and variability of publicly available\ndatasets. To solve these problems, we propose ConMatFormer, a new hybrid deep\nlearning architecture that combines ConvNeXt blocks, multiple attention\nmechanisms convolutional block attention module (CBAM) and dual attention\nnetwork (DANet), and transformer modules in a way that works together. This\ndesign facilitates the extraction of better local features and understanding of\nthe global context, which allows us to model small skin patterns across\ndifferent types of DFU very accurately. To address the class imbalance, we used\ndata augmentation methods. A ConvNeXt block was used to obtain detailed local\nfeatures in the initial stages. Subsequently, we compiled the model by adding a\ntransformer module to enhance long-range dependency. This enabled us to\npinpoint the DFU classes that were underrepresented or constituted minorities.\nTests on the DS1 (DFUC2021) and DS2 (diabetic foot ulcer (DFU)) datasets showed\nthat ConMatFormer outperformed state-of-the-art (SOTA) convolutional neural\nnetwork (CNN) and Vision Transformer (ViT) models in terms of accuracy,\nreliability, and flexibility. The proposed method achieved an accuracy of\n0.8961 and a precision of 0.9160 in a single experiment, which is a significant\nimprovement over the current standards for classifying DFUs. In addition, by\n4-fold cross-validation, the proposed model achieved an accuracy of 0.9755 with\na standard deviation of only 0.0031. We further applied explainable artificial\nintelligence (XAI) methods, such as Grad-CAM, Grad-CAM++, and LIME, to\nconsistently monitor the transparency and trustworthiness of the\ndecision-making process.. Our findings set a new benchmark for DFU\nclassification and provide a hybrid attention transformer framework for medical\nimage analysis.",
        "url": "http://arxiv.org/abs/2510.22743v1",
        "published_date": "2025-10-26T16:34:43+00:00",
        "updated_date": "2025-10-26T16:34:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Raihan Ahamed Rifat",
            "Fuyad Hasan Bhoyan",
            "Md Humaion Kabir Mehedi",
            "Md Kaviul Hossain",
            "Md. Jakir Hossen",
            "M. F. Mridha"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces ConMatFormer, a deep learning model for Diabetic Foot Ulcer classification that outperforms existing models with high accuracy and precision.",
        "tldr_zh": "该论文介绍了ConMatFormer，一种深度学习模型，用于糖尿病足溃疡的分类，其准确度和精密度高于现有模型。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Estimation of Fireproof Structure Class and Construction Year for Disaster Risk Assessment",
        "summary": "Structural fireproof classification is vital for disaster risk assessment and\ninsurance pricing in Japan. However, key building metadata such as construction\nyear and structure type are often missing or outdated, particularly in the\nsecond-hand housing market. This study proposes a multi-task learning model\nthat predicts these attributes from facade images. The model jointly estimates\nthe construction year, building structure, and property type, from which the\nstructural fireproof class - defined as H (non-fireproof), T (semi-fireproof),\nor M (fireproof) - is derived via a rule-based mapping based on official\ninsurance criteria. We trained and evaluated the model using a large-scale\ndataset of Japanese residential images, applying rigorous filtering and\ndeduplication. The model achieved high accuracy in construction-year regression\nand robust classification across imbalanced categories. Qualitative analyses\nshow that it captures visual cues related to building age and materials. Our\napproach demonstrates the feasibility of scalable, interpretable, image-based\nrisk-profiling systems, offering potential applications in insurance, urban\nplanning, and disaster preparedness.",
        "url": "http://arxiv.org/abs/2510.22683v1",
        "published_date": "2025-10-26T13:54:41+00:00",
        "updated_date": "2025-10-26T13:54:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hibiki Ayabe",
            "Kazushi Okamoto",
            "Koki Karube",
            "Atsushi Shibata",
            "Kei Harada"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper proposes a model to estimate building attributes from images for disaster risk assessment in Japan.",
        "tldr_zh": "该论文提出了一个从图像中估计建筑属性的模型，用于日本的灾害风险评估。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "DAMap: Distance-aware MapNet for High Quality HD Map Construction",
        "summary": "Predicting High-definition (HD) map elements with high quality (high\nclassification and localization scores) is crucial to the safety of autonomous\ndriving vehicles. However, current methods perform poorly in high quality\npredictions due to inherent task misalignment. Two main factors are responsible\nfor misalignment: 1) inappropriate task labels due to one-to-many matching\nqueries sharing the same labels, and 2) sub-optimal task features due to\ntask-shared sampling mechanism. In this paper, we reveal two inherent defects\nin current methods and develop a novel HD map construction method named DAMap\nto address these problems. Specifically, DAMap consists of three components:\nDistance-aware Focal Loss (DAFL), Hybrid Loss Scheme (HLS), and Task Modulated\nDeformable Attention (TMDA). The DAFL is introduced to assign appropriate\nclassification labels for one-to-many matching samples. The TMDA is proposed to\nobtain discriminative task-specific features. Furthermore, the HLS is proposed\nto better utilize the advantages of the DAFL. We perform extensive experiments\nand consistently achieve performance improvement on the NuScenes and Argoverse2\nbenchmarks under different metrics, baselines, splits, backbones, and\nschedules. Code will be available at https://github.com/jpdong-xjtu/DAMap.",
        "url": "http://arxiv.org/abs/2510.22675v1",
        "published_date": "2025-10-26T13:29:26+00:00",
        "updated_date": "2025-10-26T13:29:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinpeng Dong",
            "Chen Li",
            "Yutong Lin",
            "Jingwen Fu",
            "Sanping Zhou",
            "Nanning Zheng"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces DAMap, a novel method for constructing high-quality HD maps for autonomous driving by addressing task misalignment issues through various components.",
        "tldr_zh": "本文介绍了 DAMap，这是一种新颖的方法，通过各种组件解决任务错位问题，用于为自动驾驶构建高质量的高清地图。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "SARCLIP: A Vision Language Foundation Model for Semantic Understanding and Target Recognition in SAR Imagery",
        "summary": "Synthetic Aperture Radar (SAR) has emerged as a crucial imaging modality due\nto its all-weather capabilities. While recent advancements in self-supervised\nlearning and Masked Image Modeling (MIM) have paved the way for SAR foundation\nmodels, these approaches primarily focus on low-level visual features, often\noverlooking multimodal alignment and zero-shot target recognition within SAR\nimagery. To address this limitation, we construct SARCLIP-1M, a large-scale\nvision language dataset comprising over one million text-image pairs aggregated\nfrom existing datasets. We further introduce SARCLIP, the first vision language\nfoundation model tailored for the SAR domain. Our SARCLIP model is trained\nusing a contrastive vision language learning approach by domain transferring\nstrategy, enabling it to bridge the gap between SAR imagery and textual\ndescriptions. Extensive experiments on image-text retrieval and zero-shot\nclassification tasks demonstrate the superior performance of SARCLIP in feature\nextraction and interpretation, significantly outperforming state-of-the-art\nfoundation models and advancing the semantic understanding of SAR imagery. The\ncode and datasets will be released soon.",
        "url": "http://arxiv.org/abs/2510.22665v1",
        "published_date": "2025-10-26T13:04:50+00:00",
        "updated_date": "2025-10-26T13:04:50+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Qiwei Ma",
            "Zhiyu Wang",
            "Wang Liu",
            "Xukun Lu",
            "Bin Deng",
            "Puhong Duan",
            "Xudong Kang",
            "Shutao Li"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces SARCLIP, a vision language foundation model tailored for semantic understanding and target recognition in SAR imagery, outperforming state-of-the-art models.",
        "tldr_zh": "本文介绍了SARCLIP，这是一种针对合成孔径雷达图像的视觉语言基础模型，优于现有模型。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.0
    },
    {
        "title": "PSScreen V2: Partially Supervised Multiple Retinal Disease Screening",
        "summary": "In this work, we propose PSScreen V2, a partially supervised self-training\nframework for multiple retinal disease screening. Unlike previous methods that\nrely on fully labelled or single-domain datasets, PSScreen V2 is designed to\nlearn from multiple partially labelled datasets with different distributions,\naddressing both label absence and domain shift challenges. To this end,\nPSScreen V2 adopts a three-branch architecture with one teacher and two student\nnetworks. The teacher branch generates pseudo labels from weakly augmented\nimages to address missing labels, while the two student branches introduce\nnovel feature augmentation strategies: Low-Frequency Dropout (LF-Dropout),\nwhich enhances domain robustness by randomly discarding domain-related\nlow-frequency components, and Low-Frequency Uncertainty (LF-Uncert), which\nestimates uncertain domain variability via adversarially learned Gaussian\nperturbations of low-frequency statistics. Extensive experiments on multiple\nin-domain and out-of-domain fundus datasets demonstrate that PSScreen V2\nachieves state-of-the-art performance and superior domain generalization\nability. Furthermore, compatibility tests with diverse backbones, including the\nvision foundation model DINOv2, as well as evaluations on chest X-ray datasets,\nhighlight the universality and adaptability of the proposed framework. The\ncodes are available at https://github.com/boyiZheng99/PSScreen_V2.",
        "url": "http://arxiv.org/abs/2510.22589v1",
        "published_date": "2025-10-26T09:09:52+00:00",
        "updated_date": "2025-10-26T09:09:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Boyi Zheng",
            "Yalin Zheng",
            "Hrvoje Bogunović",
            "Qing Liu"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "PSScreen V2 is a partially supervised framework for retinal disease screening that learns from multiple datasets with different distributions, achieving state-of-the-art performance and domain generalization ability.",
        "tldr_zh": "PSScreen V2是一个部分监督的框架，用于视网膜疾病筛查，能够从不同分布的多个数据集中学习，实现了最先进的性能和领域泛化能力。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    },
    {
        "title": "Cross-View UAV Geo-Localization with Precision-Focused Efficient Design: A Hierarchical Distillation Approach with Multi-view Refinement",
        "summary": "Cross-view geo-localization (CVGL) enables UAV localization by matching\naerial images to geo-tagged satellite databases, which is critical for\nautonomous navigation in GNSS-denied environments. However, existing methods\nrely on resource-intensive fine-grained feature extraction and alignment, where\nmultiple branches and modules significantly increase inference costs, limiting\ntheir deployment on edge devices. We propose Precision-Focused Efficient Design\n(PFED), a resource-efficient framework combining hierarchical knowledge\ntransfer and multi-view representation refinement. This innovative method\ncomprises two key components: 1) During training, Hierarchical Distillation\nparadigm for fast and accurate CVGL (HD-CVGL), coupled with Uncertainty-Aware\nPrediction Alignment (UAPA) to distill essential information and mitigate the\ndata imbalance without incurring additional inference overhead. 2) During\ninference, an efficient Multi-view Refinement Module (MRM) leverages mutual\ninformation to filter redundant samples and effectively utilize the multi-view\ndata. Extensive experiments show that PFED achieves state-of-the-art\nperformance in both accuracy and efficiency, reaching 97.15\\% Recall@1 on\nUniversity-1652 while being over $5 \\times$ more efficient in FLOPs and $3\n\\times$ faster than previous top methods. Furthermore, PFED runs at 251.5 FPS\non the AGX Orin edge device, demonstrating its practical viability for\nreal-time UAV applications. The project is available at\nhttps://github.com/SkyEyeLoc/PFED",
        "url": "http://arxiv.org/abs/2510.22582v1",
        "published_date": "2025-10-26T08:47:20+00:00",
        "updated_date": "2025-10-26T08:47:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jian Sun",
            "Kangdao Liu",
            "Chi Zhang",
            "Chuangquan Chen",
            "Junge Shen",
            "Chi-Man Vong"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a resource-efficient framework for UAV geo-localization using a hierarchical distillation approach and multi-view refinement, achieving state-of-the-art performance in accuracy and efficiency.",
        "tldr_zh": "本文提出了一种资源有效的框架，利用分层蒸馏方法和多视角优化进行无人机地理定位，实现了在准确性和效率方面的最新性能。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "A Fully Interpretable Statistical Approach for Roadside LiDAR Background Subtraction",
        "summary": "We present a fully interpretable and flexible statistical method for\nbackground subtraction in roadside LiDAR data, aimed at enhancing\ninfrastructure-based perception in automated driving. Our approach introduces\nboth a Gaussian distribution grid (GDG), which models the spatial statistics of\nthe background using background-only scans, and a filtering algorithm that uses\nthis representation to classify LiDAR points as foreground or background. The\nmethod supports diverse LiDAR types, including multiline 360 degree and\nmicro-electro-mechanical systems (MEMS) sensors, and adapts to various\nconfigurations. Evaluated on the publicly available RCooper dataset, it\noutperforms state-of-the-art techniques in accuracy and flexibility, even with\nminimal background data. Its efficient implementation ensures reliable\nperformance on low-resource hardware, enabling scalable real-world deployment.",
        "url": "http://arxiv.org/abs/2510.22390v1",
        "published_date": "2025-10-25T18:18:10+00:00",
        "updated_date": "2025-10-25T18:18:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aitor Iglesias",
            "Nerea Aranjuelo",
            "Patricia Javierre",
            "Ainhoa Menendez",
            "Ignacio Arganda-Carreras",
            "Marcos Nieto"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper introduces a statistical method for roadside LiDAR background subtraction to enhance automated driving infrastructure-based perception, outperforming state-of-the-art techniques in accuracy and flexibility.",
        "tldr_zh": "本文介绍了一种用于路边LiDAR背景减除的统计方法，旨在增强自动驾驶基础设施感知， 在精度和灵活性方面优于现有技术。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Understanding What Is Not Said:Referring Remote Sensing Image Segmentation with Scarce Expressions",
        "summary": "Referring Remote Sensing Image Segmentation (RRSIS) aims to segment instances\nin remote sensing images according to referring expressions. Unlike Referring\nImage Segmentation on general images, acquiring high-quality referring\nexpressions in the remote sensing domain is particularly challenging due to the\nprevalence of small, densely distributed objects and complex backgrounds. This\npaper introduces a new learning paradigm, Weakly Referring Expression Learning\n(WREL) for RRSIS, which leverages abundant class names as weakly referring\nexpressions together with a small set of accurate ones to enable efficient\ntraining under limited annotation conditions. Furthermore, we provide a\ntheoretical analysis showing that mixed-referring training yields a provable\nupper bound on the performance gap relative to training with fully annotated\nreferring expressions, thereby establishing the validity of this new setting.\nWe also propose LRB-WREL, which integrates a Learnable Reference Bank (LRB) to\nrefine weakly referring expressions through sample-specific prompt embeddings\nthat enrich coarse class-name inputs. Combined with a teacher-student\noptimization framework using dynamically scheduled EMA updates, LRB-WREL\nstabilizes training and enhances cross-modal generalization under noisy weakly\nreferring supervision. Extensive experiments on our newly constructed benchmark\nwith varying weakly referring data ratios validate both the theoretical\ninsights and the practical effectiveness of WREL and LRB-WREL, demonstrating\nthat they can approach or even surpass models trained with fully annotated\nreferring expressions.",
        "url": "http://arxiv.org/abs/2510.22760v1",
        "published_date": "2025-10-26T17:18:48+00:00",
        "updated_date": "2025-10-26T17:18:48+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Kai Ye",
            "Bowen Liu",
            "Jianghang Lin",
            "Jiayi Ji",
            "Pingyang Dai",
            "Liujuan Cao"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "This paper introduces Weakly Referring Expression Learning (WREL) and LRB-WREL for Referring Remote Sensing Image Segmentation, which leverage weakly referring expressions for efficient training. Experimental results show that these methods can approach or surpass models trained with fully annotated referring expressions.",
        "tldr_zh": "本文引入了Weakly Referring Expression Learning (WREL)和LRB-WREL方法，用于处理Refering Remote Sensing Image Segmentation问题，能够有效训练。实验结果表明这些方法可以接近或超越使用完全标记的表达式训练的模型。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "LRW-Persian: Lip-reading in the Wild Dataset for Persian Language",
        "summary": "Lipreading has emerged as an increasingly important research area for\ndeveloping robust speech recognition systems and assistive technologies for the\nhearing-impaired. However, non-English resources for visual speech recognition\nremain limited. We introduce LRW-Persian, the largest in-the-wild Persian\nword-level lipreading dataset, comprising $743$ target words and over\n$414{,}000$ video samples extracted from more than $1{,}900$ hours of footage\nacross $67$ television programs. Designed as a benchmark-ready resource,\nLRW-Persian provides speaker-disjoint training and test splits, wide regional\nand dialectal coverage, and rich per-clip metadata including head pose, age,\nand gender. To ensure large-scale data quality, we establish a fully automated\nend-to-end curation pipeline encompassing transcription based on Automatic\nSpeech Recognition(ASR), active-speaker localization, quality filtering, and\npose/mask screening. We further fine-tune two widely used lipreading\narchitectures on LRW-Persian, establishing reference performance and\ndemonstrating the difficulty of Persian visual speech recognition. By filling a\ncritical gap in low-resource languages, LRW-Persian enables rigorous\nbenchmarking, supports cross-lingual transfer, and provides a foundation for\nadvancing multimodal speech research in underrepresented linguistic contexts.\nThe dataset is publicly available at: https://lrw-persian.vercel.app.",
        "url": "http://arxiv.org/abs/2510.22716v1",
        "published_date": "2025-10-26T15:21:42+00:00",
        "updated_date": "2025-10-26T15:21:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zahra Taghizadeh",
            "Mohammad Shahverdikondori",
            "Arian Noori",
            "Alireza Dadgarnia"
        ],
        "ai_categories": [
            "Dataset",
            "Multimodality"
        ],
        "tldr": "The paper introduces LRW-Persian, a large dataset for Persian lipreading, addressing the lack of resources for non-English visual speech recognition. It includes over 414,000 video samples from various television programs and provides detailed metadata for benchmarking and research purposes.",
        "tldr_zh": "该论文介绍了LRW-Persian，这是一个用于波斯语唇读的大型数据集，解决了非英语视觉语音识别资源不足的问题。它包含来自各种电视节目的超过41.4万个视频样本，并提供详细的元数据供基准测试和研究目的。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "A Critical Study on Tea Leaf Disease Detection using Deep Learning Techniques",
        "summary": "The proposed solution is Deep Learning Technique that will be able classify\nthree types of tea leaves diseases from which two diseases are caused by the\npests and one due to pathogens (infectious organisms) and environmental\nconditions and also show the area damaged by a disease in leaves. Namely Red\nRust, Helopeltis and Red spider mite respectively. In this paper we have\nevaluated two models namely SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for\nthe object detection. The SSD MobileNet V2 gave precision of 0.209 for IOU\nrange of 0.50:0.95 with recall of 0.02 on IOU 0.50:0.95 and final mAP of 20.9%.\nWhile Faster R-CNN ResNet50 V1 has precision of 0.252 on IOU range of 0.50:0.95\nand recall of 0.044 on IOU of 0.50:0.95 with a mAP of 25%, which is better than\nSSD. Also used Mask R-CNN for Object Instance Segmentation where we have\nimplemented our custom method to calculate the damaged diseased portion of\nleaves. Keywords: Tea Leaf Disease, Deep Learning, Red Rust, Helopeltis and Red\nSpider Mite, SSD MobileNet V2, Faster R-CNN ResNet50 V1 and Mask RCNN.",
        "url": "http://arxiv.org/abs/2510.22647v1",
        "published_date": "2025-10-26T12:18:15+00:00",
        "updated_date": "2025-10-26T12:18:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Nabajyoti Borah",
            "Raju Moni Borah",
            "Bandan Boruah",
            "Purnendu Bikash Acharjee",
            "Sajal Saha",
            "Ripjyoti Hazarika"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper presents a deep learning approach for detecting tea leaf diseases using object detection and instance segmentation techniques.",
        "tldr_zh": "该论文提出了一种利用目标检测和实例分割技术检测茶叶疾病的深度学习方法。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.0
    },
    {
        "title": "3D Roadway Scene Object Detection with LIDARs in Snowfall Conditions",
        "summary": "Because 3D structure of a roadway environment can be characterized directly\nby a Light Detection and Ranging (LiDAR) sensors, they can be used to obtain\nexceptional situational awareness for assitive and autonomous driving systems.\nAlthough LiDARs demonstrate good performance in clean and clear weather\nconditions, their performance significantly deteriorates in adverse weather\nconditions such as those involving atmospheric precipitation. This may render\nperception capabilities of autonomous systems that use LiDAR data in learning\nbased models to perform object detection and ranging ineffective. While efforts\nhave been made to enhance the accuracy of these models, the extent of signal\ndegradation under various weather conditions remains largely not quantified. In\nthis study, we focus on the performance of an automotive grade LiDAR in snowy\nconditions in order to develop a physics-based model that examines failure\nmodes of a LiDAR sensor. Specifically, we investigated how the LiDAR signal\nattenuates with different snowfall rates and how snow particles near the source\nserve as small but efficient reflectors. Utilizing our model, we transform data\nfrom clear conditions to simulate snowy scenarios, enabling a comparison of our\nsynthetic data with actual snowy conditions. Furthermore, we employ this\nsynthetic data, representative of different snowfall rates, to explore the\nimpact on a pre-trained object detection model, assessing its performance under\nvarying levels of snowfall",
        "url": "http://arxiv.org/abs/2510.22436v1",
        "published_date": "2025-10-25T21:14:45+00:00",
        "updated_date": "2025-10-25T21:14:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ghazal Farhani",
            "Taufiq Rahman",
            "Syed Mostaquim Ali",
            "Andrew Liu",
            "Mohamed Zaki",
            "Dominique Charlebois",
            "Benoit Anctil"
        ],
        "ai_categories": [
            "Dataset",
            "Diffusion"
        ],
        "tldr": "The paper investigates how LiDAR sensors perform in snowy conditions and develops a physics-based model to analyze signal degradation and object detection under varying snowfall rates.",
        "tldr_zh": "本文研究了LiDAR传感器在雪天中的性能，并开发了基于物理的模型来分析不同降雪速率下的信号衰减和物体检测。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "Privacy-Aware Federated nnU-Net for ECG Page Digitization",
        "summary": "Deep neural networks can convert ECG page images into analyzable waveforms,\nyet centralized training often conflicts with cross-institutional privacy and\ndeployment constraints. A cross-silo federated digitization framework is\npresented that trains a full-model nnU-Net segmentation backbone without\nsharing images and aggregates updates across sites under realistic non-IID\nheterogeneity (layout, grid style, scanner profile, noise).\n  The protocol integrates three standard server-side aggregators--FedAvg,\nFedProx, and FedAdam--and couples secure aggregation with central, user-level\ndifferential privacy to align utility with formal guarantees. Key features\ninclude: (i) end-to-end full-model training and synchronization across clients;\n(ii) secure aggregation so the server only observes a clipped, weighted sum\nonce a participation threshold is met; (iii) central Gaussian DP with Renyi\naccounting applied post-aggregation for auditable user-level privacy; and (iv)\na calibration-aware digitization pipeline comprising page normalization, trace\nsegmentation, grid-leakage suppression, and vectorization to twelve-lead\nsignals.\n  Experiments on ECG pages rendered from PTB-XL show consistently faster\nconvergence and higher late-round plateaus with adaptive server updates\n(FedAdam) relative to FedAvg and FedProx, while approaching centralized\nperformance. The privacy mechanism maintains competitive accuracy while\npreventing exposure of raw images or per-client updates, yielding deployable,\nauditable guarantees suitable for multi-institution settings.",
        "url": "http://arxiv.org/abs/2510.22387v1",
        "published_date": "2025-10-25T18:10:05+00:00",
        "updated_date": "2025-10-25T18:10:05+00:00",
        "categories": [
            "cs.CR",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Nader Nemati"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper presents a privacy-aware federated learning framework for digitizing ECG pages using deep neural networks without sharing images, ensuring user privacy and model performance.",
        "tldr_zh": "本文提出了一个隐私感知的联邦学习框架，使用深度神经网络对心电图页面进行数字化，不共享图像，确保用户隐私和模型性能。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]