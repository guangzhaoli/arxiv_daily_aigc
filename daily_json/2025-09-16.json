[
    {
        "title": "A Fully Open and Generalizable Foundation Model for Ultrasound Clinical Applications",
        "summary": "Artificial intelligence (AI) that can effectively learn ultrasound\nrepresentations by integrating multi-source data holds significant promise for\nadvancing clinical care. However, the scarcity of large labeled datasets in\nreal-world clinical environments and the limited generalizability of\ntask-specific models have hindered the development of generalizable clinical AI\nmodels for ultrasound applications. In this study, we present EchoCare, a novel\nultrasound foundation model for generalist clinical use, developed via\nself-supervised learning on our curated, publicly available, large-scale\ndataset EchoCareData. EchoCareData comprises 4.5 million ultrasound images,\nsourced from over 23 countries across 5 continents and acquired via a diverse\nrange of distinct imaging devices, thus encompassing global cohorts that are\nmulti-center, multi-device, and multi-ethnic. Unlike prior studies that adopt\noff-the-shelf vision foundation model architectures, we introduce a\nhierarchical classifier into EchoCare to enable joint learning of pixel-level\nand representation-level features, capturing both global anatomical contexts\nand local ultrasound characteristics. With minimal training, EchoCare\noutperforms state-of-the-art comparison models across 10 representative\nultrasound benchmarks of varying diagnostic difficulties, spanning disease\ndiagnosis, lesion segmentation, organ detection, landmark prediction,\nquantitative regression, imaging enhancement and report generation. The code\nand pretrained model are publicly released, rendering EchoCare accessible for\nfine-tuning and local adaptation, supporting extensibility to additional\napplications. EchoCare provides a fully open and generalizable foundation model\nto boost the development of AI technologies for diverse clinical ultrasound\napplications.",
        "url": "http://arxiv.org/abs/2509.11752v1",
        "published_date": "2025-09-15T10:05:31+00:00",
        "updated_date": "2025-09-15T10:05:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hongyuan Zhang",
            "Yuheng Wu",
            "Mingyang Zhao",
            "Zhiwei Chen",
            "Rebecca Li",
            "Fei Zhu",
            "Haohan Zhao",
            "Xiaohua Yuan",
            "Meng Yang",
            "Chunli Qiu",
            "Xiang Cong",
            "Haiyan Chen",
            "Lina Luan",
            "Randolph H. L. Wong",
            "Huai Liao",
            "Colin A Graham",
            "Shi Chang",
            "Guowei Tao",
            "Dong Yi",
            "Zhen Lei",
            "Nassir Navab",
            "Sebastien Ourselin",
            "Jiebo Luo",
            "Hongbin Liu",
            "Gaofeng Meng"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper presents EchoCare, a novel ultrasound foundation model developed using self-supervised learning on a large dataset, outperforming state-of-the-art models across various ultrasound benchmarks.",
        "tldr_zh": "该论文提出了EchoCare，这是一个利用自监督学习在大型数据集上开发的新型超声基础模型，优于各种超声基准的最新模型。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "MVQA-68K: A Multi-dimensional and Causally-annotated Dataset with Quality Interpretability for Video Assessment",
        "summary": "With the rapid advancement of video generation models such as Sora, video\nquality assessment (VQA) is becoming increasingly crucial for selecting\nhigh-quality videos from large-scale datasets used in pre-training. Traditional\nVQA methods, typically producing single numerical scores, often lack\ncomprehensiveness and interpretability. To address these challenges, we\nintroduce MVQA-68K, a novel multi-dimensional VQA dataset comprising over\n68,000 carefully annotated videos, covering seven essential quality dimensions:\noverall aesthetics, camera movement, dynamic degree, texture detail,\ncomposition, visual quality, and factual consistency. Each annotation includes\ndetailed chain-of-thought reasoning to facilitate interpretability and\ncomprehensive understanding. Extensive experiments demonstrate that MVQA-68K\nsignificantly enhances the performance of various multimodal large language\nmodels (MLLMs) on the VQA task, achieving state-of-the-art results not only on\nour internal test set (Fig.1) but also on public benchmarks including\nLSVQ-test, LSVQ-1080p, and LIVE-VQC. Meantime, incorporating explicit reasoning\nprocess during VQA training substantially boosts the zero-shot generalization.\nCode and dataset will be available at github:\nhttps://github.com/Controller01-ai/MVQA-68K",
        "url": "http://arxiv.org/abs/2509.11589v1",
        "published_date": "2025-09-15T05:16:54+00:00",
        "updated_date": "2025-09-15T05:16:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanyun Pu",
            "Kehan Li",
            "Zeyi Huang",
            "Zhijie Zhong",
            "Kaixiang Yang"
        ],
        "ai_categories": [
            "Dataset",
            "Multimodality"
        ],
        "tldr": "MVQA-68K is a new dataset for video quality assessment with detailed annotations across seven quality dimensions, improving performance of multimodal large language models.",
        "tldr_zh": "MVQA-68K是一个新的视频质量评估数据集，具有跨七个质量维度的详细注释，提高了多模态大语言模型的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "HoloGarment: 360° Novel View Synthesis of In-the-Wild Garments",
        "summary": "Novel view synthesis (NVS) of in-the-wild garments is a challenging task due\nsignificant occlusions, complex human poses, and cloth deformations. Prior\nmethods rely on synthetic 3D training data consisting of mostly unoccluded and\nstatic objects, leading to poor generalization on real-world clothing. In this\npaper, we propose HoloGarment (Hologram-Garment), a method that takes 1-3\nimages or a continuous video of a person wearing a garment and generates\n360{\\deg} novel views of the garment in a canonical pose. Our key insight is to\nbridge the domain gap between real and synthetic data with a novel implicit\ntraining paradigm leveraging a combination of large-scale real video data and\nsmall-scale synthetic 3D data to optimize a shared garment embedding space.\nDuring inference, the shared embedding space further enables dynamic\nvideo-to-360{\\deg} NVS through the construction of a garment \"atlas\"\nrepresentation by finetuning a garment embedding on a specific real-world\nvideo. The atlas captures garment-specific geometry and texture across all\nviewpoints, independent of body pose or motion. Extensive experiments show that\nHoloGarment achieves state-of-the-art performance on NVS of in-the-wild\ngarments from images and videos. Notably, our method robustly handles\nchallenging real-world artifacts -- such as wrinkling, pose variation, and\nocclusion -- while maintaining photorealism, view consistency, fine texture\ndetails, and accurate geometry. Visit our project page for additional results:\nhttps://johannakarras.github.io/HoloGarment",
        "url": "http://arxiv.org/abs/2509.12187v1",
        "published_date": "2025-09-15T17:50:57+00:00",
        "updated_date": "2025-09-15T17:50:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.GR",
            "cs.LG"
        ],
        "authors": [
            "Johanna Karras",
            "Yingwei Li",
            "Yasamin Jafarian",
            "Ira Kemelmacher-Shlizerman"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces HoloGarment, a method for generating 360° novel views of garments from just 1-3 images or a continuous video of a person wearing the garment. It bridges the gap between real and synthetic data to achieve state-of-the-art results in generating realistic garment views.",
        "tldr_zh": "本文介绍了HoloGarment，一种从仅1-3张图像或人穿着该服装的连续视频中生成服装360°新视图的方法。它建立了真实数据和合成数据之间的联系，实现了在生成逼真服装视图方面的最新成果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling",
        "summary": "The field of 4D world modeling - aiming to jointly capture spatial geometry\nand temporal dynamics - has witnessed remarkable progress in recent years,\ndriven by advances in large-scale generative models and multimodal learning.\nHowever, the development of truly general 4D world models remains fundamentally\nconstrained by the availability of high-quality data. Existing datasets and\nbenchmarks often lack the dynamic complexity, multi-domain diversity, and\nspatial-temporal annotations required to support key tasks such as 4D geometric\nreconstruction, future prediction, and camera-control video generation. To\naddress this gap, we introduce OmniWorld, a large-scale, multi-domain,\nmulti-modal dataset specifically designed for 4D world modeling. OmniWorld\nconsists of a newly collected OmniWorld-Game dataset and several curated public\ndatasets spanning diverse domains. Compared with existing synthetic datasets,\nOmniWorld-Game provides richer modality coverage, larger scale, and more\nrealistic dynamic interactions. Based on this dataset, we establish a\nchallenging benchmark that exposes the limitations of current state-of-the-art\n(SOTA) approaches in modeling complex 4D environments. Moreover, fine-tuning\nexisting SOTA methods on OmniWorld leads to significant performance gains\nacross 4D reconstruction and video generation tasks, strongly validating\nOmniWorld as a powerful resource for training and evaluation. We envision\nOmniWorld as a catalyst for accelerating the development of general-purpose 4D\nworld models, ultimately advancing machines' holistic understanding of the\nphysical world.",
        "url": "http://arxiv.org/abs/2509.12201v1",
        "published_date": "2025-09-15T17:59:19+00:00",
        "updated_date": "2025-09-15T17:59:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yang Zhou",
            "Yifan Wang",
            "Jianjun Zhou",
            "Wenzheng Chang",
            "Haoyu Guo",
            "Zizun Li",
            "Kaijing Ma",
            "Xinyue Li",
            "Yating Wang",
            "Haoyi Zhu",
            "Mingyu Liu",
            "Dingning Liu",
            "Jiange Yang",
            "Zhoujie Fu",
            "Junyi Chen",
            "Chunhua Shen",
            "Jiangmiao Pang",
            "Kaipeng Zhang",
            "Tong He"
        ],
        "ai_categories": [
            "Dataset",
            "Multimodality"
        ],
        "tldr": "The paper introduces OmniWorld, a dataset for 4D world modeling, and establishes a benchmark for evaluating state-of-the-art approaches in modeling complex environments.",
        "tldr_zh": "该论文介绍了OmniWorld，这是一个用于4D世界建模的数据集，并建立了一个评估模拟复杂环境中最先进方法的基准。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "AvatarSync: Rethinking Talking-Head Animation through Autoregressive Perspective",
        "summary": "Existing talking-head animation approaches based on Generative Adversarial\nNetworks (GANs) or diffusion models often suffer from inter-frame flicker,\nidentity drift, and slow inference. These limitations inherent to their video\ngeneration pipelines restrict their suitability for applications. To address\nthis, we introduce AvatarSync, an autoregressive framework on phoneme\nrepresentations that generates realistic and controllable talking-head\nanimations from a single reference image, driven directly text or audio input.\nIn addition, AvatarSync adopts a two-stage generation strategy, decoupling\nsemantic modeling from visual dynamics, which is a deliberate \"Divide and\nConquer\" design. The first stage, Facial Keyframe Generation (FKG), focuses on\nphoneme-level semantic representation by leveraging the many-to-one mapping\nfrom text or audio to phonemes. A Phoneme-to-Visual Mapping is constructed to\nanchor abstract phonemes to character-level units. Combined with a customized\nText-Frame Causal Attention Mask, the keyframes are generated. The second\nstage, inter-frame interpolation, emphasizes temporal coherence and visual\nsmoothness. We introduce a timestamp-aware adaptive strategy based on a\nselective state space model, enabling efficient bidirectional context\nreasoning. To support deployment, we optimize the inference pipeline to reduce\nlatency without compromising visual fidelity. Extensive experiments show that\nAvatarSync outperforms existing talking-head animation methods in visual\nfidelity, temporal consistency, and computational efficiency, providing a\nscalable and controllable solution.",
        "url": "http://arxiv.org/abs/2509.12052v1",
        "published_date": "2025-09-15T15:34:02+00:00",
        "updated_date": "2025-09-15T15:34:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuchen Deng",
            "Xiuyang Wu",
            "Hai-Tao Zheng",
            "Suiyang Zhang",
            "Yi He",
            "Yuxing Han"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Other"
        ],
        "tldr": "AvatarSync is an autoregressive framework for generating realistic talking-head animations using phoneme representations and a two-stage generation strategy.",
        "tldr_zh": "AvatarSync是一个自回归框架，通过音素表示和两阶段生成策略生成逼真的说唱动画。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "DTGen: Generative Diffusion-Based Few-Shot Data Augmentation for Fine-Grained Dirty Tableware Recognition",
        "summary": "Intelligent tableware cleaning is a critical application in food safety and\nsmart homes, but existing methods are limited by coarse-grained classification\nand scarcity of few-shot data, making it difficult to meet industrialization\nrequirements. We propose DTGen, a few-shot data augmentation scheme based on\ngenerative diffusion models, specifically designed for fine-grained dirty\ntableware recognition. DTGen achieves efficient domain specialization through\nLoRA, generates diverse dirty images via structured prompts, and ensures data\nquality through CLIP-based cross-modal filtering. Under extremely limited real\nfew-shot conditions, DTGen can synthesize virtually unlimited high-quality\nsamples, significantly improving classifier performance and supporting\nfine-grained dirty tableware recognition. We further elaborate on lightweight\ndeployment strategies, promising to transfer DTGen's benefits to embedded\ndishwashers and integrate with cleaning programs to intelligently regulate\nenergy consumption and detergent usage. Research results demonstrate that DTGen\nnot only validates the value of generative AI in few-shot industrial vision but\nalso provides a feasible deployment path for automated tableware cleaning and\nfood safety monitoring.",
        "url": "http://arxiv.org/abs/2509.11661v1",
        "published_date": "2025-09-15T07:59:34+00:00",
        "updated_date": "2025-09-15T07:59:34+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Lifei Hao",
            "Yue Cheng",
            "Baoqi Huang",
            "Bing Jia",
            "Xuandong Zhao"
        ],
        "ai_categories": [
            "Diffusion",
            "LoRA",
            "GAN",
            "Dataset"
        ],
        "tldr": "DTGen proposes a generative diffusion-based few-shot data augmentation method for fine-grained dirty tableware recognition, showing significant improvements in performance and potential applications in intelligent tableware cleaning.",
        "tldr_zh": "DTGen提出了一种基于生成扩散的少样本数据增强方法，用于细粒度脏餐具识别，在性能上取得显著改进，并在智能餐具清洁方面具有潜在应用。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Disentanglement of Biological and Technical Factors via Latent Space Rotation in Clinical Imaging Improves Disease Pattern Discovery",
        "summary": "Identifying new disease-related patterns in medical imaging data with the\nhelp of machine learning enlarges the vocabulary of recognizable findings. This\nsupports diagnostic and prognostic assessment. However, image appearance varies\nnot only due to biological differences, but also due to imaging technology\nlinked to vendors, scanning- or re- construction parameters. The resulting\ndomain shifts impedes data representation learning strategies and the discovery\nof biologically meaningful cluster appearances. To address these challenges, we\nintroduce an approach to actively learn the domain shift via post-hoc rotation\nof the data latent space, enabling disentanglement of biological and technical\nfactors. Results on real-world heterogeneous clinical data showcase that the\nlearned disentangled representation leads to stable clusters representing\ntissue-types across different acquisition settings. Cluster consistency is\nimproved by +19.01% (ARI), +16.85% (NMI), and +12.39% (Dice) compared to the\nentangled representation, outperforming four state-of-the-art harmonization\nmethods. When using the clusters to quantify tissue composition on idiopathic\npulmonary fibrosis patients, the learned profiles enhance Cox survival\nprediction. This indicates that the proposed label-free framework facilitates\nbiomarker discovery in multi-center routine imaging data. Code is available on\nGitHub https://github.com/cirmuw/latent-space-rotation-disentanglement.",
        "url": "http://arxiv.org/abs/2509.11436v1",
        "published_date": "2025-09-14T21:16:15+00:00",
        "updated_date": "2025-09-14T21:16:15+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Jeanny Pan",
            "Philipp Seeböck",
            "Christoph Fürböck",
            "Svitlana Pochepnia",
            "Jennifer Straub",
            "Lucian Beer",
            "Helmut Prosch",
            "Georg Langs"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a method that disentangles biological and technical factors in clinical imaging data using latent space rotation, leading to better disease pattern discovery and biomarker prediction.",
        "tldr_zh": "本文介绍了一种利用潜在空间旋转来解开临床成像数据中的生物和技术因素的方法，从而更好地发现疾病模式和生物标志预测。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Character-Centric Understanding of Animated Movies",
        "summary": "Animated movies are captivating for their unique character designs and\nimaginative storytelling, yet they pose significant challenges for existing\nrecognition systems. Unlike the consistent visual patterns detected by\nconventional face recognition methods, animated characters exhibit extreme\ndiversity in their appearance, motion, and deformation. In this work, we\npropose an audio-visual pipeline to enable automatic and robust animated\ncharacter recognition, and thereby enhance character-centric understanding of\nanimated movies. Central to our approach is the automatic construction of an\naudio-visual character bank from online sources. This bank contains both visual\nexemplars and voice (audio) samples for each character, enabling subsequent\nmulti-modal character recognition despite long-tailed appearance distributions.\nBuilding on accurate character recognition, we explore two downstream\napplications: Audio Description (AD) generation for visually impaired\naudiences, and character-aware subtitling for the hearing impaired. To support\nresearch in this domain, we introduce CMD-AM, a new dataset of 75 animated\nmovies with comprehensive annotations. Our character-centric pipeline\ndemonstrates significant improvements in both accessibility and narrative\ncomprehension for animated content over prior face-detection-based approaches.\nFor the code and dataset, visit\nhttps://www.robots.ox.ac.uk/~vgg/research/animated_ad/.",
        "url": "http://arxiv.org/abs/2509.12204v1",
        "published_date": "2025-09-15T17:59:51+00:00",
        "updated_date": "2025-09-15T17:59:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhongrui Gui",
            "Junyu Xie",
            "Tengda Han",
            "Weidi Xie",
            "Andrew Zisserman"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes an audio-visual pipeline for automated recognition of animated characters in movies, with applications for visually and hearing impaired audiences.",
        "tldr_zh": "本文提出了一种用于自动识别动画电影中角色的音频-视觉流水线，适用于视觉障碍和听力障碍观众。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence",
        "summary": "The reliance on implicit point matching via attention has become a core\nbottleneck in drag-based editing, resulting in a fundamental compromise on\nweakened inversion strength and costly test-time optimization (TTO). This\ncompromise severely limits the generative capabilities of diffusion models,\nsuppressing high-fidelity inpainting and text-guided creation. In this paper,\nwe introduce LazyDrag, the first drag-based image editing method for\nMulti-Modal Diffusion Transformers, which directly eliminates the reliance on\nimplicit point matching. In concrete terms, our method generates an explicit\ncorrespondence map from user drag inputs as a reliable reference to boost the\nattention control. This reliable reference opens the potential for a stable\nfull-strength inversion process, which is the first in the drag-based editing\ntask. It obviates the necessity for TTO and unlocks the generative capability\nof models. Therefore, LazyDrag naturally unifies precise geometric control with\ntext guidance, enabling complex edits that were previously out of reach:\nopening the mouth of a dog and inpainting its interior, generating new objects\nlike a ``tennis ball'', or for ambiguous drags, making context-aware changes\nlike moving a hand into a pocket. Additionally, LazyDrag supports multi-round\nworkflows with simultaneous move and scale operations. Evaluated on the\nDragBench, our method outperforms baselines in drag accuracy and perceptual\nquality, as validated by VIEScore and human evaluation. LazyDrag not only\nestablishes new state-of-the-art performance, but also paves a new way to\nediting paradigms.",
        "url": "http://arxiv.org/abs/2509.12203v1",
        "published_date": "2025-09-15T17:59:47+00:00",
        "updated_date": "2025-09-15T17:59:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zixin Yin",
            "Xili Dai",
            "Duomin Wang",
            "Xianfang Zeng",
            "Lionel M. Ni",
            "Gang Yu",
            "Heung-Yeung Shum"
        ],
        "ai_categories": [
            "Multimodality",
            "Diffusion",
            "Transformer"
        ],
        "tldr": "LazyDrag introduces a drag-based image editing method for Multi-Modal Diffusion Transformers, enabling stable editing by generating an explicit correspondence map from user drag inputs.",
        "tldr_zh": "LazyDrag为多模态扩散变换器引入了基于拖拽的图像编辑方法，通过从用户拖拽输入生成显式对应图，实现稳定编辑。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Advancing Medical Artificial Intelligence Using a Century of Cases",
        "summary": "BACKGROUND: For over a century, the New England Journal of Medicine\nClinicopathological Conferences (CPCs) have tested the reasoning of expert\nphysicians and, recently, artificial intelligence (AI). However, prior AI\nevaluations have focused on final diagnoses without addressing the multifaceted\nreasoning and presentation skills required of expert discussants.\n  METHODS: Using 7102 CPCs (1923-2025) and 1021 Image Challenges (2006-2025),\nwe conducted extensive physician annotation and automated processing to create\nCPC-Bench, a physician-validated benchmark spanning 10 text-based and\nmultimodal tasks, against which we evaluated leading large language models\n(LLMs). Then, we developed \"Dr. CaBot,\" an AI discussant designed to produce\nwritten and slide-based video presentations using only the case presentation,\nmodeling the role of the human expert in these cases.\n  RESULTS: When challenged with 377 contemporary CPCs, o3 (OpenAI) ranked the\nfinal diagnosis first in 60% of cases and within the top ten in 84% of cases,\noutperforming a 20-physician baseline; next-test selection accuracy reached\n98%. Event-level physician annotations quantified AI diagnostic accuracy per\nunit of information. Performance was lower on literature search and image\ntasks; o3 and Gemini 2.5 Pro (Google) achieved 67% accuracy on image\nchallenges. In blinded comparisons of CaBot vs. human expert-generated text,\nphysicians misclassified the source of the differential in 46 of 62 (74%) of\ntrials, and scored CaBot more favorably across quality dimensions. To promote\nresearch, we are releasing CaBot and CPC-Bench.\n  CONCLUSIONS: LLMs exceed physician performance on complex text-based\ndifferential diagnosis and convincingly emulate expert medical presentations,\nbut image interpretation and literature retrieval remain weaker. CPC-Bench and\nCaBot may enable transparent and continued tracking of progress in medical AI.",
        "url": "http://arxiv.org/abs/2509.12194v1",
        "published_date": "2025-09-15T17:54:51+00:00",
        "updated_date": "2025-09-15T17:54:51+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Thomas A. Buckley",
            "Riccardo Conci",
            "Peter G. Brodeur",
            "Jason Gusdorf",
            "Sourik Beltrán",
            "Bita Behrouzi",
            "Byron Crowe",
            "Jacob Dockterman",
            "Muzzammil Muhammad",
            "Sarah Ohnigian",
            "Andrew Sanchez",
            "James A. Diao",
            "Aashna P. Shah",
            "Daniel Restrepo",
            "Eric S. Rosenberg",
            "Andrew S. Lea",
            "Marinka Zitnik",
            "Scott H. Podolsky",
            "Zahir Kanjee",
            "Raja-Elie E. Abdulnour",
            "Jacob M. Koshy",
            "Adam Rodman",
            "Arjun K. Manrai"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces CPC-Bench, a benchmark for assessing AI performance in medical reasoning, and presents an AI discussant named Dr. CaBot that emulates expert medical presentations.",
        "tldr_zh": "该论文介绍了CPC-Bench，这是一个评估医学推理中人工智能表现的基准，并呈现了一个名为Dr. CaBot的人工智能讨论者，模仿专家医学展示。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LoRA-fine-tuned Large Vision Models for Automated Assessment of Post-SBRT Lung Injury",
        "summary": "This study investigates the efficacy of Low-Rank Adaptation (LoRA) for\nfine-tuning large Vision Models, DinoV2 and SwinV2, to diagnose\nRadiation-Induced Lung Injury (RILI) from X-ray CT scans following Stereotactic\nBody Radiation Therapy (SBRT). To evaluate the robustness and efficiency of\nthis approach, we compare LoRA with traditional full fine-tuning and\ninference-only (no fine-tuning) methods. Cropped images of two sizes (50 mm3\nand 75 mm3), centered at the treatment isocenter, in addition to different\nadaptation techniques for adapting the 2D LVMs for 3D data were used to\ndetermine the sensitivity of the models to spatial context. Experimental\nresults show that LoRA achieves comparable or superior performance to\ntraditional fine-tuning while significantly reducing computational costs and\ntraining times by requiring fewer trainable parameters.",
        "url": "http://arxiv.org/abs/2509.12155v1",
        "published_date": "2025-09-15T17:21:22+00:00",
        "updated_date": "2025-09-15T17:21:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "M. Bolhassani",
            "B. Veasey",
            "E. Daugherty",
            "S. Keltner",
            "N. Kumar",
            "N. Dunlap",
            "A. Amini"
        ],
        "ai_categories": [
            "LoRA",
            "Dataset"
        ],
        "tldr": "The paper explores using LoRA to fine-tune large Vision Models for diagnosing lung injury from CT scans post-SBRT, achieving comparable performance with reduced computational costs.",
        "tldr_zh": "本文探讨了使用LoRA对大型视觉模型进行微调，以诊断SBRT后的CT扫描中的肺部损伤，实现了性能可比的同时减少计算成本。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multi Anatomy X-Ray Foundation Model",
        "summary": "X-ray imaging is a ubiquitous in radiology, yet most existing AI foundation\nmodels are limited to chest anatomy and fail to generalize across broader\nclinical tasks. In this work, we introduce XR-0, the multi-anatomy X-ray\nfoundation model using self-supervised learning on a large, private dataset of\n1.15 million images spanning diverse anatomical regions and evaluated across 12\ndatasets and 20 downstream tasks, including classification, retrieval,\nsegmentation, localization, visual grounding, and report generation. XR-0\nachieves state-of-the-art performance on most multi-anatomy tasks and remains\ncompetitive on chest-specific benchmarks. Our results demonstrate that\nanatomical diversity and supervision are critical for building robust,\ngeneral-purpose medical vision models, paving the way for scalable and\nadaptable AI systems in radiology.",
        "url": "http://arxiv.org/abs/2509.12146v1",
        "published_date": "2025-09-15T17:12:26+00:00",
        "updated_date": "2025-09-15T17:12:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Nishank Singla",
            "Krisztian Koos",
            "Farzin Haddadpour",
            "Amin Honarmandi Shandiz",
            "Lovish Chum",
            "Xiaojian Xu",
            "Qing Jin",
            "Erhan Bas"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces XR-0, a multi-anatomy X-ray foundation model trained on a diverse dataset for various clinical tasks, achieving state-of-the-art performance.",
        "tldr_zh": "本文介绍了XR-0，一种多解剖X射线基础模型，通过在多样化数据集上进行训练，在各种临床任务中取得了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Open-ended Hierarchical Streaming Video Understanding with Vision Language Models",
        "summary": "We introduce Hierarchical Streaming Video Understanding, a task that combines\nonline temporal action localization with free-form description generation.\nGiven the scarcity of datasets with hierarchical and fine-grained temporal\nannotations, we demonstrate that LLMs can effectively group atomic actions into\nhigher-level events, enriching existing datasets. We then propose OpenHOUSE\n(Open-ended Hierarchical Online Understanding System for Events), which extends\nstreaming action perception beyond action classification. OpenHOUSE features a\nspecialized streaming module that accurately detects boundaries between closely\nadjacent actions, nearly doubling the performance of direct extensions of\nexisting methods. We envision the future of streaming action perception in the\nintegration of powerful generative models, with OpenHOUSE representing a key\nstep in that direction.",
        "url": "http://arxiv.org/abs/2509.12145v1",
        "published_date": "2025-09-15T17:11:06+00:00",
        "updated_date": "2025-09-15T17:11:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hyolim Kang",
            "Yunsu Park",
            "Youngbeom Yoo",
            "Yeeun Choi",
            "Seon Joo Kim"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a task of combining online temporal action localization with description generation using vision language models, proposing a system named OpenHOUSE for streaming action perception.",
        "tldr_zh": "该论文介绍了一个任务，将在线时间动作定位与描述生成结合起来，利用视觉语言模型提出了一个名为OpenHOUSE的流式动作感知系统。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Progressive Flow-inspired Unfolding for Spectral Compressive Imaging",
        "summary": "Coded aperture snapshot spectral imaging (CASSI) retrieves a 3D hyperspectral\nimage (HSI) from a single 2D compressed measurement, which is a highly\nchallenging reconstruction task. Recent deep unfolding networks (DUNs),\nempowered by explicit data-fidelity updates and implicit deep denoisers, have\nachieved the state of the art in CASSI reconstruction. However, existing\nunfolding approaches suffer from uncontrollable reconstruction trajectories,\nleading to abrupt quality jumps and non-gradual refinement across stages.\nInspired by diffusion trajectories and flow matching, we propose a novel\ntrajectory-controllable unfolding framework that enforces smooth, continuous\noptimization paths from noisy initial estimates to high-quality\nreconstructions. To achieve computational efficiency, we design an efficient\nspatial-spectral Transformer tailored for hyperspectral reconstruction, along\nwith a frequency-domain fusion module to gurantee feature consistency.\nExperiments on simulation and real data demonstrate that our method achieves\nbetter reconstruction quality and efficiency than prior state-of-the-art\napproaches.",
        "url": "http://arxiv.org/abs/2509.12079v1",
        "published_date": "2025-09-15T16:10:50+00:00",
        "updated_date": "2025-09-15T16:10:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaodong Wang",
            "Ping Wang",
            "Zijun He",
            "Mengjie Qin",
            "Xin Yuan"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a new trajectory-controllable unfolding framework for hyperspectral image reconstruction, which outperforms existing state-of-the-art methods in terms of quality and efficiency.",
        "tldr_zh": "本文提出了一种新的轨迹可控展开框架，用于高光谱图像重建，其在质量和效率方面优于现有的最先进方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "End-to-End Learning of Multi-Organ Implicit Surfaces from 3D Medical Imaging Data",
        "summary": "The fine-grained surface reconstruction of different organs from 3D medical\nimaging can provide advanced diagnostic support and improved surgical planning.\nHowever, the representation of the organs is often limited by the resolution,\nwith a detailed higher resolution requiring more memory and computing\nfootprint. Implicit representations of objects have been proposed to alleviate\nthis problem in general computer vision by providing compact and differentiable\nfunctions to represent the 3D object shapes. However, architectural and\ndata-related differences prevent the direct application of these methods to\nmedical images. This work introduces ImplMORe, an end-to-end deep learning\nmethod using implicit surface representations for multi-organ reconstruction\nfrom 3D medical images. ImplMORe incorporates local features using a 3D CNN\nencoder and performs multi-scale interpolation to learn the features in the\ncontinuous domain using occupancy functions. We apply our method for single and\nmultiple organ reconstructions using the totalsegmentator dataset. By\nleveraging the continuous nature of occupancy functions, our approach\noutperforms the discrete explicit representation based surface reconstruction\napproaches, providing fine-grained surface details of the organ at a resolution\nhigher than the given input image. The source code will be made publicly\navailable at: https://github.com/CAMMA-public/ImplMORe",
        "url": "http://arxiv.org/abs/2509.12068v1",
        "published_date": "2025-09-15T15:52:20+00:00",
        "updated_date": "2025-09-15T15:52:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Farahdiba Zarin",
            "Nicolas Padoy",
            "Jérémy Dana",
            "Vinkle Srivastav"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces ImplMORe, an end-to-end deep learning method for reconstructing multiple organs from 3D medical images using implicit surface representations, outperforming traditional methods in fine-grained surface details.",
        "tldr_zh": "该论文介绍了ImplMORe，一种利用隐式表面表示从3D医学图像中重建多个器官的端到端深度学习方法，优于传统方法在细节方面的表现。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Layout-Conditioned Autoregressive Text-to-Image Generation via Structured Masking",
        "summary": "While autoregressive (AR) models have demonstrated remarkable success in\nimage generation, extending them to layout-conditioned generation remains\nchallenging due to the sparse nature of layout conditions and the risk of\nfeature entanglement. We present Structured Masking for AR-based\nLayout-to-Image (SMARLI), a novel framework for layoutto-image generation that\neffectively integrates spatial layout constraints into AR-based image\ngeneration. To equip AR model with layout control, a specially designed\nstructured masking strategy is applied to attention computation to govern the\ninteraction among the global prompt, layout, and image tokens. This design\nprevents mis-association between different regions and their descriptions while\nenabling sufficient injection of layout constraints into the generation\nprocess. To further enhance generation quality and layout accuracy, we\nincorporate Group Relative Policy Optimization (GRPO) based post-training\nscheme with specially designed layout reward functions for next-set-based AR\nmodels. Experimental results demonstrate that SMARLI is able to seamlessly\nintegrate layout tokens with text and image tokens without compromising\ngeneration quality. It achieves superior layoutaware control while maintaining\nthe structural simplicity and generation efficiency of AR models.",
        "url": "http://arxiv.org/abs/2509.12046v1",
        "published_date": "2025-09-15T15:27:29+00:00",
        "updated_date": "2025-09-15T15:27:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zirui Zheng",
            "Takashi Isobe",
            "Tong Shen",
            "Xu Jia",
            "Jianbin Zhao",
            "Xiaomin Li",
            "Mengmeng Ge",
            "Baolu Li",
            "Qinghe Wang",
            "Dong Li",
            "Dong Zhou",
            "Yunzhi Zhuge",
            "Huchuan Lu",
            "Emad Barsoum"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer"
        ],
        "tldr": "The paper introduces Structured Masking for AR-based Layout-to-Image generation, which effectively integrates spatial layout constraints into image generation without compromising quality.",
        "tldr_zh": "本文介绍了基于结构化屏蔽的自回归布局到图像生成方法，有效地将空间布局约束集成到图像生成中，而不会影响质量。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restoration",
        "summary": "This work presents Robust Representation Learning via Adaptive Mask (RAM++),\na two-stage framework for all-in-one image restoration. RAM++ integrates\nhigh-level semantic understanding with low-level texture generation to achieve\ncontent-oriented robust restoration. It addresses the limitations of existing\ndegradation-oriented methods in extreme scenarios (e.g., degradations strongly\ncoupled with image structures). RAM++ also mitigates common challenges such as\nunbalanced performance across tasks, overfitting to seen degradations, and weak\ngeneralization to unseen ones through three key designs: 1) Adaptive\nSemantic-Aware Mask (AdaSAM): a pretraining strategy that applies pixel-level\nmasks to semantically rich and textured regions. This design enables the\nnetwork to learn both generative priors and image content priors from various\ndegradations. 2) Mask Attribute Conductance (MAC): a selective fine-tuning\nstrategy that adjusts the layers with higher contributions to bridge the\nintegrity gap between masked pretraining and full-image fine-tuning while\nretaining learned priors. 3) Robust Feature Regularization (RFR): a strategy\nthat leverages DINOv2's semantically consistent and degradation-invariant\nrepresentations, together with efficient feature fusion, to achieve faithful\nand semantically coherent restoration. With these designs, RAM++ achieves\nrobust, well-balanced, and state-of-the-art performance across seen, unseen,\nextreme, and mixed degradations. Our code and model will be released at\nhttps://github.com/DragonisCV/RAM",
        "url": "http://arxiv.org/abs/2509.12039v1",
        "published_date": "2025-09-15T15:24:15+00:00",
        "updated_date": "2025-09-15T15:24:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zilong Zhang",
            "Chujie Qin",
            "Chunle Guo",
            "Yong Zhang",
            "Chao Xue",
            "Ming-Ming Cheng",
            "Chongyi Li"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "RAM++ is a framework for all-in-one image restoration that integrates high-level semantic understanding with low-level texture generation for robust restoration.",
        "tldr_zh": "RAM++是一个全方位图像恢复框架，将高级语义理解与低级纹理生成相结合，实现强大的恢复效果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Robust Concept Erasure in Diffusion Models: A Theoretical Perspective on Security and Robustness",
        "summary": "Diffusion models have achieved unprecedented success in image generation but\npose increasing risks in terms of privacy, fairness, and security. A growing\ndemand exists to \\emph{erase} sensitive or harmful concepts (e.g., NSFW\ncontent, private individuals, artistic styles) from these models while\npreserving their overall generative capabilities. We introduce \\textbf{SCORE}\n(Secure and Concept-Oriented Robust Erasure), a novel framework for robust\nconcept removal in diffusion models. SCORE formulates concept erasure as an\n\\emph{adversarial independence} problem, theoretically guaranteeing that the\nmodel's outputs become statistically independent of the erased concept. Unlike\nprior heuristic methods, SCORE minimizes the mutual information between a\ntarget concept and generated outputs, yielding provable erasure guarantees. We\nprovide formal proofs establishing convergence properties and derive upper\nbounds on residual concept leakage. Empirically, we evaluate SCORE on Stable\nDiffusion and FLUX across four challenging benchmarks: object erasure, NSFW\nremoval, celebrity face suppression, and artistic style unlearning. SCORE\nconsistently outperforms state-of-the-art methods including EraseAnything, ANT,\nMACE, ESD, and UCE, achieving up to \\textbf{12.5\\%} higher erasure efficacy\nwhile maintaining comparable or superior image quality. By integrating\nadversarial optimization, trajectory consistency, and saliency-driven\nfine-tuning, SCORE sets a new standard for secure and robust concept erasure in\ndiffusion models.",
        "url": "http://arxiv.org/abs/2509.12024v1",
        "published_date": "2025-09-15T15:05:50+00:00",
        "updated_date": "2025-09-15T15:05:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zixuan Fu",
            "Yan Ren",
            "Finn Carter",
            "Chenyue Wen",
            "Le Ku",
            "Daheng Yu",
            "Emily Davis",
            "Bo Zhang"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN",
            "Security"
        ],
        "tldr": "The paper introduces SCORE, a framework for concept erasure in diffusion models to remove sensitive content while maintaining generative capabilities.",
        "tldr_zh": "本文介绍了SCORE，这是一个用于扩散模型中的概念擦除的框架，旨在删除敏感内容同时保持生成能力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Data-driven Smile Design: Personalized Dental Aesthetics Outcomes Using Deep Learning",
        "summary": "A healthy smile plays a significant role in functional as well as esthetic\nconsiderations, improving confidence. It is difficult for dental professionals\nto strike a balance between esthetic requirements and functional requirements.\nTraditional smile design has had heavy reliance on dentist expertise and used\nplaster models and hand drawings, raising questions about the outcome for\npatients. Digital technology, led by Dr. Christian Coachman in 2007, allows\nphotographic and videographic assessments, enabling improved intercommunication\namong specialists and patients. Advances in artificial intelligence (AI) and\nbig data have supported analysis of facial features and development of\npersonalized smile designs in the last few years. Outputs are, however,\nsusceptible to practitioner bias or limitations of training data, and may be\nsuboptimal for individual users. The study presented here suggests a\ncomprehensive system integrating AI, big data, and recognition technologies to\nautomate the smile design process so that both experienced and inexperienced\ndentists can generate pleasing aesthetics with ease. The system has a Facial\nFeature Extraction Module and an Image Generation Module, serving diverse\npractitioner and patient needs. User data can be incorporated in future\nresearch for design optimization and testing of virtual and augmented reality\nfor real-time previewing. Data gathered can also be employed in aesthetic\npreference analyses, which can enhance our knowledge of smile design in dental\npractice.",
        "url": "http://arxiv.org/abs/2509.12001v1",
        "published_date": "2025-09-15T14:49:27+00:00",
        "updated_date": "2025-09-15T14:49:27+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "I.2.6; I.2.10; J.3"
        ],
        "authors": [
            "Marcus Lin",
            "Jennifer Lai"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a system using AI and big data to automate personalized smile design, catering to both experienced and inexperienced dentists.",
        "tldr_zh": "本文提出了一个利用人工智能和大数据自动化个性化微笑设计的系统，满足了经验丰富和不熟练的牙医的需求。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Lost in Embeddings: Information Loss in Vision-Language Models",
        "summary": "Vision--language models (VLMs) often process visual inputs through a\npretrained vision encoder, followed by a projection into the language model's\nembedding space via a connector component. While crucial for modality fusion,\nthe potential information loss induced by this projection step and its direct\nimpact on model capabilities remain understudied. We introduce two\ncomplementary approaches to examine and quantify this loss by analyzing the\nlatent representation space. First, we evaluate semantic information\npreservation by analyzing changes in k-nearest neighbor relationships between\nimage representations, before and after projection. Second, we directly measure\ninformation loss by reconstructing visual embeddings from the projected\nrepresentation, localizing loss at an image patch level. Experiments reveal\nthat connectors substantially distort the local geometry of visual\nrepresentations, with k-nearest neighbors diverging by 40--60\\%\npost-projection, correlating with degradation in retrieval performance. The\npatch-level embedding reconstruction provides interpretable insights for model\nbehavior on visually grounded question-answering tasks, finding that areas of\nhigh information loss reliably predict instances where models struggle.",
        "url": "http://arxiv.org/abs/2509.11986v1",
        "published_date": "2025-09-15T14:38:06+00:00",
        "updated_date": "2025-09-15T14:38:06+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Wenyan Li",
            "Raphael Tang",
            "Chengzu Li",
            "Caiqi Zhang",
            "Ivan Vulić",
            "Anders Søgaard"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer"
        ],
        "tldr": "The paper explores information loss in Vision-Language Models due to projection steps, impacting model capabilities and retrieval performance.",
        "tldr_zh": "本文研究了视觉-语言模型中由于投影步骤导致的信息丢失问题，影响模型能力和检索性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Learning to Generate 4D LiDAR Sequences",
        "summary": "While generative world models have advanced video and occupancy-based data\nsynthesis, LiDAR generation remains underexplored despite its importance for\naccurate 3D perception. Extending generation to 4D LiDAR data introduces\nchallenges in controllability, temporal stability, and evaluation. We present\nLiDARCrafter, a unified framework that converts free-form language into\neditable LiDAR sequences. Instructions are parsed into ego-centric scene\ngraphs, which a tri-branch diffusion model transforms into object layouts,\ntrajectories, and shapes. A range-image diffusion model generates the initial\nscan, and an autoregressive module extends it into a temporally coherent\nsequence. The explicit layout design further supports object-level editing,\nsuch as insertion or relocation. To enable fair assessment, we provide\nEvalSuite, a benchmark spanning scene-, object-, and sequence-level metrics. On\nnuScenes, LiDARCrafter achieves state-of-the-art fidelity, controllability, and\ntemporal consistency, offering a foundation for LiDAR-based simulation and data\naugmentation.",
        "url": "http://arxiv.org/abs/2509.11959v1",
        "published_date": "2025-09-15T14:14:48+00:00",
        "updated_date": "2025-09-15T14:14:48+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Ao Liang",
            "Youquan Liu",
            "Yu Yang",
            "Dongyue Lu",
            "Linfeng Li",
            "Lingdong Kong",
            "Huaici Zhao",
            "Wei Tsang Ooi"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces LiDARCrafter, a framework for generating 4D LiDAR sequences from text instructions. It achieves state-of-the-art fidelity, controllability, and temporal consistency.",
        "tldr_zh": "本文介绍了LiDARCrafter，一个从文本指令生成4D LiDAR序列的框架。它在保真度、可控性和时间一致性方面达到了最先进的水平。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CLAIRE: A Dual Encoder Network with RIFT Loss and Phi-3 Small Language Model Based Interpretability for Cross-Modality Synthetic Aperture Radar and Optical Land Cover Segmentation",
        "summary": "Accurate land cover classification from satellite imagery is crucial in\nenvironmental monitoring and sustainable resource management. However, it\nremains challenging due to the complexity of natural landscapes, the visual\nsimilarity between classes, and the significant class imbalance in the\navailable datasets. To address these issues, we propose a dual encoder\narchitecture that independently extracts modality-specific features from\noptical and Synthetic Aperture Radar (SAR) imagery, which are then fused using\na cross-modality attention-fusion module named Cross-modality Land cover\nsegmentation with Attention and Imbalance-aware Reasoning-Enhanced Explanations\n(CLAIRE). This fusion mechanism highlights complementary spatial and textural\nfeatures, enabling the network to better capture detailed and diverse land\ncover patterns. We incorporate a hybrid loss function that utilizes Weighted\nFocal Loss and Tversky Loss named RIFT (Rare-Instance Focal-Tversky) to address\nclass imbalance and improve segmentation performance across underrepresented\ncategories. Our model achieves competitive performance across multiple\nbenchmarks: a mean Intersection over Union (mIoU) of 56.02% and Overall\nAccuracy (OA) of 84.56% on the WHU-OPT-SAR dataset; strong generalization with\na mIoU of 59.89% and OA of 73.92% on the OpenEarthMap-SAR dataset; and\nremarkable robustness under cloud-obstructed conditions, achieving an mIoU of\n86.86% and OA of 94.58% on the PIE-RGB-SAR dataset. Additionally, we introduce\na metric-driven reasoning module generated by a Small Language Model (Phi-3),\nwhich generates expert-level, sample-specific justifications for model\npredictions, thereby enhancing transparency and interpretability.",
        "url": "http://arxiv.org/abs/2509.11952v1",
        "published_date": "2025-09-15T14:10:52+00:00",
        "updated_date": "2025-09-15T14:10:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Debopom Sutradhar",
            "Arefin Ittesafun Abian",
            "Mohaimenul Azam Khan Raiaan",
            "Reem E. Mohamed",
            "Sheikh Izzal Azid",
            "Sami Azam"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "AIGC"
        ],
        "tldr": "Proposes a dual encoder network for land cover segmentation using optical and SAR imagery with a novel fusion mechanism named CLAIRE. Incorporates a hybrid loss function and a metric-driven reasoning module for improved performance and interpretability.",
        "tldr_zh": "提出了一种双编码器网络，用于利用光学和SAR图像进行土地覆盖分割，采用名为CLAIRE的新型融合机制。结合了混合损失函数和度量驱动的推理模块，以提高性能和解释性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Sphere-GAN: a GAN-based Approach for Saliency Estimation in 360° Videos",
        "summary": "The recent success of immersive applications is pushing the research\ncommunity to define new approaches to process 360{\\deg} images and videos and\noptimize their transmission. Among these, saliency estimation provides a\npowerful tool that can be used to identify visually relevant areas and,\nconsequently, adapt processing algorithms. Although saliency estimation has\nbeen widely investigated for 2D content, very few algorithms have been proposed\nfor 360{\\deg} saliency estimation. Towards this goal, we introduce Sphere-GAN,\na saliency detection model for 360{\\deg} videos that leverages a Generative\nAdversarial Network with spherical convolutions. Extensive experiments were\nconducted using a public 360{\\deg} video saliency dataset, and the results\ndemonstrate that Sphere-GAN outperforms state-of-the-art models in accurately\npredicting saliency maps.",
        "url": "http://arxiv.org/abs/2509.11948v1",
        "published_date": "2025-09-15T14:07:33+00:00",
        "updated_date": "2025-09-15T14:07:33+00:00",
        "categories": [
            "cs.CV",
            "cs.MM",
            "eess.IV"
        ],
        "authors": [
            "Mahmoud Z. A. Wahba",
            "Sara Baldoni",
            "Federica Battisti"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "Sphere-GAN is a GAN-based model for saliency estimation in 360° videos, outperforming existing models in predicting saliency maps.",
        "tldr_zh": "Sphere-GAN 是一个用于在 360° 视频中估计显著性的基于 GAN 的模型，优于现有模型在预测显著性图中的表现。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Graph Algorithm Unrolling with Douglas-Rachford Iterations for Image Interpolation with Guaranteed Initialization",
        "summary": "Conventional deep neural nets (DNNs) initialize network parameters at random\nand then optimize each one via stochastic gradient descent (SGD), resulting in\nsubstantial risk of poor-performing local minima.Focusing on the image\ninterpolation problem and leveraging a recent theorem that maps a\n(pseudo-)linear interpolator {\\Theta} to a directed graph filter that is a\nsolution to a MAP problem regularized with a graph shift variation (GSV) prior,\nwe first initialize a directed graph adjacency matrix A based on a known\ninterpolator {\\Theta}, establishing a baseline performance.Then, towards\nfurther gain, we learn perturbation matrices P and P(2) from data to augment A,\nwhose restoration effects are implemented via Douglas-Rachford (DR) iterations,\nwhich we unroll into a lightweight interpretable neural net.Experimental\nresults demonstrate state-of-the-art image interpolation results, while\ndrastically reducing network parameters.",
        "url": "http://arxiv.org/abs/2509.11926v1",
        "published_date": "2025-09-15T13:43:55+00:00",
        "updated_date": "2025-09-15T13:43:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xue Zhang",
            "Bingshuo Hu",
            "Gene Cheung"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer"
        ],
        "tldr": "The paper introduces a graph algorithm unrolling method using Douglas-Rachford iterations for image interpolation with guaranteed initialization, achieving state-of-the-art results with reduced network parameters.",
        "tldr_zh": "本文介绍了一种使用Douglas-Rachford迭代的图算法展开方法，用于具有保证初始化的图像插值，实现了最先进的结果并减少了网络参数。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Do It Yourself (DIY): Modifying Images for Poems in a Zero-Shot Setting Using Weighted Prompt Manipulation",
        "summary": "Poetry is an expressive form of art that invites multiple interpretations, as\nreaders often bring their own emotions, experiences, and cultural backgrounds\ninto their understanding of a poem. Recognizing this, we aim to generate images\nfor poems and improve these images in a zero-shot setting, enabling audiences\nto modify images as per their requirements. To achieve this, we introduce a\nnovel Weighted Prompt Manipulation (WPM) technique, which systematically\nmodifies attention weights and text embeddings within diffusion models. By\ndynamically adjusting the importance of specific words, WPM enhances or\nsuppresses their influence in the final generated image, leading to\nsemantically richer and more contextually accurate visualizations. Our approach\nexploits diffusion models and large language models (LLMs) such as GPT in\nconjunction with existing poetry datasets, ensuring a comprehensive and\nstructured methodology for improved image generation in the literary domain. To\nthe best of our knowledge, this is the first attempt at integrating weighted\nprompt manipulation for enhancing imagery in poetic language.",
        "url": "http://arxiv.org/abs/2509.11878v1",
        "published_date": "2025-09-15T12:58:38+00:00",
        "updated_date": "2025-09-15T12:58:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sofia Jamil",
            "Kotla Sai Charan",
            "Sriparna Saha",
            "Koustava Goswami",
            "K J Joseph"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a novel technique called Weighted Prompt Manipulation for modifying images for poems in a zero-shot setting, enhancing visualizations based on specific words.",
        "tldr_zh": "本文引入一种名为加权提示操作的新技术，用于在零射设置中修改诗歌图片，根据特定单词增强视觉效果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding",
        "summary": "Recent advancements in large video models (LVMs) have significantly enhance\nvideo understanding. However, these models continue to suffer from\nhallucinations, producing content that conflicts with input videos. To address\nthis issue, we propose Dr.V, a hierarchical framework covering perceptive,\ntemporal, and cognitive levels to diagnose video hallucination by fine-grained\nspatial-temporal grounding. Dr.V comprises of two key components: a benchmark\ndataset Dr.V-Bench and a satellite video agent Dr.V-Agent. Dr.V-Bench includes\n10k instances drawn from 4,974 videos spanning diverse tasks, each enriched\nwith detailed spatial-temporal annotation. Dr.V-Agent detects hallucinations in\nLVMs by systematically applying fine-grained spatial-temporal grounding at the\nperceptive and temporal levels, followed by cognitive level reasoning. This\nstep-by-step pipeline mirrors human-like video comprehension and effectively\nidentifies hallucinations. Extensive experiments demonstrate that Dr.V-Agent is\neffective in diagnosing hallucination while enhancing interpretability and\nreliability, offering a practical blueprint for robust video understanding in\nreal-world scenarios. All our data and code are available at\nhttps://github.com/Eurekaleo/Dr.V.",
        "url": "http://arxiv.org/abs/2509.11866v1",
        "published_date": "2025-09-15T12:39:19+00:00",
        "updated_date": "2025-09-15T12:39:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Meng Luo",
            "Shengqiong Wu",
            "Liqiang Jing",
            "Tianjie Ju",
            "Li Zheng",
            "Jinxiang Lai",
            "Tianlong Wu",
            "Xinya Du",
            "Jian Li",
            "Siyuan Yan",
            "Jiebo Luo",
            "William Yang Wang",
            "Hao Fei",
            "Mong-Li Lee",
            "Wynne Hsu"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer",
            "AIGC"
        ],
        "tldr": "Dr.V proposes a hierarchical framework to diagnose video hallucination by fine-grained spatial-temporal grounding, offering a practical blueprint for robust video understanding in real-world scenarios.",
        "tldr_zh": "Dr.V提出了一个层次结构框架，通过精细的时空基础诊断视频幻觉，在实际场景中提供强大的视频理解蓝图。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Bridging Vision Language Models and Symbolic Grounding for Video Question Answering",
        "summary": "Video Question Answering (VQA) requires models to reason over spatial,\ntemporal, and causal cues in videos. Recent vision language models (VLMs)\nachieve strong results but often rely on shallow correlations, leading to weak\ntemporal grounding and limited interpretability. We study symbolic scene graphs\n(SGs) as intermediate grounding signals for VQA. SGs provide structured\nobject-relation representations that complement VLMs holistic reasoning. We\nintroduce SG-VLM, a modular framework that integrates frozen VLMs with scene\ngraph grounding via prompting and visual localization. Across three benchmarks\n(NExT-QA, iVQA, ActivityNet-QA) and multiple VLMs (QwenVL, InternVL), SG-VLM\nimproves causal and temporal reasoning and outperforms prior baselines, though\ngains over strong VLMs are limited. These findings highlight both the promise\nand current limitations of symbolic grounding, and offer guidance for future\nhybrid VLM-symbolic approaches in video understanding.",
        "url": "http://arxiv.org/abs/2509.11862v1",
        "published_date": "2025-09-15T12:35:56+00:00",
        "updated_date": "2025-09-15T12:35:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Haodi Ma",
            "Vyom Pathak",
            "Daisy Zhe Wang"
        ],
        "ai_categories": [
            "Multimodality"
        ],
        "tldr": "The paper introduces SG-VLM, a framework that combines vision language models with symbolic scene graphs for better video question answering by improving temporal reasoning and surpassing previous baselines.",
        "tldr_zh": "该论文介绍了SG-VLM框架，它将视觉语言模型与符号场景图结合起来，通过提高时间推理能力并超越先前的基线，从而改善视频问答。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Segmentation-Driven Initialization for Sparse-view 3D Gaussian Splatting",
        "summary": "Sparse-view synthesis remains a challenging problem due to the difficulty of\nrecovering accurate geometry and appearance from limited observations. While\nrecent advances in 3D Gaussian Splatting (3DGS) have enabled real-time\nrendering with competitive quality, existing pipelines often rely on\nStructure-from-Motion (SfM) for camera pose estimation, an approach that\nstruggles in genuinely sparse-view settings. Moreover, several SfM-free methods\nreplace SfM with multi-view stereo (MVS) models, but generate massive numbers\nof 3D Gaussians by back-projecting every pixel into 3D space, leading to high\nmemory costs. We propose Segmentation-Driven Initialization for Gaussian\nSplatting (SDI-GS), a method that mitigates inefficiency by leveraging\nregion-based segmentation to identify and retain only structurally significant\nregions. This enables selective downsampling of the dense point cloud,\npreserving scene fidelity while substantially reducing Gaussian count.\nExperiments across diverse benchmarks show that SDI-GS reduces Gaussian count\nby up to 50% and achieves comparable or superior rendering quality in PSNR and\nSSIM, with only marginal degradation in LPIPS. It further enables faster\ntraining and lower memory footprint, advancing the practicality of 3DGS for\nconstrained-view scenarios.",
        "url": "http://arxiv.org/abs/2509.11853v1",
        "published_date": "2025-09-15T12:31:33+00:00",
        "updated_date": "2025-09-15T12:31:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yi-Hsin Li",
            "Thomas Sikora",
            "Sebastian Knorr",
            "Måarten Sjöström"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes a method called Segmentation-Driven Initialization for Gaussian Splatting (SDI-GS) to reduce memory costs and improve rendering quality in sparse-view 3D Gaussian splatting.",
        "tldr_zh": "本文提出了一种名为分割驱动初始化高斯分层（SDI-GS）的方法，以减少稀疏视图3D高斯飞溅中的内存成本并提高渲染质量。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Synthetic Captions for Open-Vocabulary Zero-Shot Segmentation",
        "summary": "Generative vision-language models (VLMs) exhibit strong high-level image\nunderstanding but lack spatially dense alignment between vision and language\nmodalities, as our findings indicate. Orthogonal to advancements in generative\nVLMs, another line of research has focused on representation learning for\nvision-language alignment, targeting zero-shot inference for dense tasks like\nsegmentation. In this work, we bridge these two directions by densely aligning\nimages with synthetic descriptions generated by VLMs. Synthetic captions are\ninexpensive, scalable, and easy to generate, making them an excellent source of\nhigh-level semantic understanding for dense alignment methods. Empirically, our\napproach outperforms prior work on standard zero-shot open-vocabulary\nsegmentation benchmarks/datasets, while also being more data-efficient.",
        "url": "http://arxiv.org/abs/2509.11840v1",
        "published_date": "2025-09-15T12:26:47+00:00",
        "updated_date": "2025-09-15T12:26:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tim Lebailly",
            "Vijay Veerabadran",
            "Satwik Kottur",
            "Karl Ridgeway",
            "Michael Louis Iuzzolino"
        ],
        "ai_categories": [
            "Multimodality",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a method that aligns images with synthetic captions generated by vision-language models for zero-shot segmentation, achieving better performance than previous methods.",
        "tldr_zh": "本文提出了一种方法，通过视觉-语言模型生成的合成描述来对齐图像，用于零样本分割，比先前的方法表现更好。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MAFS: Masked Autoencoder for Infrared-Visible Image Fusion and Semantic Segmentation",
        "summary": "Infrared-visible image fusion methods aim at generating fused images with\ngood visual quality and also facilitate the performance of high-level tasks.\nIndeed, existing semantic-driven methods have considered semantic information\ninjection for downstream applications. However, none of them investigates the\npotential for reciprocal promotion between pixel-wise image fusion and\ncross-modal feature fusion perception tasks from a macroscopic task-level\nperspective. To address this limitation, we propose a unified network for image\nfusion and semantic segmentation. MAFS is a parallel structure, containing a\nfusion sub-network and a segmentation sub-network. On the one hand, We devise a\nheterogeneous feature fusion strategy to enhance semantic-aware capabilities\nfor image fusion. On the other hand, by cascading the fusion sub-network and a\nsegmentation backbone, segmentation-related knowledge is transferred to promote\nfeature-level fusion-based segmentation. Within the framework, we design a\nnovel multi-stage Transformer decoder to aggregate fine-grained multi-scale\nfused features efficiently. Additionally, a dynamic factor based on the max-min\nfairness allocation principle is introduced to generate adaptive weights of two\ntasks and guarantee smooth training in a multi-task manner. Extensive\nexperiments demonstrate that our approach achieves competitive results compared\nwith state-of-the-art methods. The code is available at\nhttps://github.com/Abraham-Einstein/MAFS/.",
        "url": "http://arxiv.org/abs/2509.11817v1",
        "published_date": "2025-09-15T11:55:55+00:00",
        "updated_date": "2025-09-15T11:55:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Liying Wang",
            "Xiaoli Zhang",
            "Chuanmin Jia",
            "Siwei Ma"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "Proposed MAFS model combines image fusion and semantic segmentation for reciprocal promotion, achieving competitive results with state-of-the-art methods.",
        "tldr_zh": "提出的MAFS模型将图像融合和语义分割相结合，实现了与最先进方法竞争的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SpecVLM: Fast Speculative Decoding in Vision-Language Models",
        "summary": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM.",
        "url": "http://arxiv.org/abs/2509.11815v1",
        "published_date": "2025-09-15T11:53:56+00:00",
        "updated_date": "2025-09-15T11:53:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Haiduo Huang",
            "Fuwei Yang",
            "Zhenhua Liu",
            "Xuanwu Yin",
            "Dong Li",
            "Pengju Ren",
            "Emad Barsoum"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "SpecVLM introduces fast speculative decoding for vision-language models, achieving significant speedups without sacrificing model output quality.",
        "tldr_zh": "SpecVLM为视觉-语言模型引入了快速的推测解码，实现了显著的加速，同时保持了模型输出质量。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Pseudo-D: Informing Multi-View Uncertainty Estimation with Calibrated Neural Training Dynamics",
        "summary": "Computer-aided diagnosis systems must make critical decisions from medical\nimages that are often noisy, ambiguous, or conflicting, yet today's models are\ntrained on overly simplistic labels that ignore diagnostic uncertainty. One-hot\nlabels erase inter-rater variability and force models to make overconfident\npredictions, especially when faced with incomplete or artifact-laden inputs. We\naddress this gap by introducing a novel framework that brings uncertainty back\ninto the label space. Our method leverages neural network training dynamics\n(NNTD) to assess the inherent difficulty of each training sample. By\naggregating and calibrating model predictions during training, we generate\nuncertainty-aware pseudo-labels that reflect the ambiguity encountered during\nlearning. This label augmentation approach is architecture-agnostic and can be\napplied to any supervised learning pipeline to enhance uncertainty estimation\nand robustness. We validate our approach on a challenging echocardiography\nclassification benchmark, demonstrating superior performance over specialized\nbaselines in calibration, selective classification, and multi-view fusion.",
        "url": "http://arxiv.org/abs/2509.11800v1",
        "published_date": "2025-09-15T11:30:12+00:00",
        "updated_date": "2025-09-15T11:30:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ang Nan Gu",
            "Michael Tsang",
            "Hooman Vaseli",
            "Purang Abolmaesumi",
            "Teresa Tsang"
        ],
        "ai_categories": [
            "Other"
        ],
        "tldr": "The paper proposes a framework to incorporate uncertainty into label space for better model performance in medical image classification.",
        "tldr_zh": "本文提出了一个框架，将不确定性纳入标签空间，以提高医学图像分类模型的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MSMA: Multi-Scale Feature Fusion For Multi-Attribute 3D Face Reconstruction From Unconstrained Images",
        "summary": "Reconstructing 3D face from a single unconstrained image remains a\nchallenging problem due to diverse conditions in unconstrained environments.\nRecently, learning-based methods have achieved notable results by effectively\ncapturing complex facial structures and details across varying conditions.\nConsequently, many existing approaches employ projection-based losses between\ngenerated and input images to constrain model training. However, learning-based\nmethods for 3D face reconstruction typically require substantial amounts of 3D\nfacial data, which is difficult and costly to obtain. Consequently, to reduce\nreliance on labeled 3D face datasets, many existing approaches employ\nprojection-based losses between generated and input images to constrain model\ntraining. Nonetheless, despite these advancements, existing approaches\nfrequently struggle to capture detailed and multi-scale features under diverse\nfacial attributes and conditions, leading to incomplete or less accurate\nreconstructions. In this paper, we propose a Multi-Scale Feature Fusion with\nMulti-Attribute (MSMA) framework for 3D face reconstruction from unconstrained\nimages. Our method integrates multi-scale feature fusion with a focus on\nmulti-attribute learning and leverages a large-kernel attention module to\nenhance the precision of feature extraction across scales, enabling accurate 3D\nfacial parameter estimation from a single 2D image. Comprehensive experiments\non the MICC Florence, Facewarehouse and custom-collect datasets demonstrate\nthat our approach achieves results on par with current state-of-the-art\nmethods, and in some instances, surpasses SOTA performance across challenging\nconditions.",
        "url": "http://arxiv.org/abs/2509.11763v1",
        "published_date": "2025-09-15T10:30:08+00:00",
        "updated_date": "2025-09-15T10:30:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Danling Cao"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a Multi-Scale Feature Fusion with Multi-Attribute framework for 3D face reconstruction from unconstrained images, achieving results on par with state-of-the-art methods.",
        "tldr_zh": "本文介绍了一个用于从无约束图像中进行3D面部重建的多尺度特征融合与多属性框架，其结果与现有最先进的方法相当。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "The Quest for Universal Master Key Filters in DS-CNNs",
        "summary": "A recent study has proposed the \"Master Key Filters Hypothesis\" for\nconvolutional neural network filters. This paper extends this hypothesis by\nradically constraining its scope to a single set of just 8 universal filters\nthat depthwise separable convolutional networks inherently converge to. While\nconventional DS-CNNs employ thousands of distinct trained filters, our analysis\nreveals these filters are predominantly linear shifts (ax+b) of our discovered\nuniversal set. Through systematic unsupervised search, we extracted these\nfundamental patterns across different architectures and datasets. Remarkably,\nnetworks initialized with these 8 unique frozen filters achieve over 80%\nImageNet accuracy, and even outperform models with thousands of trainable\nparameters when applied to smaller datasets. The identified master key filters\nclosely match Difference of Gaussians (DoGs), Gaussians, and their derivatives,\nstructures that are not only fundamental to classical image processing but also\nstrikingly similar to receptive fields in mammalian visual systems. Our\nfindings provide compelling evidence that depthwise convolutional layers\nnaturally gravitate toward this fundamental set of spatial operators regardless\nof task or architecture. This work offers new insights for understanding\ngeneralization and transfer learning through the universal language of these\nmaster key filters.",
        "url": "http://arxiv.org/abs/2509.11711v1",
        "published_date": "2025-09-15T09:10:13+00:00",
        "updated_date": "2025-09-15T09:10:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zahra Babaiee",
            "Peyman M. Kiassari",
            "Daniela Rus",
            "Radu Grosu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes the existence of a set of 8 universal master key filters that depthwise separable convolutional networks naturally converge to, showing superior performance compared to traditional networks with thousands of filters.",
        "tldr_zh": "本文提出了一组仅包含8个通用主键滤波器的深度可分卷积网络，显示出与具有数千个滤波器的传统网络相比更高的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MindVL: Towards Efficient and Effective Training of Multimodal Large Language Models on Ascend NPUs",
        "summary": "We propose MindVL, a multimodal large langauge model trained on Ascend NPUs.\nSimilar to Qwen2.5-VL, MindVL adopts native-resolution Vision Transformers,\nwhich enables it to process images at their original variable resolutions. This\ndesign avoids the degradation caused by fixed-resolution tiling while\npreserving fine-grained details and global layouts, which is crucial for\nvisually dense content such as complex charts and diagrams. To ensure the\nsmooth training of MindVL on Ascend NPUs, we develop Mindspeed-MLLM, a\ndistributed multimodal training framework tailored for Ascend NPUs. To maintain\ntraining accuracy, we implement equivalent replacements for certain operators.\nMindVL undergoes a three-phase training process, namely the warm-up phase,\nmultitask training phase, and supervised instruction tuning phase, to gradually\nenhance its capabilities. This process starts with basic visual and multimodal\npre-training, followed by large-scale multiask trainging and instruction\ntuning. We also adopt multimodal data packaging and hybrid parallelism\ntechniques, which significantly improve end-to-end training speed. To further\nboost model performance, we specifically introduce test-time resolution search\nand model weight averaging. Notably, despite using about 1/10 of the training\ndata required by Qwen2.5-VL, MindVL achieves performance on par with Qwen2.5-VL\nin evaluations of general multimodal understanding and document/table\ncomprehension. Beyond overall scores, MindVL also delivers leading performance\nin OCR assessments.",
        "url": "http://arxiv.org/abs/2509.11662v1",
        "published_date": "2025-09-15T08:00:31+00:00",
        "updated_date": "2025-09-15T08:00:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "eess.IV"
        ],
        "authors": [
            "Feilong Chen",
            "Yijiang Liu",
            "Yi Huang",
            "Hao Wang",
            "Miren Tian",
            "Ya-Qi Yu",
            "Minghui Liao",
            "Jihao Wu"
        ],
        "ai_categories": [
            "Transformer",
            "AIGC",
            "Dataset"
        ],
        "tldr": "MindVL is a multimodal large language model trained on Ascend NPUs, achieving performance on par with Qwen2.5-VL with improved speed.",
        "tldr_zh": "MindVL是在Ascend NPUs上训练的多模态大型语言模型，在速度上与Qwen2.5-VL相当，并有所提高。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Joint-octamamba:an octa joint segmentation network based on feature enhanced mamba",
        "summary": "OCTA is a crucial non-invasive imaging technique for diagnosing and\nmonitoring retinal diseases like diabetic retinopathy, age-related macular\ndegeneration, and glaucoma. Current 2D-based methods for retinal vessel (RV)\nsegmentation offer insufficient accuracy. To address this, we propose RVMamba,\na novel architecture integrating multiple feature extraction modules with the\nMamba state-space model. Moreover, existing joint segmentation models for OCTA\ndata exhibit performance imbalance between different tasks. To simultaneously\nimprove the segmentation of the foveal avascular zone (FAZ) and mitigate this\nimbalance, we introduce FAZMamba and a unified Joint-OCTAMamba framework.\nExperimental results on the OCTA-500 dataset demonstrate that Joint-OCTAMamba\noutperforms existing models across evaluation metrics.The code is available at\nhttps://github.com/lc-sfis/Joint-OCTAMamba.",
        "url": "http://arxiv.org/abs/2509.11649v1",
        "published_date": "2025-09-15T07:36:21+00:00",
        "updated_date": "2025-09-15T07:36:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chuang Liu",
            "Nan Guo"
        ],
        "ai_categories": [
            "Dataset",
            "GAN",
            "Transformer"
        ],
        "tldr": "The paper introduces Joint-OCTAMamba, a segmentation network for retinal images using OCTA data to improve accuracy and balance performance, outperforming existing models on the OCTA-500 dataset.",
        "tldr_zh": "该论文介绍了Joint-OCTAMamba，一种利用OCTA数据进行视网膜图像分割的网络，以提高准确性和平衡性能，在OCTA-500数据集上优于现有模型。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "WeatherBench: A Real-World Benchmark Dataset for All-in-One Adverse Weather Image Restoration",
        "summary": "Existing all-in-one image restoration approaches, which aim to handle\nmultiple weather degradations within a single framework, are predominantly\ntrained and evaluated using mixed single-weather synthetic datasets. However,\nthese datasets often differ significantly in resolution, style, and domain\ncharacteristics, leading to substantial domain gaps that hinder the development\nand fair evaluation of unified models. Furthermore, the lack of a large-scale,\nreal-world all-in-one weather restoration dataset remains a critical bottleneck\nin advancing this field. To address these limitations, we present a real-world\nall-in-one adverse weather image restoration benchmark dataset, which contains\nimage pairs captured under various weather conditions, including rain, snow,\nand haze, as well as diverse outdoor scenes and illumination settings. The\nresulting dataset provides precisely aligned degraded and clean images,\nenabling supervised learning and rigorous evaluation. We conduct comprehensive\nexperiments by benchmarking a variety of task-specific, task-general, and\nall-in-one restoration methods on our dataset. Our dataset offers a valuable\nfoundation for advancing robust and practical all-in-one image restoration in\nreal-world scenarios. The dataset has been publicly released and is available\nat https://github.com/guanqiyuan/WeatherBench.",
        "url": "http://arxiv.org/abs/2509.11642v1",
        "published_date": "2025-09-15T07:24:29+00:00",
        "updated_date": "2025-09-15T07:24:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qiyuan Guan",
            "Qianfeng Yang",
            "Xiang Chen",
            "Tianyu Song",
            "Guiyue Jin",
            "Jiyu Jin"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper introduces a real-world benchmark dataset for all-in-one adverse weather image restoration, aiming to address the limitations of existing synthetic datasets and advance the field of weather restoration.",
        "tldr_zh": "该论文介绍了一个真实世界的基准数据集，用于全方位逆天气图像恢复，旨在解决现有合成数据集的局限性并推进天气恢复领域。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "IS-Diff: Improving Diffusion-Based Inpainting with Better Initial Seed",
        "summary": "Diffusion models have shown promising results in free-form inpainting. Recent\nstudies based on refined diffusion samplers or novel architectural designs led\nto realistic results and high data consistency. However, random initialization\nseed (noise) adopted in vanilla diffusion process may introduce mismatched\nsemantic information in masked regions, leading to biased inpainting results,\ne.g., low consistency and low coherence with the other unmasked area. To\naddress this issue, we propose the Initial Seed refined Diffusion Model\n(IS-Diff), a completely training-free approach incorporating distributional\nharmonious seeds to produce harmonious results. Specifically, IS-Diff employs\ninitial seeds sampled from unmasked areas to imitate the masked data\ndistribution, thereby setting a promising direction for the diffusion\nprocedure. Moreover, a dynamic selective refinement mechanism is proposed to\ndetect severe unharmonious inpaintings in intermediate latent and adjust the\nstrength of our initialization prior dynamically. We validate our method on\nboth standard and large-mask inpainting tasks using the CelebA-HQ, ImageNet,\nand Places2 datasets, demonstrating its effectiveness across all metrics\ncompared to state-of-the-art inpainting methods.",
        "url": "http://arxiv.org/abs/2509.11638v1",
        "published_date": "2025-09-15T07:16:03+00:00",
        "updated_date": "2025-09-15T07:16:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yongzhe Lyu",
            "Yu Wu",
            "Yutian Lin",
            "Bo Du"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces IS-Diff, a training-free approach for inpainting that improves results by using initial seeds sampled from unmasked areas to imitate masked data distribution, leading to more harmonious inpainting results.",
        "tldr_zh": "该论文介绍了IS-Diff，一个无需训练的修复方法，通过使用从未遮挡区域采样的初始种子来模拟被遮挡数据分布，从而改善修复结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SpeCa: Accelerating Diffusion Transformers with Speculative Feature Caching",
        "summary": "Diffusion models have revolutionized high-fidelity image and video synthesis,\nyet their computational demands remain prohibitive for real-time applications.\nThese models face two fundamental challenges: strict temporal dependencies\npreventing parallelization, and computationally intensive forward passes\nrequired at each denoising step. Drawing inspiration from speculative decoding\nin large language models, we present SpeCa, a novel 'Forecast-then-verify'\nacceleration framework that effectively addresses both limitations. SpeCa's\ncore innovation lies in introducing Speculative Sampling to diffusion models,\npredicting intermediate features for subsequent timesteps based on fully\ncomputed reference timesteps. Our approach implements a parameter-free\nverification mechanism that efficiently evaluates prediction reliability,\nenabling real-time decisions to accept or reject each prediction while\nincurring negligible computational overhead. Furthermore, SpeCa introduces\nsample-adaptive computation allocation that dynamically modulates resources\nbased on generation complexity, allocating reduced computation for simpler\nsamples while preserving intensive processing for complex instances.\nExperiments demonstrate 6.34x acceleration on FLUX with minimal quality\ndegradation (5.5% drop), 7.3x speedup on DiT while preserving generation\nfidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The\nverification mechanism incurs minimal overhead (1.67%-3.5% of full inference\ncosts), establishing a new paradigm for efficient diffusion model inference\nwhile maintaining generation quality even at aggressive acceleration ratios.\nOur codes have been released in Github:\n\\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}",
        "url": "http://arxiv.org/abs/2509.11628v1",
        "published_date": "2025-09-15T06:46:22+00:00",
        "updated_date": "2025-09-15T06:46:22+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Jiacheng Liu",
            "Chang Zou",
            "Yuanhuiyi Lyu",
            "Fei Ren",
            "Shaobo Wang",
            "Kaixin Li",
            "Linfeng Zhang"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces SpeCa, a framework for accelerating diffusion models in image and video synthesis by predicting intermediate features and dynamically allocating computation resources.",
        "tldr_zh": "本文介绍了SpeCa，一种用于加速扩散模型在图像和视频合成中的框架，通过预测中间特征和动态分配计算资源。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Controllable 3D Deepfake Generation Framework with Gaussian Splatting",
        "summary": "We propose a novel 3D deepfake generation framework based on 3D Gaussian\nSplatting that enables realistic, identity-preserving face swapping and\nreenactment in a fully controllable 3D space. Compared to conventional 2D\ndeepfake approaches that suffer from geometric inconsistencies and limited\ngeneralization to novel view, our method combines a parametric head model with\ndynamic Gaussian representations to support multi-view consistent rendering,\nprecise expression control, and seamless background integration. To address\nediting challenges in point-based representations, we explicitly separate the\nhead and background Gaussians and use pre-trained 2D guidance to optimize the\nfacial region across views. We further introduce a repair module to enhance\nvisual consistency under extreme poses and expressions. Experiments on\nNeRSemble and additional evaluation videos demonstrate that our method achieves\ncomparable performance to state-of-the-art 2D approaches in identity\npreservation, as well as pose and expression consistency, while significantly\noutperforming them in multi-view rendering quality and 3D consistency. Our\napproach bridges the gap between 3D modeling and deepfake synthesis, enabling\nnew directions for scene-aware, controllable, and immersive visual forgeries,\nrevealing the threat that emerging 3D Gaussian Splatting technique could be\nused for manipulation attacks.",
        "url": "http://arxiv.org/abs/2509.11624v1",
        "published_date": "2025-09-15T06:34:17+00:00",
        "updated_date": "2025-09-15T06:34:17+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Wending Liu",
            "Siyun Liang",
            "Huy H. Nguyen",
            "Isao Echizen"
        ],
        "ai_categories": [
            "GAN",
            "Transformer"
        ],
        "tldr": "The paper introduces a novel 3D deepfake generation framework using Gaussian Splatting for realistic and controllable face swapping in a 3D space.",
        "tldr_zh": "该论文介绍了一种使用高斯Splatting的新型3D深度伪造生成框架，用于在3D空间中进行逼真和可控的人脸交换。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Disentangling Content from Style to Overcome Shortcut Learning: A Hybrid Generative-Discriminative Learning Framework",
        "summary": "Despite the remarkable success of Self-Supervised Learning (SSL), its\ngeneralization is fundamentally hindered by Shortcut Learning, where models\nexploit superficial features like texture instead of intrinsic structure. We\nexperimentally verify this flaw within the generative paradigm (e.g., MAE) and\nargue it is a systemic issue also affecting discriminative methods, identifying\nit as the root cause of their failure on unseen domains. While existing methods\noften tackle this at a surface level by aligning or separating domain-specific\nfeatures, they fail to alter the underlying learning mechanism that fosters\nshortcut dependency. To address this at its core, we propose HyGDL (Hybrid\nGenerative-Discriminative Learning Framework), a hybrid framework that achieves\nexplicit content-style disentanglement. Our approach is guided by the\nInvariance Pre-training Principle: forcing a model to learn an invariant\nessence by systematically varying a bias (e.g., style) at the input while\nkeeping the supervision signal constant. HyGDL operates on a single encoder and\nanalytically defines style as the component of a representation that is\northogonal to its style-invariant content, derived via vector projection.",
        "url": "http://arxiv.org/abs/2509.11598v1",
        "published_date": "2025-09-15T05:28:32+00:00",
        "updated_date": "2025-09-15T05:28:32+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Siming Fu",
            "Sijun Dong",
            "Xiaoliang Meng"
        ],
        "ai_categories": [
            "GAN"
        ],
        "tldr": "The paper proposes a hybrid learning framework to disentangle content from style in order to overcome shortcut learning in models, aiming to improve generalization in self-supervised learning.",
        "tldr_zh": "本文提出了一种混合学习框架，以解耦内容和风格，以克服模型中的捷径学习，旨在改善自监督学习的泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Modality-Aware Infrared and Visible Image Fusion with Target-Aware Supervision",
        "summary": "Infrared and visible image fusion (IVIF) is a fundamental task in multi-modal\nperception that aims to integrate complementary structural and textural cues\nfrom different spectral domains. In this paper, we propose FusionNet, a novel\nend-to-end fusion framework that explicitly models inter-modality interaction\nand enhances task-critical regions. FusionNet introduces a modality-aware\nattention mechanism that dynamically adjusts the contribution of infrared and\nvisible features based on their discriminative capacity. To achieve\nfine-grained, interpretable fusion, we further incorporate a pixel-wise alpha\nblending module, which learns spatially-varying fusion weights in an adaptive\nand content-aware manner. Moreover, we formulate a target-aware loss that\nleverages weak ROI supervision to preserve semantic consistency in regions\ncontaining important objects (e.g., pedestrians, vehicles). Experiments on the\npublic M3FD dataset demonstrate that FusionNet generates fused images with\nenhanced semantic preservation, high perceptual quality, and clear\ninterpretability. Our framework provides a general and extensible solution for\nsemantic-aware multi-modal image fusion, with benefits for downstream tasks\nsuch as object detection and scene understanding.",
        "url": "http://arxiv.org/abs/2509.11476v1",
        "published_date": "2025-09-14T23:44:15+00:00",
        "updated_date": "2025-09-14T23:44:15+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Tianyao Sun",
            "Dawei Xiang",
            "Tianqi Ding",
            "Xiang Fang",
            "Yijiashun Qi",
            "Zunduo Zhao"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces FusionNet, a model for infrared and visible image fusion that enhances task-critical regions by adjusting feature contributions and utilizing target-aware supervision.",
        "tldr_zh": "本文介绍了FusionNet，一种用于红外和可见图像融合的模型，通过调整特征贡献并利用目标感知监督来增强关键区域。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MultiMAE for Brain MRIs: Robustness to Missing Inputs Using Multi-Modal Masked Autoencoder",
        "summary": "Missing input sequences are common in medical imaging data, posing a\nchallenge for deep learning models reliant on complete input data. In this\nwork, inspired by MultiMAE [2], we develop a masked autoencoder (MAE) paradigm\nfor multi-modal, multi-task learning in 3D medical imaging with brain MRIs. Our\nmethod treats each MRI sequence as a separate input modality, leveraging a\nlate-fusion-style transformer encoder to integrate multi-sequence information\n(multi-modal) and individual decoder streams for each modality for multi-task\nreconstruction. This pretraining strategy guides the model to learn rich\nrepresentations per modality while also equipping it to handle missing inputs\nthrough cross-sequence reasoning. The result is a flexible and generalizable\nencoder for brain MRIs that infers missing sequences from available inputs and\ncan be adapted to various downstream applications. We demonstrate the\nperformance and robustness of our method against an MAE-ViT baseline in\ndownstream segmentation and classification tasks, showing absolute improvement\nof $10.1$ overall Dice score and $0.46$ MCC over the baselines with missing\ninput sequences. Our experiments demonstrate the strength of this pretraining\nstrategy. The implementation is made available.",
        "url": "http://arxiv.org/abs/2509.11442v1",
        "published_date": "2025-09-14T21:33:59+00:00",
        "updated_date": "2025-09-14T21:33:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ayhan Can Erdur",
            "Christian Beischl",
            "Daniel Scholz",
            "Jiazhen Pan",
            "Benedikt Wiestler",
            "Daniel Rueckert",
            "Jan C Peeken"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a masked autoencoder paradigm for multi-task learning in brain MRIs, which can handle missing input sequences through cross-sequence reasoning and shows improved performance in segmentation and classification tasks.",
        "tldr_zh": "本文引入了一种用于脑MRI的遮蔽自动编码器范例，可以通过跨序列推理处理缺失的输入序列，并在分割和分类任务中表现出改善的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations",
        "summary": "Vision-language-action (VLA) models finetuned from vision-language models\n(VLMs) hold the promise of leveraging rich pretrained representations to build\ngeneralist robots across diverse tasks and environments. However, direct\nfine-tuning on robot data often disrupts these representations and limits\ngeneralization. We present a framework that better preserves pretrained\nfeatures while adapting them for robot manipulation. Our approach introduces\nthree components: (i) a dual-encoder design with one frozen vision encoder to\nretain pretrained features and another trainable for task adaptation, (ii) a\nstring-based action tokenizer that casts continuous actions into character\nsequences aligned with the model's pretraining domain, and (iii) a co-training\nstrategy that combines robot demonstrations with vision-language datasets\nemphasizing spatial reasoning and affordances. Evaluations in simulation and on\nreal robots show that our method improves robustness to visual perturbations,\ngeneralization to novel instructions and environments, and overall task success\ncompared to baselines.",
        "url": "http://arxiv.org/abs/2509.11417v1",
        "published_date": "2025-09-14T20:08:56+00:00",
        "updated_date": "2025-09-14T20:08:56+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Shresth Grover",
            "Akshay Gopalkrishnan",
            "Bo Ai",
            "Henrik I. Christensen",
            "Hao Su",
            "Xuanlin Li"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a framework to enhance generalization in vision-language-action models by preserving pretrained features and adapting them for robot manipulation.",
        "tldr_zh": "该论文介绍了一种框架，通过保留预训练特征并将其适应机器人操作，从而增强视觉-语言-动作模型的泛化能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "On the Skinning of Gaussian Avatars",
        "summary": "Radiance field-based methods have recently been used to reconstruct human\navatars, showing that we can significantly downscale the systems needed for\ncreating animated human avatars. Although this progress has been initiated by\nneural radiance fields, their slow rendering and backward mapping from the\nobservation space to the canonical space have been the main challenges. With\nGaussian splatting overcoming both challenges, a new family of approaches has\nemerged that are faster to train and render, while also straightforward to\nimplement using forward skinning from the canonical to the observation space.\nHowever, the linear blend skinning required for the deformation of the\nGaussians does not provide valid results for their non-linear rotation\nproperties. To address such artifacts, recent works use mesh properties to\nrotate the non-linear Gaussian properties or train models to predict corrective\noffsets. Instead, we propose a weighted rotation blending approach that\nleverages quaternion averaging. This leads to simpler vertex-based Gaussians\nthat can be efficiently animated and integrated in any engine by only modifying\nthe linear blend skinning technique, and using any Gaussian rasterizer.",
        "url": "http://arxiv.org/abs/2509.11411v1",
        "published_date": "2025-09-14T19:58:48+00:00",
        "updated_date": "2025-09-14T19:58:48+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Nikolaos Zioulis",
            "Nikolaos Kotarelas",
            "Georgios Albanis",
            "Spyridon Thermos",
            "Anargyros Chatzitofis"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a new approach for animating human avatars using Gaussian splatting and weighted rotation blending for more efficient and accurate results.",
        "tldr_zh": "本文介绍了一种使用高斯分割和加权旋转混合的新方法，用于更高效和准确地实现人类化身的动画。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "No Modality Left Behind: Dynamic Model Generation for Incomplete Medical Data",
        "summary": "In real world clinical environments, training and applying deep learning\nmodels on multi-modal medical imaging data often struggles with partially\nincomplete data. Standard approaches either discard missing samples, require\nimputation or repurpose dropout learning schemes, limiting robustness and\ngeneralizability. To address this, we propose a hypernetwork-based method that\ndynamically generates task-specific classification models conditioned on the\nset of available modalities. Instead of training a fixed model, a hypernetwork\nlearns to predict the parameters of a task model adapted to available\nmodalities, enabling training and inference on all samples, regardless of\ncompleteness. We compare this approach with (1) models trained only on complete\ndata, (2) state of the art channel dropout methods, and (3) an imputation-based\nmethod, using artificially incomplete datasets to systematically analyze\nrobustness to missing modalities. Results demonstrate superior adaptability of\nour method, outperforming state of the art approaches with an absolute increase\nin accuracy of up to 8% when trained on a dataset with 25% completeness (75% of\ntraining data with missing modalities). By enabling a single model to\ngeneralize across all modality configurations, our approach provides an\nefficient solution for real-world multi-modal medical data analysis.",
        "url": "http://arxiv.org/abs/2509.11406v1",
        "published_date": "2025-09-14T19:47:45+00:00",
        "updated_date": "2025-09-14T19:47:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Christoph Fürböck",
            "Paul Weiser",
            "Branko Mitic",
            "Philipp Seeböck",
            "Thomas Helbich",
            "Georg Langs"
        ],
        "ai_categories": [
            "Multimodality",
            "GAN"
        ],
        "tldr": "The paper proposes a method for generating dynamic models for medical data analysis, addressing the issue of incomplete modalities in multi-modal data.",
        "tldr_zh": "本文提出了一种针对医学数据分析的动态模型生成方法，解决了多模态数据中存在的不完整性问题。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MixANT: Observation-dependent Memory Propagation for Stochastic Dense Action Anticipation",
        "summary": "We present MixANT, a novel architecture for stochastic long-term dense\nanticipation of human activities. While recent State Space Models (SSMs) like\nMamba have shown promise through input-dependent selectivity on three key\nparameters, the critical forget-gate ($\\textbf{A}$ matrix) controlling temporal\nmemory remains static. We address this limitation by introducing a mixture of\nexperts approach that dynamically selects contextually relevant $\\textbf{A}$\nmatrices based on input features, enhancing representational capacity without\nsacrificing computational efficiency. Extensive experiments on the 50Salads,\nBreakfast, and Assembly101 datasets demonstrate that MixANT consistently\noutperforms state-of-the-art methods across all evaluation settings. Our\nresults highlight the importance of input-dependent forget-gate mechanisms for\nreliable prediction of human behavior in diverse real-world scenarios.",
        "url": "http://arxiv.org/abs/2509.11394v1",
        "published_date": "2025-09-14T19:07:24+00:00",
        "updated_date": "2025-09-14T19:07:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Syed Talal Wasim",
            "Hamid Suleman",
            "Olga Zatsarynna",
            "Muzammal Naseer",
            "Juergen Gall"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "MixANT is a novel architecture for anticipating human activities by dynamically selecting relevant forget-gate mechanisms based on input features, outperforming state-of-the-art methods in various datasets.",
        "tldr_zh": "MixANT 是一种新颖的架构，通过动态选择基于输入特征的相关遗忘门机制来预测人类活动，在各种数据集中表现优于现有的方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review",
        "summary": "In this paper, we present a comprehensive review of 3D human pose estimation\nand human mesh recovery from in-the-wild LiDAR point clouds. We compare\nexisting approaches across several key dimensions, and propose a structured\ntaxonomy to classify these methods. Following this taxonomy, we analyze each\nmethod's strengths, limitations, and design choices. In addition, (i) we\nperform a quantitative comparison of the three most widely used datasets,\ndetailing their characteristics; (ii) we compile unified definitions of all\nevaluation metrics; and (iii) we establish benchmark tables for both tasks on\nthese datasets to enable fair comparisons and promote progress in the field. We\nalso outline open challenges and research directions critical for advancing\nLiDAR-based 3D human understanding. Moreover, we maintain an accompanying\nwebpage that organizes papers according to our taxonomy and continuously update\nit with new studies:\nhttps://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR",
        "url": "http://arxiv.org/abs/2509.12197v1",
        "published_date": "2025-09-15T17:56:33+00:00",
        "updated_date": "2025-09-15T17:56:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Salma Galaaoui",
            "Eduardo Valle",
            "David Picard",
            "Nermin Samet"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "The paper reviews methods for estimating 3D human pose and shape from LiDAR point clouds, providing insights into strengths, limitations, and design choices, along with benchmark comparisons and future research directions.",
        "tldr_zh": "本文综述了从LiDAR点云中估计3D人体姿势和形状的方法，提供了方法的优势、限制和设计选择的见解，以及基准比较和未来研究方向。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "RailSafeNet: Visual Scene Understanding for Tram Safety",
        "summary": "Tram-human interaction safety is an important challenge, given that trams\nfrequently operate in densely populated areas, where collisions can range from\nminor injuries to fatal outcomes. This paper addresses the issue from the\nperspective of designing a solution leveraging digital image processing, deep\nlearning, and artificial intelligence to improve the safety of pedestrians,\ndrivers, cyclists, pets, and tram passengers. We present RailSafeNet, a\nreal-time framework that fuses semantic segmentation, object detection and a\nrule-based Distance Assessor to highlight track intrusions. Using only\nmonocular video, the system identifies rails, localises nearby objects and\nclassifies their risk by comparing projected distances with the standard 1435mm\nrail gauge. Experiments on the diverse RailSem19 dataset show that a\nclass-filtered SegFormer B3 model achieves 65% intersection-over-union (IoU),\nwhile a fine-tuned YOLOv8 attains 75.6% mean average precision (mAP) calculated\nat an intersection over union (IoU) threshold of 0.50. RailSafeNet therefore\ndelivers accurate, annotation-light scene understanding that can warn drivers\nbefore dangerous situations escalate. Code available at\nhttps://github.com/oValach/RailSafeNet.",
        "url": "http://arxiv.org/abs/2509.12125v1",
        "published_date": "2025-09-15T16:51:21+00:00",
        "updated_date": "2025-09-15T16:51:21+00:00",
        "categories": [
            "cs.CV",
            "68T45 (Primary), 68T07",
            "I.4.8"
        ],
        "authors": [
            "Ing. Ondrej Valach",
            "Ing. Ivan Gruber"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "RailSafeNet is a real-time framework that uses digital image processing, deep learning, and AI to improve tram safety by identifying track intrusions and warning drivers of potential dangers.",
        "tldr_zh": "RailSafeNet是一个实时框架，利用数字图像处理、深度学习和人工智能来改善有轨电车的安全性，通过识别轨道侵入并警告驾驶员潜在的危险。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "A Computer Vision Pipeline for Individual-Level Behavior Analysis: Benchmarking on the Edinburgh Pig Dataset",
        "summary": "Animal behavior analysis plays a crucial role in understanding animal\nwelfare, health status, and productivity in agricultural settings. However,\ntraditional manual observation methods are time-consuming, subjective, and\nlimited in scalability. We present a modular pipeline that leverages\nopen-sourced state-of-the-art computer vision techniques to automate animal\nbehavior analysis in a group housing environment. Our approach combines\nstate-of-the-art models for zero-shot object detection, motion-aware tracking\nand segmentation, and advanced feature extraction using vision transformers for\nrobust behavior recognition. The pipeline addresses challenges including animal\nocclusions and group housing scenarios as demonstrated in indoor pig\nmonitoring. We validated our system on the Edinburgh Pig Behavior Video Dataset\nfor multiple behavioral tasks. Our temporal model achieved 94.2% overall\naccuracy, representing a 21.2 percentage point improvement over existing\nmethods. The pipeline demonstrated robust tracking capabilities with 93.3%\nidentity preservation score and 89.3% object detection precision. The modular\ndesign suggests potential for adaptation to other contexts, though further\nvalidation across species would be required. The open-source implementation\nprovides a scalable solution for behavior monitoring, contributing to precision\npig farming and welfare assessment through automated, objective, and continuous\nanalysis.",
        "url": "http://arxiv.org/abs/2509.12047v1",
        "published_date": "2025-09-15T15:31:12+00:00",
        "updated_date": "2025-09-15T15:31:12+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Haiyu Yang",
            "Enhong Liu",
            "Jennifer Sun",
            "Sumit Sharma",
            "Meike van Leerdam",
            "Sebastien Franceschini",
            "Puchun Niu",
            "Miel Hostens"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper presents a computer vision pipeline for automated individual-level behavior analysis in animal group housing environments, achieving significant improvements over existing methods on pig behavior recognition and tracking.",
        "tldr_zh": "该论文提出了一个计算机视觉管道，用于在动物集体居住环境中自动分析个体级行为，在室内猪行为识别和跟踪方面取得了显著改进。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Enriched text-guided variational multimodal knowledge distillation network (VMD) for automated diagnosis of plaque vulnerability in 3D carotid artery MRI",
        "summary": "Multimodal learning has attracted much attention in recent years due to its\nability to effectively utilize data features from a variety of different\nmodalities. Diagnosing the vulnerability of atherosclerotic plaques directly\nfrom carotid 3D MRI images is relatively challenging for both radiologists and\nconventional 3D vision networks. In clinical practice, radiologists assess\npatient conditions using a multimodal approach that incorporates various\nimaging modalities and domain-specific expertise, paving the way for the\ncreation of multimodal diagnostic networks. In this paper, we have developed an\neffective strategy to leverage radiologists' domain knowledge to automate the\ndiagnosis of carotid plaque vulnerability through Variation inference and\nMultimodal knowledge Distillation (VMD). This method excels in harnessing\ncross-modality prior knowledge from limited image annotations and radiology\nreports within training data, thereby enhancing the diagnostic network's\naccuracy for unannotated 3D MRI images. We conducted in-depth experiments on\nthe dataset collected in-house and verified the effectiveness of the VMD\nstrategy we proposed.",
        "url": "http://arxiv.org/abs/2509.11924v1",
        "published_date": "2025-09-15T13:38:35+00:00",
        "updated_date": "2025-09-15T13:38:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bo Cao",
            "Fan Yu",
            "Mengmeng Feng",
            "SenHao Zhang",
            "Xin Meng",
            "Yue Zhang",
            "Zhen Qian",
            "Jie Lu"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper presents a method called VMD for automated diagnosis of plaque vulnerability in 3D carotid artery MRI, leveraging domain knowledge from radiologists and multimodal learning.",
        "tldr_zh": "这篇论文提出了一种名为VMD的方法，用于通过利用放射科医生和多模态学习的领域知识，自动诊断3D颈动脉MRI中的斑块脆弱性。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning",
        "summary": "Imitation learning (IL) enables efficient skill acquisition from\ndemonstrations but often struggles with long-horizon tasks and high-precision\ncontrol due to compounding errors. Residual policy learning offers a promising,\nmodel-agnostic solution by refining a base policy through closed-loop\ncorrections. However, existing approaches primarily focus on local corrections\nto the base policy, lacking a global understanding of state evolution, which\nlimits robustness and generalization to unseen scenarios. To address this, we\npropose incorporating global dynamics modeling to guide residual policy\nupdates. Specifically, we leverage Koopman operator theory to impose linear\ntime-invariant structure in a learned latent space, enabling reliable state\ntransitions and improved extrapolation for long-horizon prediction and unseen\nenvironments. We introduce KORR (Koopman-guided Online Residual Refinement), a\nsimple yet effective framework that conditions residual corrections on\nKoopman-predicted latent states, enabling globally informed and stable action\nrefinement. We evaluate KORR on long-horizon, fine-grained robotic furniture\nassembly tasks under various perturbations. Results demonstrate consistent\ngains in performance, robustness, and generalization over strong baselines. Our\nfindings further highlight the potential of Koopman-based modeling to bridge\nmodern learning methods with classical control theory. For more details, please\nrefer to https://jiachengliu3.github.io/TrajBooster.",
        "url": "http://arxiv.org/abs/2509.11839v1",
        "published_date": "2025-09-15T12:25:39+00:00",
        "updated_date": "2025-09-15T12:25:39+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Jiacheng Liu",
            "Pengxiang Ding",
            "Qihang Zhou",
            "Yuxuan Wu",
            "Da Huang",
            "Zimian Peng",
            "Wei Xiao",
            "Weinan Zhang",
            "Lixin Yang",
            "Cewu Lu",
            "Donglin Wang"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper introduces a method called KORR which incorporates global dynamics modeling to improve residual policy learning for humanoid whole-body manipulation tasks.",
        "tldr_zh": "本文介绍了一种名为KORR的方法，通过全局动态建模来改进用于人形机器人全身操作任务的残差策略学习。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "Hierarchical Identity Learning for Unsupervised Visible-Infrared Person Re-Identification",
        "summary": "Unsupervised visible-infrared person re-identification (USVI-ReID) aims to\nlearn modality-invariant image features from unlabeled cross-modal person\ndatasets by reducing the modality gap while minimizing reliance on costly\nmanual annotations. Existing methods typically address USVI-ReID using\ncluster-based contrastive learning, which represents a person by a single\ncluster center. However, they primarily focus on the commonality of images\nwithin each cluster while neglecting the finer-grained differences among them.\nTo address the limitation, we propose a Hierarchical Identity Learning (HIL)\nframework. Since each cluster may contain several smaller sub-clusters that\nreflect fine-grained variations among images, we generate multiple memories for\neach existing coarse-grained cluster via a secondary clustering. Additionally,\nwe propose Multi-Center Contrastive Learning (MCCL) to refine representations\nfor enhancing intra-modal clustering and minimizing cross-modal discrepancies.\nTo further improve cross-modal matching quality, we design a Bidirectional\nReverse Selection Transmission (BRST) mechanism, which establishes reliable\ncross-modal correspondences by performing bidirectional matching of\npseudo-labels. Extensive experiments conducted on the SYSU-MM01 and RegDB\ndatasets demonstrate that the proposed method outperforms existing approaches.\nThe source code is available at: https://github.com/haonanshi0125/HIL.",
        "url": "http://arxiv.org/abs/2509.11587v1",
        "published_date": "2025-09-15T05:10:43+00:00",
        "updated_date": "2025-09-15T05:10:43+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Haonan Shi",
            "Yubin Wang",
            "De Cheng",
            "Lingfeng He",
            "Nannan Wang",
            "Xinbo Gao"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes a novel framework called Hierarchical Identity Learning for Unsupervised Visible-Infrared Person Re-Identification, which outperforms existing methods in cross-modal person re-identification.",
        "tldr_zh": "本文提出了一种新的框架，名为层次身份学习，用于无监督可见-红外人员再识别，优于现有方法。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Beyond Frame-wise Tracking: A Trajectory-based Paradigm for Efficient Point Cloud Tracking",
        "summary": "LiDAR-based 3D single object tracking (3D SOT) is a critical task in robotics\nand autonomous systems. Existing methods typically follow frame-wise motion\nestimation or a sequence-based paradigm. However, the two-frame methods are\nefficient but lack long-term temporal context, making them vulnerable in sparse\nor occluded scenes, while sequence-based methods that process multiple point\nclouds gain robustness at a significant computational cost. To resolve this\ndilemma, we propose a novel trajectory-based paradigm and its instantiation,\nTrajTrack. TrajTrack is a lightweight framework that enhances a base two-frame\ntracker by implicitly learning motion continuity from historical bounding box\ntrajectories alone-without requiring additional, costly point cloud inputs. It\nfirst generates a fast, explicit motion proposal and then uses an implicit\nmotion modeling module to predict the future trajectory, which in turn refines\nand corrects the initial proposal. Extensive experiments on the large-scale\nNuScenes benchmark show that TrajTrack achieves new state-of-the-art\nperformance, dramatically improving tracking precision by 4.48% over a strong\nbaseline while running at 56 FPS. Besides, we also demonstrate the strong\ngeneralizability of TrajTrack across different base trackers. Video is\navailable at https://www.bilibili.com/video/BV1ahYgzmEWP.",
        "url": "http://arxiv.org/abs/2509.11453v1",
        "published_date": "2025-09-14T21:57:16+00:00",
        "updated_date": "2025-09-14T21:57:16+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "BaiChen Fan",
            "Sifan Zhou",
            "Jian Li",
            "Shibo Zhao",
            "Muqing Cao",
            "Qin Wang"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "TrajTrack is a novel trajectory-based paradigm for efficient point cloud tracking that outperforms existing methods, improving tracking precision while running at high frame rates.",
        "tldr_zh": "TrajTrack是一种新颖的基于轨迹的范式，用于高效追踪点云，优于现有方法，在高帧率下提高了跟踪精度。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "In-Vivo Skin 3-D Surface Reconstruction and Wrinkle Depth Estimation using Handheld High Resolution Tactile Sensing",
        "summary": "Three-dimensional (3-D) skin surface reconstruction offers promise for\nobjective and quantitative dermatological assessment, but no portable,\nhigh-resolution device exists that has been validated and used for depth\nreconstruction across various body locations. We present a compact 3-D skin\nreconstruction probe based on GelSight tactile imaging with a custom elastic\ngel and a learning-based reconstruction algorithm for micron-level wrinkle\nheight estimation. Our probe, integrated into a handheld probe with force\nsensing for consistent contact, achieves a mean absolute error of 12.55 micron\non wrinkle-like test objects. In a study with 15 participants without skin\ndisorders, we provide the first validated wrinkle depth metrics across multiple\nbody regions. We further demonstrate statistically significant reductions in\nwrinkle height at three locations following over-the-counter moisturizer\napplication. Our work offers a validated tool for clinical and cosmetic skin\nanalysis, with potential applications in diagnosis, treatment monitoring, and\nskincare efficacy evaluation.",
        "url": "http://arxiv.org/abs/2509.11385v1",
        "published_date": "2025-09-14T18:37:31+00:00",
        "updated_date": "2025-09-14T18:37:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Akhil Padmanabha",
            "Arpit Agarwal",
            "Catherine Li",
            "Austin Williams",
            "Dinesh K. Patel",
            "Sankalp Chopkar",
            "Achu Wilson",
            "Ahmet Ozkan",
            "Wenzhen Yuan",
            "Sonal Choudhary",
            "Arash Mostaghimi",
            "Zackory Erickson",
            "Carmel Majidi"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper presents a portable device for 3-D skin surface reconstruction and wrinkle depth estimation, with potential applications in clinical and cosmetic skin analysis.",
        "tldr_zh": "本文提出了一个便携设备，用于三维皮肤表面重建和皱纹深度估计，具有在临床和美容皮肤分析中的潜在应用。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "3DViT-GAT: A Unified Atlas-Based 3D Vision Transformer and Graph Learning Framework for Major Depressive Disorder Detection Using Structural MRI Data",
        "summary": "Major depressive disorder (MDD) is a prevalent mental health condition that\nnegatively impacts both individual well-being and global public health.\nAutomated detection of MDD using structural magnetic resonance imaging (sMRI)\nand deep learning (DL) methods holds increasing promise for improving\ndiagnostic accuracy and enabling early intervention. Most existing methods\nemploy either voxel-level features or handcrafted regional representations\nbuilt from predefined brain atlases, limiting their ability to capture complex\nbrain patterns. This paper develops a unified pipeline that utilizes Vision\nTransformers (ViTs) for extracting 3D region embeddings from sMRI data and\nGraph Neural Network (GNN) for classification. We explore two strategies for\ndefining regions: (1) an atlas-based approach using predefined structural and\nfunctional brain atlases, and (2) an cube-based method by which ViTs are\ntrained directly to identify regions from uniformly extracted 3D patches.\nFurther, cosine similarity graphs are generated to model interregional\nrelationships, and guide GNN-based classification. Extensive experiments were\nconducted using the REST-meta-MDD dataset to demonstrate the effectiveness of\nour model. With stratified 10-fold cross-validation, the best model obtained\n78.98% accuracy, 76.54% sensitivity, 81.58% specificity, 81.58% precision, and\n78.98% F1-score. Further, atlas-based models consistently outperformed the\ncube-based approach, highlighting the importance of using domain-specific\nanatomical priors for MDD detection.",
        "url": "http://arxiv.org/abs/2509.12143v1",
        "published_date": "2025-09-15T17:10:39+00:00",
        "updated_date": "2025-09-15T17:10:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "62P10, 68T07, 92B20",
            "I.2.6; J.3"
        ],
        "authors": [
            "Nojod M. Alotaibi",
            "Areej M. Alhothali",
            "Manar S. Ali"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper presents a unified framework using Vision Transformers and Graph Neural Networks for detecting major depressive disorder from structural MRI data, achieving promising results.",
        "tldr_zh": "该论文提出了一个使用Vision Transformers和图神经网络检测主要抑郁症的统一框架，取得了令人鼓舞的结果。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models",
        "summary": "Recent advances in text-only \"slow-thinking\" reasoning have prompted efforts\nto transfer this capability to vision-language models (VLMs), for training\nvisual reasoning models (\\textbf{VRMs}). owever, such transfer faces critical\nchallenges: Effective \"slow thinking\" in VRMs requires \\textbf{visual\nreflection}, the ability to check the reasoning process based on visual\ninformation. Through quantitative analysis, we observe that current VRMs\nexhibit limited visual reflection, as their attention to visual information\ndiminishes rapidly with longer generated responses. To address this challenge,\nwe propose a new VRM \\textbf{Reflection-V}, which enhances visual reflection\nbased on reasoning data construction for cold-start and reward design for\nreinforcement learning (RL). Firstly, we construct vision-centered reasoning\ndata by leveraging an agent that interacts between VLMs and reasoning LLMs,\nenabling cold-start learning of visual reflection patterns. Secondly, a visual\nattention based reward model is employed during RL to encourage reasoning based\non visual information. Therefore, \\textbf{Reflection-V} demonstrates\nsignificant improvements across multiple visual reasoning benchmarks.\nFurthermore, \\textbf{Reflection-V} maintains a stronger and more consistent\nreliance on visual information during visual reasoning, indicating effective\nenhancement in visual reflection capabilities.",
        "url": "http://arxiv.org/abs/2509.12132v1",
        "published_date": "2025-09-15T16:57:25+00:00",
        "updated_date": "2025-09-15T16:57:25+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Pu Jian",
            "Junhong Wu",
            "Wei Sun",
            "Chen Wang",
            "Shuo Ren",
            "Jiajun Zhang"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper introduces a new vision-language model, Reflection-V, that enhances visual reflection in reasoning tasks. It shows improvements in visual reasoning benchmarks and maintains a stronger reliance on visual information.",
        "tldr_zh": "本文介绍了一种新的视觉-语言模型Reflection-V，可以增强推理任务中的视觉反思能力。它在视觉推理基准测试中展示了改进，并保持了对视觉信息的更强依赖。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "End-to-End 4D Heart Mesh Recovery Across Full-Stack and Sparse Cardiac MRI",
        "summary": "Reconstructing cardiac motion from cine CMR sequences is critical for\ndiagnosis, prediction, and intervention. Existing methods rely on complete CMR\nstacks to infer full heart motion, limiting their utility in intra-procedural\nscenarios where only sparse observations are available. We present TetHeart,\nthe first end-to-end framework that unifies full 4D multi-structure heart mesh\nrecovery from both offline full-stack acquisitions and intra-procedural\nsparse-slice observations. Our method leverages deep deformable tetrahedra, an\nexplicit-implicit hybrid representation, to capture shape and motion in a\ncoherent space shared across cardiac structures. It is initialized from\nhigh-quality pre-procedural or offline-acquired full stacks to build detailed,\npatient-specific heart meshes, which can then be updated using whatever slices\nare available, from full stacks down to a single slice. We further incorporate\nseveral key innovations: (i) an attentive mechanism for slice-adaptive 2D-3D\nfeature assembly that dynamically integrates information from arbitrary numbers\nof slices at any position, combined with a distillation strategy from\nfull-slice to sparse-slice settings to ensure accurate reconstruction under\nextreme sparsity; and (ii) a two-stage weakly supervised motion learning scheme\nrequiring only keyframe (e.g., ED and ES) annotations. Trained and validated on\nthree large public datasets and externally evaluated zero-shot on additional\nprivate interventional and public CMR datasets, TetHeart achieves\nstate-of-the-art accuracy and strong generalization in both pre- and\nintra-procedural settings.",
        "url": "http://arxiv.org/abs/2509.12090v1",
        "published_date": "2025-09-15T16:17:45+00:00",
        "updated_date": "2025-09-15T16:17:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yihong Chen",
            "Jiancheng Yang",
            "Deniz Sayin Mercadier",
            "Hieu Le",
            "Juerg Schwitter",
            "Pascal Fua"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces TetHeart, an end-to-end framework for recovering 4D heart mesh from both full-stack and sparse cardiac MRI data, achieving state-of-the-art accuracy in pre- and intra-procedural settings.",
        "tldr_zh": "该论文介绍了TetHeart，一种从完整堆栈和稀疏心脏MRI数据中恢复4D心脏网格的端到端框架，在术前和术中环境中实现了最先进的准确性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "U-Mamba2: Scaling State Space Models for Dental Anatomy Segmentation in CBCT",
        "summary": "Cone-Beam Computed Tomography (CBCT) is a widely used 3D imaging technique in\ndentistry, providing volumetric information about the anatomical structures of\njaws and teeth. Accurate segmentation of these anatomies is critical for\nclinical applications such as diagnosis and surgical planning, but remains\ntime-consuming and challenging. In this paper, we present U-Mamba2, a new\nneural network architecture designed for multi-anatomy CBCT segmentation in the\ncontext of the ToothFairy3 challenge. U-Mamba2 integrates the Mamba2 state\nspace models into the U-Net architecture, enforcing stronger structural\nconstraints for higher efficiency without compromising performance. In\naddition, we integrate interactive click prompts with cross-attention blocks,\npre-train U-Mamba2 using self-supervised learning, and incorporate dental\ndomain knowledge into the model design to address key challenges of dental\nanatomy segmentation in CBCT. Extensive experiments, including independent\ntests, demonstrate that U-Mamba2 is both effective and efficient, securing top\n3 places in both tasks of the Toothfairy3 challenge. In Task 1, U-Mamba2\nachieved a mean Dice of 0.792, HD95 of 93.19 with the held-out test data, with\nan average inference time of XX (TBC during the ODIN workshop). In Task 2,\nU-Mamba2 achieved the mean Dice of 0.852 and HD95 of 7.39 with the held-out\ntest data. The code is publicly available at\nhttps://github.com/zhiqin1998/UMamba2.",
        "url": "http://arxiv.org/abs/2509.12069v1",
        "published_date": "2025-09-15T15:52:43+00:00",
        "updated_date": "2025-09-15T15:52:43+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhi Qin Tan",
            "Xiatian Zhu",
            "Owen Addison",
            "Yunpeng Li"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces U-Mamba2, a neural network architecture for dental anatomy segmentation in CBCT, achieving top rankings in a challenge.",
        "tldr_zh": "本文介绍了U-Mamba2，这是一个用于CBCT中牙齿解剖分割的神经网络架构，在挑战赛中获得了前三名。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Robust Fetal Pose Estimation across Gestational Ages via Cross-Population Augmentation",
        "summary": "Fetal motion is a critical indicator of neurological development and\nintrauterine health, yet its quantification remains challenging, particularly\nat earlier gestational ages (GA). Current methods track fetal motion by\npredicting the location of annotated landmarks on 3D echo planar imaging (EPI)\ntime-series, primarily in third-trimester fetuses. The predicted landmarks\nenable simplification of the fetal body for downstream analysis. While these\nmethods perform well within their training age distribution, they consistently\nfail to generalize to early GAs due to significant anatomical changes in both\nmother and fetus across gestation, as well as the difficulty of obtaining\nannotated early GA EPI data. In this work, we develop a cross-population data\naugmentation framework that enables pose estimation models to robustly\ngeneralize to younger GA clinical cohorts using only annotated images from\nolder GA cohorts. Specifically, we introduce a fetal-specific augmentation\nstrategy that simulates the distinct intrauterine environment and fetal\npositioning of early GAs. Our experiments find that cross-population\naugmentation yields reduced variability and significant improvements across\nboth older GA and challenging early GA cases. By enabling more reliable pose\nestimation across gestation, our work potentially facilitates early clinical\ndetection and intervention in challenging 4D fetal imaging settings. Code is\navailable at https://github.com/sebodiaz/cross-population-pose.",
        "url": "http://arxiv.org/abs/2509.12062v1",
        "published_date": "2025-09-15T15:42:28+00:00",
        "updated_date": "2025-09-15T15:42:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sebastian Diaz",
            "Benjamin Billot",
            "Neel Dey",
            "Molin Zhang",
            "Esra Abaci Turk",
            "P. Ellen Grant",
            "Polina Golland",
            "Elfar Adalsteinsson"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper introduces a cross-population data augmentation framework to improve fetal pose estimation, aiding in early clinical detection in challenging fetal imaging settings.",
        "tldr_zh": "该论文引入了一个跨人群数据增强框架，改善胎儿姿势估计，有助于在挑战性胎儿成像环境中进行早期临床检测。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "NeuroGaze-Distill: Brain-informed Distillation and Depression-Inspired Geometric Priors for Robust Facial Emotion Recognition",
        "summary": "Facial emotion recognition (FER) models trained only on pixels often fail to\ngeneralize across datasets because facial appearance is an indirect and biased\nproxy for underlying affect. We present NeuroGaze-Distill, a cross-modal\ndistillation framework that transfers brain-informed priors into an image-only\nFER student via static Valence/Arousal (V/A) prototypes and a\ndepression-inspired geometric prior (D-Geo). A teacher trained on EEG\ntopographic maps from DREAMER (with MAHNOB-HCI as unlabeled support) produces a\nconsolidated 5x5 V/A prototype grid that is frozen and reused; no EEG-face\npairing and no non-visual signals at deployment are required. The student\n(ResNet-18/50) is trained on FERPlus with conventional CE/KD and two\nlightweight regularizers: (i) Proto-KD (cosine) aligns student features to the\nstatic prototypes; (ii) D-Geo softly shapes the embedding geometry in line with\naffective findings often reported in depression research (e.g., anhedonia-like\ncontraction in high-valence regions). We evaluate both within-domain (FERPlus\nvalidation) and cross-dataset protocols (AffectNet-mini; optional CK+),\nreporting standard 8-way scores alongside present-only Macro-F1 and balanced\naccuracy to fairly handle label-set mismatch. Ablations attribute consistent\ngains to prototypes and D-Geo, and favor 5x5 over denser grids for stability.\nThe method is simple, deployable, and improves robustness without architectural\ncomplexity.",
        "url": "http://arxiv.org/abs/2509.11916v1",
        "published_date": "2025-09-15T13:33:54+00:00",
        "updated_date": "2025-09-15T13:33:54+00:00",
        "categories": [
            "cs.CV",
            "I.2.10; I.4.8; I.5.4"
        ],
        "authors": [
            "Zilin Li",
            "Weiwei Xu",
            "Xuanqi Zhao",
            "Yiran Zhu"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "NeuroGaze-Distill is a framework that transfers brain-informed priors for facial emotion recognition, improving robustness without complexity.",
        "tldr_zh": "NeuroGaze-Distill是一个将大脑先验信息转移到面部情绪识别的框架，提高了鲁棒性而不增加复杂性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "BREA-Depth: Bronchoscopy Realistic Airway-geometric Depth Estimation",
        "summary": "Monocular depth estimation in bronchoscopy can significantly improve\nreal-time navigation accuracy and enhance the safety of interventions in\ncomplex, branching airways. Recent advances in depth foundation models have\nshown promise for endoscopic scenarios, yet these models often lack anatomical\nawareness in bronchoscopy, overfitting to local textures rather than capturing\nthe global airway structure, particularly under ambiguous depth cues and poor\nlighting. To address this, we propose Brea-Depth, a novel framework that\nintegrates airway-specific geometric priors into foundation model adaptation\nfor bronchoscopic depth estimation. Our method introduces a depth-aware\nCycleGAN, refining the translation between real bronchoscopic images and airway\ngeometries from anatomical data, effectively bridging the domain gap. In\naddition, we introduce an airway structure awareness loss to enforce depth\nconsistency within the airway lumen while preserving smooth transitions and\nstructural integrity. By incorporating anatomical priors, Brea-Depth enhances\nmodel generalization and yields more robust, accurate 3D airway\nreconstructions. To assess anatomical realism, we introduce Airway Depth\nStructure Evaluation, a new metric for structural consistency. We validate\nBREA-Depth on a collected ex vivo human lung dataset and an open bronchoscopic\ndataset, where it outperforms existing methods in anatomical depth\npreservation.",
        "url": "http://arxiv.org/abs/2509.11885v1",
        "published_date": "2025-09-15T13:02:42+00:00",
        "updated_date": "2025-09-15T13:02:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Francis Xiatian Zhang",
            "Emile Mackute",
            "Mohammadreza Kasaei",
            "Kevin Dhaliwal",
            "Robert Thomson",
            "Mohsen Khadem"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes a novel framework, Brea-Depth, for improving depth estimation in bronchoscopy by integrating airway-specific geometric priors and introducing a depth-aware CycleGAN.",
        "tldr_zh": "本文提出了一种新颖的框架，Brea-Depth，通过整合特定于气道的几何先验信息，并引入了一种深度感知的CycleGAN，来改进支气管镜下的深度估计。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "SAM-TTT: Segment Anything Model via Reverse Parameter Configuration and Test-Time Training for Camouflaged Object Detection",
        "summary": "This paper introduces a new Segment Anything Model (SAM) that leverages\nreverse parameter configuration and test-time training to enhance its\nperformance on Camouflaged Object Detection (COD), named SAM-TTT. While most\nexisting SAM-based COD models primarily focus on enhancing SAM by extracting\nfavorable features and amplifying its advantageous parameters, a crucial gap is\nidentified: insufficient attention to adverse parameters that impair SAM's\nsemantic understanding in downstream tasks. To tackle this issue, the Reverse\nSAM Parameter Configuration Module is proposed to effectively mitigate the\ninfluence of adverse parameters in a train-free manner by configuring SAM's\nparameters. Building on this foundation, the T-Visioner Module is unveiled to\nstrengthen advantageous parameters by integrating Test-Time Training layers,\noriginally developed for language tasks, into vision tasks. Test-Time Training\nlayers represent a new class of sequence modeling layers characterized by\nlinear complexity and an expressive hidden state. By integrating two modules,\nSAM-TTT simultaneously suppresses adverse parameters while reinforcing\nadvantageous ones, significantly improving SAM's semantic understanding in COD\ntask. Our experimental results on various COD benchmarks demonstrate that the\nproposed approach achieves state-of-the-art performance, setting a new\nbenchmark in the field. The code will be available at\nhttps://github.com/guobaoxiao/SAM-TTT.",
        "url": "http://arxiv.org/abs/2509.11884v1",
        "published_date": "2025-09-15T13:02:27+00:00",
        "updated_date": "2025-09-15T13:02:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhenni Yu",
            "Li Zhao",
            "Guobao Xiao",
            "Xiaoqin Zhang"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "SAM-TTT is a new model that enhances Camouflaged Object Detection using reverse parameter configuration and test-time training, achieving state-of-the-art results.",
        "tldr_zh": "SAM-TTT是一种利用反向参数配置和测试时间训练增强伪装目标检测的新模型，实现了业界领先的结果。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Probabilistic Robustness Analysis in High Dimensional Space: Application to Semantic Segmentation Network",
        "summary": "Semantic segmentation networks (SSNs) play a critical role in domains such as\nmedical imaging, autonomous driving, and environmental monitoring, where safety\nhinges on reliable model behavior under uncertainty. Yet, existing\nprobabilistic verification approaches struggle to scale with the complexity and\ndimensionality of modern segmentation tasks, often yielding guarantees that are\ntoo conservative to be practical. We introduce a probabilistic verification\nframework that is both architecture-agnostic and scalable to high-dimensional\noutputs. Our approach combines sampling-based reachability analysis with\nconformal inference (CI) to deliver provable guarantees while avoiding the\nexcessive conservatism of prior methods. To counteract CI's limitations in\nhigh-dimensional settings, we propose novel strategies that reduce conservatism\nwithout compromising rigor. Empirical evaluation on large-scale segmentation\nmodels across CamVid, OCTA-500, Lung Segmentation, and Cityscapes demonstrates\nthat our method provides reliable safety guarantees while substantially\ntightening bounds compared to SOTA. We also provide a toolbox implementing this\ntechnique, available on Github.",
        "url": "http://arxiv.org/abs/2509.11838v1",
        "published_date": "2025-09-15T12:25:25+00:00",
        "updated_date": "2025-09-15T12:25:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Navid Hashemi",
            "Samuel Sasaki",
            "Diego Manzanas Lopez",
            "Ipek Oguz",
            "Meiyi Ma",
            "Taylor T. Johnson"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a probabilistic verification framework for semantic segmentation networks that is scalable and provides reliable safety guarantees.",
        "tldr_zh": "该论文引入了一种可扩展的概率验证框架，用于语义分割网络，可提供可靠性安全保证。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "FineQuest: Adaptive Knowledge-Assisted Sports Video Understanding via Agent-of-Thoughts Reasoning",
        "summary": "Video Question Answering (VideoQA) based on Large Language Models (LLMs) has\nshown potential in general video understanding but faces significant challenges\nwhen applied to the inherently complex domain of sports videos. In this work,\nwe propose FineQuest, the first training-free framework that leverages\ndual-mode reasoning inspired by cognitive science: i) Reactive Reasoning for\nstraightforward sports queries and ii) Deliberative Reasoning for more complex\nones. To bridge the knowledge gap between general-purpose models and\ndomain-specific sports understanding, FineQuest incorporates SSGraph, a\nmultimodal sports knowledge scene graph spanning nine sports, which encodes\nboth visual instances and domain-specific terminology to enhance reasoning\naccuracy. Furthermore, we introduce two new sports VideoQA benchmarks, Gym-QA\nand Diving-QA, derived from the FineGym and FineDiving datasets, enabling\ndiverse and comprehensive evaluation. FineQuest achieves state-of-the-art\nperformance on these benchmarks as well as the existing SPORTU dataset, while\nmaintains strong general VideoQA capabilities.",
        "url": "http://arxiv.org/abs/2509.11796v1",
        "published_date": "2025-09-15T11:27:23+00:00",
        "updated_date": "2025-09-15T11:27:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haodong Chen",
            "Haojian Huang",
            "XinXiang Yin",
            "Dian Shao"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces FineQuest, a training-free framework for sports video understanding using dual-mode reasoning and domain-specific knowledge. It achieves state-of-the-art results on new sports VideoQA benchmarks.",
        "tldr_zh": "本文介绍了FineQuest，一个利用双模态推理和领域特定知识进行体育视频理解的无需训练的框架。它在新的体育视频问答基准上取得了最先进的结果。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "How Auxiliary Reasoning Unleashes GUI Grounding in VLMs",
        "summary": "Graphical user interface (GUI) grounding is a fundamental task for building\nGUI agents. However, general vision-language models (VLMs) struggle with this\ntask due to a lack of specific optimization. We identify a key gap in this\npaper: while VLMs exhibit significant latent grounding potential, as\ndemonstrated by their performance measured by Pointing Game, they underperform\nwhen tasked with outputting explicit coordinates. To address this discrepancy,\nand bypass the high data and annotation costs of current fine-tuning\napproaches, we propose three zero-shot auxiliary reasoning methods. By\nproviding explicit spatial cues such as axes, grids and labeled intersections\nas part of the input image, these methods enable VLMs to articulate their\nimplicit spatial understanding capabilities. We evaluate these methods on four\nGUI grounding benchmarks across seven open-source and proprietary VLMs. The\nevaluation results demonstrate that the proposed methods substantially improve\nthe performance of GUI grounding.",
        "url": "http://arxiv.org/abs/2509.11548v1",
        "published_date": "2025-09-15T03:28:29+00:00",
        "updated_date": "2025-09-15T03:28:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weiming Li",
            "Yan Shao",
            "Jing Yang",
            "Yujing Lu",
            "Ling Zhong",
            "Yuhan Wang",
            "Manni Duan"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes auxiliary reasoning methods to improve GUI grounding in vision-language models, with positive results across various benchmarks.",
        "tldr_zh": "本文提出了辅助推理方法来改进视觉-语言模型中的GUI基础，通过对多个基准测试的积极结果。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Domain-Adaptive Pretraining Improves Primate Behavior Recognition",
        "summary": "Computer vision for animal behavior offers promising tools to aid research in\necology, cognition, and to support conservation efforts. Video camera traps\nallow for large-scale data collection, but high labeling costs remain a\nbottleneck to creating large-scale datasets. We thus need data-efficient\nlearning approaches. In this work, we show that we can utilize self-supervised\nlearning to considerably improve action recognition on primate behavior. On two\ndatasets of great ape behavior (PanAf and ChimpACT), we outperform published\nstate-of-the-art action recognition models by 6.1 %pt. accuracy and 6.3 %pt.\nmAP, respectively. We achieve this by utilizing a pretrained V-JEPA model and\napplying domain-adaptive pretraining (DAP), i.e. continuing the pretraining\nwith in-domain data. We show that most of the performance gain stems from the\nDAP. Our method promises great potential for improving the recognition of\nanimal behavior, as DAP does not require labeled samples. Code is available at\nhttps://github.com/ecker-lab/dap-behavior",
        "url": "http://arxiv.org/abs/2509.12193v1",
        "published_date": "2025-09-15T17:54:20+00:00",
        "updated_date": "2025-09-15T17:54:20+00:00",
        "categories": [
            "cs.CV",
            "I.4.8; I.2.10; I.5"
        ],
        "authors": [
            "Felix B. Mueller",
            "Timo Lueddecke",
            "Richard Vogg",
            "Alexander S. Ecker"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a method called domain-adaptive pretraining to improve primate behavior recognition using self-supervised learning, showing significant performance gains without the need for labeled samples.",
        "tldr_zh": "本文介绍了一种称为领域自适应预训练的方法，利用自监督学习来改善灵长类动物行为识别，显示出显著的性能提升，而无需标记样本。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Early Detection of Branched Broomrape (Phelipanche ramosa) Infestation in Tomato Crops Using Leaf Spectral Analysis and Machine Learning",
        "summary": "Branched broomrape (Phelipanche ramosa) is a chlorophyll-deficient parasitic\nweed that threatens tomato production by extracting nutrients from the host. We\ninvestigate early detection using leaf-level spectral reflectance (400-2500 nm)\nand ensemble machine learning. In a field experiment in Woodland, California,\nwe tracked 300 tomato plants across growth stages defined by growing degree\ndays (GDD). Leaf reflectance was acquired with a portable spectrometer and\npreprocessed (band denoising, 1 nm interpolation, Savitzky-Golay smoothing,\ncorrelation-based band reduction). Clear class differences were observed near\n1500 nm and 2000 nm water absorption features, consistent with reduced leaf\nwater content in infected plants at early stages. An ensemble combining Random\nForest, XGBoost, SVM with RBF kernel, and Naive Bayes achieved 89% accuracy at\n585 GDD, with recalls of 0.86 (infected) and 0.93 (noninfected). Accuracy\ndeclined at later stages (e.g., 69% at 1568 GDD), likely due to senescence and\nweed interference. Despite the small number of infected plants and\nenvironmental confounders, results show that proximal sensing with ensemble\nlearning enables timely detection of broomrape before canopy symptoms are\nvisible, supporting targeted interventions and reduced yield losses.",
        "url": "http://arxiv.org/abs/2509.12074v1",
        "published_date": "2025-09-15T16:00:32+00:00",
        "updated_date": "2025-09-15T16:00:32+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "eess.SP",
            "68T07, 68T45, 68U10",
            "I.5.4; I.4.6; I.2.6"
        ],
        "authors": [
            "Mohammadreza Narimani",
            "Alireza Pourreza",
            "Ali Moghimi",
            "Parastoo Farajpoor",
            "Hamid Jafarbiglu",
            "Mohsen B. Mesgaran"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer",
            "AIGC"
        ],
        "tldr": "The paper discusses using leaf spectral analysis and machine learning to detect early infestation of a parasitic weed in tomato crops, achieving high accuracy for timely intervention.",
        "tldr_zh": "本文讨论了使用叶片光谱分析和机器学习来检测番茄作物中寄生性杂草的早期侵染，实现高准确率以进行及时干预。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing",
        "summary": "Open-Vocabulary Remote Sensing Image Segmentation (OVRSIS), an emerging task\nthat adapts Open-Vocabulary Segmentation (OVS) to the remote sensing (RS)\ndomain, remains underexplored due to the absence of a unified evaluation\nbenchmark and the domain gap between natural and RS images. To bridge these\ngaps, we first establish a standardized OVRSIS benchmark (\\textbf{OVRSISBench})\nbased on widely-used RS segmentation datasets, enabling consistent evaluation\nacross methods. Using this benchmark, we comprehensively evaluate several\nrepresentative OVS/OVRSIS models and reveal their limitations when directly\napplied to remote sensing scenarios. Building on these insights, we propose\n\\textbf{RSKT-Seg}, a novel open-vocabulary segmentation framework tailored for\nremote sensing. RSKT-Seg integrates three key components: (1) a\nMulti-Directional Cost Map Aggregation (RS-CMA) module that captures\nrotation-invariant visual cues by computing vision-language cosine similarities\nacross multiple directions; (2) an Efficient Cost Map Fusion (RS-Fusion)\ntransformer, which jointly models spatial and semantic dependencies with a\nlightweight dimensionality reduction strategy; and (3) a Remote Sensing\nKnowledge Transfer (RS-Transfer) module that injects pre-trained knowledge and\nfacilitates domain adaptation via enhanced upsampling. Extensive experiments on\nthe benchmark show that RSKT-Seg consistently outperforms strong OVS baselines\nby +3.8 mIoU and +5.9 mACC, while achieving 2x faster inference through\nefficient aggregation. Our code is\n\\href{https://github.com/LiBingyu01/RSKT-Seg}{\\textcolor{blue}{here}}.",
        "url": "http://arxiv.org/abs/2509.12040v1",
        "published_date": "2025-09-15T15:24:49+00:00",
        "updated_date": "2025-09-15T15:24:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Bingyu Li",
            "Haocheng Dong",
            "Da Zhang",
            "Zhiyuan Zhao",
            "Junyu Gao",
            "Xuelong Li"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a new framework, RSKT-Seg, for open-vocabulary segmentation in remote sensing, outperforming existing models and achieving faster inference.",
        "tldr_zh": "该论文介绍了一种新的框架，RSKT-Seg，用于遥感中的开放词汇分割，在性能上超越了现有模型并实现了更快的推断速度。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Integrating Prior Observations for Incremental 3D Scene Graph Prediction",
        "summary": "3D semantic scene graphs (3DSSG) provide compact structured representations\nof environments by explicitly modeling objects, attributes, and relationships.\nWhile 3DSSGs have shown promise in robotics and embodied AI, many existing\nmethods rely mainly on sensor data, not integrating further information from\nsemantically rich environments. Additionally, most methods assume access to\ncomplete scene reconstructions, limiting their applicability in real-world,\nincremental settings. This paper introduces a novel heterogeneous graph model\nfor incremental 3DSSG prediction that integrates additional, multi-modal\ninformation, such as prior observations, directly into the message-passing\nprocess. Utilizing multiple layers, the model flexibly incorporates global and\nlocal scene representations without requiring specialized modules or full scene\nreconstructions. We evaluate our approach on the 3DSSG dataset, showing that\nGNNs enriched with multi-modal information such as semantic embeddings (e.g.,\nCLIP) and prior observations offer a scalable and generalizable solution for\ncomplex, real-world environments. The full source code of the presented\narchitecture will be made available at\nhttps://github.com/m4renz/incremental-scene-graph-prediction.",
        "url": "http://arxiv.org/abs/2509.11895v1",
        "published_date": "2025-09-15T13:10:34+00:00",
        "updated_date": "2025-09-15T13:10:34+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Marian Renz",
            "Felix Igelbrink",
            "Martin Atzmueller"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel method for incremental 3D scene graph prediction that integrates prior observations and multi-modal information into the process, showing promising results on the 3DSSG dataset.",
        "tldr_zh": "该论文介绍了一种将先前观察和多模态信息整合到增量3D场景图预测过程中的新方法，在3DSSG数据集上展现了令人期待的结果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "FedDAF: Federated Domain Adaptation Using Model Functional Distance",
        "summary": "Federated Domain Adaptation (FDA) is a federated learning (FL) approach that\nimproves model performance at the target client by collaborating with source\nclients while preserving data privacy. FDA faces two primary challenges: domain\nshifts between source and target data and limited labeled data at the target.\nMost existing FDA methods focus on domain shifts, assuming ample target data,\nyet often neglect the combined challenges of both domain shifts and data\nscarcity. Moreover, approaches that address both challenges fail to prioritize\nsharing relevant information from source clients according to the target's\nobjective. In this paper, we propose FedDAF, a novel approach addressing both\nchallenges in FDA. FedDAF uses similarity-based aggregation of the global\nsource model and target model by calculating model functional distance from\ntheir mean gradient fields computed on target data. This enables effective\nmodel aggregation based on the target objective, constructed using target data,\neven with limited data. While computing model functional distance between these\ntwo models, FedDAF computes the angle between their mean gradient fields and\nthen normalizes with the Gompertz function. To construct the global source\nmodel, all the local source models are aggregated using simple average in the\nserver. Experiments on real-world datasets demonstrate FedDAF's superiority\nover existing FL, PFL, and FDA methods in terms of achieving better test\naccuracy.",
        "url": "http://arxiv.org/abs/2509.11819v1",
        "published_date": "2025-09-15T12:03:38+00:00",
        "updated_date": "2025-09-15T12:03:38+00:00",
        "categories": [
            "cs.LG",
            "cs.CV",
            "68W15, 68T05, 90C25",
            "I.2.6; I.5.1; C.2.4"
        ],
        "authors": [
            "Mrinmay Sen",
            "Ankita Das",
            "Sidhant Nair",
            "C Krishna Mohan"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "FedDAF is a federated learning approach that addresses domain shifts and data scarcity challenges in FDA by calculating model functional distance for effective model aggregation based on target objectives.",
        "tldr_zh": "FedDAF是一种联邦学习方法，通过计算模型功能距离，有效地根据目标对象实现模型聚合，解决FDA中的领域转移和数据稀缺挑战。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7.0
    },
    {
        "title": "LFRA-Net: A Lightweight Focal and Region-Aware Attention Network for Retinal Vessel Segmentatio",
        "summary": "Retinal vessel segmentation is critical for the early diagnosis of\nvision-threatening and systemic diseases, especially in real-world clinical\nsettings with limited computational resources. Although significant\nimprovements have been made in deep learning-based segmentation methods,\ncurrent models still face challenges in extracting tiny vessels and suffer from\nhigh computational costs. In this study, we present LFRA-Net by incorporating\nfocal modulation attention at the encoder-decoder bottleneck and region-aware\nattention in the selective skip connections. LFRA-Net is a lightweight network\noptimized for precise and effective retinal vascular segmentation. It enhances\nfeature representation and regional focus by efficiently capturing local and\nglobal dependencies. LFRA-Net outperformed many state-of-the-art models while\nmaintaining lightweight characteristics with only 0.17 million parameters, 0.66\nMB memory size, and 10.50 GFLOPs. We validated it on three publicly available\ndatasets: DRIVE, STARE, and CHASE\\_DB. It performed better in terms of Dice\nscore (84.28\\%, 88.44\\%, and 85.50\\%) and Jaccard index (72.86\\%, 79.31\\%, and\n74.70\\%) on the DRIVE, STARE, and CHASE\\_DB datasets, respectively. LFRA-Net\nprovides an ideal ratio between segmentation accuracy and computational cost\ncompared to existing deep learning methods, which makes it suitable for\nreal-time clinical applications in areas with limited resources. The code can\nbe found at https://github.com/Mehwish4593/LFRA-Net.",
        "url": "http://arxiv.org/abs/2509.11811v1",
        "published_date": "2025-09-15T11:47:51+00:00",
        "updated_date": "2025-09-15T11:47:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mehwish Mehmood",
            "Shahzaib Iqbal",
            "Tariq Mahmood Khan",
            "Ivor Spence",
            "Muhammad Fahim"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "LFRA-Net is a lightweight neural network designed for retinal vessel segmentation, achieving high accuracy with low computational costs.",
        "tldr_zh": "LFRA-Net是一个轻量级神经网络，专门用于视网膜血管分割，在低计算成本下获得高准确性。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "SA-UNetv2: Rethinking Spatial Attention U-Net for Retinal Vessel Segmentation",
        "summary": "Retinal vessel segmentation is essential for early diagnosis of diseases such\nas diabetic retinopathy, hypertension, and neurodegenerative disorders.\nAlthough SA-UNet introduces spatial attention in the bottleneck, it underuses\nattention in skip connections and does not address the severe\nforeground-background imbalance. We propose SA-UNetv2, a lightweight model that\ninjects cross-scale spatial attention into all skip connections to strengthen\nmulti-scale feature fusion and adopts a weighted Binary Cross-Entropy (BCE)\nplus Matthews Correlation Coefficient (MCC) loss to improve robustness to class\nimbalance. On the public DRIVE and STARE datasets, SA-UNetv2 achieves\nstate-of-the-art performance with only 1.2MB memory and 0.26M parameters (less\nthan 50% of SA-UNet), and 1 second CPU inference on 592 x 592 x 3 images,\ndemonstrating strong efficiency and deployability in resource-constrained,\nCPU-only settings.",
        "url": "http://arxiv.org/abs/2509.11774v1",
        "published_date": "2025-09-15T10:53:28+00:00",
        "updated_date": "2025-09-15T10:53:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Changlu Guo",
            "Anders Nymark Christensen",
            "Anders Bjorholm Dahl",
            "Yugen Yi",
            "Morten Rieger Hannemose"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "SA-UNetv2 is a novel model for retinal vessel segmentation, outperforming existing models in efficiency and accuracy.",
        "tldr_zh": "SA-UNetv2 是一种用于视网膜血管分割的新型模型，效率和准确性均优于现有模型。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalization",
        "summary": "Autonomous systems require robust Multi-Object Tracking (MOT) capabilities to\noperate reliably in dynamic environments. MOT ensures consistent object\nidentity assignment and precise spatial delineation. Recent advances in\nfoundation models, such as SAM2, have demonstrated strong zero-shot\ngeneralization for video segmentation, but their direct application to MOTS\n(MOT+Segmentation) remains limited by insufficient identity management and\nmemory efficiency. This work introduces Seg2Track-SAM2, a framework that\nintegrates pre-trained object detectors with SAM2 and a novel Seg2Track module\nto address track initialization, track management, and reinforcement. The\nproposed approach requires no fine-tuning and remains detector-agnostic.\nExperimental results on KITTI MOT and KITTI MOTS benchmarks show that\nSeg2Track-SAM2 achieves state-of-the-art (SOTA) performance, ranking fourth\noverall in both car and pedestrian classes on KITTI MOTS, while establishing a\nnew benchmark in association accuracy (AssA). Furthermore, a sliding-window\nmemory strategy reduces memory usage by up to 75% with negligible performance\ndegradation, supporting deployment under resource constraints. These results\nconfirm that Seg2Track-SAM2 advances MOTS by combining robust zero-shot\ntracking, enhanced identity preservation, and efficient memory utilization. The\ncode is available at https://github.com/hcmr-lab/Seg2Track-SAM2",
        "url": "http://arxiv.org/abs/2509.11772v1",
        "published_date": "2025-09-15T10:52:27+00:00",
        "updated_date": "2025-09-15T10:52:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Diogo Mendonça",
            "Tiago Barros",
            "Cristiano Premebida",
            "Urbano J. Nunes"
        ],
        "ai_categories": [
            "AIGCB"
        ],
        "tldr": "Seg2Track-SAM2 is a framework that integrates pre-trained object detectors with SAM2 for Multi-Object Tracking and Segmentation, achieving state-of-the-art performance without fine-tuning and being detector-agnostic.",
        "tldr_zh": "Seg2Track-SAM2是一个框架，它将预训练的物体检测器与SAM2集成，用于多目标跟踪和分割，在不需要微调的情况下实现了最先进的性能，并且是检测器无关的。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "DRAG: Data Reconstruction Attack using Guided Diffusion",
        "summary": "With the rise of large foundation models, split inference (SI) has emerged as\na popular computational paradigm for deploying models across lightweight edge\ndevices and cloud servers, addressing data privacy and computational cost\nconcerns. However, most existing data reconstruction attacks have focused on\nsmaller CNN classification models, leaving the privacy risks of foundation\nmodels in SI settings largely unexplored. To address this gap, we propose a\nnovel data reconstruction attack based on guided diffusion, which leverages the\nrich prior knowledge embedded in a latent diffusion model (LDM) pre-trained on\na large-scale dataset. Our method performs iterative reconstruction on the\nLDM's learned image prior, effectively generating high-fidelity images\nresembling the original data from their intermediate representations (IR).\nExtensive experiments demonstrate that our approach significantly outperforms\nstate-of-the-art methods, both qualitatively and quantitatively, in\nreconstructing data from deep-layer IRs of the vision foundation model. The\nresults highlight the urgent need for more robust privacy protection mechanisms\nfor large models in SI scenarios. Code is available at:\nhttps://github.com/ntuaislab/DRAG.",
        "url": "http://arxiv.org/abs/2509.11724v1",
        "published_date": "2025-09-15T09:26:19+00:00",
        "updated_date": "2025-09-15T09:26:19+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Wa-Kin Lei",
            "Jun-Cheng Chen",
            "Shang-Tse Chen"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper introduces a data reconstruction attack using guided diffusion to reconstruct data from deep-layer intermediate representations of vision models.",
        "tldr_zh": "本文引入了一种使用引导扩散的数据重建攻击，以从视觉模型的深层中间表示中重建数据。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Advanced Layout Analysis Models for Docling",
        "summary": "This technical report documents the development of novel Layout Analysis\nmodels integrated into the Docling document-conversion pipeline. We trained\nseveral state-of-the-art object detectors based on the RT-DETR, RT-DETRv2 and\nDFINE architectures on a heterogeneous corpus of 150,000 documents (both openly\navailable and proprietary). Post-processing steps were applied to the raw\ndetections to make them more applicable to the document conversion task. We\nevaluated the effectiveness of the layout analysis on various document\nbenchmarks using different methodologies while also measuring the runtime\nperformance across different environments (CPU, Nvidia and Apple GPUs). We\nintroduce five new document layout models achieving 20.6% - 23.9% mAP\nimprovement over Docling's previous baseline, with comparable or better\nruntime. Our best model, \"heron-101\", attains 78% mAP with 28 ms/image\ninference time on a single NVIDIA A100 GPU. Extensive quantitative and\nqualitative experiments establish best practices for training, evaluating, and\ndeploying document-layout detectors, providing actionable guidance for the\ndocument conversion community. All trained checkpoints, code, and documentation\nare released under a permissive license on HuggingFace.",
        "url": "http://arxiv.org/abs/2509.11720v1",
        "published_date": "2025-09-15T09:20:11+00:00",
        "updated_date": "2025-09-15T09:20:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nikolaos Livathinos",
            "Christoph Auer",
            "Ahmed Nassar",
            "Rafael Teixeira de Lima",
            "Maksym Lysak",
            "Brown Ebouky",
            "Cesar Berrospi",
            "Michele Dolfi",
            "Panagiotis Vagenas",
            "Matteo Omenetti",
            "Kasper Dinkla",
            "Yusik Kim",
            "Valery Weber",
            "Lucas Morin",
            "Ingmar Meijer",
            "Viktor Kuropiatnyk",
            "Tim Strohmeyer",
            "A. Said Gurbuz",
            "Peter W. J. Staar"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces advanced layout analysis models integrated into the Docling document-conversion pipeline, achieving significant improvement in mAP with fast inference times on GPUs.",
        "tldr_zh": "本文介绍了集成到Docling文档转换流程中的先进布局分析模型，通过在GPU上取得了显着的mAP改进和快速推理时间。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CoachMe: Decoding Sport Elements with a Reference-Based Coaching Instruction Generation Model",
        "summary": "Motion instruction is a crucial task that helps athletes refine their\ntechnique by analyzing movements and providing corrective guidance. Although\nrecent advances in multimodal models have improved motion understanding,\ngenerating precise and sport-specific instruction remains challenging due to\nthe highly domain-specific nature of sports and the need for informative\nguidance. We propose CoachMe, a reference-based model that analyzes the\ndifferences between a learner's motion and a reference under temporal and\nphysical aspects. This approach enables both domain-knowledge learning and the\nacquisition of a coach-like thinking process that identifies movement errors\neffectively and provides feedback to explain how to improve. In this paper, we\nillustrate how CoachMe adapts well to specific sports such as skating and\nboxing by learning from general movements and then leveraging limited data.\nExperiments show that CoachMe provides high-quality instructions instead of\ndirections merely in the tone of a coach but without critical information.\nCoachMe outperforms GPT-4o by 31.6% in G-Eval on figure skating and by 58.3% on\nboxing. Analysis further confirms that it elaborates on errors and their\ncorresponding improvement methods in the generated instructions. You can find\nCoachMe here: https://motionxperts.github.io/",
        "url": "http://arxiv.org/abs/2509.11698v1",
        "published_date": "2025-09-15T09:01:39+00:00",
        "updated_date": "2025-09-15T09:01:39+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "I.2.7; I.2.10"
        ],
        "authors": [
            "Wei-Hsin Yeh",
            "Yu-An Su",
            "Chih-Ning Chen",
            "Yi-Hsueh Lin",
            "Calvin Ku",
            "Wen-Hsin Chiu",
            "Min-Chun Hu",
            "Lun-Wei Ku"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "CoachMe is a reference-based coaching instruction generation model that helps athletes improve their technique by analyzing movements and providing corrective guidance. It outperforms GPT-4o in figure skating and boxing.",
        "tldr_zh": "CoachMe是一种基于参考的教练指导生成模型，通过分析动作并提供纠正指导，帮助运动员改善他们的技术。它在花样滑冰和拳击方面优于GPT-4o。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.0
    },
    {
        "title": "Uncertainty-Aware Retinal Vessel Segmentation via Ensemble Distillation",
        "summary": "Uncertainty estimation is critical for reliable medical image segmentation,\nparticularly in retinal vessel analysis, where accurate predictions are\nessential for diagnostic applications. Deep Ensembles, where multiple networks\nare trained individually, are widely used to improve medical image segmentation\nperformance. However, training and testing costs increase with the number of\nensembles. In this work, we propose Ensemble Distillation as a robust\nalternative to commonly used uncertainty estimation techniques by distilling\nthe knowledge of multiple ensemble models into a single model. Through\nextensive experiments on the DRIVE and FIVES datasets, we demonstrate that\nEnsemble Distillation achieves comparable performance via calibration and\nsegmentation metrics, while significantly reducing computational complexity.\nThese findings suggest that Ensemble distillation provides an efficient and\nreliable approach for uncertainty estimation in the segmentation of the retinal\nvessels, making it a promising tool for medical imaging applications.",
        "url": "http://arxiv.org/abs/2509.11689v1",
        "published_date": "2025-09-15T08:41:19+00:00",
        "updated_date": "2025-09-15T08:41:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jeremiah Fadugba",
            "Petru Manescu",
            "Bolanle Oladejo",
            "Delmiro Fernandez-Reyes",
            "Philipp Berens"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces Ensemble Distillation as a method for uncertainty estimation in retinal vessel segmentation, achieving comparable performance with reduced computational complexity.",
        "tldr_zh": "该论文引入了Ensemble Distillation作为视网膜血管分割中的不确定性估计方法，在减少计算复杂性的同时实现了可比较的性能。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "ParaEQsA: Parallel and Asynchronous Embodied Questions Scheduling and Answering",
        "summary": "This paper formulates the Embodied Questions Answering (EQsA) problem,\nintroduces a corresponding benchmark, and proposes a system to tackle the\nproblem. Classical Embodied Question Answering (EQA) is typically formulated as\nanswering one single question by actively exploring a 3D environment. Real\ndeployments, however, often demand handling multiple questions that may arrive\nasynchronously and carry different urgencies. We formalize this setting as\nEmbodied Questions Answering (EQsA) and present ParaEQsA, a framework for\nparallel, urgency-aware scheduling and answering. ParaEQsA leverages a group\nmemory module shared among questions to reduce redundant exploration, and a\npriority-planning module to dynamically schedule questions. To evaluate this\nsetting, we contribute the Parallel Asynchronous Embodied Questions (PAEQs)\nbenchmark containing 40 indoor scenes and five questions per scene (200 in\ntotal), featuring asynchronous follow-up questions and urgency labels. We\nfurther propose metrics for EQsA performance: Direct Answer Rate (DAR), and\nNormalized Urgency-Weighted Latency (NUWL), which jointly measure efficiency\nand responsiveness of this system. ParaEQsA consistently outperforms strong\nsequential baselines adapted from recent EQA systems, while reducing\nexploration and delay. Empirical evaluations investigate the relative\ncontributions of priority, urgency modeling, spatial scope, reward estimation,\nand dependency reasoning within our framework. Together, these results\ndemonstrate that urgency-aware, parallel scheduling is key to making embodied\nagents responsive and efficient under realistic, multi-question workloads.",
        "url": "http://arxiv.org/abs/2509.11663v1",
        "published_date": "2025-09-15T08:02:55+00:00",
        "updated_date": "2025-09-15T08:02:55+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Haisheng Wang",
            "Weiming Zhi"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a system, ParaEQsA, for parallel and urgency-aware scheduling and answering of multiple questions in a 3D environment, outperforming sequential baselines and improving efficiency.",
        "tldr_zh": "本文介绍了一个系统ParaEQsA，用于在3D环境中并行且具有紧急感知的调度和回答多个问题，优于顺序基线并提高效率。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Optimizing Class Distributions for Bias-Aware Multi-Class Learning",
        "summary": "We propose BiCDO (Bias-Controlled Class Distribution Optimizer), an\niterative, data-centric framework that identifies Pareto optimized class\ndistributions for multi-class image classification. BiCDO enables performance\nprioritization for specific classes, which is useful in safety-critical\nscenarios (e.g. prioritizing 'Human' over 'Dog'). Unlike uniform distributions,\nBiCDO determines the optimal number of images per class to enhance reliability\nand minimize bias and variance in the objective function. BiCDO can be\nincorporated into existing training pipelines with minimal code changes and\nsupports any labelled multi-class dataset. We have validated BiCDO using\nEfficientNet, ResNet and ConvNeXt on CIFAR-10 and iNaturalist21 datasets,\ndemonstrating improved, balanced model performance through optimized data\ndistribution.",
        "url": "http://arxiv.org/abs/2509.11588v1",
        "published_date": "2025-09-15T05:13:21+00:00",
        "updated_date": "2025-09-15T05:13:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mirco Felske",
            "Stefan Stiene"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper introduces BiCDO, a framework for optimizing class distributions in multi-class image classification to enhance performance for specific classes and minimize bias and variance.",
        "tldr_zh": "该论文介绍了BiCDO，这是一个用于优化多类图像分类中的类分布的框架，以提高特定类别的性能并减少偏差和方差。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Gaussian-Plus-SDF SLAM: High-fidelity 3D Reconstruction at 150+ fps",
        "summary": "While recent Gaussian-based SLAM methods achieve photorealistic\nreconstruction from RGB-D data, their computational performance remains a\ncritical bottleneck. State-of-the-art techniques operate at less than 20 fps,\nsignificantly lagging behind geometry-centric approaches like KinectFusion\n(hundreds of fps). This limitation stems from the heavy computational burden:\nmodeling scenes requires numerous Gaussians and complex iterative optimization\nto fit RGB-D data, where insufficient Gaussian counts or optimization\niterations cause severe quality degradation. To address this, we propose a\nGaussian-SDF hybrid representation, combining a colorized Signed Distance Field\n(SDF) for smooth geometry and appearance with 3D Gaussians to capture\nunderrepresented details. The SDF is efficiently constructed via RGB-D fusion\n(as in geometry-centric methods), while Gaussians undergo iterative\noptimization. Our representation enables drastic Gaussian reduction (50% fewer)\nby avoiding full-scene Gaussian modeling, and efficient Gaussian optimization\n(75% fewer iterations) through targeted appearance refinement. Building upon\nthis representation, we develop GPS-SLAM (Gaussian-Plus-SDF SLAM), a real-time\n3D reconstruction system achieving over 150 fps on real-world Azure Kinect\nsequences -- delivering an order-of-magnitude speedup over state-of-the-art\ntechniques while maintaining comparable reconstruction quality. We will release\nthe source code and data to facilitate future research.",
        "url": "http://arxiv.org/abs/2509.11574v1",
        "published_date": "2025-09-15T04:37:32+00:00",
        "updated_date": "2025-09-15T04:37:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhexi Peng",
            "Kun Zhou",
            "Tianjia Shao"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces Gaussian-Plus-SDF SLAM for real-time 3D reconstruction at over 150 fps, with improved computational efficiency compared to existing methods.",
        "tldr_zh": "该论文介绍了高斯加SDF SLAM技术，可在超过150 fps下实现实时3D重建，在计算效率方面优于现有方法。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SFGNet: Semantic and Frequency Guided Network for Camouflaged Object Detection",
        "summary": "Camouflaged object detection (COD) aims to segment objects that blend into\ntheir surroundings. However, most existing studies overlook the semantic\ndifferences among textual prompts of different targets as well as fine-grained\nfrequency features. In this work, we propose a novel Semantic and Frequency\nGuided Network (SFGNet), which incorporates semantic prompts and\nfrequency-domain features to capture camouflaged objects and improve boundary\nperception. We further design Multi-Band Fourier Module(MBFM) to enhance the\nability of the network in handling complex backgrounds and blurred boundaries.\nIn addition, we design an Interactive Structure Enhancement Block (ISEB) to\nensure structural integrity and boundary details in the predictions. Extensive\nexperiments conducted on three COD benchmark datasets demonstrate that our\nmethod significantly outperforms state-of-the-art approaches. The core code of\nthe model is available at the following link:\nhttps://github.com/winter794444/SFGNetICASSP2026.",
        "url": "http://arxiv.org/abs/2509.11539v1",
        "published_date": "2025-09-15T03:15:31+00:00",
        "updated_date": "2025-09-15T03:15:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dezhen Wang",
            "Haixiang Zhao",
            "Xiang Shen",
            "Sheng Miao"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a Semantic and Frequency Guided Network for Camouflaged Object Detection, outperforming state-of-the-art approaches on benchmark datasets.",
        "tldr_zh": "该论文提出了一种用于伪装对象检测的语义和频率引导网络，在基准数据集上优于现有技术。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Multiple Instance Learning Framework with Masked Hard Instance Mining for Gigapixel Histopathology Image Analysis",
        "summary": "Digitizing pathological images into gigapixel Whole Slide Images (WSIs) has\nopened new avenues for Computational Pathology (CPath). As positive tissue\ncomprises only a small fraction of gigapixel WSIs, existing Multiple Instance\nLearning (MIL) methods typically focus on identifying salient instances via\nattention mechanisms. However, this leads to a bias towards easy-to-classify\ninstances while neglecting challenging ones. Recent studies have shown that\nhard examples are crucial for accurately modeling discriminative boundaries.\nApplying such an idea at the instance level, we elaborate a novel MIL framework\nwith masked hard instance mining (MHIM-MIL), which utilizes a Siamese structure\nwith a consistency constraint to explore the hard instances. Using a\nclass-aware instance probability, MHIM-MIL employs a momentum teacher to mask\nsalient instances and implicitly mine hard instances for training the student\nmodel. To obtain diverse, non-redundant hard instances, we adopt large-scale\nrandom masking while utilizing a global recycle network to mitigate the risk of\nlosing key features. Furthermore, the student updates the teacher using an\nexponential moving average, which identifies new hard instances for subsequent\ntraining iterations and stabilizes optimization. Experimental results on cancer\ndiagnosis, subtyping, survival analysis tasks, and 12 benchmarks demonstrate\nthat MHIM-MIL outperforms the latest methods in both performance and\nefficiency. The code is available at: https://github.com/DearCaat/MHIM-MIL.",
        "url": "http://arxiv.org/abs/2509.11526v1",
        "published_date": "2025-09-15T02:31:33+00:00",
        "updated_date": "2025-09-15T02:31:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenhao Tang",
            "Sheng Huang",
            "Heng Fang",
            "Fengtao Zhou",
            "Bo Liu",
            "Qingshan Liu"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel Multiple Instance Learning framework with masked hard instance mining for analyzing gigapixel histopathology images, outperforming existing methods in performance and efficiency.",
        "tldr_zh": "该论文介绍了一种新的多实例学习框架，采用遮蔽硬实例挖掘分析千兆像素组织病理图像，在性能和效率方面优于现有方法。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7.0
    },
    {
        "title": "Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs",
        "summary": "Vision-Language-Action (VLA) models have emerged as powerful generalist\npolicies for robotic control, yet their performance scaling across model\narchitectures and hardware platforms, as well as their associated power\nbudgets, remain poorly understood. This work presents an evaluation of five\nrepresentative VLA models -- spanning state-of-the-art baselines and two newly\nproposed architectures -- targeting edge and datacenter GPU platforms. Using\nthe LIBERO benchmark, we measure accuracy alongside system-level metrics,\nincluding latency, throughput, and peak memory usage, under varying edge power\nconstraints and high-performance datacenter GPU configurations. Our results\nidentify distinct scaling trends: (1) architectural choices, such as action\ntokenization and model backbone size, strongly influence throughput and memory\nfootprint; (2) power-constrained edge devices exhibit non-linear performance\ndegradation, with some configurations matching or exceeding older datacenter\nGPUs; and (3) high-throughput variants can be achieved without significant\naccuracy loss. These findings provide actionable insights when selecting and\noptimizing VLAs across a range of deployment constraints. Our work challenges\ncurrent assumptions about the superiority of datacenter hardware for robotic\ninference.",
        "url": "http://arxiv.org/abs/2509.11480v1",
        "published_date": "2025-09-15T00:00:37+00:00",
        "updated_date": "2025-09-15T00:00:37+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.ET",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Amir Taherin",
            "Juyi Lin",
            "Arash Akbari",
            "Arman Akbari",
            "Pu Zhao",
            "Weiwei Chen",
            "David Kaeli",
            "Yanzhi Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper evaluates different Vision-Language-Action models on edge and datacenter GPU platforms, showing how architectural choices and power constraints affect performance.",
        "tldr_zh": "本文在边缘和数据中心 GPU 平台上评估了不同的视觉-语言-动作模型，展示了架构选择和功耗约束如何影响性能。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    },
    {
        "title": "IMD: A 6-DoF Pose Estimation Benchmark for Industrial Metallic Objects",
        "summary": "Object 6DoF (6D) pose estimation is essential for robotic perception,\nespecially in industrial settings. It enables robots to interact with the\nenvironment and manipulate objects. However, existing benchmarks on object 6D\npose estimation primarily use everyday objects with rich textures and\nlow-reflectivity, limiting model generalization to industrial scenarios where\nobjects are often metallic, texture-less, and highly reflective. To address\nthis gap, we propose a novel dataset and benchmark namely \\textit{Industrial\nMetallic Dataset (IMD)}, tailored for industrial applications. Our dataset\ncomprises 45 true-to-scale industrial components, captured with an RGB-D camera\nunder natural indoor lighting and varied object arrangements to replicate\nreal-world conditions. The benchmark supports three tasks, including video\nobject segmentation, 6D pose tracking, and one-shot 6D pose estimation. We\nevaluate existing state-of-the-art models, including XMem and SAM2 for\nsegmentation, and BundleTrack and BundleSDF for pose estimation, to assess\nmodel performance in industrial contexts. Evaluation results show that our\nindustrial dataset is more challenging than existing household object datasets.\nThis benchmark provides the baseline for developing and comparing segmentation\nand pose estimation algorithms that better generalize to industrial robotics\nscenarios.",
        "url": "http://arxiv.org/abs/2509.11680v1",
        "published_date": "2025-09-15T08:28:15+00:00",
        "updated_date": "2025-09-15T08:28:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruimin Ma",
            "Sebastian Zudaire",
            "Zhen Li",
            "Chi Zhang"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper introduces a new benchmark dataset called IMD for 6-DoF pose estimation of industrial metallic objects, highlighting the challenges in generalizing existing models to industrial scenarios.",
        "tldr_zh": "本文介绍了一个名为IMD的新的基准数据集，用于工业金属物体的6DoF姿势估计，突出了将现有模型推广到工业场景中的挑战。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6.75
    },
    {
        "title": "FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptation",
        "summary": "Few-shot semantic segmentation has recently attracted great attention. The\ngoal is to develop a model capable of segmenting unseen classes using only a\nfew annotated samples. Most existing approaches adapt a pre-trained model by\ntraining from scratch an additional module. Achieving optimal performance with\nthese approaches requires extensive training on large-scale datasets. The\nSegment Anything Model 2 (SAM2) is a foundational model for zero-shot image and\nvideo segmentation with a modular design. In this paper, we propose a Few-Shot\nsegmentation method based on SAM2 (FS-SAM2), where SAM2's video capabilities\nare directly repurposed for the few-shot task. Moreover, we apply a Low-Rank\nAdaptation (LoRA) to the original modules in order to handle the diverse images\ntypically found in standard datasets, unlike the temporally connected frames\nused in SAM2's pre-training. With this approach, only a small number of\nparameters is meta-trained, which effectively adapts SAM2 while benefiting from\nits impressive segmentation performance. Our method supports any K-shot\nconfiguration. We evaluate FS-SAM2 on the PASCAL-5$^i$, COCO-20$^i$ and\nFSS-1000 datasets, achieving remarkable results and demonstrating excellent\ncomputational efficiency during inference. Code is available at\nhttps://github.com/fornib/FS-SAM2",
        "url": "http://arxiv.org/abs/2509.12105v1",
        "published_date": "2025-09-15T16:32:31+00:00",
        "updated_date": "2025-09-15T16:32:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bernardo Forni",
            "Gabriele Lombardi",
            "Federico Pozzi",
            "Mirco Planamente"
        ],
        "ai_categories": [
            "Transformer",
            "LoRA",
            "Dataset"
        ],
        "tldr": "The paper proposes FS-SAM2, a few-shot segmentation method based on SAM2, enhanced with Low-Rank Adaptation, achieving impressive results on various datasets with computational efficiency.",
        "tldr_zh": "本文提出了基于SAM2的少样本分割方法FS-SAM2，通过低秩适应以达到在各种数据集上取得出色结果并具有计算效率。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Microsurgical Instrument Segmentation for Robot-Assisted Surgery",
        "summary": "Accurate segmentation of thin structures is critical for microsurgical scene\nunderstanding but remains challenging due to resolution loss, low contrast, and\nclass imbalance. We propose Microsurgery Instrument Segmentation for Robotic\nAssistance(MISRA), a segmentation framework that augments RGB input with\nluminance channels, integrates skip attention to preserve elongated features,\nand employs an Iterative Feedback Module(IFM) for continuity restoration across\nmultiple passes. In addition, we introduce a dedicated microsurgical dataset\nwith fine-grained annotations of surgical instruments including thin objects,\nproviding a benchmark for robust evaluation Dataset available at\nhttps://huggingface.co/datasets/KIST-HARILAB/MISAW-Seg. Experiments demonstrate\nthat MISRA achieves competitive performance, improving the mean class IoU by\n5.37% over competing methods, while delivering more stable predictions at\ninstrument contacts and overlaps. These results position MISRA as a promising\nstep toward reliable scene parsing for computer-assisted and robotic\nmicrosurgery.",
        "url": "http://arxiv.org/abs/2509.11727v1",
        "published_date": "2025-09-15T09:29:27+00:00",
        "updated_date": "2025-09-15T09:29:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "I.4.6; I.4.8"
        ],
        "authors": [
            "Tae Kyeong Jeong",
            "Garam Kim",
            "Juyoun Park"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a segmentation framework for microsurgical instruments using luminance channels and attention mechanisms, achieving competitive performance and stable predictions.",
        "tldr_zh": "该论文介绍了一种使用亮度通道和注意机制的微外科工具分割框架，实现了竞争性能和稳定预测。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Geometric Analysis of Magnetic Labyrinthine Stripe Evolution via U-Net Segmentation",
        "summary": "Labyrinthine stripe patterns are common in many physical systems, yet their\nlack of long-range order makes quantitative characterization challenging. We\ninvestigate the evolution of such patterns in bismuth-doped yttrium iron garnet\n(Bi:YIG) films subjected to a magnetic field annealing protocol. A U-Net deep\nlearning model, trained with synthetic degradations including additive white\nGaussian and Simplex noise, enables robust segmentation of experimental\nmagneto-optical images despite noise and occlusions. Building on this\nsegmentation, we develop a geometric analysis pipeline based on\nskeletonization, graph mapping, and spline fitting, which quantifies local\nstripe propagation through length and curvature measurements. Applying this\nframework to 444 images from 12 annealing protocol trials, we analyze the\ntransition from the \"quenched\" state to a more parallel and coherent \"annealed\"\nstate, and identify two distinct evolution modes (Type A and Type B) linked to\nfield polarity. Our results provide a quantitative analysis of geometric and\ntopological properties in magnetic stripe patterns and offer new insights into\ntheir local structural evolution, and establish a general tool for analyzing\ncomplex labyrinthine systems.",
        "url": "http://arxiv.org/abs/2509.11485v1",
        "published_date": "2025-09-15T00:23:23+00:00",
        "updated_date": "2025-09-15T00:23:23+00:00",
        "categories": [
            "cond-mat.mtrl-sci",
            "cs.CV"
        ],
        "authors": [
            "Vinícius Yu Okubo",
            "Kotaro Shimizu",
            "B. S. Shivaran",
            "Gia-Wei Chern",
            "Hae Yong Kim"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper explores the evolution of labyrinthine stripe patterns in magnetic films using a U-Net segmentation model, providing insights into their local structural evolution.",
        "tldr_zh": "本文利用 U-Net 分割模型研究了磁膜中迷宫条带图案的演变，为了解其局部结构演变提供了见解。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Logit Mixture Outlier Exposure for Fine-grained Out-of-Distribution Detection",
        "summary": "The ability to detect out-of-distribution data is essential not only for\nensuring robustness against unknown or unexpected input data but also for\nimproving the generalization performance of the model. Among various\nout-of-distribution detection methods, Outlier Exposure and Mixture Outlier\nExposure are promising approaches that enhance out-of-distribution detection\nperformance by exposing the outlier data during training. However, even with\nthese sophisticated techniques, it remains challenging for models to learn the\nrelationships between classes effectively and to distinguish data sampling from\nin-distribution and out-of-distribution clearly. Therefore, we focus on the\nlogit space, where the properties between class-wise distributions are\ndistinctly separated from those in the input or feature spaces. Specifically,\nwe propose a linear interpolation technique in the logit space that mixes\nin-distribution and out-of-distribution data to facilitate smoothing logits\nbetween classes and improve the out-of-distribution detection performance,\nparticularly for out-of-distribution data that lie close to the in-distribution\ndata. Additionally, we enforce consistency between the logits obtained through\nmixing in the logit space and those generated via mixing in the input space.\nOur experiments demonstrate that our logit-space mixing technique reduces the\nabrupt fluctuations in the model outputs near the decision boundaries,\nresulting in smoother and more reliable separation between in-distribution and\nout-of-distribution data. Furthermore, we evaluate the effectiveness of the\nproposed method on a fine-grained out-of-distribution detection task.",
        "url": "http://arxiv.org/abs/2509.11892v1",
        "published_date": "2025-09-15T13:08:02+00:00",
        "updated_date": "2025-09-15T13:08:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Akito Shinohara",
            "Kohei Fukuda",
            "Hiroaki Aizawa"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a logit space mixing technique to improve out-of-distribution detection performance by smoothing the model outputs near decision boundaries, especially for data close to in-distribution data.",
        "tldr_zh": "本文提出了一种logit空间混合技术，通过在决策边界附近平滑模型输出来提高离群检测性能，特别适用于接近于正态分布数据的情况。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.25
    },
    {
        "title": "Multi-animal tracking in Transition: Comparative Insights into Established and Emerging Methods",
        "summary": "Precision livestock farming requires advanced monitoring tools to meet the\nincreasing management needs of the industry. Computer vision systems capable of\nlong-term multi-animal tracking (MAT) are essential for continuous behavioral\nmonitoring in livestock production. MAT, a specialized subset of multi-object\ntracking (MOT), shares many challenges with MOT, but also faces domain-specific\nissues including frequent animal occlusion, highly similar appearances among\nanimals, erratic motion patterns, and a wide range of behavior types.\n  While some existing MAT tools are user-friendly and widely adopted, they\noften underperform compared to state-of-the-art MOT methods, which can result\nin inaccurate downstream tasks such as behavior analysis, health state\nestimation, and related applications. In this study, we benchmarked both MAT\nand MOT approaches for long-term tracking of pigs. We compared tools such as\nDeepLabCut and idTracker with MOT-based methods including ByteTrack, DeepSORT,\ncross-input consistency, and newer approaches like Track-Anything and\nPromptTrack.\n  All methods were evaluated on a 10-minute pig tracking dataset. Our results\ndemonstrate that, overall, MOT approaches outperform traditional MAT tools,\neven for long-term tracking scenarios. These findings highlight the potential\nof recent MOT techniques to enhance the accuracy and reliability of automated\nlivestock tracking.",
        "url": "http://arxiv.org/abs/2509.11873v1",
        "published_date": "2025-09-15T12:52:31+00:00",
        "updated_date": "2025-09-15T12:52:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Anne Marthe Sophie Ngo Bibinbe",
            "Patrick Gagnon",
            "Jamie Ahloy-Dallaire",
            "Eric R. Paquet"
        ],
        "ai_categories": [
            "Dataset",
            "Multimodality"
        ],
        "tldr": "The paper compares traditional multi-animal tracking tools with state-of-the-art multi-object tracking methods in the context of livestock monitoring, showing that the latter outperform the former.",
        "tldr_zh": "本文比较了传统的多动物跟踪工具与最先进的多目标追踪方法，在畜牧监测领域展示出后者优于前者。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "Bridging the Gap Between Sparsity and Redundancy: A Dual-Decoding Framework with Global Context for Map Inference",
        "summary": "Trajectory data has become a key resource for automated map in-ference due to\nits low cost, broad coverage, and continuous availability. However, uneven\ntrajectory density often leads to frag-mented roads in sparse areas and\nredundant segments in dense regions, posing significant challenges for existing\nmethods. To address these issues, we propose DGMap, a dual-decoding framework\nwith global context awareness, featuring Multi-scale Grid Encoding,\nMask-enhanced Keypoint Extraction, and Global Context-aware Relation\nPrediction. By integrating global semantic context with local geometric\nfeatures, DGMap improves keypoint detection accuracy to reduce road\nfragmentation in sparse-trajectory areas. Additionally, the Global\nContext-aware Relation Prediction module suppresses false connections in\ndense-trajectory regions by modeling long-range trajectory patterns.\nExperimental results on three real-world datasets show that DGMap outperforms\nstate-of-the-art methods by 5% in APLS, with notable performance gains on\ntrajectory data from the Didi Chuxing platform",
        "url": "http://arxiv.org/abs/2509.11731v1",
        "published_date": "2025-09-15T09:31:38+00:00",
        "updated_date": "2025-09-15T09:31:38+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yudong Shen",
            "Wenyu Wu",
            "Jiali Mao",
            "Yixiao Tong",
            "Guoping Liu",
            "Chaoya Wang"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "DGMap is a dual-decoding framework that improves automated map inference using trajectory data, reducing road fragmentation in sparse areas and false connections in dense regions.",
        "tldr_zh": "DGMap是一个双解码框架，利用轨迹数据改进了自动地图推断，减少了稀疏区域的道路碎片化和密集区域中的错误连接。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "RouteExtract: A Modular Pipeline for Extracting Routes from Paper Maps",
        "summary": "Paper maps remain widely used for hiking and sightseeing because they contain\ncurated trails and locally relevant annotations that are often missing from\ndigital navigation applications such as Google Maps. We propose a pipeline to\nextract navigable trails from scanned maps, enabling their use in GPS-based\nnavigation. Our method combines georeferencing, U-Net-based binary\nsegmentation, graph construction, and an iterative refinement procedure using a\nrouting engine. We evaluate the full end-to-end pipeline as well as individual\ncomponents, showing that the approach can robustly recover trail networks from\ndiverse map styles and generate GPS routes suitable for practical use.",
        "url": "http://arxiv.org/abs/2509.11674v1",
        "published_date": "2025-09-15T08:16:12+00:00",
        "updated_date": "2025-09-15T08:16:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bjoern Kremser",
            "Yusuke Matsui"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a pipeline to extract navigable trails from paper maps for GPS-based navigation.",
        "tldr_zh": "本文介绍了一种从纸质地图中提取可通行路径的管线，用于基于GPS的导航。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.0
    },
    {
        "title": "DUAL-VAD: Dual Benchmarks and Anomaly-Focused Sampling for Video Anomaly Detection",
        "summary": "Video Anomaly Detection (VAD) is critical for surveillance and public safety.\nHowever, existing benchmarks are limited to either frame-level or video-level\ntasks, restricting a holistic view of model generalization. This work first\nintroduces a softmax-based frame allocation strategy that prioritizes\nanomaly-dense segments while maintaining full-video coverage, enabling balanced\nsampling across temporal scales. Building on this process, we construct two\ncomplementary benchmarks. The image-based benchmark evaluates frame-level\nreasoning with representative frames, while the video-based benchmark extends\nto temporally localized segments and incorporates an abnormality scoring\ntask.Experiments on UCF-Crime demonstrate improvements at both the frame and\nvideo levels, and ablation studies confirm clear advantages of anomaly-focused\nsampling over uniform and random baselines.",
        "url": "http://arxiv.org/abs/2509.11605v1",
        "published_date": "2025-09-15T05:48:22+00:00",
        "updated_date": "2025-09-15T05:48:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Seoik Jung",
            "Taekyung Song",
            "Joshua Jordan Daniel",
            "JinYoung Lee",
            "SungJun Lee"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a new anomaly-focused sampling strategy for video anomaly detection, leading to improved performance on both frame and video levels.",
        "tldr_zh": "本文提出了一种新的面向异常的抽样策略，用于视频异常检测，在帧级和视频级别上实现了性能的提升。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]