[
    {
        "title": "No Concept Left Behind: Test-Time Optimization for Compositional Text-to-Image Generation",
        "summary": "Despite recent advances in text-to-image (T2I) models, they often fail to\nfaithfully render all elements of complex prompts, frequently omitting or\nmisrepresenting specific objects and attributes. Test-time optimization has\nemerged as a promising approach to address this limitation by refining\ngeneration without the need for retraining. In this paper, we propose a\nfine-grained test-time optimization framework that enhances compositional\nfaithfulness in T2I generation. Unlike most of prior approaches that rely\nsolely on a global image/text similarity score, our method decomposes the input\nprompt into semantic concepts and evaluates alignment at both the global and\nconcept levels. A fine-grained variant of CLIP is used to compute concept-level\ncorrespondence, producing detailed feedback on missing or inaccurate concepts.\nThis feedback is fed into an iterative prompt refinement loop, enabling the\nlarge language model to propose improved prompts. Experiments on DrawBench and\nCompBench prompts demonstrate that our method significantly improves concept\ncoverage and human-judged faithfulness over both standard test-time\noptimization and the base T2I model. Code is available at:\nhttps://github.com/AmirMansurian/NoConceptLeftBehind",
        "url": "http://arxiv.org/abs/2509.23457v1",
        "published_date": "2025-09-27T18:59:49+00:00",
        "updated_date": "2025-09-27T18:59:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mohammad Hossein Sameti",
            "Amir M. Mansourian",
            "Arash Marioriyad",
            "Soheil Fadaee Oshyani",
            "Mohammad Hossein Rohban",
            "Mahdieh Soleymani Baghshah"
        ],
        "ai_categories": [
            "GAN",
            "AIGC"
        ],
        "tldr": "The paper proposes a test-time optimization framework to improve compositional faithfulness in text-to-image generation by decomposing input prompts into semantic concepts and refining the generation process iteratively.",
        "tldr_zh": "本文提出了一个测试时间优化框架，通过将输入提示分解成语义概念并迭代地优化生成过程，以提高文本到图片生成的构图忠实度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 10,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "HunyuanImage 3.0 Technical Report",
        "summary": "We present HunyuanImage 3.0, a native multimodal model that unifies\nmultimodal understanding and generation within an autoregressive framework,\nwith its image generation module publicly available. The achievement of\nHunyuanImage 3.0 relies on several key components, including meticulous data\ncuration, advanced architecture design, a native Chain-of-Thoughts schema,\nprogressive model pre-training, aggressive model post-training, and an\nefficient infrastructure that enables large-scale training and inference. With\nthese advancements, we successfully trained a Mixture-of-Experts (MoE) model\ncomprising over 80 billion parameters in total, with 13 billion parameters\nactivated per token during inference, making it the largest and most powerful\nopen-source image generative model to date. We conducted extensive experiments\nand the results of automatic and human evaluation of text-image alignment and\nvisual quality demonstrate that HunyuanImage 3.0 rivals previous\nstate-of-the-art models. By releasing the code and weights of HunyuanImage 3.0,\nwe aim to enable the community to explore new ideas with a state-of-the-art\nfoundation model, fostering a dynamic and vibrant multimodal ecosystem. All\nopen source assets are publicly available at\nhttps://github.com/Tencent-Hunyuan/HunyuanImage-3.0",
        "url": "http://arxiv.org/abs/2509.23951v1",
        "published_date": "2025-09-28T16:14:10+00:00",
        "updated_date": "2025-09-28T16:14:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Siyu Cao",
            "Hangting Chen",
            "Peng Chen",
            "Yiji Cheng",
            "Yutao Cui",
            "Xinchi Deng",
            "Ying Dong",
            "Kipper Gong",
            "Tianpeng Gu",
            "Xiusen Gu",
            "Tiankai Hang",
            "Duojun Huang",
            "Jie Jiang",
            "Zhengkai Jiang",
            "Weijie Kong",
            "Changlin Li",
            "Donghao Li",
            "Junzhe Li",
            "Xin Li",
            "Yang Li",
            "Zhenxi Li",
            "Zhimin Li",
            "Jiaxin Lin",
            "Linus",
            "Lucaz Liu",
            "Shu Liu",
            "Songtao Liu",
            "Yu Liu",
            "Yuhong Liu",
            "Yanxin Long",
            "Fanbin Lu",
            "Qinglin Lu",
            "Yuyang Peng",
            "Yuanbo Peng",
            "Xiangwei Shen",
            "Yixuan Shi",
            "Jiale Tao",
            "Yangyu Tao",
            "Qi Tian",
            "Pengfei Wan",
            "Chunyu Wang",
            "Kai Wang",
            "Lei Wang",
            "Linqing Wang",
            "Lucas Wang",
            "Qixun Wang",
            "Weiyan Wang",
            "Hao Wen",
            "Bing Wu",
            "Jianbing Wu",
            "Yue Wu",
            "Senhao Xie",
            "Fang Yang",
            "Miles Yang",
            "Xiaofeng Yang",
            "Xuan Yang",
            "Zhantao Yang",
            "Jingmiao Yu",
            "Zheng Yuan",
            "Chao Zhang",
            "Jian-Wei Zhang",
            "Peizhen Zhang",
            "Shi-Xue Zhang",
            "Tao Zhang",
            "Weigang Zhang",
            "Yepeng Zhang",
            "Yingfang Zhang",
            "Zihao Zhang",
            "Zijian Zhang",
            "Penghao Zhao",
            "Zhiyuan Zhao",
            "Xuefei Zhe",
            "Jianchen Zhu",
            "Zhao Zhong"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "HunyuanImage 3.0 is a large multimodal model for image generation, achieving state-of-the-art results and releasing code and weights for the community.",
        "tldr_zh": "HunyuanImage 3.0是一个用于图像生成的庞大多模态模型，取得了最先进的结果，并为社区发布了代码和权重。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention",
        "summary": "In Diffusion Transformer (DiT) models, particularly for video generation,\nattention latency is a major bottleneck due to the long sequence length and the\nquadratic complexity. We find that attention weights can be separated into two\nparts: a small fraction of large weights with high rank and the remaining\nweights with very low rank. This naturally suggests applying sparse\nacceleration to the first part and low-rank acceleration to the second. Based\non this finding, we propose SLA (Sparse-Linear Attention), a trainable\nattention method that fuses sparse and linear attention to accelerate diffusion\nmodels. SLA classifies attention weights into critical, marginal, and\nnegligible categories, applying O(N^2) attention to critical weights, O(N)\nattention to marginal weights, and skipping negligible ones. SLA combines these\ncomputations into a single GPU kernel and supports both forward and backward\npasses. With only a few fine-tuning steps using SLA, DiT models achieve a 20x\nreduction in attention computation, resulting in significant acceleration\nwithout loss of generation quality. Experiments show that SLA reduces attention\ncomputation by 95% without degrading end-to-end generation quality,\noutperforming baseline methods. In addition, we implement an efficient GPU\nkernel for SLA, which yields a 13.7x speedup in attention computation and a\n2.2x end-to-end speedup in video generation on Wan2.1-1.3B.",
        "url": "http://arxiv.org/abs/2509.24006v1",
        "published_date": "2025-09-28T17:58:59+00:00",
        "updated_date": "2025-09-28T17:58:59+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Jintao Zhang",
            "Haoxu Wang",
            "Kai Jiang",
            "Shuo Yang",
            "Kaiwen Zheng",
            "Haocheng Xi",
            "Ziteng Wang",
            "Hongzhou Zhu",
            "Min Zhao",
            "Ion Stoica",
            "Joseph E. Gonzalez",
            "Jun Zhu",
            "Jianfei Chen"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer"
        ],
        "tldr": "The paper introduces SLA, a method that combines sparse and linear attention to accelerate Diffusion Transformer models for video generation, achieving significant attention computation reduction without sacrificing generation quality.",
        "tldr_zh": "本文介绍了SLA方法，结合了稀疏和线性注意力，加速扩散变换器模型用于视频生成，在不损失生成质量的情况下实现了显著的注意力计算减少。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "SIE3D: Single-image Expressive 3D Avatar generation via Semantic Embedding and Perceptual Expression Loss",
        "summary": "Generating high-fidelity 3D head avatars from a single image is challenging,\nas current methods lack fine-grained, intuitive control over expressions via\ntext. This paper proposes SIE3D, a framework that generates expressive 3D\navatars from a single image and descriptive text. SIE3D fuses identity features\nfrom the image with semantic embedding from text through a novel conditioning\nscheme, enabling detailed control. To ensure generated expressions accurately\nmatch the text, it introduces an innovative perceptual expression loss\nfunction. This loss uses a pre-trained expression classifier to regularize the\ngeneration process, guaranteeing expression accuracy. Extensive experiments\nshow SIE3D significantly improves controllability and realism, outperforming\ncompetitive methods in identity preservation and expression fidelity on a\nsingle consumer-grade GPU. Project page:\nhttps://blazingcrystal1747.github.io/SIE3D/",
        "url": "http://arxiv.org/abs/2509.24004v1",
        "published_date": "2025-09-28T17:56:42+00:00",
        "updated_date": "2025-09-28T17:56:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhiqi Huang",
            "Dulongkai Cui",
            "Jinglu Hu"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Dataset"
        ],
        "tldr": "SIE3D is a framework that generates expressive 3D avatars from a single image and text by combining identity features and semantic embedding with a novel expression loss function.",
        "tldr_zh": "SIE3D是一个框架，通过组合图像的身份特征和语义嵌入以及新颖的表情损失函数，从单个图像和文本生成富有表现力的3D头像。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "EditScore: Unlocking Online RL for Image Editing via High-Fidelity Reward Modeling",
        "summary": "Instruction-guided image editing has achieved remarkable progress, yet\ncurrent models still face challenges with complex instructions and often\nrequire multiple samples to produce a desired result. Reinforcement Learning\n(RL) offers a promising solution, but its adoption in image editing has been\nseverely hindered by the lack of a high-fidelity, efficient reward signal. In\nthis work, we present a comprehensive methodology to overcome this barrier,\ncentered on the development of a state-of-the-art, specialized reward model. We\nfirst introduce EditReward-Bench, a comprehensive benchmark to systematically\nevaluate reward models on editing quality. Building on this benchmark, we\ndevelop EditScore, a series of reward models (7B-72B) for evaluating the\nquality of instruction-guided image editing. Through meticulous data curation\nand filtering, EditScore effectively matches the performance of learning\nproprietary VLMs. Furthermore, coupled with an effective self-ensemble strategy\ntailored for the generative nature of EditScore, our largest variant even\nsurpasses GPT-5 in the benchmark. We then demonstrate that a high-fidelity\nreward model is the key to unlocking online RL for image editing. Our\nexperiments show that, while even the largest open-source VLMs fail to provide\nan effective learning signal, EditScore enables efficient and robust policy\noptimization. Applying our framework to a strong base model, OmniGen2, results\nin a final model that shows a substantial and consistent performance uplift.\nOverall, this work provides the first systematic path from benchmarking to\nreward modeling to RL training in image editing, showing that a high-fidelity,\ndomain-specialized reward model is the key to unlocking the full potential of\nRL in this domain.",
        "url": "http://arxiv.org/abs/2509.23909v1",
        "published_date": "2025-09-28T14:28:24+00:00",
        "updated_date": "2025-09-28T14:28:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xin Luo",
            "Jiahao Wang",
            "Chenyuan Wu",
            "Shitao Xiao",
            "Xiyan Jiang",
            "Defu Lian",
            "Jiajun Zhang",
            "Dong Liu",
            "Zheng liu"
        ],
        "ai_categories": [
            "AIGC",
            "GAN"
        ],
        "tldr": "The paper introduces EditScore, a reward model for instruction-guided image editing using Reinforcement Learning, showing promising results in overcoming challenges faced by current models.",
        "tldr_zh": "本文介绍了EditScore，这是一个用于指导图像编辑的奖励模型，使用强化学习，展示出很好的结果，克服了当前模型所面临的挑战。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "Not All Tokens are Guided Equal: Improving Guidance in Visual Autoregressive Models",
        "summary": "Autoregressive (AR) models based on next-scale prediction are rapidly\nemerging as a powerful tool for image generation, but they face a critical\nweakness: information inconsistencies between patches across timesteps\nintroduced by progressive resolution scaling. These inconsistencies scatter\nguidance signals, causing them to drift away from conditioning information and\nleaving behind ambiguous, unfaithful features. We tackle this challenge with\nInformation-Grounding Guidance (IGG), a novel mechanism that anchors guidance\nto semantically important regions through attention. By adaptively reinforcing\ninformative patches during sampling, IGG ensures that guidance and content\nremain tightly aligned. Across both class-conditioned and text-to-image\ngeneration tasks, IGG delivers sharper, more coherent, and semantically\ngrounded images, setting a new benchmark for AR-based methods.",
        "url": "http://arxiv.org/abs/2509.23876v1",
        "published_date": "2025-09-28T13:33:49+00:00",
        "updated_date": "2025-09-28T13:33:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ky Dan Nguyen",
            "Hoang Lam Tran",
            "Anh-Dung Dinh",
            "Daochang Liu",
            "Weidong Cai",
            "Xiuying Wang",
            "Chang Xu"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper introduces Information-Grounding Guidance (IGG) to improve guidance in visual autoregressive models for image generation by anchoring guidance to semantically important regions through attention.",
        "tldr_zh": "本文引入信息基础引导（IGG）来改进视觉自回归模型中的引导，通过关注将引导锚定到语义重要区域，用于图像生成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "RIV: Recursive Introspection Mask Diffusion Vision Language Model",
        "summary": "Mask Diffusion-based Vision Language Models (MDVLMs) have achieved remarkable\nprogress in multimodal understanding tasks. However, these models are unable to\ncorrect errors in generated tokens, meaning they lack self-correction\ncapability. In this paper, we propose Recursive Introspection Mask Diffusion\nVision Language Model (RIV), which equips the model with self-correction\nability through two novel mechanisms. The first is Introspection Training,\nwhere an Introspection Model is introduced to identify errors within generated\nsequences. Introspection Training enables the model to detect not only\ngrammatical and spelling mistakes, but more importantly, logical errors. The\nsecond is Recursive Inference. Beginning with the standard unmasking step, the\nlearned Introspection Model helps to identify errors in the output sequence and\nremask them. This alternating\n($\\text{unmask}\\rightarrow\\text{introspection}\\rightarrow\\text{remask}$)\nprocess is repeated recursively until reliable results are obtained.\nExperimental results on multiple benchmarks demonstrate that the proposed RIV\nachieves state-of-the-art performance, outperforming most existing MDVLMs.",
        "url": "http://arxiv.org/abs/2509.23625v1",
        "published_date": "2025-09-28T04:01:46+00:00",
        "updated_date": "2025-09-28T04:01:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "YuQian Li",
            "Limeng Qiao",
            "Lin Ma"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "Multimodality"
        ],
        "tldr": "The RIV model introduces self-correction mechanisms to improve the performance of vision language models, surpassing existing models in various benchmarks.",
        "tldr_zh": "RIV 模型引入自我修正机制，提高视觉语言模型的性能，在各项基准测试中超越了现有模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "SPIKE-RL: Video-LLMs meet Bayesian Surprise",
        "summary": "Real-world videos often show routine activities punctuated by memorable,\nsurprising events. However, most Video-LLMs process videos by sampling frames\nuniformly, likely missing critical moments that define a video's narrative. We\nintroduce SPIKE, an inference-time framework that quantifies Bayesian Surprise\nas the belief update triggered by new visual evidence in the video stream,\nidentifying moments where new visual evidence conflicts with prior beliefs.\nSPIKE effectively localizes surprise in videos, strongly correlated with humans\non positive (FunQA) and negative (Oops!) surprise benchmarks. Since the beliefs\nof zero-shot Video-LLMs are often suboptimal, we develop SPIKE-RL, which\nleverages GRPO to optimize belief hypotheses based on a reward signal from the\nvideo caption. SPIKE and SPIKE-RL guide query-agnostic surprise-weighted frame\nsampling, which allocates more frames to interesting moments in the video. With\nthis strategy, we achieve consistent performance gains on five downstream\nbenchmarks over uniform sampling. By enabling Video-LLMs to track beliefs and\nregister surprise, our work paves the way for more robust models that can\nrevise their understanding in response to new information.",
        "url": "http://arxiv.org/abs/2509.23433v1",
        "published_date": "2025-09-27T18:02:23+00:00",
        "updated_date": "2025-09-27T18:02:23+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Sahithya Ravi",
            "Aditya Chinchure",
            "Raymond T. Ng",
            "Leonid Sigal",
            "Vered Shwartz"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "SPIKE-RL introduces a framework that quantifies Bayesian Surprise in videos to identify moments of conflicting visual evidence, improving Video-LLMs' performance on surprise benchmarks.",
        "tldr_zh": "SPIKE-RL引入了一个框架，用于量化视频中的贝叶斯惊喜，以识别出冲突视觉证据的时刻，从而提高了视频模型在惊喜基准上的表现。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "FairViT-GAN: A Hybrid Vision Transformer with Adversarial Debiasing for Fair and Explainable Facial Beauty Prediction",
        "summary": "Facial Beauty Prediction (FBP) has made significant strides with the\napplication of deep learning, yet state-of-the-art models often exhibit\ncritical limitations, including architectural constraints, inherent demographic\nbiases, and a lack of transparency. Existing methods, primarily based on\nConvolutional Neural Networks (CNNs), excel at capturing local texture but\nstruggle with global facial harmony, while Vision Transformers (ViTs)\neffectively model long-range dependencies but can miss fine-grained details.\nFurthermore, models trained on benchmark datasets can inadvertently learn and\nperpetuate societal biases related to protected attributes like ethnicity. To\naddress these interconnected challenges, we propose \\textbf{FairViT-GAN}, a\nnovel hybrid framework that synergistically integrates a CNN branch for local\nfeature extraction and a ViT branch for global context modeling. More\nsignificantly, we introduce an adversarial debiasing mechanism where the\nfeature extractor is explicitly trained to produce representations that are\ninvariant to protected attributes, thereby actively mitigating algorithmic\nbias. Our framework's transparency is enhanced by visualizing the distinct\nfocus of each architectural branch. Extensive experiments on the SCUT-FBP5500\nbenchmark demonstrate that FairViT-GAN not only sets a new state-of-the-art in\npredictive accuracy, achieving a Pearson Correlation of \\textbf{0.9230} and\nreducing RMSE to \\textbf{0.2650}, but also excels in fairness. Our analysis\nreveals a remarkable \\textbf{82.9\\% reduction in the performance gap} between\nethnic subgroups, with the adversary's classification accuracy dropping to\nnear-random chance (52.1\\%). We believe FairViT-GAN provides a robust,\ntransparent, and significantly fairer blueprint for developing responsible AI\nsystems for subjective visual assessment.",
        "url": "http://arxiv.org/abs/2509.23859v1",
        "published_date": "2025-09-28T12:55:31+00:00",
        "updated_date": "2025-09-28T12:55:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Djamel Eddine Boukhari"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "FairViT-GAN proposes a hybrid framework combining CNN and ViT branches with adversarial debiasing for fair and explainable facial beauty prediction, achieving state-of-the-art accuracy and fairness.",
        "tldr_zh": "FairViT-GAN提出了一种混合框架，结合了CNN和ViT分支，并采用对抗去偏见技术，实现了领先水平的准确性和公平性。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Uni4D-LLM: A Unified SpatioTemporal-Aware VLM for 4D Understanding and Generation",
        "summary": "Vision-language models (VLMs) have demonstrated strong performance in 2D\nscene understanding and generation, but extending this unification to the\nphysical world remains an open challenge. Existing 3D and 4D approaches\ntypically embed scene geometry into autoregressive model for semantic\nunderstanding and diffusion model for content generation. This paradigm gap\nprevents a single model from jointly handling both tasks, especially in dynamic\n4D settings where spatiotemporal modeling is critical. We propose Uni4D-LLM,\nthe first unified VLM framework with spatiotemporal awareness for 4D scene\nunderstanding and generation. Our design is guided by two key insights: 1)\nUnification requires a shared representation. We extract semantic features for\nunderstanding and noisy-injected appearance features for generation,\nincorporate 4D geometric cues, and fuse them into a spatiotemporal-aware visual\nrepresentation through adaptive cross-attention. 2) Unification requires a\nshared architecture. Both autoregression and diffusion are built on Transformer\nbackbones, and this enables integration into a single LLM with task-specific\nheads. By aligning visual and linguistic representations, our Uni4D-LLM\nproduces predictions for both understanding and generation within one\nTransformer-based framework. We further apply instruction fine-tuning on\ndiverse 4D vision-language datasets to improve generalization across tasks.\nExtensive experiments on multiple benchmarks demonstrate that Uni4D-LLM\nachieves competitive or superior results compared to state-of-the-art models\nand offers the first true unification of 4D scene understanding and generation.",
        "url": "http://arxiv.org/abs/2509.23828v1",
        "published_date": "2025-09-28T12:06:54+00:00",
        "updated_date": "2025-09-28T12:06:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hanyu Zhou",
            "Gim Hee Lee"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "Uni4D-LLM is a unified VLM framework for 4D scene understanding and generation, which outperforms existing models by incorporating spatiotemporal awareness and shared representations and architecture.",
        "tldr_zh": "Uni4D-LLM是一个用于4D场景理解和生成的统一VLM框架，通过融合时空感知和共享表示和架构，优于现有模型。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "GenView++: Unifying Adaptive View Generation and Quality-Driven Supervision for Contrastive Representation Learning",
        "summary": "The success of contrastive learning depends on the construction and\nutilization of high-quality positive pairs. However, current methods face\ncritical limitations on two fronts: on the construction side, both handcrafted\nand generative augmentations often suffer from limited diversity and risk\nsemantic corruption; on the learning side, the absence of a quality assessment\nmechanism leads to suboptimal supervision where all pairs are treated equally.\nTo tackle these challenges, we propose GenView++, a unified framework that\naddresses both fronts by introducing two synergistic innovations. To improve\npair construction, GenView++ introduces a multi-source adaptive view generation\nmechanism to synthesize diverse yet semantically coherent views by dynamically\nmodulating generative parameters across image-conditioned, text-conditioned,\nand image-text-conditioned strategies. Second, a quality-driven contrastive\nlearning mechanism assesses each pair's semantic alignment and diversity to\ndynamically reweight their training contribution, prioritizing high-quality\npairs while suppressing redundant or misaligned pairs. Extensive experiments\ndemonstrate the effectiveness of GenView++ across both vision and\nvision-language tasks. For vision representation learning, it improves MoCov2\nby +2.5% on ImageNet linear classification. For vision-language learning, it\nraises the average zero-shot classification accuracy by +12.31% over CLIP and\n+5.31% over SLIP across ten datasets, and further improves Flickr30k text\nretrieval R@5 by +3.2%. The code is available at\nhttps://github.com/xiaojieli0903/GenViewPlusPlus.",
        "url": "http://arxiv.org/abs/2509.23770v1",
        "published_date": "2025-09-28T09:35:37+00:00",
        "updated_date": "2025-09-28T09:35:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaojie Li",
            "Bei Wang",
            "Jianlong Wu",
            "Yue Yu",
            "Liqiang Nie",
            "Min Zhang"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Transformer"
        ],
        "tldr": "GenView++ proposes a unified framework to improve contrastive representation learning by generating diverse yet coherent views and assessing pair quality for optimal training supervision. It shows significant performance improvements on vision and vision-language tasks.",
        "tldr_zh": "GenView++提出了一个统一框架，通过生成多样化且连贯的视图，并评估成对质量以获取最佳训练监督，从而改进对比表示学习。在视觉和视觉-语言任务中显示出显著性能改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "UniAlignment: Semantic Alignment for Unified Image Generation, Understanding, Manipulation and Perception",
        "summary": "The remarkable success of diffusion models in text-to-image generation has\nsparked growing interest in expanding their capabilities to a variety of\nmulti-modal tasks, including image understanding, manipulation, and perception.\nThese tasks require advanced semantic comprehension across both visual and\ntextual modalities, especially in scenarios involving complex semantic\ninstructions. However, existing approaches often rely heavily on\nvision-language models (VLMs) or modular designs for semantic guidance, leading\nto fragmented architectures and computational inefficiency. To address these\nchallenges, we propose UniAlignment, a unified multimodal generation framework\nwithin a single diffusion transformer. UniAlignment introduces a dual-stream\ndiffusion training strategy that incorporates both intrinsic-modal semantic\nalignment and cross-modal semantic alignment, thereby enhancing the model's\ncross-modal consistency and instruction-following robustness. Additionally, we\npresent SemGen-Bench, a new benchmark specifically designed to evaluate\nmultimodal semantic consistency under complex textual instructions. Extensive\nexperiments across multiple tasks and benchmarks demonstrate that UniAlignment\noutperforms existing baselines, underscoring the significant potential of\ndiffusion models in unified multimodal generation.",
        "url": "http://arxiv.org/abs/2509.23760v1",
        "published_date": "2025-09-28T09:11:30+00:00",
        "updated_date": "2025-09-28T09:11:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinyang Song",
            "Libin Wang",
            "Weining Wang",
            "Shaozhen Liu",
            "Dandan Zheng",
            "Jingdong Chen",
            "Qi Li",
            "Zhenan Sun"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "UniAlignment proposes a unified multimodal generation framework using a single diffusion transformer, outperforming existing baselines in various tasks.",
        "tldr_zh": "UniAlignment提出了一个统一的多模态生成框架，使用单个扩散变换器，在各种任务中优于现有基线。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "HieraTok: Multi-Scale Visual Tokenizer Improves Image Reconstruction and Generation",
        "summary": "In this work, we present HieraTok, a novel multi-scale Vision Transformer\n(ViT)-based tokenizer that overcomes the inherent limitation of modeling\nsingle-scale representations. This is realized through two key designs: (1)\nmulti-scale downsampling applied to the token map generated by the tokenizer\nencoder, producing a sequence of multi-scale tokens, and (2) a scale-causal\nattention mechanism that enables the progressive flow of information from\nlow-resolution global semantic features to high-resolution structural details.\nCoupling these designs, HieraTok achieves significant improvements in both\nimage reconstruction and generation tasks. Under identical settings, the\nmulti-scale visual tokenizer outperforms its single-scale counterpart by a\n27.2\\% improvement in rFID ($1.47 \\rightarrow 1.07$). When integrated into\ndownstream generation frameworks, it achieves a $1.38\\times$ faster convergence\nrate and an 18.9\\% boost in gFID ($16.4 \\rightarrow 13.3$), which may be\nattributed to the smoother and more uniformly distributed latent space.\nFurthermore, by scaling up the tokenizer's training, we demonstrate its\npotential by a sota rFID of 0.45 and a gFID of 1.82 among ViT tokenizers. To\nthe best of our knowledge, we are the first to introduce multi-scale ViT-based\ntokenizer in image reconstruction and image generation. We hope our findings\nand designs advance the ViT-based tokenizers in visual generation tasks.",
        "url": "http://arxiv.org/abs/2509.23736v1",
        "published_date": "2025-09-28T08:30:26+00:00",
        "updated_date": "2025-09-28T08:30:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Cong Chen",
            "Ziyuan Huang",
            "Cheng Zou",
            "Muzhi Zhu",
            "Kaixiang Ji",
            "Jiajia Liu",
            "Jingdong Chen",
            "Hao Chen",
            "Chunhua Shen"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "HieraTok is a novel multi-scale ViT-based tokenizer that improves image reconstruction and generation tasks, outperforming single-scale counterparts.",
        "tldr_zh": "HieraTok是一种新颖的多尺度ViT基于的分词器，可以提升图像重建和生成任务的性能，优于单尺度模型。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training",
        "summary": "We present LLaVA-OneVision-1.5, a novel family of Large Multimodal Models\n(LMMs) that achieve state-of-the-art performance with significantly reduced\ncomputational and financial costs. Different from the existing works,\nLLaVA-OneVision-1.5 provides an open, efficient, and reproducible framework for\nbuilding high-quality vision-language models entirely from scratch. The\nLLaVA-OneVision-1.5 release comprises three primary components: (1) Large-Scale\nCurated Datasets: We construct an 85M concept-balanced pretraining dataset\nLLaVA-OneVision-1.5-Mid-Traning and a meticulously curated 26M instruction\ndataset LLaVA-OneVision-1.5-Instruct, collectively encompassing 64B compressed\nmultimodal tokens. (2) Efficient Training Framework: We develop a complete\nend-to-end efficient training framework leveraging an offline parallel data\npacking strategy to facilitate the training of LLaVA-OneVision-1.5 within a\n$16,000 budget. (3) State-of-the-art Performance: Experimental results\ndemonstrate that LLaVA-OneVision1.5 yields exceptionally competitive\nperformance across a broad range of downstream tasks. Specifically,\nLLaVA-OneVision-1.5-8B outperforms Qwen2.5-VL-7B on 18 of 27 benchmarks, and\nLLaVA-OneVision-1.5-4B surpasses Qwen2.5-VL-3B on all 27 benchmarks. We\nanticipate releasing LLaVA-OneVision-1.5-RL shortly and encourage the community\nto await further updates.",
        "url": "http://arxiv.org/abs/2509.23661v1",
        "published_date": "2025-09-28T05:52:55+00:00",
        "updated_date": "2025-09-28T05:52:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiang An",
            "Yin Xie",
            "Kaicheng Yang",
            "Wenkang Zhang",
            "Xiuwei Zhao",
            "Zheng Cheng",
            "Yirui Wang",
            "Songcen Xu",
            "Changrui Chen",
            "Chunsheng Wu",
            "Huajie Tan",
            "Chunyuan Li",
            "Jing Yang",
            "Jie Yu",
            "Xiyao Wang",
            "Bin Qin",
            "Yumeng Wang",
            "Zizhen Yan",
            "Ziyong Feng",
            "Ziwei Liu",
            "Bo Li",
            "Jiankang Deng"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "LLaVA-OneVision-1.5 introduces a novel framework for efficient vision-language model training with state-of-the-art performance at reduced costs.",
        "tldr_zh": "LLaVA-OneVision-1.5引入了一个新颖的框架，能够以更低成本实现高效的视觉语言模型训练，并具有领先的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "FrameMind: Frame-Interleaved Chain-of-Thought for Video Reasoning via Reinforcement Learning",
        "summary": "Current video understanding models rely on fixed frame sampling strategies,\nprocessing predetermined visual inputs regardless of the specific reasoning\nrequirements of each question. This static approach limits their ability to\nadaptively gather visual evidence, leading to suboptimal performance on tasks\nthat require either broad temporal coverage or fine-grained spatial detail. In\nthis paper, we introduce FrameMind, an end-to-end framework trained with\nreinforcement learning that enables models to dynamically request visual\ninformation during reasoning through Frame-Interleaved Chain-of-Thought\n(FiCOT). Unlike traditional approaches, FrameMind operates in multiple turns\nwhere the model alternates between textual reasoning and active visual\nperception, using tools to extract targeted frames or video clips based on\nidentified knowledge gaps. To train effective dynamic sampling policies, we\npropose Dynamic Resolution Frame Sampling (DRFS), which exposes models to\ndiverse temporal-spatial trade-offs during learning, and DRFS-GRPO, a\ngroup-relative policy optimization algorithm that learns from outcome-based\nrewards without requiring frame-level annotations. Extensive experiments on\nchallenging benchmarks like MLVU and VideoMME demonstrate that our method\nsignificantly outperforms existing models, advancing the state of the art in\nflexible and efficient video understanding.",
        "url": "http://arxiv.org/abs/2509.24008v1",
        "published_date": "2025-09-28T17:59:43+00:00",
        "updated_date": "2025-09-28T17:59:43+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Haonan Ge",
            "Yiwei Wang",
            "Kai-Wei Chang",
            "Hang Wu",
            "Yujun Cai"
        ],
        "ai_categories": [
            "Transformer",
            "Reinforcement Learning",
            "Dataset"
        ],
        "tldr": "FrameMind introduces a dynamic video reasoning framework with reinforcement learning to gather visual evidence adaptively, significantly outperforming existing models in video understanding tasks.",
        "tldr_zh": "FrameMind通过强化学习引入了动态视频推理框架，能够自适应地收集视觉证据，在视频理解任务中明显优于现有模型。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "TREAT-Net: Tabular-Referenced Echocardiography Analysis for Acute Coronary Syndrome Treatment Prediction",
        "summary": "Coronary angiography remains the gold standard for diagnosing Acute Coronary\nSyndrome (ACS). However, its resource-intensive and invasive nature can expose\npatients to procedural risks and diagnostic delays, leading to postponed\ntreatment initiation. In this work, we introduce TREAT-Net, a multimodal deep\nlearning framework for ACS treatment prediction that leverages non-invasive\nmodalities, including echocardiography videos and structured clinical records.\nTREAT-Net integrates tabular-guided cross-attention to enhance video\ninterpretation, along with a late fusion mechanism to align predictions across\nmodalities. Trained on a dataset of over 9000 ACS cases, the model outperforms\nunimodal and non-fused baselines, achieving a balanced accuracy of 67.6% and an\nAUROC of 71.1%. Cross-modality agreement analysis demonstrates 88.6% accuracy\nfor intervention prediction. These findings highlight the potential of\nTREAT-Net as a non-invasive tool for timely and accurate patient triage,\nparticularly in underserved populations with limited access to coronary\nangiography.",
        "url": "http://arxiv.org/abs/2509.23999v1",
        "published_date": "2025-09-28T17:45:01+00:00",
        "updated_date": "2025-09-28T17:45:01+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Diane Kim",
            "Minh Nguyen Nhat To",
            "Sherif Abdalla",
            "Teresa S. M. Tsang",
            "Purang Abolmaesumi",
            "and Christina Luong"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "TREAT-Net is a deep learning framework that uses echocardiography and clinical records to predict treatments for Acute Coronary Syndrome, showing promising results for patient triage.",
        "tldr_zh": "TREAT-Net是一个深度学习框架，利用心脏超声和临床记录来预测急性冠状动脉综合征的治疗，对患者分类表现出良好的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Advancing Multi-agent Traffic Simulation via R1-Style Reinforcement Fine-Tuning",
        "summary": "Scalable and realistic simulation of multi-agent traffic behavior is critical\nfor advancing autonomous driving technologies. Although existing data-driven\nsimulators have made significant strides in this domain, they predominantly\nrely on supervised learning to align simulated distributions with real-world\ndriving scenarios. A persistent challenge, however, lies in the distributional\nshift that arises between training and testing, which often undermines model\ngeneralization in unseen environments. To address this limitation, we propose\nSMART-R1, a novel R1-style reinforcement fine-tuning paradigm tailored for\nnext-token prediction models to better align agent behavior with human\npreferences and evaluation metrics. Our approach introduces a metric-oriented\npolicy optimization algorithm to improve distribution alignment and an\niterative \"SFT-RFT-SFT\" training strategy that alternates between Supervised\nFine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT) to maximize performance\ngains. Extensive experiments on the large-scale Waymo Open Motion Dataset\n(WOMD) validate the effectiveness of this simple yet powerful R1-style training\nframework in enhancing foundation models. The results on the Waymo Open Sim\nAgents Challenge (WOSAC) showcase that SMART-R1 achieves state-of-the-art\nperformance with an overall realism meta score of 0.7858, ranking first on the\nleaderboard at the time of submission.",
        "url": "http://arxiv.org/abs/2509.23993v1",
        "published_date": "2025-09-28T17:36:13+00:00",
        "updated_date": "2025-09-28T17:36:13+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Muleilan Pei",
            "Shaoshuai Shi",
            "Shaojie Shen"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel reinforcement fine-tuning approach called SMART-R1 to improve multi-agent traffic simulation for autonomous driving, achieving state-of-the-art performance on the Waymo Open Sim Agents Challenge.",
        "tldr_zh": "本文提出了一种名为SMART-R1的新型强化微调方法，以改进自动驾驶的多智能体交通模拟，在Waymo Open Sim Agents Challenge中取得了最先进的性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Redundancy Reduction in Diffusion Models for Efficient Video Super-Resolution",
        "summary": "Diffusion models have recently shown promising results for video\nsuper-resolution (VSR). However, directly adapting generative diffusion models\nto VSR can result in redundancy, since low-quality videos already preserve\nsubstantial content information. Such redundancy leads to increased\ncomputational overhead and learning burden, as the model performs superfluous\noperations and must learn to filter out irrelevant information. To address this\nproblem, we propose OASIS, an efficient $\\textbf{o}$ne-step diffusion model\nwith $\\textbf{a}$ttention $\\textbf{s}$pecialization for real-world\nv$\\textbf{i}$deo $\\textbf{s}$uper-resolution. OASIS incorporates an attention\nspecialization routing that assigns attention heads to different patterns\naccording to their intrinsic behaviors. This routing mitigates redundancy while\neffectively preserving pretrained knowledge, allowing diffusion models to\nbetter adapt to VSR and achieve stronger performance. Moreover, we propose a\nsimple yet effective progressive training strategy, which starts with\ntemporally consistent degradations and then shifts to inconsistent settings.\nThis strategy facilitates learning under complex degradations. Extensive\nexperiments demonstrate that OASIS achieves state-of-the-art performance on\nboth synthetic and real-world datasets. OASIS also provides superior inference\nspeed, offering a $\\textbf{6.2$\\times$}$ speedup over one-step diffusion\nbaselines such as SeedVR2. The code will be available at\n\\href{https://github.com/jp-guo/OASIS}{https://github.com/jp-guo/OASIS}.",
        "url": "http://arxiv.org/abs/2509.23980v1",
        "published_date": "2025-09-28T17:08:51+00:00",
        "updated_date": "2025-09-28T17:08:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinpei Guo",
            "Yifei Ji",
            "Zheng Chen",
            "Yufei Wang",
            "Sizhuo Ma",
            "Yong Guo",
            "Yulun Zhang",
            "Jian Wang"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes an efficient diffusion model with attention specialization for video super-resolution, achieving state-of-the-art performance and a significant speedup over baselines.",
        "tldr_zh": "本文提出了一种带注意力专门化的扩散模型，用于视频超分辨率，实现了创新性能和对基线的显著加速。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Reinforcement Learning with Inverse Rewards for World Model Post-training",
        "summary": "World models simulate dynamic environments, enabling agents to interact with\ndiverse input modalities. Although recent advances have improved the visual\nquality and temporal consistency of video world models, their ability of\naccurately modeling human-specified actions remains under-explored.\nReinforcement learning presents a promising approach for directly improving the\nsuboptimal action-following capability of pre-trained models, assuming that an\nappropriate reward function can be defined. However, transferring reinforcement\nlearning post-training methods to world model is impractical due to the\nprohibitive cost of large-scale preference annotations and the infeasibility of\nconstructing rule-based video verifiers. To address this gap, we propose\nReinforcement Learning with Inverse Rewards (RLIR), a post-training framework\nthat derives verifiable reward signals by recovering input actions from\ngenerated videos using an Inverse Dynamics Model. By mapping high-dimensional\nvideo modality to a low-dimensional action space, RLIR provides an objective\nand verifiable reward for optimization via Group Relative Policy Optimization.\nExperiments across autoregressive and diffusion paradigms demonstrate 5-10%\ngains in action-following, up to 10% improvements in visual quality, and higher\nhuman preference scores, establishing RLIR as the first post-training method\nspecifically designed to enhance action-following in video world models.",
        "url": "http://arxiv.org/abs/2509.23958v1",
        "published_date": "2025-09-28T16:27:47+00:00",
        "updated_date": "2025-09-28T16:27:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yang Ye",
            "Tianyu He",
            "Shuo Yang",
            "Jiang Bian"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "AIGC"
        ],
        "tldr": "The paper introduces RLIR, a post-training method to enhance action-following in video world models by deriving reward signals from generated videos using an Inverse Dynamics Model.",
        "tldr_zh": "该论文介绍了RLIR，一种通过使用逆动力学模型从生成的视频中获得奖励信号来增强视频世界模型中的动作跟踪能力的后训练方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ColLab: A Collaborative Spatial Progressive Data Engine for Referring Expression Comprehension and Generation",
        "summary": "Referring Expression Comprehension (REC) and Referring Expression Generation\n(REG) are fundamental tasks in multimodal understanding, supporting precise\nobject localization through natural language. However, existing REC and REG\ndatasets rely heavily on manual annotation, which is labor-intensive and\ndifficult to scale. In this paper, we propose ColLab, a collaborative spatial\nprogressive data engine that enables fully automated REC and REG data\ngeneration without human supervision. Specifically, our method introduces a\nCollaborative Multimodal Model Interaction (CMMI) strategy, which leverages the\nsemantic understanding of multimodal large language models (MLLMs) and large\nlanguage models (LLMs) to generate descriptions. Furthermore, we design a\nmodule termed Spatial Progressive Augmentation (SPA) to enhance spatial\nexpressiveness among duplicate instances. Experiments demonstrate that ColLab\nsignificantly accelerates the annotation process of REC and REG while improving\nthe quality and discriminability of the generated expressions. In addition to\nthe core methodological contribution, our framework was partially adopted in\nthe data generation pipeline of the ICCV 2025 MARS2 Challenge on Multimodal\nReasoning, enriching the dataset with diverse and challenging samples that\nbetter reflect real-world reasoning demands.",
        "url": "http://arxiv.org/abs/2509.23955v1",
        "published_date": "2025-09-28T16:21:29+00:00",
        "updated_date": "2025-09-28T16:21:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shilan Zhang",
            "Jirui Huang",
            "Ruilin Yao",
            "Cong Wang",
            "Yaxiong Chen",
            "Peng Xu",
            "Shengwu Xiong"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces ColLab, a system for automated data generation in referring expression comprehension and generation tasks using large language models.",
        "tldr_zh": "本文介绍了ColLab，这是一个利用大型语言模型进行自动化数据生成的系统，用于指代表达理解和生成任务。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CrashSplat: 2D to 3D Vehicle Damage Segmentation in Gaussian Splatting",
        "summary": "Automatic car damage detection has been a topic of significant interest for\nthe auto insurance industry as it promises faster, accurate, and cost-effective\ndamage assessments. However, few works have gone beyond 2D image analysis to\nleverage 3D reconstruction methods, which have the potential to provide a more\ncomprehensive and geometrically accurate representation of the damage.\nMoreover, recent methods employing 3D representations for novel view synthesis,\nparticularly 3D Gaussian Splatting (3D-GS), have demonstrated the ability to\ngenerate accurate and coherent 3D reconstructions from a limited number of\nviews. In this work we introduce an automatic car damage detection pipeline\nthat performs 3D damage segmentation by up-lifting 2D masks. Additionally, we\npropose a simple yet effective learning-free approach for single-view 3D-GS\nsegmentation. Specifically, Gaussians are projected onto the image plane using\ncamera parameters obtained via Structure from Motion (SfM). They are then\nfiltered through an algorithm that utilizes Z-buffering along with a normal\ndistribution model of depth and opacities. Through experiments we found that\nthis method is particularly effective for challenging scenarios like car damage\ndetection, where target objects (e.g., scratches, small dents) may only be\nclearly visible in a single view, making multi-view consistency approaches\nimpractical or impossible. The code is publicly available at:\nhttps://github.com/DragosChileban/CrashSplat.",
        "url": "http://arxiv.org/abs/2509.23947v1",
        "published_date": "2025-09-28T15:49:33+00:00",
        "updated_date": "2025-09-28T15:49:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dragoş-Andrei Chileban",
            "Andrei-Ştefan Bulzan",
            "Cosmin Cernǎzanu-Glǎvan"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces CrashSplat, a 3D vehicle damage segmentation method using 2D masks and 3D Gaussian Splatting, showing effectiveness in detecting car damages from a single view.",
        "tldr_zh": "该论文介绍了CrashSplat，一种使用2D掩模和3D高斯点云技术进行车辆损伤分割的方法，表明在从单个视图中检测汽车损伤方面的有效性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AutoPrune: Each Complexity Deserves a Pruning Policy",
        "summary": "The established redundancy in visual tokens within large vision-language\nmodels allows pruning to effectively reduce their substantial computational\ndemands. Previous methods typically employ heuristic layer-specific pruning\nstrategies where, although the number of tokens removed may differ across\ndecoder layers, the overall pruning schedule is fixed and applied uniformly to\nall input samples and tasks, failing to align token elimination with the\nmodel's holistic reasoning trajectory. Cognitive science indicates that human\nvisual processing often begins with broad exploration to accumulate evidence\nbefore narrowing focus as the target becomes distinct. Our experiments reveal\nan analogous pattern in these models. This observation suggests that neither a\nfixed pruning schedule nor a heuristic layer-wise strategy can optimally\naccommodate the diverse complexities inherent in different inputs. To overcome\nthis limitation, we introduce Complexity-Adaptive Pruning (AutoPrune), a\ntraining-free, plug-and-play framework that tailors pruning policies to varying\nsample and task complexities. Specifically, AutoPrune quantifies the mutual\ninformation between visual and textual tokens, then projects this signal to a\nbudget-constrained logistic retention curve. Each such logistic curve, defined\nby its unique shape, corresponds to the specific complexity of different tasks\nand can guarantee adherence to predefined computational constraints. We\nevaluate AutoPrune on standard vision-language tasks and on\nVision-Language-Action models for autonomous driving. Notably, when applied to\nLLaVA-1.5-7B, our method prunes 89% of visual tokens and reduces inference\nFLOPs by 76.8% while retaining 96.7% of the original accuracy averaged over all\ntasks. This corresponds to a 9.1% improvement over the recent work PDrop,\ndemonstrating the effectiveness. Code is available at\nhttps://github.com/AutoLab-SAI-SJTU/AutoPrune.",
        "url": "http://arxiv.org/abs/2509.23931v1",
        "published_date": "2025-09-28T15:09:00+00:00",
        "updated_date": "2025-09-28T15:09:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hanshi Wang",
            "Yuhao Xu",
            "Zekun Xu",
            "Jin Gao",
            "Yufan Liu",
            "Weiming Hu",
            "Ke Wang",
            "Zhipeng Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "AutoPrune introduces Complexity-Adaptive Pruning for vision-language models, tailoring pruning policies to varying complexities to optimize computational efficiency.",
        "tldr_zh": "AutoPrune 提出了适用于视觉-语言模型的复杂自适应修剪，根据不同复杂性定制修剪策略以优化计算效率。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SAR-KnowLIP: Towards Multimodal Foundation Models for Remote Sensing",
        "summary": "Cross-modal artificial intelligence has garnered widespread attention in\nrecent years, achieving significant progress in the study of natural images.\nHowever, existing methods are mostly designed for RGB imagery, leaving a\nsignificant gap in modeling synthetic aperture radar (SAR) imagery. SAR, with\nits all-day, all-weather imaging capabilities, plays an irreplaceable role in\nremote sensing scene understanding. To address this gap, this paper proposes\nSAR-KnowLIP, the first universal SAR multimodal foundational model, along with\nreusable data and evaluation baselines. Specifically: (1) This work introduces\nthe critical yet long-overlooked attribute of geographic information into\nremote sensing research, constructing SAR-GEOVL-1M (the first large-scale SAR\ndataset with complete geographic projection properties), covering multiple\nsatellite platforms, 120,000 images, and 135 cities. (2) Aligned structured\ntext is generated through a hierarchical cognitive chain-of-thought (HCoT),\nproviding more than one million multi-dimensional semantic annotations of\nlandforms, regional functions, target attributes, and spatial relationships.\n(3) We design a Self-Consistent Iterative Optimization mechanism that\ncontinuously enhances cross-modal alignment through a self-supervised closed\nloop of contrastive, matching, and reconstruction learning on a transferable\nmultimodal encoder. (4) A unified evaluation benchmark is established across 11\nrepresentative downstream vision and vision-language tasks, with comparisons\nagainst 14 leading foundation models, where SAR-KnowLIP demonstrates leading\nperformance, particularly in object counting and land-cover classification. We\nexpect that SAR-KnowLIP's large-scale multimodal data, transferable model\narchitecture, and comprehensive experimental benchmark will significantly\nadvance the development of SAR multimodal baseline models.",
        "url": "http://arxiv.org/abs/2509.23927v1",
        "published_date": "2025-09-28T15:03:25+00:00",
        "updated_date": "2025-09-28T15:03:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yi Yang",
            "Xiaokun Zhang",
            "Qingchen Fang",
            "Ziqi Ye",
            "Rui Li",
            "Li Liu",
            "Haipeng Wang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces SAR-KnowLIP, a universal SAR multimodal foundational model, with a focus on SAR imagery for remote sensing scene understanding and geographic information.",
        "tldr_zh": "该论文介绍了SAR-KnowLIP，一个通用的SAR多模态基础模型，重点关注合成孔径雷达图像在遥感场景理解和地理信息中的应用。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Learning Encoding-Decoding Direction Pairs to Unveil Concepts of Influence in Deep Vision Networks",
        "summary": "Empirical evidence shows that deep vision networks represent concepts as\ndirections in latent space, vectors we call concept embeddings. Each concept\nhas a latent factor-a scalar-indicating its presence in an input patch. For a\ngiven patch, multiple latent factors are encoded into a compact representation\nby linearly combining concept embeddings, with the factors as coefficients.\nSince these embeddings enable such encoding, we call them encoding directions.\nA latent factor can be recovered via the inner product with a filter, a vector\nwe call a decoding direction. These encoding-decoding direction pairs are not\ndirectly accessible, but recovering them helps open the black box of deep\nnetworks, enabling understanding, debugging, and improving models. Decoder\ndirections attribute meaning to latent codes, while encoding directions assess\nconcept influence on predictions, with both enabling model correction by\nunlearning irrelevant concepts. Unlike prior matrix decomposition, autoencoder,\nor dictionary learning methods that rely on feature reconstruction, we propose\na new perspective: decoding directions are identified via directional\nclustering of activations, and encoding directions are estimated with signal\nvectors under a probabilistic view. We further leverage network weights through\na novel technique, Uncertainty Region Alignment, which reveals interpretable\ndirections affecting predictions. Our analysis shows that (a) on synthetic\ndata, our method recovers ground-truth direction pairs; (b) on real data,\ndecoding directions map to monosemantic, interpretable concepts and outperform\nunsupervised baselines; and (c) signal vectors faithfully estimate encoding\ndirections, validated via activation maximization. Finally, we demonstrate\napplications in understanding global model behavior, explaining individual\npredictions, and intervening to produce counterfactuals or correct errors.",
        "url": "http://arxiv.org/abs/2509.23926v1",
        "published_date": "2025-09-28T15:02:34+00:00",
        "updated_date": "2025-09-28T15:02:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Alexandros Doumanoglou",
            "Kurt Driessens",
            "Dimitrios Zarpalas"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper proposes a method to recover encoding-decoding direction pairs to understand the concepts of influence in deep vision networks, enabling model improvement and correction by unlearning irrelevant concepts.",
        "tldr_zh": "本文提出了一种方法，通过恢复编码 - 解码方向对来理解深度视觉网络中的概念影响，实现通过遗忘无关概念进行模型改进和纠正。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DriveE2E: Closed-Loop Benchmark for End-to-End Autonomous Driving through Real-to-Simulation",
        "summary": "Closed-loop evaluation is increasingly critical for end-to-end autonomous\ndriving. Current closed-loop benchmarks using the CARLA simulator rely on\nmanually configured traffic scenarios, which can diverge from real-world\nconditions, limiting their ability to reflect actual driving performance. To\naddress these limitations, we introduce a simple yet challenging closed-loop\nevaluation framework that closely integrates real-world driving scenarios into\nthe CARLA simulator with infrastructure cooperation. Our approach involves\nextracting 800 dynamic traffic scenarios selected from a comprehensive 100-hour\nvideo dataset captured by high-mounted infrastructure sensors, and creating\nstatic digital twin assets for 15 real-world intersections with consistent\nvisual appearance. These digital twins accurately replicate the traffic and\nenvironmental characteristics of their real-world counterparts, enabling more\nrealistic simulations in CARLA. This evaluation is challenging due to the\ndiversity of driving behaviors, locations, weather conditions, and times of day\nat complex urban intersections. In addition, we provide a comprehensive\nclosed-loop benchmark for evaluating end-to-end autonomous driving models.\nProject URL:\n\\href{https://github.com/AIR-THU/DriveE2E}{https://github.com/AIR-THU/DriveE2E}.",
        "url": "http://arxiv.org/abs/2509.23922v1",
        "published_date": "2025-09-28T14:55:14+00:00",
        "updated_date": "2025-09-28T14:55:14+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Haibao Yu",
            "Wenxian Yang",
            "Ruiyang Hao",
            "Chuanye Wang",
            "Jiaru Zhong",
            "Ping Luo",
            "Zaiqing Nie"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a closed-loop evaluation framework for end-to-end autonomous driving that integrates real-world driving scenarios into the CARLA simulator for more realistic simulations.",
        "tldr_zh": "该论文引入了一个封闭环评估框架，用于端到端自动驾驶，在CARLA模拟器中整合了真实世界的驾驶场景，以实现更真实的模拟。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Token Painter: Training-Free Text-Guided Image Inpainting via Mask Autoregressive Models",
        "summary": "Text-guided image inpainting aims to inpaint masked image regions based on a\ntextual prompt while preserving the background. Although diffusion-based\nmethods have become dominant, their property of modeling the entire image in\nlatent space makes it challenging for the results to align well with prompt\ndetails and maintain a consistent background. To address these issues, we\nexplore Mask AutoRegressive (MAR) models for this task. MAR naturally supports\nimage inpainting by generating latent tokens corresponding to mask regions,\nenabling better local controllability without altering the background. However,\ndirectly applying MAR to this task makes the inpainting content either ignore\nthe prompts or be disharmonious with the background context. Through analysis\nof the attention maps from the inpainting images, we identify the impact of\nbackground tokens on text tokens during the MAR generation, and leverage this\nto design \\textbf{Token Painter}, a training-free text-guided image inpainting\nmethod based on MAR. Our approach introduces two key components: (1)\nDual-Stream Encoder Information Fusion (DEIF), which fuses the semantic and\ncontext information from text and background in frequency domain to produce\nnovel guidance tokens, allowing MAR to generate text-faithful inpainting\ncontent while keeping harmonious with background context. (2) Adaptive Decoder\nAttention Score Enhancing (ADAE), which adaptively enhances attention scores on\nguidance tokens and inpainting tokens to further enhance the alignment of\nprompt details and the content visual quality. Extensive experiments\ndemonstrate that our training-free method outperforms prior state-of-the-art\nmethods across almost all metrics and delivers superior visual results. Codes\nwill be released.",
        "url": "http://arxiv.org/abs/2509.23919v1",
        "published_date": "2025-09-28T14:48:52+00:00",
        "updated_date": "2025-09-28T14:48:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Longtao Jiang",
            "Mingfei Han",
            "Lei Chen",
            "Yongqiang Yu",
            "Feng Zhao",
            "Xiaojun Chang",
            "Zhihui Li"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "AIGC"
        ],
        "tldr": "Token Painter is a training-free text-guided image inpainting method that outperforms previous methods by leveraging Mask AutoRegressive models for improved inpainting accuracy and background preservation.",
        "tldr_zh": "Token Painter是一种无需训练的文字引导图像修复方法，通过利用Mask AutoRegressive模型提高修复精度和背景保留，优于先前的方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Bridging the Task Gap: Multi-Task Adversarial Transferability in CLIP and Its Derivatives",
        "summary": "As a general-purpose vision-language pretraining model, CLIP demonstrates\nstrong generalization ability in image-text alignment tasks and has been widely\nadopted in downstream applications such as image classification and image-text\nretrieval. However, it struggles with fine-grained tasks such as object\ndetection and semantic segmentation. While many variants aim to improve CLIP on\nthese tasks, its robustness to adversarial perturbations remains underexplored.\nUnderstanding how adversarial examples transfer across tasks is key to\nassessing CLIP's generalization limits and security risks. In this work, we\nconduct a systematic empirical analysis of the cross-task transfer behavior of\nCLIP-based models on image-text retrieval, object detection, and semantic\nsegmentation under adversarial perturbations. We find that adversarial examples\ngenerated from fine-grained tasks (e.g., object detection and semantic\nsegmentation) often exhibit stronger transfer potential than those from\ncoarse-grained tasks, enabling more effective attacks against the original CLIP\nmodel. Motivated by this observation, we propose a novel framework, Multi-Task\nAdversarial CLIP (MT-AdvCLIP), which introduces a task-aware feature\naggregation loss and generates perturbations with enhanced cross-task\ngeneralization capability. This design strengthens the attack effectiveness of\nfine-grained task models on the shared CLIP backbone. Experimental results on\nmultiple public datasets show that MT-AdvCLIP significantly improves the\nadversarial transfer success rate (The average attack success rate across\nmultiple tasks is improved by over 39%.) against various CLIP-derived models,\nwithout increasing the perturbation budget. This study reveals the transfer\nmechanism of adversarial examples in multi-task CLIP models, offering new\ninsights into multi-task robustness evaluation and adversarial example design.",
        "url": "http://arxiv.org/abs/2509.23917v1",
        "published_date": "2025-09-28T14:46:52+00:00",
        "updated_date": "2025-09-28T14:46:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kuanrong Liu",
            "Siyuan Liang",
            "Cheng Qian",
            "Ming Zhang",
            "Xiaochun Cao"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper explores the transferability of adversarial examples across different tasks in CLIP-based models and proposes a novel framework to improve attack effectiveness on fine-grained tasks.",
        "tldr_zh": "本文探讨了在基于CLIP模型的不同任务之间对抗样本的可转移性，并提出了一种新的框架来提高对细粒度任务的攻击效果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Revisit the Imbalance Optimization in Multi-task Learning: An Experimental Analysis",
        "summary": "Multi-task learning (MTL) aims to build general-purpose vision systems by\ntraining a single network to perform multiple tasks jointly. While promising,\nits potential is often hindered by \"unbalanced optimization\", where task\ninterference leads to subpar performance compared to single-task models. To\nfacilitate research in MTL, this paper presents a systematic experimental\nanalysis to dissect the factors contributing to this persistent problem. Our\ninvestigation confirms that the performance of existing optimization methods\nvaries inconsistently across datasets, and advanced architectures still rely on\ncostly grid-searched loss weights. Furthermore, we show that while powerful\nVision Foundation Models (VFMs) provide strong initialization, they do not\ninherently resolve the optimization imbalance, and merely increasing data\nquantity offers limited benefits. A crucial finding emerges from our analysis:\na strong correlation exists between the optimization imbalance and the norm of\ntask-specific gradients. We demonstrate that this insight is directly\napplicable, showing that a straightforward strategy of scaling task losses\naccording to their gradient norms can achieve performance comparable to that of\nan extensive and computationally expensive grid search. Our comprehensive\nanalysis suggests that understanding and controlling gradient dynamics is a\nmore direct path to stable MTL than developing increasingly complex methods.",
        "url": "http://arxiv.org/abs/2509.23915v1",
        "published_date": "2025-09-28T14:40:06+00:00",
        "updated_date": "2025-09-28T14:40:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yihang Guo",
            "Tianyuan Yu",
            "Liang Bai",
            "Yanming Guo",
            "Yirun Ruan",
            "William Li",
            "Weishi Zheng"
        ],
        "ai_categories": [
            "Multimodality"
        ],
        "tldr": "The paper investigates the imbalance optimization issue in Multi-task Learning and proposes a simple strategy to address it by scaling task losses based on gradient norms.",
        "tldr_zh": "本文研究了多任务学习中的不平衡优化问题，并提出了一种简单的解决方案，即根据梯度范数调整任务损失。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MoReact: Generating Reactive Motion from Textual Descriptions",
        "summary": "Modeling and generating human reactions poses a significant challenge with\nbroad applications for computer vision and human-computer interaction. Existing\nmethods either treat multiple individuals as a single entity, directly\ngenerating interactions, or rely solely on one person's motion to generate the\nother's reaction, failing to integrate the rich semantic information that\nunderpins human interactions. Yet, these methods often fall short in adaptive\nresponsiveness, i.e., the ability to accurately respond to diverse and dynamic\ninteraction scenarios. Recognizing this gap, our work introduces an approach\ntailored to address the limitations of existing models by focusing on\ntext-driven human reaction generation. Our model specifically generates\nrealistic motion sequences for individuals that responding to the other's\nactions based on a descriptive text of the interaction scenario. The goal is to\nproduce motion sequences that not only complement the opponent's movements but\nalso semantically fit the described interactions. To achieve this, we present\nMoReact, a diffusion-based method designed to disentangle the generation of\nglobal trajectories and local motions sequentially. This approach stems from\nthe observation that generating global trajectories first is crucial for\nguiding local motion, ensuring better alignment with given action and text.\nFurthermore, we introduce a novel interaction loss to enhance the realism of\ngenerated close interactions. Our experiments, utilizing data adapted from a\ntwo-person motion dataset, demonstrate the efficacy of our approach for this\nnovel task, which is capable of producing realistic, diverse, and controllable\nreactions that not only closely match the movements of the counterpart but also\nadhere to the textual guidance. Please find our webpage at\nhttps://xiyan-xu.github.io/MoReactWebPage.",
        "url": "http://arxiv.org/abs/2509.23911v1",
        "published_date": "2025-09-28T14:31:41+00:00",
        "updated_date": "2025-09-28T14:31:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiyan Xu",
            "Sirui Xu",
            "Yu-Xiong Wang",
            "Liang-Yan Gui"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "MoReact introduces a text-driven approach to generate realistic human reactions in motion sequences, enhancing adaptiveness and semantic coherence.",
        "tldr_zh": "MoReact引入了一种文本驱动方法来生成真实的人类反应运动序列，提高了适应性和语义连贯性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Adversarial Versus Federated: An Adversarial Learning based Multi-Modality Cross-Domain Federated Medical Segmentation",
        "summary": "Federated learning enables collaborative training of machine learning models\namong different clients while ensuring data privacy, emerging as the mainstream\nfor breaking data silos in the healthcare domain. However, the imbalance of\nmedical resources, data corruption or improper data preservation may lead to a\nsituation where different clients possess medical images of different modality.\nThis heterogeneity poses a significant challenge for cross-domain medical image\nsegmentation within the federated learning framework. To address this\nchallenge, we propose a new Federated Domain Adaptation (FedDA) segmentation\ntraining framework. Specifically, we propose a feature-level adversarial\nlearning among clients by aligning feature maps across clients through\nembedding an adversarial training mechanism. This design can enhance the\nmodel's generalization on multiple domains and alleviate the negative impact\nfrom domain-shift. Comprehensive experiments on three medical image datasets\ndemonstrate that our proposed FedDA substantially achieves cross-domain\nfederated aggregation, endowing single modality client with cross-modality\nprocessing capabilities, and consistently delivers robust performance compared\nto state-of-the-art federated aggregation algorithms in objective and\nsubjective assessment. Our code are available at\nhttps://github.com/GGbond-study/FedDA.",
        "url": "http://arxiv.org/abs/2509.23907v1",
        "published_date": "2025-09-28T14:26:04+00:00",
        "updated_date": "2025-09-28T14:26:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "You Zhou",
            "Lijiang Chen",
            "Shuchang Lyu",
            "Guangxia Cui",
            "Wenpei Bai",
            "Zheng Zhou",
            "Meng Li",
            "Guangliang Cheng",
            "Huiyu Zhou",
            "Qi Zhao"
        ],
        "ai_categories": [
            "Multimodality",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a Federated Domain Adaptation training framework for cross-domain medical image segmentation in federated learning, achieving superior performance compared to existing methods.",
        "tldr_zh": "本文提出了一种针对联邦学习中跨领域医学图像分割的联邦域自适应训练框架，相较于现有方法表现更优。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Q-FSRU: Quantum-Augmented Frequency-Spectral For Medical Visual Question Answering",
        "summary": "Solving tough clinical questions that require both image and text\nunderstanding is still a major challenge in healthcare AI. In this work, we\npropose Q-FSRU, a new model that combines Frequency Spectrum Representation and\nFusion (FSRU) with a method called Quantum Retrieval-Augmented Generation\n(Quantum RAG) for medical Visual Question Answering (VQA). The model takes in\nfeatures from medical images and related text, then shifts them into the\nfrequency domain using Fast Fourier Transform (FFT). This helps it focus on\nmore meaningful data and filter out noise or less useful information. To\nimprove accuracy and ensure that answers are based on real knowledge, we add a\nquantum inspired retrieval system. It fetches useful medical facts from\nexternal sources using quantum-based similarity techniques. These details are\nthen merged with the frequency-based features for stronger reasoning. We\nevaluated our model using the VQA-RAD dataset, which includes real radiology\nimages and questions. The results showed that Q-FSRU outperforms earlier\nmodels, especially on complex cases needing image text reasoning. The mix of\nfrequency and quantum information improves both performance and explainability.\nOverall, this approach offers a promising way to build smart, clear, and\nhelpful AI tools for doctors.",
        "url": "http://arxiv.org/abs/2509.23899v1",
        "published_date": "2025-09-28T14:09:00+00:00",
        "updated_date": "2025-09-28T14:09:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rakesh Thakur",
            "Yusra Tariq",
            "Rakesh Chandra Joshi"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces Q-FSRU, a model combining Frequency Spectrum Representation and Quantum Retrieval-Augmented Generation for medical Visual Question Answering, showing improved performance on complex cases.",
        "tldr_zh": "本文介绍了Q-FSRU模型，该模型结合了频谱表示和量子检索增强生成，用于医学视觉问答，表现出在复杂案例上的性能提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Preserving Cross-Modal Stability for Visual Unlearning in Multimodal Scenarios",
        "summary": "Visual modality is the most vulnerable to privacy leakage in real-world\nmultimodal applications like autonomous driving with visual and radar data;\nMachine unlearning removes specific training data from pre-trained models to\naddress privacy leakage, however, existing methods fail to preserve cross-modal\nknowledge and maintain intra-class structural stability of retain data, leading\nto reduced overall and other modalities' performance during visual unlearning;\nto address these challenges, we propose a Cross-modal Contrastive Unlearning\n(CCU) framework, which integrates three key components: (a) selective visual\nunlearning: employing inverse contrastive learning to dissociate visual\nrepresentations from their original semantics, (b) cross-modal knowledge\nretention: preserving other modalities' discriminability through semantic\nconsistency, and (c) dual-set contrastive separation: preserving the model\nperformance via isolation of structural perturbations between the unlearn set\nand retain set; extensive experiments on three datasets demonstrate the\nsuperiority of CCU, and our method achieves a 7.12% accuracy improvement with\nonly 7% of the unlearning time compared to the top-accuracy baseline.",
        "url": "http://arxiv.org/abs/2509.23895v1",
        "published_date": "2025-09-28T14:03:37+00:00",
        "updated_date": "2025-09-28T14:03:37+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jinghan Xu Yuyang Zhang Qixuan Cai Jiancheng Chen Keqiu Li"
        ],
        "ai_categories": [
            "Multimodality",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a framework called Cross-modal Contrastive Unlearning (CCU) to address privacy leakage in multimodal scenarios by selectively unlearning visual data, retaining cross-modal knowledge, and isolating structural perturbations, showing a 7.12% accuracy improvement with only 7% of the unlearning time compared to the baseline.",
        "tldr_zh": "本文提出了一种名为Cross-modal Contrastive Unlearning (CCU)的框架，通过选择性地遗忘视觉数据、保留跨模态知识并隔离结构扰动来解决多模态场景中的隐私泄露问题，与基准相比，准确率提高了7.12%，仅用了基准的7%的遗忘时间。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Tunable-Generalization Diffusion Powered by Self-Supervised Contextual Sub-Data for Low-Dose CT Reconstruction",
        "summary": "Current models based on deep learning for low-dose CT denoising rely heavily\non paired data and generalize poorly. Even the more concerned diffusion models\nneed to learn the distribution of clean data for reconstruction, which is\ndifficult to satisfy in medical clinical applications. At the same time,\nself-supervised-based methods face the challenge of significant degradation of\ngeneralizability of models pre-trained for the current dose to expand to other\ndoses. To address these issues, this paper proposes a novel method of\ntunable-generalization diffusion powered by self-supervised contextual sub-data\nfor low-dose CT reconstruction, named SuperDiff. Firstly, a contextual subdata\nsimilarity adaptive sensing strategy is designed for denoising centered on the\nLDCT projection domain, which provides an initial prior for the subsequent\nprogress. Subsequently, the initial prior is used to combine knowledge\ndistillation with a deep combination of latent diffusion models for optimizing\nimage details. The pre-trained model is used for inference reconstruction, and\nthe pixel-level self-correcting fusion technique is proposed for fine-grained\nreconstruction of the image domain to enhance the image fidelity, using the\ninitial prior and the LDCT image as a guide. In addition, the technique is\nflexibly applied to the generalization of upper and lower doses or even unseen\ndoses. Dual-domain strategy cascade for self-supervised LDCT denoising,\nSuperDiff requires only LDCT projection domain data for training and testing.\nFull qualitative and quantitative evaluations on both datasets and real data\nshow that SuperDiff consistently outperforms existing state-of-the-art methods\nin terms of reconstruction and generalization performance.",
        "url": "http://arxiv.org/abs/2509.23885v1",
        "published_date": "2025-09-28T13:50:29+00:00",
        "updated_date": "2025-09-28T13:50:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Guoquan Wei",
            "Zekun Zhou",
            "Liu Shi",
            "Wenzhe Shan",
            "Qiegen Liu"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel method for low-dose CT reconstruction using self-supervised contextual sub-data, outperforming existing state-of-the-art methods in terms of reconstruction and generalization performance.",
        "tldr_zh": "本文提出了一种新颖的方法，利用自监督情境子数据进行低剂量CT重建，在重建和泛化性能方面优于现有最先进方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PCRI: Measuring Context Robustness in Multimodal Models for Enterprise Applications",
        "summary": "The reliability of Multimodal Large Language Models (MLLMs) in real-world\nsettings is often undermined by sensitivity to irrelevant or distracting visual\ncontext, an aspect not captured by existing evaluation metrics. We introduce\nthe \\textbf{Patch Context Robustness Index (PCRI)}, the first systematic and\ninterpretable score for quantifying MLLM robustness to variations in visual\ncontext granularity, measuring performance changes between localized image\npatches and full-image input.\n  Applying PCRI to 19 state-of-the-art MLLMs across 15 vision-language\nbenchmarks, we find that most leading models remain brittle to background\nnoise, with only a few, such as InternVL2-26B and Qwen2VL-72B, demonstrating\nconsistent robustness across tasks. PCRI analysis also highlights how different\nmodel architectures handle and integrate visual context, offering actionable\ndiagnostic insight for both researchers and practitioners.\n  PCRI enables rigorous comparison of context robustness, supporting principled\nmodel selection and guiding the development of future architectures and\ntraining strategies for robust, real-world deployment.",
        "url": "http://arxiv.org/abs/2509.23879v1",
        "published_date": "2025-09-28T13:39:57+00:00",
        "updated_date": "2025-09-28T13:39:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.MM",
            "68T50, 68T45",
            "I.2.7; I.2.10; I.4.8; I.4.10; I.4.0"
        ],
        "authors": [
            "Hitesh Laxmichand Patel",
            "Amit Agarwal",
            "Srikant Panda",
            "Hansa Meghwani",
            "Karan Dua",
            "Paul Li",
            "Tao Sheng",
            "Sujith Ravi",
            "Dan Roth"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a new metric, PCRI, to measure the robustness of Multimodal Large Language Models to variations in visual context. It highlights the brittleness of current models to background noise and offers diagnostic insights for improvement.",
        "tldr_zh": "本文引入了一种新的度量标准PCRI，用于衡量多模式大语言模型对视觉上下文变化的稳健性。它强调了目前模型对背景噪音的脆弱性，并为改进提供了诊断性见解。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Sim-DETR: Unlock DETR for Temporal Sentence Grounding",
        "summary": "Temporal sentence grounding aims to identify exact moments in a video that\ncorrespond to a given textual query, typically addressed with detection\ntransformer (DETR) solutions. However, we find that typical strategies designed\nto enhance DETR do not improve, and may even degrade, its performance in this\ntask. We systematically analyze and identify the root causes of this abnormal\nbehavior: (1) conflicts between queries from similar target moments and (2)\ninternal query conflicts due to the tension between global semantics and local\nlocalization. Building on these insights, we propose a simple yet powerful\nbaseline, Sim-DETR, which extends the standard DETR with two minor\nmodifications in the decoder layers: (1) constraining self-attention between\nqueries based on their semantic and positional overlap and (2) adding\nquery-to-frame alignment to bridge the global and local contexts. Experiments\ndemonstrate that Sim-DETR unlocks the full potential of DETR for temporal\nsentence grounding, offering a strong baseline for future research.",
        "url": "http://arxiv.org/abs/2509.23867v1",
        "published_date": "2025-09-28T13:21:10+00:00",
        "updated_date": "2025-09-28T13:21:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiajin Tang",
            "Zhengxuan Wei",
            "Yuchen Zhu",
            "Cheng Shi",
            "Guanbin Li",
            "Liang Lin",
            "Sibei Yang"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper introduces Sim-DETR, a modification of DETR for temporal sentence grounding, addressing issues in global semantics and local localization.",
        "tldr_zh": "本文介绍了Sim-DETR，这是DETR在时间句子基准中的一种修改，解决了全局语义和本地定位的问题。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CE-FAM: Concept-Based Explanation via Fusion of Activation Maps",
        "summary": "Although saliency maps can highlight important regions to explain the\nreasoning behind image classification in artificial intelligence (AI), the\nmeaning of these regions is left to the user's interpretation. In contrast,\nconceptbased explanations decompose AI predictions into humanunderstandable\nconcepts, clarifying their contributions. However, few methods can\nsimultaneously reveal what concepts an image classifier learns, which regions\nare associated with them, and how they contribute to predictions. We propose a\nnovel concept-based explanation method, Concept-based Explanation via Fusion of\nActivation Maps (CE-FAM). It employs a branched network that shares activation\nmaps with an image classifier and learns to mimic the embeddings of a Vision\nand Language Model (VLM). The branch network predicts concepts in an image, and\ntheir corresponding regions are represented by a weighted sum of activation\nmaps, with weights given by the gradients of the concept prediction scores.\nTheir contributions are quantified based on their impact on the image\nclassification score. Our method provides a general framework for identifying\nthe concept regions and their contributions while leveraging VLM knowledge to\nhandle arbitrary concepts without requiring an annotated dataset. Furthermore,\nwe introduce a novel evaluation metric to assess the accuracy of the concept\nregions. Our qualitative and quantitative evaluations demonstrate our method\noutperforms existing approaches and excels in zero-shot inference for unseen\nconcepts.",
        "url": "http://arxiv.org/abs/2509.23849v1",
        "published_date": "2025-09-28T12:40:53+00:00",
        "updated_date": "2025-09-28T12:40:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Michihiro Kuroki",
            "Toshihiko Yamasaki"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces CE-FAM, a method for explaining image classification through concepts, outperforming existing approaches and excelling in zero-shot inference for unseen concepts.",
        "tldr_zh": "该论文介绍了CE-FAM，一种通过概念解释图像分类的方法，优于现有方法，在未知概念的零击推理中表现出色。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Fine-Grained Text-to-3D Quality Assessment: A Benchmark and A Two-Stage Rank-Learning Metric",
        "summary": "Recent advances in Text-to-3D (T23D) generative models have enabled the\nsynthesis of diverse, high-fidelity 3D assets from textual prompts. However,\nexisting challenges restrict the development of reliable T23D quality\nassessment (T23DQA). First, existing benchmarks are outdated, fragmented, and\ncoarse-grained, making fine-grained metric training infeasible. Moreover,\ncurrent objective metrics exhibit inherent design limitations, resulting in\nnon-representative feature extraction and diminished metric robustness. To\naddress these limitations, we introduce T23D-CompBench, a comprehensive\nbenchmark for compositional T23D generation. We define five components with\ntwelve sub-components for compositional prompts, which are used to generate\n3,600 textured meshes from ten state-of-the-art generative models. A\nlarge-scale subjective experiment is conducted to collect 129,600 reliable\nhuman ratings across different perspectives. Based on T23D-CompBench, we\nfurther propose Rank2Score, an effective evaluator with two-stage training for\nT23DQA. Rank2Score enhances pairwise training via supervised contrastive\nregression and curriculum learning in the first stage, and subsequently refines\npredictions using mean opinion scores to achieve closer alignment with human\njudgments in the second stage. Extensive experiments and downstream\napplications demonstrate that Rank2Score consistently outperforms existing\nmetrics across multiple dimensions and can additionally serve as a reward\nfunction to optimize generative models. The project is available at\nhttps://cbysjtu.github.io/Rank2Score/.",
        "url": "http://arxiv.org/abs/2509.23841v1",
        "published_date": "2025-09-28T12:30:47+00:00",
        "updated_date": "2025-09-28T12:30:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bingyang Cui",
            "Yujie Zhang",
            "Qi Yang",
            "Zhu Li",
            "Yiling Xu"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a new benchmark for evaluating the quality of Text-to-3D generation models using a novel two-stage rank-learning metric called Rank2Score.",
        "tldr_zh": "该论文介绍了一个新的基准，用于评估文本到3D生成模型的质量，使用了一种名为Rank2Score的新型两阶段排名学习度量。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "AISHELL6-whisper: A Chinese Mandarin Audio-visual Whisper Speech Dataset with Speech Recognition Baselines",
        "summary": "Whisper speech recognition is crucial not only for ensuring privacy in\nsensitive communications but also for providing a critical communication bridge\nfor patients under vocal restraint and enabling discrete interaction in\nnoise-sensitive environments. The development of Chinese mandarin audio-visual\nwhisper speech recognition is hindered by the lack of large-scale datasets. We\npresent AISHELL6-Whisper, a large-scale open-source audio-visual whisper speech\ndataset, featuring 30 hours each of whisper speech and parallel normal speech,\nwith synchronized frontal facial videos. Moreover, we propose an audio-visual\nspeech recognition (AVSR) baseline based on the Whisper-Flamingo framework,\nwhich integrates a parallel training strategy to align embeddings across speech\ntypes, and employs a projection layer to adapt to whisper speech's spectral\nproperties. The model achieves a Character Error Rate (CER) of 4.13% for\nwhisper speech and 1.11% for normal speech in the test set of our dataset, and\nestablishes new state-of-the-art results on the wTIMIT benchmark. The dataset\nand the AVSR baseline codes are open-sourced at\nhttps://zutm.github.io/AISHELL6-Whisper.",
        "url": "http://arxiv.org/abs/2509.23833v1",
        "published_date": "2025-09-28T12:14:06+00:00",
        "updated_date": "2025-09-28T12:14:06+00:00",
        "categories": [
            "eess.AS",
            "cs.CV",
            "cs.MM",
            "cs.SD"
        ],
        "authors": [
            "Cancan Li",
            "Fei Su",
            "Juan Liu",
            "Hui Bu",
            "Yulong Wan",
            "Hongbin Suo",
            "Ming Li"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces AISHELL6-Whisper, a Chinese Mandarin audio-visual whisper speech dataset, and proposes an AVSR baseline achieving state-of-the-art results on the wTIMIT benchmark.",
        "tldr_zh": "本文介绍了AISHELL6-Whisper，一个中文普通话音频-视觉低语音数据集，并提出了一个基于AVSR的基线，在wTIMIT基准上取得了最新的成果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Multi-Camera Vision-Based Approach for Fine-Grained Assembly Quality Control",
        "summary": "Quality control is a critical aspect of manufacturing, particularly in\nensuring the proper assembly of small components in production lines. Existing\nsolutions often rely on single-view imaging or manual inspection, which are\nprone to errors due to occlusions, restricted perspectives, or lighting\ninconsistencies. These limitations require the installation of additional\ninspection stations, which could disrupt the assembly line and lead to\nincreased downtime and costs. This paper introduces a novel multi-view quality\ncontrol module designed to address these challenges, integrating a multi-camera\nimaging system with advanced object detection algorithms. By capturing images\nfrom three camera views, the system provides comprehensive visual coverage of\ncomponents of an assembly process. A tailored image fusion methodology combines\nresults from multiple views, effectively resolving ambiguities and enhancing\ndetection reliability. To support this system, we developed a unique dataset\ncomprising annotated images across diverse scenarios, including varied lighting\nconditions, occlusions, and angles, to enhance applicability in real-world\nmanufacturing environments. Experimental results show that our approach\nsignificantly outperforms single-view methods, achieving high precision and\nrecall rates in the identification of improperly fastened small assembly parts\nsuch as screws. This work contributes to industrial automation by overcoming\nsingle-view limitations, and providing a scalable, cost-effective, and accurate\nquality control mechanism that ensures the reliability and safety of the\nassembly line. The dataset used in this study is publicly available to\nfacilitate further research in this domain.",
        "url": "http://arxiv.org/abs/2509.23815v1",
        "published_date": "2025-09-28T11:37:48+00:00",
        "updated_date": "2025-09-28T11:37:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "68T45",
            "I.4.8; I.4.1; I.2.10"
        ],
        "authors": [
            "Ali Nazeri",
            "Shashank Mishra",
            "Achim Wagner",
            "Martin Ruskowski",
            "Didier Stricker",
            "Jason Rambach"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a multi-camera vision-based system for fine-grained assembly quality control, outperforming single-view methods and providing a cost-effective solution for manufacturing environments.",
        "tldr_zh": "本文提出了一种多摄像头视觉系统，用于细粒度装配质量控制，优于单视角方法，并为制造环境提供了具有成本效益的解决方案。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "FedAgentBench: Towards Automating Real-world Federated Medical Image Analysis with Server-Client LLM Agents",
        "summary": "Federated learning (FL) allows collaborative model training across healthcare\nsites without sharing sensitive patient data. However, real-world FL deployment\nis often hindered by complex operational challenges that demand substantial\nhuman efforts. This includes: (a) selecting appropriate clients (hospitals),\n(b) coordinating between the central server and clients, (c) client-level data\npre-processing, (d) harmonizing non-standardized data and labels across\nclients, and (e) selecting FL algorithms based on user instructions and\ncross-client data characteristics. However, the existing FL works overlook\nthese practical orchestration challenges. These operational bottlenecks\nmotivate the need for autonomous, agent-driven FL systems, where intelligent\nagents at each hospital client and the central server agent collaboratively\nmanage FL setup and model training with minimal human intervention. To this\nend, we first introduce an agent-driven FL framework that captures key phases\nof real-world FL workflows from client selection to training completion and a\nbenchmark dubbed FedAgentBench that evaluates the ability of LLM agents to\nautonomously coordinate healthcare FL. Our framework incorporates 40 FL\nalgorithms, each tailored to address diverse task-specific requirements and\ncross-client characteristics. Furthermore, we introduce a diverse set of\ncomplex tasks across 201 carefully curated datasets, simulating 6\nmodality-specific real-world healthcare environments, viz., Dermatoscopy,\nUltrasound, Fundus, Histopathology, MRI, and X-Ray. We assess the agentic\nperformance of 14 open-source and 10 proprietary LLMs spanning small, medium,\nand large model scales. While some agent cores such as GPT-4.1 and DeepSeek V3\ncan automate various stages of the FL pipeline, our results reveal that more\ncomplex, interdependent tasks based on implicit goals remain challenging for\neven the strongest models.",
        "url": "http://arxiv.org/abs/2509.23803v1",
        "published_date": "2025-09-28T11:06:07+00:00",
        "updated_date": "2025-09-28T11:06:07+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.DC",
            "cs.MA"
        ],
        "authors": [
            "Pramit Saha",
            "Joshua Strong",
            "Divyanshu Mishra",
            "Cheng Ouyang",
            "J. Alison Noble"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces an agent-driven framework for automating federated learning in medical image analysis, addressing practical challenges in real-world deployment.",
        "tldr_zh": "本文介绍了一种代理驱动的框架，用于自动化医学图像分析中的联邦学习，解决了实际部署中的挑战。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GroupCoOp: Group-robust Fine-tuning via Group Prompt Learning",
        "summary": "Parameter-efficient fine-tuning (PEFT) of vision-language models (VLMs)\nexcels in various vision tasks thanks to the rich knowledge and generalization\nability of VLMs. However, recent studies revealed that such fine-tuned VLMs are\nvulnerable to spurious correlations stemming from the subgroup imbalance in the\nfine-tuning datasets. To resolve this issue, we propose Group Context\nOptimization (GroupCoOp), a simple and effective debiased fine-tuning algorithm\nthat enhances the group robustness of fine-tuned VLMs. Its key idea is to\nemploy group-specific text prompts as group representatives serving as multiple\nclassifiers for their target class. The rich semantic knowledge of the text\nencoder of VLM enables the discovery of effective group prompts even for groups\nwith a small number of training samples. Leveraging the group prompts for each\nclass addresses the issues caused by the group-imbalanced training set, such as\nthe neglect of minority groups and the scattered distribution of each class in\nthe embedding space. GroupCoOp achieved the best results on five benchmarks\nacross five CLIP architectures and occasionally outperformed prior methods that\nfine-tune the entire network, despite training only 0.016\\% of the network's\nparameters.",
        "url": "http://arxiv.org/abs/2509.23781v1",
        "published_date": "2025-09-28T09:54:30+00:00",
        "updated_date": "2025-09-28T09:54:30+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Nayeong Kim",
            "Seong Joon Oh",
            "Suha Kwak"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes GroupCoOp, a method to improve the group robustness of fine-tuned vision-language models by using group-specific text prompts.",
        "tldr_zh": "本文提出了一种新方法GroupCoOp，通过使用特定于组的文本提示来改善细化的视觉语言模型的组稳健性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Texture Vector-Quantization and Reconstruction Aware Prediction for Generative Super-Resolution",
        "summary": "Vector-quantized based models have recently demonstrated strong potential for\nvisual prior modeling. However, existing VQ-based methods simply encode visual\nfeatures with nearest codebook items and train index predictor with code-level\nsupervision. Due to the richness of visual signal, VQ encoding often leads to\nlarge quantization error. Furthermore, training predictor with code-level\nsupervision can not take the final reconstruction errors into consideration,\nresult in sub-optimal prior modeling accuracy. In this paper we address the\nabove two issues and propose a Texture Vector-Quantization and a Reconstruction\nAware Prediction strategy. The texture vector-quantization strategy leverages\nthe task character of super-resolution and only introduce codebook to model the\nprior of missing textures. While the reconstruction aware prediction strategy\nmakes use of the straight-through estimator to directly train index predictor\nwith image-level supervision. Our proposed generative SR model (TVQ&RAP) is\nable to deliver photo-realistic SR results with small computational cost.",
        "url": "http://arxiv.org/abs/2509.23774v1",
        "published_date": "2025-09-28T09:40:38+00:00",
        "updated_date": "2025-09-28T09:40:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qifan Li",
            "Jiale Zou",
            "Jinhua Zhang",
            "Wei Long",
            "Xinyu Zhou",
            "Shuhang Gu"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a Texture Vector-Quantization and Reconstruction Aware Prediction strategy for generative super-resolution, delivering photo-realistic results with low computational cost.",
        "tldr_zh": "该论文引入了一种面向生成超分辨率的纹理向量量化和重建感知预测策略，以低计算成本提供了逼真的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Modality-Tailored Graph Modeling Framework for Urban Region Representation via Contrastive Learning",
        "summary": "Graph-based models have emerged as a powerful paradigm for modeling\nmultimodal urban data and learning region representations for various\ndownstream tasks. However, existing approaches face two major limitations. (1)\nThey typically employ identical graph neural network architectures across all\nmodalities, failing to capture modality-specific structures and\ncharacteristics. (2) During the fusion stage, they often neglect spatial\nheterogeneity by assuming that the aggregation weights of different modalities\nremain invariant across regions, resulting in suboptimal representations. To\naddress these issues, we propose MTGRR, a modality-tailored graph modeling\nframework for urban region representation, built upon a multimodal dataset\ncomprising point of interest (POI), taxi mobility, land use, road element,\nremote sensing, and street view images. (1) MTGRR categorizes modalities into\ntwo groups based on spatial density and data characteristics: aggregated-level\nand point-level modalities. For aggregated-level modalities, MTGRR employs a\nmixture-of-experts (MoE) graph architecture, where each modality is processed\nby a dedicated expert GNN to capture distinct modality-specific\ncharacteristics. For the point-level modality, a dual-level GNN is constructed\nto extract fine-grained visual semantic features. (2) To obtain effective\nregion representations under spatial heterogeneity, a spatially-aware\nmultimodal fusion mechanism is designed to dynamically infer region-specific\nmodality fusion weights. Building on this graph modeling framework, MTGRR\nfurther employs a joint contrastive learning strategy that integrates region\naggregated-level, point-level, and fusion-level objectives to optimize region\nrepresentations. Experiments on two real-world datasets across six modalities\nand three tasks demonstrate that MTGRR consistently outperforms\nstate-of-the-art baselines, validating its effectiveness.",
        "url": "http://arxiv.org/abs/2509.23772v1",
        "published_date": "2025-09-28T09:38:08+00:00",
        "updated_date": "2025-09-28T09:38:08+00:00",
        "categories": [
            "cs.CV",
            "stat.AP"
        ],
        "authors": [
            "Yaya Zhao",
            "Kaiqi Zhao",
            "Zixuan Tang",
            "Zhiyuan Liu",
            "Xiaoling Lu",
            "Yalei Du"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes a framework, MTGRR, for urban region representation using multimodal data and contrastive learning, outperforming existing methods on real-world datasets.",
        "tldr_zh": "本文提出了一种利用多模态数据和对比学习的框架MTGRR来进行城市区域表示，在真实数据集上表现优异。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ReLumix: Extending Image Relighting to Video via Video Diffusion Models",
        "summary": "Controlling illumination during video post-production is a crucial yet\nelusive goal in computational photography. Existing methods often lack\nflexibility, restricting users to certain relighting models. This paper\nintroduces ReLumix, a novel framework that decouples the relighting algorithm\nfrom temporal synthesis, thereby enabling any image relighting technique to be\nseamlessly applied to video. Our approach reformulates video relighting into a\nsimple yet effective two-stage process: (1) an artist relights a single\nreference frame using any preferred image-based technique (e.g., Diffusion\nModels, physics-based renderers); and (2) a fine-tuned stable video diffusion\n(SVD) model seamlessly propagates this target illumination throughout the\nsequence. To ensure temporal coherence and prevent artifacts, we introduce a\ngated cross-attention mechanism for smooth feature blending and a temporal\nbootstrapping strategy that harnesses SVD's powerful motion priors. Although\ntrained on synthetic data, ReLumix shows competitive generalization to\nreal-world videos. The method demonstrates significant improvements in visual\nfidelity, offering a scalable and versatile solution for dynamic lighting\ncontrol.",
        "url": "http://arxiv.org/abs/2509.23769v1",
        "published_date": "2025-09-28T09:35:33+00:00",
        "updated_date": "2025-09-28T09:35:33+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Lezhong Wang",
            "Shutong Jin",
            "Ruiqi Cui",
            "Anders Bjorholm Dahl",
            "Jeppe Revall Frisvad",
            "Siavash Bigdeli"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "ReLumix is a new framework that allows any image relighting technique to be applied to videos, achieving significant improvements in visual fidelity and offering dynamic lighting control.",
        "tldr_zh": "ReLumix是一个新框架，允许任何图像照明技术应用于视频，实现显著的视觉保真度改进并提供动态光照控制。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FastViDAR: Real-Time Omnidirectional Depth Estimation via Alternative Hierarchical Attention",
        "summary": "In this paper we propose FastViDAR, a novel framework that takes four fisheye\ncamera inputs and produces a full $360^\\circ$ depth map along with per-camera\ndepth, fusion depth, and confidence estimates. Our main contributions are: (1)\nWe introduce Alternative Hierarchical Attention (AHA) mechanism that\nefficiently fuses features across views through separate intra-frame and\ninter-frame windowed self-attention, achieving cross-view feature mixing with\nreduced overhead. (2) We propose a novel ERP fusion approach that projects\nmulti-view depth estimates to a shared equirectangular coordinate system to\nobtain the final fusion depth. (3) We generate ERP image-depth pairs using HM3D\nand 2D3D-S datasets for comprehensive evaluation, demonstrating competitive\nzero-shot performance on real datasets while achieving up to 20 FPS on NVIDIA\nOrin NX embedded hardware. Project page:\n\\href{https://3f7dfc.github.io/FastVidar/}{https://3f7dfc.github.io/FastVidar/}",
        "url": "http://arxiv.org/abs/2509.23733v1",
        "published_date": "2025-09-28T08:25:27+00:00",
        "updated_date": "2025-09-28T08:25:27+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Hangtian Zhao",
            "Xiang Chen",
            "Yizhe Li",
            "Qianhao Wang",
            "Haibo Lu",
            "Fei Gao"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "FastViDAR proposes a framework for real-time omnidirectional depth estimation using four fisheye cameras, achieving competitive performance on real datasets and high FPS on embedded hardware.",
        "tldr_zh": "FastViDAR提出了一种利用四个鱼眼摄像头进行实时全方位深度估计的框架，实现了在真实数据集上竞争性性能，并在嵌入式硬件上实现高帧率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LUQ: Layerwise Ultra-Low Bit Quantization for Multimodal Large Language Models",
        "summary": "Large Language Models (LLMs) with multimodal capabilities have revolutionized\nvision-language tasks, but their deployment often requires huge memory and\ncomputational resources. While post-training quantization (PTQ) has\nsuccessfully compressed language models to as low as 1-bit precision without\nsignificant performance loss, its effectiveness for multimodal LLMs (MLLMs)\nremains relatively unexplored. In this paper, we present the first study on\nultra-low bit (<4-bit) quantization for multimodal LLMs. Our analysis reveals\nthat multimodal tokens and intermediate layer activations produced by them\nexhibit significantly higher statistical variance and entropy compared to text\ntokens, making them less tolerant to ultra-low bit quantization. However, the\nactivation distributions of multimodal tokens varies significantly over\ndifferent layers, with some layers having lower entropy activation\ndistributions. We empirically show that such layers in these models can better\ntolerate ultra-low bit quantization. Building on these insights, we propose a\nnovel strategy for MLLM quantization, LUQ: Layerwise Ultra-Low Bit\nQuantization, which selectively applies ultra-low bit quantization to layers\nthat are more resilient to it. Additionally, we also show that using a mix of\nmultimodal tokens (image and text) for PTQ boosts VQA performance in the\nultra-low bit regime. We evaluate our method on LLaVA-1.5 and Qwen-2.5-VL\nacross 9 popular VQA benchmarks. The resulting LUQ models use 40% and 31% less\nmemory than their 4-bit counterparts, respectively, while exhibiting a\nperformance degradation of less than 10% on the MME benchmark.",
        "url": "http://arxiv.org/abs/2509.23729v1",
        "published_date": "2025-09-28T08:20:00+00:00",
        "updated_date": "2025-09-28T08:20:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "eess.IV"
        ],
        "authors": [
            "Shubhang Bhatnagar",
            "Andy Xu",
            "Kar-Han Tan",
            "Narendra Ahuja"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a new quantization method, LUQ, for compressing multimodal large language models with minimal performance loss.",
        "tldr_zh": "本文提出了一种新的量化方法LUQ，用于压缩多模态大型语言模型，减少性能损失。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "M3DLayout: A Multi-Source Dataset of 3D Indoor Layouts and Structured Descriptions for 3D Generation",
        "summary": "In text-driven 3D scene generation, object layout serves as a crucial\nintermediate representation that bridges high-level language instructions with\ndetailed geometric output. It not only provides a structural blueprint for\nensuring physical plausibility but also supports semantic controllability and\ninteractive editing. However, the learning capabilities of current 3D indoor\nlayout generation models are constrained by the limited scale, diversity, and\nannotation quality of existing datasets. To address this, we introduce\nM3DLayout, a large-scale, multi-source dataset for 3D indoor layout generation.\nM3DLayout comprises 15,080 layouts and over 258k object instances, integrating\nthree distinct sources: real-world scans, professional CAD designs, and\nprocedurally generated scenes. Each layout is paired with detailed structured\ntext describing global scene summaries, relational placements of large\nfurniture, and fine-grained arrangements of smaller items. This diverse and\nrichly annotated resource enables models to learn complex spatial and semantic\npatterns across a wide variety of indoor environments. To assess the potential\nof M3DLayout, we establish a benchmark using a text-conditioned diffusion\nmodel. Experimental results demonstrate that our dataset provides a solid\nfoundation for training layout generation models. Its multi-source composition\nenhances diversity, notably through the Inf3DLayout subset which provides rich\nsmall-object information, enabling the generation of more complex and detailed\nscenes. We hope that M3DLayout can serve as a valuable resource for advancing\nresearch in text-driven 3D scene synthesis.",
        "url": "http://arxiv.org/abs/2509.23728v1",
        "published_date": "2025-09-28T08:16:08+00:00",
        "updated_date": "2025-09-28T08:16:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yiheng Zhang",
            "Zhuojiang Cai",
            "Mingdao Wang",
            "Meitong Guo",
            "Tianxiao Li",
            "Li Lin",
            "Yuwang Wang"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces M3DLayout, a dataset for 3D indoor layout generation with detailed text descriptions, aiming to improve the training of layout generation models.",
        "tldr_zh": "本文介绍了M3DLayout，一个针对3D室内布局生成的数据集，具有详细的文本描述，旨在提高布局生成模型的训练。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Video Panels for Long Video Understanding",
        "summary": "Recent Video-Language Models (VLMs) achieve promising results on long-video\nunderstanding, but their performance still lags behind that achieved on tasks\ninvolving images or short videos. This has led to great interest in improving\nthe long context modeling of VLMs by introducing novel modules and additional\ncomplexity. % additional training time. In this paper, we take a different\napproach: rather than fine-tuning VLMs with the limited data available, we\nattempt to maximize the performance of existing models. To this end, we propose\na novel visual prompting strategy specifically designed for long-video\nunderstanding. By combining multiple frames as panels into one image, we\neffectively trade off spatial details for temporal resolution. Our approach is\ntraining-free, parameter-free, and model-agnostic, and can be seamlessly\nintegrated into existing VLMs. Extensive experiments on five established\nbenchmarks across a wide range of model architectures, sizes, and context\nwindows confirm the consistency of our approach. For the TimeScope (Long)\ndataset, which has the longest videos, the accuracy for video question\nanswering is improved by up to 19.4\\%. Overall, our method raises the bar for\nlong video understanding models. We will make our code available upon\nacceptance.",
        "url": "http://arxiv.org/abs/2509.23724v1",
        "published_date": "2025-09-28T08:05:55+00:00",
        "updated_date": "2025-09-28T08:05:55+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Lars Doorenbos",
            "Federico Spurio",
            "Juergen Gall"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper presents a novel visual prompting strategy for improving long-video understanding without additional training or model complexity.",
        "tldr_zh": "本文提出了一种新颖的视觉提示策略，可在不增加额外训练或模型复杂性的情况下改善长视频理解能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DiffPCN: Latent Diffusion Model Based on Multi-view Depth Images for Point Cloud Completion",
        "summary": "Latent diffusion models (LDMs) have demonstrated remarkable generative\ncapabilities across various low-level vision tasks. However, their potential\nfor point cloud completion remains underexplored due to the unstructured and\nirregular nature of point clouds. In this work, we propose DiffPCN, a novel\ndiffusion-based coarse-to-fine framework for point cloud completion. Our\napproach comprises two stages: an initial stage for generating coarse point\nclouds, and a refinement stage that improves their quality through point\ndenoising and upsampling. Specifically, we first project the unordered and\nirregular partial point cloud into structured depth images, which serve as\nconditions for a well-designed DepthLDM to synthesize completed multi-view\ndepth images that are used to form coarse point clouds. In this way, our\nDiffPCN can yield high-quality and high-completeness coarse point clouds by\nleveraging LDM' s powerful generation and comprehension capabilities. Then,\nsince LDMs inevitably introduce outliers into the generated depth maps, we\ndesign a Point Denoising Network to remove artifacts from the coarse point\ncloud by predicting a per-point distance score. Finally, we devise an\nAssociation-Aware Point Upsampler, which guides the upsampling process by\nleveraging local association features between the input point cloud and the\ncorresponding coarse points, further yielding a dense and high-fidelity output.\nExperimental results demonstrate that our DiffPCN achieves state-of-the-art\nperformance in geometric accuracy and shape completeness, significantly\nimproving the robustness and consistency of point cloud completion.",
        "url": "http://arxiv.org/abs/2509.23723v1",
        "published_date": "2025-09-28T08:05:43+00:00",
        "updated_date": "2025-09-28T08:05:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zijun Li",
            "Hongyu Yan",
            "Shijie Li",
            "Kunming Luo",
            "Li Lu",
            "Xulei Yang",
            "Weisi Lin"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel diffusion-based framework for completing point clouds using multi-view depth images, achieving state-of-the-art performance in geometric accuracy and shape completeness.",
        "tldr_zh": "本文提出了一种新的基于扩散的框架，利用多视图深度图像完成点云，实现了几何精度和形状完整性的最先进性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Diff-3DCap: Shape Captioning with Diffusion Models",
        "summary": "The task of 3D shape captioning occupies a significant place within the\ndomain of computer graphics and has garnered considerable interest in recent\nyears. Traditional approaches to this challenge frequently depend on the\nutilization of costly voxel representations or object detection techniques, yet\noften fail to deliver satisfactory outcomes. To address the above challenges,\nin this paper, we introduce Diff-3DCap, which employs a sequence of projected\nviews to represent a 3D object and a continuous diffusion model to facilitate\nthe captioning process. More precisely, our approach utilizes the continuous\ndiffusion model to perturb the embedded captions during the forward phase by\nintroducing Gaussian noise and then predicts the reconstructed annotation\nduring the reverse phase. Embedded within the diffusion framework is a\ncommitment to leveraging a visual embedding obtained from a pre-trained\nvisual-language model, which naturally allows the embedding to serve as a\nguiding signal, eliminating the need for an additional classifier. Extensive\nresults of our experiments indicate that Diff-3DCap can achieve performance\ncomparable to that of the current state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2509.23718v1",
        "published_date": "2025-09-28T07:59:22+00:00",
        "updated_date": "2025-09-28T07:59:22+00:00",
        "categories": [
            "cs.GR",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Zhenyu Shu",
            "Jiawei Wen",
            "Shiyang Li",
            "Shiqing Xin",
            "Ligang Liu"
        ],
        "ai_categories": [
            "Diffusion",
            "AIGC"
        ],
        "tldr": "The paper introduces Diff-3DCap, a method for shape captioning using diffusion models, achieving comparable performance to state-of-the-art methods.",
        "tldr_zh": "本文介绍了Diff-3DCap，这是一种利用扩散模型进行形状字幕处理的方法，其性能可与当前最先进的方法相媲美。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "StrucADT: Generating Structure-controlled 3D Point Clouds with Adjacency Diffusion Transformer",
        "summary": "In the field of 3D point cloud generation, numerous 3D generative models have\ndemonstrated the ability to generate diverse and realistic 3D shapes. However,\nthe majority of these approaches struggle to generate controllable 3D point\ncloud shapes that meet user-specific requirements, hindering the large-scale\napplication of 3D point cloud generation. To address the challenge of lacking\ncontrol in 3D point cloud generation, we are the first to propose controlling\nthe generation of point clouds by shape structures that comprise part\nexistences and part adjacency relationships. We manually annotate the adjacency\nrelationships between the segmented parts of point cloud shapes, thereby\nconstructing a StructureGraph representation. Based on this StructureGraph\nrepresentation, we introduce StrucADT, a novel structure-controllable point\ncloud generation model, which consists of StructureGraphNet module to extract\nstructure-aware latent features, cCNF Prior module to learn the distribution of\nthe latent features controlled by the part adjacency, and Diffusion Transformer\nmodule conditioned on the latent features and part adjacency to generate\nstructure-consistent point cloud shapes. Experimental results demonstrate that\nour structure-controllable 3D point cloud generation method produces\nhigh-quality and diverse point cloud shapes, enabling the generation of\ncontrollable point clouds based on user-specified shape structures and\nachieving state-of-the-art performance in controllable point cloud generation\non the ShapeNet dataset.",
        "url": "http://arxiv.org/abs/2509.23709v1",
        "published_date": "2025-09-28T07:45:51+00:00",
        "updated_date": "2025-09-28T07:45:51+00:00",
        "categories": [
            "cs.GR",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Zhenyu Shu",
            "Jiajun Shen",
            "Zhongui Chen",
            "Xiaoguang Han",
            "Shiqing Xin"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces StrucADT, a new model for generating 3D point clouds controlled by shape structures, achieving high-quality and diverse results.",
        "tldr_zh": "本文介绍了StrucADT，一种通过形状结构控制生成3D点云的新模型，实现了高质量和多样化的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CrimEdit: Controllable Editing for Counterfactual Object Removal, Insertion, and Movement",
        "summary": "Recent works on object removal and insertion have enhanced their performance\nby handling object effects such as shadows and reflections, using diffusion\nmodels trained on counterfactual datasets. However, the performance impact of\napplying classifier-free guidance to handle object effects across removal and\ninsertion tasks within a unified model remains largely unexplored. To address\nthis gap and improve efficiency in composite editing, we propose CrimEdit,\nwhich jointly trains the task embeddings for removal and insertion within a\nsingle model and leverages them in a classifier-free guidance scheme --\nenhancing the removal of both objects and their effects, and enabling\ncontrollable synthesis of object effects during insertion. CrimEdit also\nextends these two task prompts to be applied to spatially distinct regions,\nenabling object movement (repositioning) within a single denoising step. By\nemploying both guidance techniques, extensive experiments show that CrimEdit\nachieves superior object removal, controllable effect insertion, and efficient\nobject movement without requiring additional training or separate removal and\ninsertion stages.",
        "url": "http://arxiv.org/abs/2509.23708v1",
        "published_date": "2025-09-28T07:41:25+00:00",
        "updated_date": "2025-09-28T07:41:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Boseong Jeon",
            "Junghyuk Lee",
            "Jimin Park",
            "Kwanyoung Kim",
            "Jingi Jung",
            "Sangwon Lee",
            "Hyunbo Shim"
        ],
        "ai_categories": [
            "GAN",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "CrimEdit is a model that improves object removal and insertion tasks by jointly training a single model to handle both tasks with guidance, allowing for efficient object movement and controllable synthesis of object effects.",
        "tldr_zh": "CrimEdit 是一个模型，通过联合训练一个模型来处理对象的移除和插入任务，并提供指导，实现对象移动和可控制对象效果的综合合成。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "QuantSparse: Comprehensively Compressing Video Diffusion Transformer with Model Quantization and Attention Sparsification",
        "summary": "Diffusion transformers exhibit remarkable video generation capability, yet\ntheir prohibitive computational and memory costs hinder practical deployment.\nModel quantization and attention sparsification are two promising directions\nfor compression, but each alone suffers severe performance degradation under\naggressive compression. Combining them promises compounded efficiency gains,\nbut naive integration is ineffective. The sparsity-induced information loss\nexacerbates quantization noise, leading to amplified attention shifts. To\naddress this, we propose \\textbf{QuantSparse}, a unified framework that\nintegrates model quantization with attention sparsification. Specifically, we\nintroduce \\textit{Multi-Scale Salient Attention Distillation}, which leverages\nboth global structural guidance and local salient supervision to mitigate\nquantization-induced bias. In addition, we develop \\textit{Second-Order Sparse\nAttention Reparameterization}, which exploits the temporal stability of\nsecond-order residuals to efficiently recover information lost under sparsity.\nExperiments on HunyuanVideo-13B demonstrate that QuantSparse achieves 20.88\nPSNR, substantially outperforming the state-of-the-art quantization baseline\nQ-VDiT (16.85 PSNR), while simultaneously delivering a \\textbf{3.68$\\times$}\nreduction in storage and \\textbf{1.88$\\times$} acceleration in end-to-end\ninference. Our code will be released in\nhttps://github.com/wlfeng0509/QuantSparse.",
        "url": "http://arxiv.org/abs/2509.23681v1",
        "published_date": "2025-09-28T06:49:44+00:00",
        "updated_date": "2025-09-28T06:49:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weilun Feng",
            "Chuanguang Yang",
            "Haotong Qin",
            "Mingqiang Wu",
            "Yuqi Li",
            "Xiangqi Li",
            "Zhulin An",
            "Libo Huang",
            "Yulun Zhang",
            "Michele Magno",
            "Yongjun Xu"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes QuantSparse, a framework that combines model quantization and attention sparsification to compress video diffusion transformers effectively, outperforming existing methods in terms of performance, storage, and acceleration.",
        "tldr_zh": "本文提出了QuantSparse框架，将模型量化和注意力稀疏化结合起来，有效地压缩视频扩散变换器，在性能、存储和加速方面优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "MSD-KMamba: Bidirectional Spatial-Aware Multi-Modal 3D Brain Segmentation via Multi-scale Self-Distilled Fusion Strategy",
        "summary": "Numerous CNN-Transformer hybrid models rely on high-complexity global\nattention mechanisms to capture long-range dependencies, which introduces\nnon-linear computational complexity and leads to significant resource\nconsumption. Although knowledge distillation and sparse attention mechanisms\ncan improve efficiency, they often fall short of delivering the high\nsegmentation accuracy necessary for complex tasks. Balancing model performance\nwith computational efficiency remains a critical challenge. In this work, we\npropose a novel 3D multi-modal image segmentation framework, termed MSD-KMamba,\nwhich integrates bidirectional spatial perception with multi-scale\nself-distillation. The bidirectional spatial aware branch effectively captures\nlong-range spatial context dependencies across brain regions, while also\nincorporating a powerful nonlinear feature extraction mechanism that further\nenhances the model's ability to learn complex and heterogeneous patterns. In\naddition, the proposed multi-scale self-distilled fusion strategy strengthens\nhierarchical feature representations and improves the transfer of semantic\ninformation at different resolution levels. By jointly leveraging the\nbidirectional spatial perception branch and the multi-scale self-distilled\nfusion strategy, our framework effectively mitigates the bottleneck of\nquadratic computational complexity in volumetric segmentation, while\nsimultaneously addressing the limitation of insufficient global perception.\nExtensive experiments on multiple standard benchmark datasets demonstrate that\nMSD-KMamba consistently outperforms state-of-the-art methods in segmentation\naccuracy, robustness, and generalization, while maintaining high computational\nefficiency and favorable scalability. The source code of MSD-KMamba is publicly\navailable at https://github.com/daimao-zhang/MSD-KMamba.",
        "url": "http://arxiv.org/abs/2509.23677v1",
        "published_date": "2025-09-28T06:34:01+00:00",
        "updated_date": "2025-09-28T06:34:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dayu Tan",
            "Ziwei Zhang",
            "Yansan Su",
            "Xin Peng",
            "Yike Dai",
            "Chunhou Zheng",
            "Weimin Zhong"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "MSD-KMamba is a new 3D brain segmentation framework that integrates bidirectional spatial perception with multi-scale self-distillation, showing superior performance in accuracy, efficiency, and scalability.",
        "tldr_zh": "MSD-KMamba是一个新的3D脑分割框架，将双向空间感知与多尺度自我蒸馏相结合，表现出在准确性、效率和可扩展性方面的优越性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "RCI: A Score for Evaluating Global and Local Reasoning in Multimodal Benchmarks",
        "summary": "Multimodal Large Language Models (MLLMs) have achieved impressive results on\nvision-language benchmarks, yet it remains unclear whether these benchmarks\nassess genuine global reasoning or allow success via localized visual cues.\nExisting evaluation methods do not explicitly measure this distinction,\nhindering effective dataset curation and real-world focused model development.\n  We introduce Region Comprehension Index (RCI), the first model-based score to\ndirectly quantify a dataset's reliance on global versus local visual\ninformation. RCI systematically compares reference-model performance on image\npatches versus full images, revealing if tasks require holistic image\nunderstanding or can be solved with partial or localized visual cues.\n  When applying RCI to 13 widely used multimodal benchmarks, we observed that\nmost of them favor localized reasoning and exhibit significant spatial biases,\nindicating potential risks in real-world applications. RCI equips researchers &\npractitioners with an actionable tool for diagnosing & mitigating these biases,\nenabling the construction of datasets and benchmarks to foster the development\nof robust, enterprise-ready multimodal systems.",
        "url": "http://arxiv.org/abs/2509.23673v1",
        "published_date": "2025-09-28T06:26:11+00:00",
        "updated_date": "2025-09-28T06:26:11+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.MM",
            "68T45, 68T50",
            "I.2.7; I.2.10; I.4.7; I.4.8"
        ],
        "authors": [
            "Amit Agarwal",
            "Hitesh Laxmichand Patel",
            "Srikant Panda",
            "Hansa Meghwani",
            "Jyotika Singh",
            "Karan Dua",
            "Paul Li",
            "Tao Sheng",
            "Sujith Ravi",
            "Dan Roth"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a new score called RCI to evaluate global and local reasoning in multimodal benchmarks, highlighting potential biases and risks in current benchmarks.",
        "tldr_zh": "该论文介绍了一种名为RCI的新评分标准，评估多模态基准中的全局和局部推理，凸显当前基准中潜在的偏见和风险。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Token Merging via Spatiotemporal Information Mining for Surgical Video Understanding",
        "summary": "Vision Transformer models have shown impressive effectiveness in the surgical\nvideo understanding tasks through long-range dependency modeling. However,\ncurrent methods suffer from prohibitive computational costs due to processing\nmassive spatiotemporal tokens across video frames. While prior work on token\nmerging has advanced model efficiency, they fail to adequately consider the\ninherent spatiotemporal structure of video data and overlook the heterogeneous\nnature of information distribution, leading to suboptimal performance. In this\npaper, we propose a spatiotemporal information mining token merging (STIM-TM)\nmethod, representing the first dedicated approach for surgical video\nunderstanding. STIM-TM introduces a decoupled strategy that reduces token\nredundancy along temporal and spatial dimensions independently. Specifically,\nthe temporal component merges spatially corresponding tokens from consecutive\nframes using saliency weighting, preserving critical sequential information and\nmaintaining continuity. Meanwhile, the spatial component prioritizes merging\nstatic tokens through temporal stability analysis, protecting dynamic regions\ncontaining essential surgical information. Operating in a training-free manner,\nSTIM-TM achieves significant efficiency gains with over $65\\%$ GFLOPs reduction\nwhile preserving competitive accuracy across comprehensive surgical video\ntasks. Our method also supports efficient training of long-sequence surgical\nvideos, addressing computational bottlenecks in surgical applications.",
        "url": "http://arxiv.org/abs/2509.23672v1",
        "published_date": "2025-09-28T06:24:57+00:00",
        "updated_date": "2025-09-28T06:24:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xixi Jiang",
            "Chen Yang",
            "Dong Zhang",
            "Pingcheng Dong",
            "Xin Yang",
            "Kwang-Ting Cheng"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper proposes a method called STIM-TM for surgical video understanding by efficiently merging tokens based on spatiotemporal information mining, achieving significant efficiency gains while preserving competitive accuracy.",
        "tldr_zh": "本文提出了一种名为STIM-TM的方法，通过有效地合并基于时空信息挖掘的令牌，实现了显著的效率提升，同时保持竞争性准确性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HIVTP: A Training-Free Method to Improve VLMs Efficiency via Hierarchical Visual Token Pruning Using Middle-Layer-Based Importance Score",
        "summary": "Vision-Language Models (VLMs) have shown strong capabilities on diverse\nmultimodal tasks. However, the large number of visual tokens output by the\nvision encoder severely hinders inference efficiency, and prior studies have\nshown that many of these tokens are not important and can therefore be safely\npruned. In this work, we propose HIVTP, a training-free method to improve VLMs\nefficiency via hierarchical visual token pruning using a novel\nmiddle-layer-based importance score. Specifically, we utilize attention maps\nextracted from the middle layers of the vision encoder, which better reflect\nfine-grained and object-level attention, to estimate visual token importance.\nBased on this, we propose a hierarchical visual token pruning method to retain\nboth globally and locally important visual tokens. Specifically, we reshape the\n1-D visual token sequence output by the vision encoder into a 2-D spatial\nlayout. In the global retaining stage, we divide the image into regions and\nretain tokens with higher importance scores in each region; in the local\nretaining stage, we then divide the image into small windows and retain the\nmost important token in each local window. Experimental results show that our\nproposed method, HIVTP, can reduce the time-to-first-token (TTFT) of\nLLaVA-v1.5-7B and LLaVA-Next-7B by up to 50.0% and 55.1%, respectively, and\nimprove the token generation throughput by up to 60.9% and 47.3%, without\nsacrificing accuracy, and even achieving improvements on certain benchmarks.\nCompared with prior works, HIVTP achieves better accuracy while offering higher\ninference efficiency.",
        "url": "http://arxiv.org/abs/2509.23663v1",
        "published_date": "2025-09-28T05:53:39+00:00",
        "updated_date": "2025-09-28T05:53:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingqi Xu",
            "Jingxi Lu",
            "Chenghao Li",
            "Sreetama Sarkar",
            "Peter A. Beerel"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a training-free method called HIVTP to improve efficiency in Vision-Language Models by pruning unimportant visual tokens using a novel middle-layer-based importance score. Experimental results show significant improvements in efficiency without sacrificing accuracy.",
        "tldr_zh": "本文提出了一种名为HIVTP的无需训练的方法，通过使用基于中间层的重要性评分修剪不重要的视觉令牌，从而改善视觉语言模型的效率。实验结果显示，在不牺牲准确性的情况下，效率有显著提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Focusing on What Matters: Object-Agent-centric Tokenization for Vision Language Action models",
        "summary": "Vision-Language-Action (VLA) models offer a pivotal approach to learning\nrobotic manipulation at scale by repurposing large pre-trained\nVision-Language-Models (VLM) to output robotic actions. However, adapting VLMs\nfor robotic domains comes with an unnecessarily high computational cost, which\nwe attribute to the tokenization scheme of visual inputs. In this work, we aim\nto enable efficient VLA training by proposing Oat-VLA, an Object-Agent-centric\nTokenization for VLAs. Building on the insights of object-centric\nrepresentation learning, our method introduces an inductive bias towards scene\nobjects and the agent's own visual information. As a result, we find that\nOat-VLA can drastically reduce the number of visual tokens to just a few tokens\nwithout sacrificing performance. We reveal that Oat-VLA converges at least\ntwice as fast as OpenVLA on the LIBERO suite, as well as outperform OpenVLA in\ndiverse real-world pick and place tasks.",
        "url": "http://arxiv.org/abs/2509.23655v1",
        "published_date": "2025-09-28T05:42:53+00:00",
        "updated_date": "2025-09-28T05:42:53+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Rokas Bendikas",
            "Daniel Dijkman",
            "Markus Peschl",
            "Sanjay Haresh",
            "Pietro Mazzaglia"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer"
        ],
        "tldr": "The paper introduces a new tokenization scheme, Oat-VLA, for Vision-Language-Action models that significantly reduces computational cost without sacrificing performance.",
        "tldr_zh": "本文介绍了一种新的令牌化方案，Oat-VLA，用于视觉-语言-动作模型，可以显著降低计算成本而不影响性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ReWatch-R1: Boosting Complex Video Reasoning in Large Vision-Language Models through Agentic Data Synthesis",
        "summary": "While Reinforcement Learning with Verifiable Reward (RLVR) significantly\nadvances image reasoning in Large Vision-Language Models (LVLMs), its\napplication to complex video reasoning remains underdeveloped. This gap stems\nprimarily from a critical data bottleneck: existing datasets lack the\nchallenging, multi-hop questions and high-quality, video-grounded\nChain-of-Thought (CoT) data necessary to effectively bootstrap RLVR. To address\nthis, we introduce ReWatch, a large-scale dataset built to foster advanced\nvideo reasoning. We propose a novel multi-stage synthesis pipeline to\nsynthesize its three components: ReWatch-Caption, ReWatch-QA, and ReWatch-CoT.\nA core innovation is our Multi-Agent ReAct framework for CoT synthesis, which\nsimulates a human-like \"re-watching\" process to generate video-grounded\nreasoning traces by explicitly modeling information retrieval and verification.\nBuilding on this dataset, we develop ReWatch-R1 by post-training a strong\nbaseline LVLM with Supervised Fine-Tuning (SFT) and our RLVR framework. This\nframework incorporates a novel Observation \\& Reasoning (O\\&R) reward mechanism\nthat evaluates both the final answer's correctness and the reasoning's\nalignment with video content, directly penalizing hallucination. Our\nexperiments show that ReWatch-R1 achieves state-of-the-art average performance\non five challenging video reasoning benchmarks.",
        "url": "http://arxiv.org/abs/2509.23652v1",
        "published_date": "2025-09-28T05:38:16+00:00",
        "updated_date": "2025-09-28T05:38:16+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Congzhi Zhang",
            "Zhibin Wang",
            "Yinchao Ma",
            "Jiawei Peng",
            "Yihan Wang",
            "Qiang Zhou",
            "Jun Song",
            "Bo Zheng"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces ReWatch-R1, a dataset and framework for improving video reasoning in large vision-language models, achieving state-of-the-art performance on challenging benchmarks.",
        "tldr_zh": "本文介绍了ReWatch-R1，一个数据集和框架，用于改进大型视觉语言模型中的视频推理，在具有挑战性的基准测试中实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Sparse-Up: Learnable Sparse Upsampling for 3D Generation with High-Fidelity Textures",
        "summary": "The creation of high-fidelity 3D assets is often hindered by a 'pixel-level\npain point': the loss of high-frequency details. Existing methods often trade\noff one aspect for another: either sacrificing cross-view consistency,\nresulting in torn or drifting textures, or remaining trapped by the resolution\nceiling of explicit voxels, forfeiting fine texture detail. In this work, we\npropose Sparse-Up, a memory-efficient, high-fidelity texture modeling framework\nthat effectively preserves high-frequency details. We use sparse voxels to\nguide texture reconstruction and ensure multi-view consistency, while\nleveraging surface anchoring and view-domain partitioning to break through\nresolution constraints. Surface anchoring employs a learnable upsampling\nstrategy to constrain voxels to the mesh surface, eliminating over 70% of\nredundant voxels present in traditional voxel upsampling. View-domain\npartitioning introduces an image patch-guided voxel partitioning scheme,\nsupervising and back-propagating gradients only on visible local patches.\nThrough these two strategies, we can significantly reduce memory consumption\nduring high-resolution voxel training without sacrificing geometric\nconsistency, while preserving high-frequency details in textures.",
        "url": "http://arxiv.org/abs/2509.23646v1",
        "published_date": "2025-09-28T05:06:03+00:00",
        "updated_date": "2025-09-28T05:06:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lu Xiao",
            "Jiale Zhang",
            "Yang Liu",
            "Taicheng Huang",
            "Xin Tian"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Dataset"
        ],
        "tldr": "Sparse-Up is a memory-efficient 3D texture modeling framework that preserves high-frequency details using sparse voxels and surface anchoring, while reducing memory consumption during high-resolution training.",
        "tldr_zh": "Sparse-Up是一种内存高效的3D纹理建模框架，利用稀疏体素和表面锚定来保留高频细节，同时在高分辨率训练过程中减少内存消耗。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Griffin: Generative Reference and Layout Guided Image Composition",
        "summary": "Text-to-image models have achieved a level of realism that enables the\ngeneration of highly convincing images. However, text-based control can be a\nlimiting factor when more explicit guidance is needed. Defining both the\ncontent and its precise placement within an image is crucial for achieving\nfiner control. In this work, we address the challenge of multi-image layout\ncontrol, where the desired content is specified through images rather than\ntext, and the model is guided on where to place each element. Our approach is\ntraining-free, requires a single image per reference, and provides explicit and\nsimple control for object and part-level composition. We demonstrate its\neffectiveness across various image composition tasks.",
        "url": "http://arxiv.org/abs/2509.23643v1",
        "published_date": "2025-09-28T04:54:06+00:00",
        "updated_date": "2025-09-28T04:54:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aryan Mikaeili",
            "Amirhossein Alimohammadi",
            "Negar Hassanpour",
            "Ali Mahdavi-Amiri",
            "Andrea Tagliasacchi"
        ],
        "ai_categories": [
            "GAN",
            "Multimodality"
        ],
        "tldr": "The paper introduces a training-free approach for image composition that uses images as references for content placement, providing explicit control at object and part levels.",
        "tldr_zh": "本文介绍了一种使用图像作为参考的图像合成方法，可以在对象和部分级别提供明确的控制。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LightFair: Towards an Efficient Alternative for Fair T2I Diffusion via Debiasing Pre-trained Text Encoders",
        "summary": "This paper explores a novel lightweight approach LightFair to achieve fair\ntext-to-image diffusion models (T2I DMs) by addressing the adverse effects of\nthe text encoder. Most existing methods either couple different parts of the\ndiffusion model for full-parameter training or rely on auxiliary networks for\ncorrection. They incur heavy training or sampling burden and unsatisfactory\nperformance. Since T2I DMs consist of multiple components, with the text\nencoder being the most fine-tunable and front-end module, this paper focuses on\nmitigating bias by fine-tuning text embeddings. To validate feasibility, we\nobserve that the text encoder's neutral embedding output shows substantial\nskewness across image embeddings of various attributes in the CLIP space. More\nimportantly, the noise prediction network further amplifies this imbalance. To\nfinetune the text embedding, we propose a collaborative distance-constrained\ndebiasing strategy that balances embedding distances to improve fairness\nwithout auxiliary references. However, mitigating bias can compromise the\noriginal generation quality. To address this, we introduce a two-stage\ntext-guided sampling strategy to limit when the debiased text encoder\nintervenes. Extensive experiments demonstrate that LightFair is effective and\nefficient. Notably, on Stable Diffusion v1.5, our method achieves SOTA\ndebiasing at just $1/4$ of the training burden, with virtually no increase in\nsampling burden. The code is available at https://github.com/boyuh/LightFair.",
        "url": "http://arxiv.org/abs/2509.23639v1",
        "published_date": "2025-09-28T04:46:39+00:00",
        "updated_date": "2025-09-28T04:46:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Boyu Han",
            "Qianqian Xu",
            "Shilong Bao",
            "Zhiyong Yang",
            "Kangli Zi",
            "Qingming Huang"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion"
        ],
        "tldr": "The paper introduces LightFair, a lightweight approach that aims to achieve fair text-to-image diffusion models by debiasing pre-trained text encoders.",
        "tldr_zh": "本文介绍了一种轻量级方法LightFair，旨在通过去偏置预训练文本编码器来实现公平的文本到图像扩散模型。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MotionVerse: A Unified Multimodal Framework for Motion Comprehension, Generation and Editing",
        "summary": "This paper proposes MotionVerse, a unified framework that harnesses the\ncapabilities of Large Language Models (LLMs) to comprehend, generate, and edit\nhuman motion in both single-person and multi-person scenarios. To efficiently\nrepresent motion data, we employ a motion tokenizer with residual quantization,\nwhich converts continuous motion sequences into multi-stream discrete tokens.\nFurthermore, we introduce a \\textit{Delay Parallel} Modeling strategy, which\ntemporally staggers the encoding of residual token streams. This design enables\nLLMs to effectively capture inter-stream dependencies while maintaining\ncomputational efficiency comparable to single-stream modeling. Moreover, to\nalleviate modality interference between motion and language, we design a\n\\textit{dual-tower architecture} with modality-specific parameters, ensuring\nstable integration of motion information for both comprehension and generation\ntasks. Comprehensive ablation studies demonstrate the effectiveness of each\ncomponent in MotionVerse, and extensive experiments showcase its superior\nperformance across a wide range of motion-relevant tasks.",
        "url": "http://arxiv.org/abs/2509.23635v1",
        "published_date": "2025-09-28T04:20:56+00:00",
        "updated_date": "2025-09-28T04:20:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruibing Hou",
            "Mingshuang Luo",
            "Hongyu Pan",
            "Hong Chang",
            "Shiguang Shan"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "MotionVerse is a unified framework that uses Large Language Models to comprehend, generate, and edit human motion, showing superior performance in various motion-related tasks.",
        "tldr_zh": "MotionVerse是一个统一框架，利用大型语言模型来理解、生成和编辑人类动作，在各种动作相关任务中展现出优越性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Efficient Domain-Adaptive Multi-Task Dense Prediction with Vision Foundation Models",
        "summary": "Multi-task dense prediction, which aims to jointly solve tasks like semantic\nsegmentation and depth estimation, is crucial for robotics applications but\nsuffers from domain shift when deploying models in new environments. While\nunsupervised domain adaptation (UDA) addresses this challenge for single tasks,\nexisting multi-task UDA methods primarily rely on adversarial learning\napproaches that are less effective than recent self-training techniques. In\nthis paper, we introduce FAMDA, a simple yet effective UDA framework that\nbridges this gap by leveraging Vision Foundation Models (VFMs) as powerful\nteachers. Our approach integrates Segmentation and Depth foundation models into\na self-training paradigm to generate high-quality pseudo-labels for the target\ndomain, effectively distilling their robust generalization capabilities into a\nsingle, efficient student network. Extensive experiments show that FAMDA\nachieves state-of-the-art (SOTA) performance on standard synthetic-to-real UDA\nmulti-task learning (MTL) benchmarks and a challenging new day-to-night\nadaptation task. Our framework enables the training of highly efficient models;\na lightweight variant achieves SOTA accuracy while being more than 10$\\times$\nsmaller than foundation models, highlighting FAMDA's suitability for creating\ndomain-adaptive and efficient models for resource-constrained robotics\napplications.",
        "url": "http://arxiv.org/abs/2509.23626v1",
        "published_date": "2025-09-28T04:02:36+00:00",
        "updated_date": "2025-09-28T04:02:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Beomseok Kang",
            "Niluthpol Chowdhury Mithun",
            "Mikhail Sizintsev",
            "Han-Pang Chiu",
            "Supun Samarasekera"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "This paper introduces FAMDA, a framework that leverages Vision Foundation Models to improve domain-adaptive multi-task dense prediction for robotics applications.",
        "tldr_zh": "本文介绍了FAMDA，一种利用视觉基础模型改进机器人应用领域的领域自适应多任务密集预测的框架。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "BioVessel-Net and RetinaMix: Unsupervised Retinal Vessel Segmentation from OCTA Images",
        "summary": "Structural changes in retinal blood vessels are critical biomarkers for the\nonset and progression of glaucoma and other ocular diseases. However, current\nvessel segmentation approaches largely rely on supervised learning and\nextensive manual annotations, which are costly, error-prone, and difficult to\nobtain in optical coherence tomography angiography. Here we present\nBioVessel-Net, an unsupervised generative framework that integrates vessel\nbiostatistics with adversarial refinement and a radius-guided segmentation\nstrategy. Unlike pixel-based methods, BioVessel-Net directly models vascular\nstructures with biostatistical coherence, achieving accurate and explainable\nvessel extraction without labeled data or high-performance computing. To\nsupport training and evaluation, we introduce RetinaMix, a new benchmark\ndataset of 2D and 3D OCTA images with high-resolution vessel details from\ndiverse populations. Experimental results demonstrate that BioVessel-Net\nachieves near-perfect segmentation accuracy across RetinaMix and existing\ndatasets, substantially outperforming state-of-the-art supervised and\nsemi-supervised methods. Together, BioVessel-Net and RetinaMix provide a\nlabel-free, computationally efficient, and clinically interpretable solution\nfor retinal vessel analysis, with broad potential for glaucoma monitoring,\nblood flow modeling, and progression prediction. Code and dataset are\navailable: https://github.com/VikiXie/SatMar8.",
        "url": "http://arxiv.org/abs/2509.23617v1",
        "published_date": "2025-09-28T03:46:20+00:00",
        "updated_date": "2025-09-28T03:46:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Cheng Huang",
            "Weizheng Xie",
            "Fan Gao",
            "Yutong Liu",
            "Ruoling Wu",
            "Zeyu Han",
            "Jingxi Qiu",
            "Xiangxiang Wang",
            "Zhenglin Yang",
            "Hao Wang",
            "Yongbin Yu"
        ],
        "ai_categories": [
            "AIGC",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper presents BioVessel-Net, an unsupervised generative framework for retinal vessel segmentation, along with a new benchmark dataset called RetinaMix. It achieves accurate vessel extraction without labeled data, outperforming existing methods.",
        "tldr_zh": "本文介绍了BioVessel-Net，一种用于视网膜血管分割的无监督生成框架，以及一个名为RetinaMix的新基准数据集。它在没有标记数据的情况下实现准确的血管提取，优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "InteractMove: Text-Controlled Human-Object Interaction Generation in 3D Scenes with Movable Objects",
        "summary": "We propose a novel task of text-controlled human object interaction\ngeneration in 3D scenes with movable objects. Existing human-scene interaction\ndatasets suffer from insufficient interaction categories and typically only\nconsider interactions with static objects (do not change object positions), and\nthe collection of such datasets with movable objects is difficult and costly.\nTo address this problem, we construct the InteractMove dataset for Movable\nHuman-Object Interaction in 3D Scenes by aligning existing human object\ninteraction data with scene contexts, featuring three key characteristics: 1)\nscenes containing multiple movable objects with text-controlled interaction\nspecifications (including same-category distractors requiring spatial and 3D\nscene context understanding), 2) diverse object types and sizes with varied\ninteraction patterns (one-hand, two-hand, etc.), and 3) physically plausible\nobject manipulation trajectories. With the introduction of various movable\nobjects, this task becomes more challenging, as the model needs to identify\nobjects to be interacted with accurately, learn to interact with objects of\ndifferent sizes and categories, and avoid collisions between movable objects\nand the scene. To tackle such challenges, we propose a novel pipeline solution.\nWe first use 3D visual grounding models to identify the interaction object.\nThen, we propose a hand-object joint affordance learning to predict contact\nregions for different hand joints and object parts, enabling accurate grasping\nand manipulation of diverse objects. Finally, we optimize interactions with\nlocal-scene modeling and collision avoidance constraints, ensuring physically\nplausible motions and avoiding collisions between objects and the scene.\nComprehensive experiments demonstrate our method's superiority in generating\nphysically plausible, text-compliant interactions compared to existing\napproaches.",
        "url": "http://arxiv.org/abs/2509.23612v1",
        "published_date": "2025-09-28T03:29:15+00:00",
        "updated_date": "2025-09-28T03:29:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xinhao Cai",
            "Minghang Zheng",
            "Xin Jin",
            "Yang Liu"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer",
            "AIGC"
        ],
        "tldr": "The paper introduces a new task of text-controlled human-object interaction generation in 3D scenes with movable objects, proposing a dataset and a pipeline solution to tackle the challenges involved.",
        "tldr_zh": "本文介绍了一个新的任务，即在具有可移动对象的3D场景中生成由文本控制的人物物体交互，提出了一个数据集和处理管线解决这些挑战。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and Multi-Scale Global-Local Attention",
        "summary": "Audio-visual speech separation (AVSS) methods leverage visual cues to extract\ntarget speech and have demonstrated strong separation quality in noisy acoustic\nenvironments. However, these methods usually involve a large number of\nparameters and require high computational cost, which is unacceptable in many\napplications where speech separation serves as only a preprocessing step for\nfurther speech processing. To address this issue, we propose an efficient AVSS\nmethod, named Dolphin. For visual feature extraction, we develop DP-LipCoder, a\ndual-path lightweight video encoder that transforms lip-motion into discrete\naudio-aligned semantic tokens. For audio separation, we construct a lightweight\nencoder-decoder separator, in which each layer incorporates a global-local\nattention (GLA) block to efficiently capture multi-scale dependencies.\nExperiments on three benchmark datasets showed that Dolphin not only surpassed\nthe current state-of-the-art (SOTA) model in separation quality but also\nachieved remarkable improvements in efficiency: over 50% fewer parameters, more\nthan 2.4x reduction in MACs, and over 6x faster GPU inference speed. These\nresults indicate that Dolphin offers a practical and deployable solution for\nhigh-performance AVSS in real-world scenarios. Our code and demo page are\npublicly available at http://cslikai.cn/Dolphin/.",
        "url": "http://arxiv.org/abs/2509.23610v1",
        "published_date": "2025-09-28T03:25:34+00:00",
        "updated_date": "2025-09-28T03:25:34+00:00",
        "categories": [
            "cs.SD",
            "cs.CV"
        ],
        "authors": [
            "Kai Li",
            "Kejun Gao",
            "Xiaolin Hu"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces an efficient audio-visual speech separation method named Dolphin, which outperforms existing models in separation quality and efficiency.",
        "tldr_zh": "该论文介绍了一种名为Dolphin的高效音视频语音分离方法，该方法在分离质量和效率方面优于现有模型。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ZeroScene: A Zero-Shot Framework for 3D Scene Generation from a Single Image and Controllable Texture Editing",
        "summary": "In the field of 3D content generation, single image scene reconstruction\nmethods still struggle to simultaneously ensure the quality of individual\nassets and the coherence of the overall scene in complex environments, while\ntexture editing techniques often fail to maintain both local continuity and\nmulti-view consistency. In this paper, we propose a novel system ZeroScene,\nwhich leverages the prior knowledge of large vision models to accomplish both\nsingle image-to-3D scene reconstruction and texture editing in a zero-shot\nmanner. ZeroScene extracts object-level 2D segmentation and depth information\nfrom input images to infer spatial relationships within the scene. It then\njointly optimizes 3D and 2D projection losses of the point cloud to update\nobject poses for precise scene alignment, ultimately constructing a coherent\nand complete 3D scene that encompasses both foreground and background.\nMoreover, ZeroScene supports texture editing of objects in the scene. By\nimposing constraints on the diffusion model and introducing a mask-guided\nprogressive image generation strategy, we effectively maintain texture\nconsistency across multiple viewpoints and further enhance the realism of\nrendered results through Physically Based Rendering (PBR) material estimation.\nExperimental results demonstrate that our framework not only ensures the\ngeometric and appearance accuracy of generated assets, but also faithfully\nreconstructs scene layouts and produces highly detailed textures that closely\nalign with text prompts.",
        "url": "http://arxiv.org/abs/2509.23607v1",
        "published_date": "2025-09-28T03:21:12+00:00",
        "updated_date": "2025-09-28T03:21:12+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Xiang Tang",
            "Ruotong Li",
            "Xiaopeng Fan"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper presents ZeroScene, a system that generates 3D scenes from single images and allows for texture editing using large vision models. It ensures scene coherence and quality while maintaining texture consistency across viewpoints.",
        "tldr_zh": "本文介绍了ZeroScene，一种系统，可以利用大视觉模型从单一图像生成3D场景，并允许进行纹理编辑。它确保场景的连贯性和质量，同时保持纹理在各个视角上的一致性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VMDiff: Visual Mixing Diffusion for Limitless Cross-Object Synthesis",
        "summary": "Creating novel images by fusing visual cues from multiple sources is a\nfundamental yet underexplored problem in image-to-image generation, with broad\napplications in artistic creation, virtual reality and visual media. Existing\nmethods often face two key challenges: coexistent generation, where multiple\nobjects are simply juxtaposed without true integration, and bias generation,\nwhere one object dominates the output due to semantic imbalance. To address\nthese issues, we propose Visual Mixing Diffusion (VMDiff), a simple yet\neffective diffusion-based framework that synthesizes a single, coherent object\nby integrating two input images at both noise and latent levels. Our approach\ncomprises: (1) a hybrid sampling process that combines guided denoising,\ninversion, and spherical interpolation with adjustable parameters to achieve\nstructure-aware fusion, mitigating coexistent generation; and (2) an efficient\nadaptive adjustment module, which introduces a novel similarity-based score to\nautomatically and adaptively search for optimal parameters, countering semantic\nbias. Experiments on a curated benchmark of 780 concept pairs demonstrate that\nour method outperforms strong baselines in visual quality, semantic\nconsistency, and human-rated creativity.",
        "url": "http://arxiv.org/abs/2509.23605v1",
        "published_date": "2025-09-28T03:17:58+00:00",
        "updated_date": "2025-09-28T03:17:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zeren Xiong",
            "Yue Yu",
            "Zedong Zhang",
            "Shuo Chen",
            "Jian Yang",
            "Jun Li"
        ],
        "ai_categories": [
            "GAN",
            "Multimodality"
        ],
        "tldr": "The paper introduces Visual Mixing Diffusion (VMDiff) for synthesizing coherent objects from multiple input images, addressing challenges of coexistent generation and bias generation in image-to-image generation tasks.",
        "tldr_zh": "本文介绍了一种名为Visual Mixing Diffusion (VMDiff)的方法，用于从多个输入图像中合成连贯的对象，解决图像生成任务中的共存生成和偏见生成挑战。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MAN: Latent Diffusion Enhanced Multistage Anti-Noise Network for Efficient and High-Quality Low-Dose CT Image Denoising",
        "summary": "While diffusion models have set a new benchmark for quality in Low-Dose\nComputed Tomography (LDCT) denoising, their clinical adoption is critically\nhindered by extreme computational costs, with inference times often exceeding\nthousands of seconds per scan. To overcome this barrier, we introduce MAN, a\nLatent Diffusion Enhanced Multistage Anti-Noise Network for Efficient and\nHigh-Quality Low-Dose CT Image Denoising task. Our method operates in a\ncompressed latent space via a perceptually-optimized autoencoder, enabling an\nattention-based conditional U-Net to perform the fast, deterministic\nconditional denoising diffusion process with drastically reduced overhead. On\nthe LDCT and Projection dataset, our model achieves superior perceptual\nquality, surpassing CNN/GAN-based methods while rivaling the reconstruction\nfidelity of computationally heavy diffusion models like DDPM and Dn-Dp. Most\ncritically, in the inference stage, our model is over 60x faster than\nrepresentative pixel space diffusion denoisers, while remaining competitive on\nPSNR/SSIM scores. By bridging the gap between high fidelity and clinical\nviability, our work demonstrates a practical path forward for advanced\ngenerative models in medical imaging.",
        "url": "http://arxiv.org/abs/2509.23603v1",
        "published_date": "2025-09-28T03:13:39+00:00",
        "updated_date": "2025-09-28T03:13:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tangtangfang Fang",
            "Jingxi Hu",
            "Xiangjian He",
            "Jiaqi Yang"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces MAN, a network for efficient and high-quality denoising of low-dose CT images using a compressed latent space and attention-based conditional U-Net.",
        "tldr_zh": "本文介绍了MAN，一种利用压缩潜在空间和基于注意力的条件U-Net进行低剂量CT图像去噪的高效高质量网络。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Deep Taxonomic Networks for Unsupervised Hierarchical Prototype Discovery",
        "summary": "Inspired by the human ability to learn and organize knowledge into\nhierarchical taxonomies with prototypes, this paper addresses key limitations\nin current deep hierarchical clustering methods. Existing methods often tie the\nstructure to the number of classes and underutilize the rich prototype\ninformation available at intermediate hierarchical levels. We introduce deep\ntaxonomic networks, a novel deep latent variable approach designed to bridge\nthese gaps. Our method optimizes a large latent taxonomic hierarchy,\nspecifically a complete binary tree structured mixture-of-Gaussian prior within\na variational inference framework, to automatically discover taxonomic\nstructures and associated prototype clusters directly from unlabeled data\nwithout assuming true label sizes. We analytically show that optimizing the\nELBO of our method encourages the discovery of hierarchical relationships among\nprototypes. Empirically, our learned models demonstrate strong hierarchical\nclustering performance, outperforming baselines across diverse image\nclassification datasets using our novel evaluation mechanism that leverages\nprototype clusters discovered at all hierarchical levels. Qualitative results\nfurther reveal that deep taxonomic networks discover rich and interpretable\nhierarchical taxonomies, capturing both coarse-grained semantic categories and\nfine-grained visual distinctions.",
        "url": "http://arxiv.org/abs/2509.23602v1",
        "published_date": "2025-09-28T03:13:32+00:00",
        "updated_date": "2025-09-28T03:13:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zekun Wang",
            "Ethan Haarer",
            "Zhiyi Dai",
            "Tianyi Zhu",
            "Christopher J. MacLellan"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces deep taxonomic networks for unsupervised hierarchical prototype discovery, outperforming existing methods in hierarchical clustering performance on diverse image classification datasets.",
        "tldr_zh": "该论文引入了深层分类网络，用于无监督的层次原型发现，在各种图像分类数据集上的层次聚类性能优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration",
        "summary": "Recent Mamba-based image restoration methods have achieved promising results\nbut remain\n  limited by fixed scanning patterns and inefficient feature utilization.\nConventional Mamba\n  architectures rely on predetermined paths that cannot adapt to diverse\ndegradations, constraining\n  both restoration performance and computational efficiency. To overcome these\nlimitations, we\n  propose VAMamba, a Visual Adaptive Mamba framework with two key innovations.\nFirst,\n  QCLAM(Queue-basedCacheLow-rankAdaptiveMemory)enhancesfeaturelearningthrougha\n  FIFO cache that stores historical representations. Similarity between current\nLoRA-adapted and\n  cached features guides intelligent fusion, enabling dynamic reuse while\neffectively controlling\n  memorygrowth.Second, GPS-SS2D(GreedyPathScanSS2D)introducesadaptive scanning.\nA\n  Vision Transformer generates score maps to estimate pixel importance, and a\ngreedy strategy de termines optimal forward and backward scanning paths. These\nlearned trajectories replace rigid\n  patterns, enabling SS2D to perform targeted feature extraction. The\nintegration of QCLAM and\n  GPS-SS2D allows VAMamba to adaptively focus on degraded regions while\nmaintaining high\n  computational efficiency. Extensive experiments across diverse restoration\ntasks demonstrate\n  that VAMamba consistently outperforms existing approaches in both restoration\nquality and\n  efficiency, establishing new benchmarks for adaptive image restoration. Our\ncode is available\n  at https://github.com/WaterHQH/VAMamba.",
        "url": "http://arxiv.org/abs/2509.23601v1",
        "published_date": "2025-09-28T03:12:43+00:00",
        "updated_date": "2025-09-28T03:12:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Han Hu",
            "Zhuoran Zheng",
            "Liang Li",
            "Chen Lyu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces VAMamba, a Visual Adaptive Mamba framework for image restoration that outperforms existing methods in restoration quality and efficiency.",
        "tldr_zh": "该论文介绍了VAMamba，一种用于图像恢复的视觉自适应 Mamba 框架，其在恢复质量和效率方面优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VividFace: High-Quality and Efficient One-Step Diffusion For Video Face Enhancement",
        "summary": "Video Face Enhancement (VFE) seeks to reconstruct high-quality facial regions\nfrom degraded video sequences, a capability that underpins numerous\napplications including video conferencing, film restoration, and surveillance.\nDespite substantial progress in the field, current methods that primarily rely\non video super-resolution and generative frameworks continue to face three\nfundamental challenges: (1) faithfully modeling intricate facial textures while\npreserving temporal consistency; (2) restricted model generalization due to the\nlack of high-quality face video training data; and (3) low efficiency caused by\nrepeated denoising steps during inference. To address these challenges, we\npropose VividFace, a novel and efficient one-step diffusion framework for video\nface enhancement. Built upon the pretrained WANX video generation model, our\nmethod leverages powerful spatiotemporal priors through a single-step flow\nmatching paradigm, enabling direct mapping from degraded inputs to high-quality\noutputs with significantly reduced inference time. To further boost efficiency,\nwe propose a Joint Latent-Pixel Face-Focused Training strategy that employs\nstochastic switching between facial region optimization and global\nreconstruction, providing explicit supervision in both latent and pixel spaces\nthrough a progressive two-stage training process. Additionally, we introduce an\nMLLM-driven data curation pipeline for automated selection of high-quality\nvideo face datasets, enhancing model generalization. Extensive experiments\ndemonstrate that VividFace achieves state-of-the-art results in perceptual\nquality, identity preservation, and temporal stability, while offering\npractical resources for the research community.",
        "url": "http://arxiv.org/abs/2509.23584v1",
        "published_date": "2025-09-28T02:39:48+00:00",
        "updated_date": "2025-09-28T02:39:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shulian Zhang",
            "Yong Guo",
            "Long Peng",
            "Ziyang Wang",
            "Ye Chen",
            "Wenbo Li",
            "Xiao Zhang",
            "Yulun Zhang",
            "Jian Chen"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces VividFace, a one-step diffusion framework for video face enhancement that achieves state-of-the-art results in perceptual quality, identity preservation, and temporal stability.",
        "tldr_zh": "本文介绍了VividFace，这是一个用于视频人脸增强的一步扩散框架，其在感知质量、身份保留和时间稳定性方面达到了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RobuQ: Pushing DiTs to W1.58A2 via Robust Activation Quantization",
        "summary": "Diffusion Transformers (DiTs) have recently emerged as a powerful backbone\nfor image generation, demonstrating superior scalability and performance over\nU-Net architectures. However, their practical deployment is hindered by\nsubstantial computational and memory costs. While Quantization-Aware Training\n(QAT) has shown promise for U-Nets, its application to DiTs faces unique\nchallenges, primarily due to the sensitivity and distributional complexity of\nactivations. In this work, we identify activation quantization as the primary\nbottleneck for pushing DiTs to extremely low-bit settings. To address this, we\npropose a systematic QAT framework for DiTs, named RobuQ. We start by\nestablishing a strong ternary weight (W1.58A4) DiT baseline. Building upon\nthis, we propose RobustQuantizer to achieve robust activation quantization. Our\ntheoretical analyses show that the Hadamard transform can convert unknown\nper-token distributions into per-token normal distributions, providing a strong\nfoundation for this method. Furthermore, we propose AMPN, the first\nActivation-only Mixed-Precision Network pipeline for DiTs. This method applies\nternary weights across the entire network while allocating different activation\nprecisions to each layer to eliminate information bottlenecks. Through\nextensive experiments on unconditional and conditional image generation, our\nRobuQ framework achieves state-of-the-art performance for DiT quantization in\nsub-4-bit quantization configuration. To the best of our knowledge, RobuQ is\nthe first achieving stable and competitive image generation on large datasets\nlike ImageNet-1K with activations quantized to average 2 bits. The code and\nmodels will be available at https://github.com/racoonykc/RobuQ .",
        "url": "http://arxiv.org/abs/2509.23582v1",
        "published_date": "2025-09-28T02:35:12+00:00",
        "updated_date": "2025-09-28T02:35:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaicheng Yang",
            "Xun Zhang",
            "Haotong Qin",
            "Yucheng Lin",
            "Kaisen Yang",
            "Xianglong Yan",
            "Yulun Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "GAN",
            "Dataset"
        ],
        "tldr": "This paper introduces RobuQ, a framework for quantization of Diffusion Transformers (DiTs) for image generation, achieving state-of-the-art performance with sub-4-bit quantization on large datasets like ImageNet-1K.",
        "tldr_zh": "本文介绍了RobuQ，一个用于图像生成的扩散变换器（DiTs）量化框架，在大型数据集如ImageNet-1K上实现了亚4位量化的最先进性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Interpretable Visual Decoding with Attention to Brain Representations",
        "summary": "Recent work has demonstrated that complex visual stimuli can be decoded from\nhuman brain activity using deep generative models, helping brain science\nresearchers interpret how the brain represents real-world scenes. However, most\ncurrent approaches leverage mapping brain signals into intermediate image or\ntext feature spaces before guiding the generative process, masking the effect\nof contributions from different brain areas on the final reconstruction output.\nIn this work, we propose NeuroAdapter, a visual decoding framework that\ndirectly conditions a latent diffusion model on brain representations,\nbypassing the need for intermediate feature spaces. Our method demonstrates\ncompetitive visual reconstruction quality on public fMRI datasets compared to\nprior work, while providing greater transparency into how brain signals shape\nthe generation process. To this end, we contribute an Image-Brain\nBI-directional interpretability framework (IBBI) which investigates\ncross-attention mechanisms across diffusion denoising steps to reveal how\ndifferent cortical areas influence the unfolding generative trajectory. Our\nresults highlight the potential of end-to-end brain-to-image decoding and\nestablish a path toward interpreting diffusion models through the lens of\nvisual neuroscience.",
        "url": "http://arxiv.org/abs/2509.23566v1",
        "published_date": "2025-09-28T01:55:55+00:00",
        "updated_date": "2025-09-28T01:55:55+00:00",
        "categories": [
            "cs.CV",
            "I.2.0; I.4.9"
        ],
        "authors": [
            "Pinyuan Feng",
            "Hossein Adeli",
            "Wenxuan Guo",
            "Fan Cheng",
            "Ethan Hwang",
            "Nikolaus Kriegeskorte"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces NeuroAdapter, a framework for visual decoding from brain representations without intermediate feature spaces. It highlights the potential of end-to-end brain-to-image decoding for understanding diffusion models through visual neuroscience.",
        "tldr_zh": "本文介绍了NeuroAdapter，这是一个从大脑表示中解码图像的框架，不需要中间的特征空间。它突出了端到端从大脑到图像解码的潜力，用于通过视觉神经科学理解扩散模型。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From Fields to Splats: A Cross-Domain Survey of Real-Time Neural Scene Representations",
        "summary": "Neural scene representations such as Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS) have transformed how 3D environments are modeled,\nrendered, and interpreted. NeRF introduced view-consistent photorealism via\nvolumetric rendering; 3DGS has rapidly emerged as an explicit, efficient\nalternative that supports high-quality rendering, faster optimization, and\nintegration into hybrid pipelines for enhanced photorealism and task-driven\nscene understanding. This survey examines how 3DGS is being adopted across\nSLAM, telepresence and teleoperation, robotic manipulation, and 3D content\ngeneration. Despite their differences, these domains share common goals:\nphotorealistic rendering, meaningful 3D structure, and accurate downstream\ntasks. We organize the review around unified research questions that explain\nwhy 3DGS is increasingly displacing NeRF-based approaches: What technical\nadvantages drive its adoption? How does it adapt to different input modalities\nand domain-specific constraints? What limitations remain? By systematically\ncomparing domain-specific pipelines, we show that 3DGS balances photorealism,\ngeometric fidelity, and computational efficiency. The survey offers a roadmap\nfor leveraging neural rendering not only for image synthesis but also for\nperception, interaction, and content creation across real and virtual\nenvironments.",
        "url": "http://arxiv.org/abs/2509.23555v1",
        "published_date": "2025-09-28T01:30:50+00:00",
        "updated_date": "2025-09-28T01:30:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Javed Ahmad",
            "Penggang Gao",
            "Donatien Delehelle",
            "Mennuti Canio",
            "Nikhil Deshpande",
            "Jesús Ortiz",
            "Darwin G. Caldwell",
            "Yonas Teodros Tefera"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper discusses the adoption of 3D Gaussian Splatting (3DGS) in various domains for photorealistic rendering, 3D structure, and computational efficiency.",
        "tldr_zh": "本文探讨了3D高斯散射（3DGS）在各个领域中的应用，以实现逼真的渲染、3D结构和计算效率。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Imaging-Based Mortality Prediction in Patients with Systemic Sclerosis",
        "summary": "Interstitial lung disease (ILD) is a leading cause of morbidity and mortality\nin systemic sclerosis (SSc). Chest computed tomography (CT) is the primary\nimaging modality for diagnosing and monitoring lung complications in SSc\npatients. However, its role in disease progression and mortality prediction has\nnot yet been fully clarified. This study introduces a novel, large-scale\nlongitudinal chest CT analysis framework that utilizes radiomics and deep\nlearning to predict mortality associated with lung complications of SSc. We\ncollected and analyzed 2,125 CT scans from SSc patients enrolled in the\nNorthwestern Scleroderma Registry, conducting mortality analyses at one, three,\nand five years using advanced imaging analysis techniques. Death labels were\nassigned based on recorded deaths over the one-, three-, and five-year\nintervals, confirmed by expert physicians. In our dataset, 181, 326, and 428 of\nthe 2,125 CT scans were from patients who died within one, three, and five\nyears, respectively. Using ResNet-18, DenseNet-121, and Swin Transformer we use\npre-trained models, and fine-tuned on 2,125 images of SSc patients. Models\nachieved an AUC of 0.769, 0.801, 0.709 for predicting mortality within one-,\nthree-, and five-years, respectively. Our findings highlight the potential of\nboth radiomics and deep learning computational methods to improve early\ndetection and risk assessment of SSc-related interstitial lung disease, marking\na significant advancement in the literature.",
        "url": "http://arxiv.org/abs/2509.23530v1",
        "published_date": "2025-09-27T23:46:57+00:00",
        "updated_date": "2025-09-27T23:46:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Alec K. Peltekian",
            "Karolina Senkow",
            "Gorkem Durak",
            "Kevin M. Grudzinski",
            "Bradford C. Bemiss",
            "Jane E. Dematte",
            "Carrie Richardson",
            "Nikolay S. Markov",
            "Mary Carns",
            "Kathleen Aren",
            "Alexandra Soriano",
            "Matthew Dapas",
            "Harris Perlman",
            "Aaron Gundersheimer",
            "Kavitha C. Selvan",
            "John Varga",
            "Monique Hinchcliff",
            "Krishnan Warrior",
            "Catherine A. Gao",
            "Richard G. Wunderink",
            "GR Scott Budinger",
            "Alok N. Choudhary",
            "Anthony J. Esposito",
            "Alexander V. Misharin",
            "Ankit Agrawal",
            "Ulas Bagci"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel imaging analysis framework using radiomics and deep learning to predict mortality in systemic sclerosis patients with lung complications.",
        "tldr_zh": "该论文引入了一种新颖的影像分析框架，利用放射组学和深度学习预测系统性硬化症患者肺部并发症的死亡率。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multi-modal Data Spectrum: Multi-modal Datasets are Multi-dimensional",
        "summary": "Understanding the interplay between intra-modality dependencies (the\ncontribution of an individual modality to a target task) and inter-modality\ndependencies (the relationships between modalities and the target task) is\nfundamental to advancing multi-modal learning. However, the nature of and\ninteraction between these dependencies within current benchmark evaluations\nremains poorly characterized. In this work, we present a large-scale empirical\nstudy to quantify these dependencies across 23 visual question-answering\nbenchmarks using multi-modal large language models (MLLMs) covering domains\nsuch as general and expert knowledge reasoning, optical character recognition,\nand document understanding. Our findings show that the reliance on vision,\nquestion (text), and their interaction varies significantly, both across and\nwithin benchmarks. We discover that numerous benchmarks intended to mitigate\ntext-only biases have inadvertently amplified image-only dependencies. This\ncharacterization persists across model sizes, as larger models often use these\nintra-modality dependencies to achieve high performance that mask an underlying\nlack of multi-modal reasoning. We provide a quantitative characterization of\nmulti-modal datasets, enabling a principled approach to multi-modal benchmark\ndesign and evaluation.",
        "url": "http://arxiv.org/abs/2509.23499v1",
        "published_date": "2025-09-27T21:13:29+00:00",
        "updated_date": "2025-09-27T21:13:29+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Divyam Madaan",
            "Varshan Muhunthan",
            "Kyunghyun Cho",
            "Sumit Chopra"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper investigates dependencies within multi-modal datasets and their impact on benchmark evaluations, highlighting how certain biases can be inadvertently amplified. It provides insights for improved multi-modal benchmark design and evaluation.",
        "tldr_zh": "本文研究了多模态数据集内部的依赖关系及其对基准评估的影响，强调了某些偏见可能会被无意中放大。为改善多模态基准设计和评估提供了见解。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Orientation-anchored Hyper-Gaussian for 4D Reconstruction from Casual Videos",
        "summary": "We present Orientation-anchored Gaussian Splatting (OriGS), a novel framework\nfor high-quality 4D reconstruction from casually captured monocular videos.\nWhile recent advances extend 3D Gaussian Splatting to dynamic scenes via\nvarious motion anchors, such as graph nodes or spline control points, they\noften rely on low-rank assumptions and fall short in modeling complex,\nregion-specific deformations inherent to unconstrained dynamics. OriGS\naddresses this by introducing a hyperdimensional representation grounded in\nscene orientation. We first estimate a Global Orientation Field that propagates\nprincipal forward directions across space and time, serving as stable\nstructural guidance for dynamic modeling. Built upon this, we propose\nOrientation-aware Hyper-Gaussian, a unified formulation that embeds time,\nspace, geometry, and orientation into a coherent probabilistic state. This\nenables inferring region-specific deformation through principled conditioned\nslicing, adaptively capturing diverse local dynamics in alignment with global\nmotion intent. Experiments demonstrate the superior reconstruction fidelity of\nOriGS over mainstream methods in challenging real-world dynamic scenes.",
        "url": "http://arxiv.org/abs/2509.23492v1",
        "published_date": "2025-09-27T20:43:43+00:00",
        "updated_date": "2025-09-27T20:43:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junyi Wu",
            "Jiachen Tao",
            "Haoxuan Wang",
            "Gaowen Liu",
            "Ramana Rao Kompella",
            "Yan Yan"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel framework called OriGS for high-quality 4D reconstruction from monocular videos by incorporating scene orientation to model complex deformations. OriGS outperforms mainstream methods in challenging dynamic scenes.",
        "tldr_zh": "该论文提出了一种名为OriGS的新框架，通过引入场景方向来从单目视频中进行高质量的4D重建，以模拟复杂的变形。在挑战性动态场景中，OriGS优于主流方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RestoRect: Degraded Image Restoration via Latent Rectified Flow & Feature Distillation",
        "summary": "Current approaches for restoration of degraded images face a critical\ntrade-off: high-performance models are too slow for practical use, while fast\nmodels produce poor results. Knowledge distillation transfers teacher knowledge\nto students, but existing static feature matching methods cannot capture how\nmodern transformer architectures dynamically generate features. We propose\n'RestoRect', a novel Latent Rectified Flow Feature Distillation method for\nrestoring degraded images. We apply rectified flow to reformulate feature\ndistillation as a generative process where students learn to synthesize\nteacher-quality features through learnable trajectories in latent space. Our\nframework combines Retinex theory for physics-based decomposition with\nlearnable anisotropic diffusion constraints, and trigonometric color space\npolarization. We introduce a Feature Layer Extraction loss for robust knowledge\ntransfer between different network architectures through cross-normalized\ntransformer feature alignment with percentile-based outlier detection.\nRestoRect achieves better training stability, and faster convergence and\ninference while preserving restoration quality. We demonstrate superior results\nacross 15 image restoration datasets, covering 4 tasks, on 8 metrics.",
        "url": "http://arxiv.org/abs/2509.23480v1",
        "published_date": "2025-09-27T20:04:41+00:00",
        "updated_date": "2025-09-27T20:04:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shourya Verma",
            "Mengbo Wang",
            "Nadia Atallah Lanman",
            "Ananth Grama"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "RestoRect proposes a novel method for restoring degraded images using Latent Rectified Flow Feature Distillation, achieving better training stability, faster convergence and inference, while preserving restoration quality.",
        "tldr_zh": "RestoRect提出了一种新的方法，使用Latent Rectified Flow Feature Distillation来恢复退化的图像，实现更好的训练稳定性、更快的收敛和推断，并保持恢复质量。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Robust Multi-Modal Face Anti-Spoofing with Domain Adaptation: Tackling Missing Modalities, Noisy Pseudo-Labels, and Model Degradation",
        "summary": "Recent multi-modal face anti-spoofing (FAS) methods have investigated the\npotential of leveraging multiple modalities to distinguish live and spoof\nfaces. However, pre-adapted multi-modal FAS models often fail to detect unseen\nattacks from new target domains. Although a more realistic domain adaptation\n(DA) scenario has been proposed for single-modal FAS to learn specific spoof\nattacks during inference, DA remains unexplored in multi-modal FAS methods. In\nthis paper, we propose a novel framework, MFAS-DANet, to address three major\nchallenges in multi-modal FAS under the DA scenario: missing modalities, noisy\npseudo labels, and model degradation. First, to tackle the issue of missing\nmodalities, we propose extracting complementary features from other modalities\nto substitute missing modality features or enhance existing ones. Next, to\nreduce the impact of noisy pseudo labels during model adaptation, we propose\nderiving reliable pseudo labels by leveraging prediction uncertainty across\ndifferent modalities. Finally, to prevent model degradation, we design an\nadaptive mechanism that decreases the loss weight during unstable adaptations\nand increasing it during stable ones. Extensive experiments demonstrate the\neffectiveness and state-of-the-art performance of our proposed MFAS-DANet.",
        "url": "http://arxiv.org/abs/2509.23475v1",
        "published_date": "2025-09-27T19:52:31+00:00",
        "updated_date": "2025-09-27T19:52:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ming-Tsung Hsu",
            "Fang-Yu Hsu",
            "Yi-Ting Lin",
            "Kai-Heng Chien",
            "Jun-Ren Chen",
            "Cheng-Hsiang Su",
            "Yi-Chen Ou",
            "Chiou-Ting Hsu",
            "Pei-Kai Huang"
        ],
        "ai_categories": [
            "Multimodality"
        ],
        "tldr": "The paper introduces a novel framework, MFAS-DANet, to address challenges in multi-modal face anti-spoofing with domain adaptation.",
        "tldr_zh": "该论文引入了一个新的框架MFAS-DANet，以应对多模态人脸反欺诈中的挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FoR-SALE: Frame of Reference-guided Spatial Adjustment in LLM-based Diffusion Editing",
        "summary": "Frame of Reference (FoR) is a fundamental concept in spatial reasoning that\nhumans utilize to comprehend and describe space. With the rapid progress in\nMultimodal Language models, the moment has come to integrate this\nlong-overlooked dimension into these models. In particular, in text-to-image\n(T2I) generation, even state-of-the-art models exhibit a significant\nperformance gap when spatial descriptions are provided from perspectives other\nthan the camera. To address this limitation, we propose Frame of\nReference-guided Spatial Adjustment in LLM-based Diffusion Editing (FoR-SALE),\nan extension of the Self-correcting LLM-controlled Diffusion (SLD) framework\nfor T2I. For-Sale evaluates the alignment between a given text and an initially\ngenerated image, and refines the image based on the Frame of Reference\nspecified in the spatial expressions. It employs vision modules to extract the\nspatial configuration of the image, while simultaneously mapping the spatial\nexpression to a corresponding camera perspective. This unified perspective\nenables direct evaluation of alignment between language and vision. When\nmisalignment is detected, the required editing operations are generated and\napplied. FoR-SALE applies novel latent-space operations to adjust the facing\ndirection and depth of the generated images. We evaluate FoR-SALE on two\nbenchmarks specifically designed to assess spatial understanding with FoR. Our\nframework improves the performance of state-of-the-art T2I models by up to 5.3%\nusing only a single round of correction.",
        "url": "http://arxiv.org/abs/2509.23452v1",
        "published_date": "2025-09-27T18:42:04+00:00",
        "updated_date": "2025-09-27T18:42:04+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Tanawan Premsri",
            "Parisa Kordjamshidi"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Diffusion"
        ],
        "tldr": "FoR-SALE proposes a new method for adjusting spatial expressions in text-to-image generation models to improve alignment between text and images, achieving up to 5.3% performance improvement.",
        "tldr_zh": "FoR-SALE提出了一种新的方法，用于调整文本到图像生成模型中的空间表达，以改善文本和图像之间的对齐，实现了高达5.3%的性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "S$^3$F-Net: A Multi-Modal Approach to Medical Image Classification via Spatial-Spectral Summarizer Fusion Network",
        "summary": "Convolutional Neural Networks have become a cornerstone of medical image\nanalysis due to their proficiency in learning hierarchical spatial features.\nHowever, this focus on a single domain is inefficient at capturing global,\nholistic patterns and fails to explicitly model an image's frequency-domain\ncharacteristics. To address these challenges, we propose the Spatial-Spectral\nSummarizer Fusion Network (S$^3$F-Net), a dual-branch framework that learns\nfrom both spatial and spectral representations simultaneously. The S$^3$F-Net\nperforms a fusion of a deep spatial CNN with our proposed shallow spectral\nencoder, SpectraNet. SpectraNet features the proposed SpectralFilter layer,\nwhich leverages the Convolution Theorem by applying a bank of learnable filters\ndirectly to an image's full Fourier spectrum via a computation-efficient\nelement-wise multiplication. This allows the SpectralFilter layer to attain a\nglobal receptive field instantaneously, with its output being distilled by a\nlightweight summarizer network. We evaluate S$^3$F-Net across four medical\nimaging datasets spanning different modalities to validate its efficacy and\ngeneralizability. Our framework consistently and significantly outperforms its\nstrong spatial-only baseline in all cases, with accuracy improvements of up to\n5.13%. With a powerful Bilinear Fusion, S$^3$F-Net achieves a SOTA competitive\naccuracy of 98.76% on the BRISC2025 dataset. Concatenation Fusion performs\nbetter on the texture-dominant Chest X-Ray Pneumonia dataset, achieving 93.11%\naccuracy, surpassing many top-performing, much deeper models. Our\nexplainability analysis also reveals that the S$^3$F-Net learns to dynamically\nadjust its reliance on each branch based on the input pathology. These results\nverify that our dual-domain approach is a powerful and generalizable paradigm\nfor medical image analysis.",
        "url": "http://arxiv.org/abs/2509.23442v1",
        "published_date": "2025-09-27T18:18:39+00:00",
        "updated_date": "2025-09-27T18:18:39+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "eess.SP"
        ],
        "authors": [
            "Md. Saiful Bari Siddiqui",
            "Mohammed Imamul Hassan Bhuiyan"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces S$^3$F-Net, a dual-branch framework for medical image classification that combines spatial and spectral representations, outperforming spatial-only models on various datasets.",
        "tldr_zh": "该论文介绍了 S$^3$F-Net，一种结合空间和频谱表示的医学图像分类双分支框架，在各种数据集上优于仅空间模型。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "FM-SIREN & FM-FINER: Nyquist-Informed Frequency Multiplier for Implicit Neural Representation with Periodic Activation",
        "summary": "Existing periodic activation-based implicit neural representation (INR)\nnetworks, such as SIREN and FINER, suffer from hidden feature redundancy, where\nneurons within a layer capture overlapping frequency components due to the use\nof a fixed frequency multiplier. This redundancy limits the expressive capacity\nof multilayer perceptrons (MLPs). Drawing inspiration from classical signal\nprocessing methods such as the Discrete Sine Transform (DST), we propose\nFM-SIREN and FM-FINER, which assign Nyquist-informed, neuron-specific frequency\nmultipliers to periodic activations. Unlike existing approaches, our design\nintroduces frequency diversity without requiring hyperparameter tuning or\nadditional network depth. This simple yet principled modification reduces the\nredundancy of features by nearly 50% and consistently improves signal\nreconstruction across diverse INR tasks, including fitting 1D audio, 2D image\nand 3D shape, and synthesis of neural radiance fields (NeRF), outperforming\ntheir baseline counterparts while maintaining efficiency.",
        "url": "http://arxiv.org/abs/2509.23438v1",
        "published_date": "2025-09-27T18:14:47+00:00",
        "updated_date": "2025-09-27T18:14:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mohammed Alsakabi",
            "Wael Mobeirek",
            "John M. Dolan",
            "Ozan K. Tonguz"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Multimodality"
        ],
        "tldr": "FM-SIREN & FM-FINER propose Nyquist-informed frequency multipliers for implicit neural representations to reduce feature redundancy and improve signal reconstruction in various tasks.",
        "tldr_zh": "FM-SIREN和FM-FINER提出了尼奎斯特信息频率倍增器，以减少特征冗余，并在各种任务中改善信号重建。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A University of Texas Medical Branch Case Study on Aortic Calcification Detection",
        "summary": "This case study details The University of Texas Medical Branch (UTMB)'s\npartnership with Zauron Labs, Inc. to enhance detection and coding of aortic\ncalcifications (ACs) using chest radiographs. ACs are often underreported\ndespite their significant prognostic value for cardiovascular disease, and UTMB\npartnered with Zauron to apply its advanced AI tools, including a\nhigh-performing image model (AUC = 0.938) and a fine-tuned language model based\non Meta's Llama 3.2, to retrospectively analyze imaging and report data. The\neffort identified 495 patients out of 3,988 unique patients assessed (5,000\ntotal exams) whose reports contained indications of aortic calcifications that\nwere not properly coded for reimbursement (12.4% miscode rate) as well as an\nadditional 84 patients who had aortic calcifications that were missed during\ninitial review (2.1% misdiagnosis rate). Identification of these patients\nprovided UTMB with the potential to impact clinical care for these patients and\npursue $314k in missed annual revenue. These findings informed UTMB's decision\nto adopt Zauron's Guardian Pro software system-wide to ensure accurate,\nAI-enhanced peer review and coding, improving both patient care and financial\nsolvency. This study is covered under University of Texas Health San Antonio's\nInstitutional Review Board Study ID 00001887.",
        "url": "http://arxiv.org/abs/2509.23930v1",
        "published_date": "2025-09-28T15:08:53+00:00",
        "updated_date": "2025-09-28T15:08:53+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "92C55"
        ],
        "authors": [
            "Eric Walser",
            "Peter McCaffrey",
            "Kal Clark",
            "Nicholas Czarnek"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "This case study illustrates the use of AI tools to improve aortic calcification detection in medical imaging, leading to better patient care and increased revenue for the University of Texas Medical Branch.",
        "tldr_zh": "这项案例研究展示了利用人工智能工具改善医学影像中主动脉钙化检测的方法，为德克萨斯大学医学分支机构提供了更好的患者护理和增加收入的机会。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7.5
    },
    {
        "title": "From Unstable to Playable: Stabilizing Angry Birds Levels via Object Segmentation",
        "summary": "Procedural Content Generation (PCG) techniques enable automatic creation of\ndiverse and complex environments. While PCG facilitates more efficient content\ncreation, ensuring consistently high-quality, industry-standard content remains\na significant challenge. In this research, we propose a method to identify and\nrepair unstable levels generated by existing PCG models. We use Angry Birds as\na case study, demonstrating our method on game levels produced by established\nPCG approaches. Our method leverages object segmentation and visual analysis of\nlevel images to detect structural gaps and perform targeted repairs. We\nevaluate multiple object segmentation models and select the most effective one\nas the basis for our repair pipeline. Experimental results show that our method\nimproves the stability and playability of AI-generated levels. Although our\nevaluation is specific to Angry Birds, our image-based approach is designed to\nbe applicable to a wide range of 2D games with similar level structures.",
        "url": "http://arxiv.org/abs/2509.23787v1",
        "published_date": "2025-09-28T10:15:19+00:00",
        "updated_date": "2025-09-28T10:15:19+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mahdi Farrokhimaleki",
            "Parsa Rahmati",
            "Richard Zhao"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper proposes a method to stabilize unstable game levels generated by AI using object segmentation. It focuses on Angry Birds but can be applied to other 2D games.",
        "tldr_zh": "该论文提出了一种通过目标分割来稳定AI生成的游戏关卡的方法。重点是愤怒的小鸟，但可以应用于其他2D游戏。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7.5
    },
    {
        "title": "Transparent Visual Reasoning via Object-Centric Agent Collaboration",
        "summary": "A central challenge in explainable AI, particularly in the visual domain, is\nproducing explanations grounded in human-understandable concepts. To tackle\nthis, we introduce OCEAN (Object-Centric Explananda via Agent Negotiation), a\nnovel, inherently interpretable framework built on object-centric\nrepresentations and a transparent multi-agent reasoning process. The\ngame-theoretic reasoning process drives agents to agree on coherent and\ndiscriminative evidence, resulting in a faithful and interpretable\ndecision-making process. We train OCEAN end-to-end and benchmark it against\nstandard visual classifiers and popular posthoc explanation tools like GradCAM\nand LIME across two diagnostic multi-object datasets. Our results demonstrate\ncompetitive performance with respect to state-of-the-art black-box models with\na faithful reasoning process, which was reflected by our user study, where\nparticipants consistently rated OCEAN's explanations as more intuitive and\ntrustworthy.",
        "url": "http://arxiv.org/abs/2509.23757v1",
        "published_date": "2025-09-28T09:06:52+00:00",
        "updated_date": "2025-09-28T09:06:52+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Benjamin Teoh",
            "Ben Glocker",
            "Francesca Toni",
            "Avinash Kori"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces OCEAN, a transparent framework for visual reasoning using object-centric representations and multi-agent collaboration, which outperforms black-box models in terms of interpretability and user trust.",
        "tldr_zh": "本文介绍了一种通过物体中心表示和多智能体协作的透明框架OCEAN，其在解释性和用户信任方面优于黑盒模型。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "PVTAdpNet: Polyp Segmentation using Pyramid vision transformer with a novel Adapter block",
        "summary": "Colorectal cancer ranks among the most common and deadly cancers, emphasizing\nthe need for effective early detection and treatment. To address the\nlimitations of traditional colonoscopy, including high miss rates due to polyp\nvariability, we introduce the Pyramid Vision Transformer Adapter Residual\nNetwork (PVTAdpNet). This model integrates a U-Net-style encoder-decoder\nstructure with a Pyramid Vision Transformer backbone, novel residual blocks,\nand adapter-based skip connections. The design enhances feature extraction,\ndense prediction, and gradient flow, supported by squeeze-and-excitation\nattention for improved channel-wise feature refinement. PVTAdpNet achieves\nreal-time, accurate polyp segmentation, demonstrating superior performance on\nbenchmark datasets with high mDice and mIoU scores, making it highly suitable\nfor clinical applications. PVTAdpNet obtains a high Dice coefficient of 0.8851\nand a mean Intersection over Union (mIoU) of 0.8167 on out-of-distribution\npolyp datasets. Evaluation of the PolypGen dataset demonstrates PVTAdpNet's\ncapability for real-time, accurate performance within familiar distributions.\nThe source code of our network is available at\nhttps://github.com/ayousefinejad/PVTAdpNet.git",
        "url": "http://arxiv.org/abs/2509.23751v1",
        "published_date": "2025-09-28T08:55:50+00:00",
        "updated_date": "2025-09-28T08:55:50+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Arshia Yousefi Nezhad",
            "Helia Aghaei",
            "Hedieh Sajedi"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "PVTAdpNet is a model for real-time, accurate polyp segmentation using a Pyramid Vision Transformer with novel adapter blocks.",
        "tldr_zh": "PVTAdpNet是使用金字塔视觉变换器和新型适配器块进行实时、准确的息肉分割的模型。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "GBSK: Skeleton Clustering via Granular-ball Computing and Multi-Sampling for Large-Scale Data",
        "summary": "To effectively handle clustering task for large-scale datasets, we propose a\nnovel scalable skeleton clustering algorithm, namely GBSK, which leverages the\ngranular-ball technique to capture the underlying structure of data. By\nmulti-sampling the dataset and constructing multi-grained granular-balls, GBSK\nprogressively uncovers a statistical \"skeleton\" -- a spatial abstraction that\napproximates the essential structure and distribution of the original data.\nThis strategy enables GBSK to dramatically reduce computational overhead while\nmaintaining high clustering accuracy. In addition, we introduce an adaptive\nversion, AGBSK, with simplified parameter settings to enhance usability and\nfacilitate deployment in real-world scenarios. Extensive experiments conducted\non standard computing hardware demonstrate that GBSK achieves high efficiency\nand strong clustering performance on large-scale datasets, including one with\nup to 100 million instances across 256 dimensions. Our implementation and\nexperimental results are available at: https://github.com/XFastDataLab/GBSK/.",
        "url": "http://arxiv.org/abs/2509.23742v1",
        "published_date": "2025-09-28T08:41:15+00:00",
        "updated_date": "2025-09-28T08:41:15+00:00",
        "categories": [
            "cs.LG",
            "cs.CV",
            "cs.IR"
        ],
        "authors": [
            "Yewang Chen",
            "Junfeng Li",
            "Shuyin Xia",
            "Qinghong Lai",
            "Xinbo Gao",
            "Guoyin Wang",
            "Dongdong Cheng",
            "Yi Liu",
            "Yi Wang"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "GBSK is a novel clustering algorithm for large-scale datasets that uses granular-ball computing and multi-sampling to create a statistical 'skeleton' of data, reducing computational overhead while maintaining accuracy.",
        "tldr_zh": "GBSK是一种针对大规模数据集的新型聚类算法，利用颗粒球计算和多采样技术创建数据的统计“骨架”，降低计算开销同时保持准确性。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "ResAD++: Towards Class Agnostic Anomaly Detection via Residual Feature Learning",
        "summary": "This paper explores the problem of class-agnostic anomaly detection (AD),\nwhere the objective is to train one class-agnostic AD model that can generalize\nto detect anomalies in diverse new classes from different domains without any\nretraining or fine-tuning on the target data. When applied for new classes, the\nperformance of current single- and multi-class AD methods is still\nunsatisfactory. One fundamental reason is that representation learning in\nexisting methods is still class-related, namely, feature correlation. To\naddress this issue, we propose residual features and construct a simple but\neffective framework, termed ResAD. Our core insight is to learn the residual\nfeature distribution rather than the initial feature distribution. Residual\nfeatures are formed by matching and then subtracting normal reference features.\nIn this way, we can effectively realize feature decorrelation. Even in new\nclasses, the distribution of normal residual features would not remarkably\nshift from the learned distribution. In addition, we think that residual\nfeatures still have one issue: scale correlation. To this end, we propose a\nfeature hypersphere constraining approach, which learns to constrain initial\nnormal residual features into a spatial hypersphere for enabling the feature\nscales of different classes as consistent as possible. Furthermore, we propose\na novel logbarrier bidirectional contraction OCC loss and vector quantization\nbased feature distribution matching module to enhance ResAD, leading to the\nimproved version of ResAD (ResAD++). Comprehensive experiments on eight\nreal-world AD datasets demonstrate that our ResAD++ can achieve remarkable AD\nresults when directly used in new classes, outperforming state-of-the-art\ncompeting methods and also surpassing ResAD. The code is available at\nhttps://github.com/xcyao00/ResAD.",
        "url": "http://arxiv.org/abs/2509.23741v1",
        "published_date": "2025-09-28T08:41:05+00:00",
        "updated_date": "2025-09-28T08:41:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xincheng Yao",
            "Chao Shi",
            "Muming Zhao",
            "Guangtao Zhai",
            "Chongyang Zhang"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel Residual Feature Learning framework, ResAD++, for class-agnostic anomaly detection, outperforming existing methods on diverse datasets.",
        "tldr_zh": "本文提出了一种新的残差特征学习框架ResAD++，用于类别无关异常检测，在多个数据集上表现优异。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "PD-Diag-Net: Clinical-Priors guided Network on Brain MRI for Auxiliary Diagnosis of Parkinson's Disease",
        "summary": "Parkinson's disease (PD) is a common neurodegenerative disorder that severely\ndiminishes patients' quality of life. Its global prevalence has increased\nmarkedly in recent decades. Current diagnostic workflows are complex and\nheavily reliant on neurologists' expertise, often resulting in delays in early\ndetection and missed opportunities for timely intervention. To address these\nissues, we propose an end-to-end automated diagnostic method for PD, termed\nPD-Diag-Net, which performs risk assessment and auxiliary diagnosis directly\nfrom raw MRI scans. This framework first introduces an MRI Pre-processing\nModule (MRI-Processor) to mitigate inter-subject and inter-scanner variability\nby flexibly integrating established medical imaging preprocessing tools. It\nthen incorporates two forms of clinical prior knowledge: (1)\nBrain-Region-Relevance-Prior (Relevance-Prior), which specifies brain regions\nstrongly associated with PD; and (2) Brain-Region-Aging-Prior (Aging-Prior),\nwhich reflects the accelerated aging typically observed in PD-associated\nregions. Building on these priors, we design two dedicated modules: the\nRelevance-Prior Guided Feature Aggregation Module (Aggregator), which guides\nthe model to focus on PD-associated regions at the inter-subject level, and the\nAge-Prior Guided Diagnosis Module (Diagnoser), which leverages brain age gaps\nas auxiliary constraints at the intra-subject level to enhance diagnostic\naccuracy and clinical interpretability. Furthermore, we collected external test\ndata from our collaborating hospital. Experimental results show that\nPD-Diag-Net achieves 86\\% accuracy on external tests and over 96% accuracy in\nearly-stage diagnosis, outperforming existing advanced methods by more than\n20%.",
        "url": "http://arxiv.org/abs/2509.23719v1",
        "published_date": "2025-09-28T08:00:03+00:00",
        "updated_date": "2025-09-28T08:00:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuai Shao",
            "Shu Jiang",
            "Shiyuan Zhao",
            "Di Yang",
            "Yan Wang",
            "Yutong Bai",
            "Jianguo Zhang",
            "Jiangtao Wang"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "PD-Diag-Net is an automated diagnostic method for Parkinson's Disease using MRI scans, achieving high accuracy in early-stage diagnosis.",
        "tldr_zh": "PD-Diag-Net是一种利用MRI扫描的帕金森氏病自动诊断方法，在早期诊断方面取得高准确度。",
        "relevance_score": 1,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7.5
    },
    {
        "title": "INSTINCT: Instance-Level Interaction Architecture for Query-Based Collaborative Perception",
        "summary": "Collaborative perception systems overcome single-vehicle limitations in\nlong-range detection and occlusion scenarios by integrating multi-agent sensory\ndata, improving accuracy and safety. However, frequent cooperative interactions\nand real-time requirements impose stringent bandwidth constraints. Previous\nworks proves that query-based instance-level interaction reduces bandwidth\ndemands and manual priors, however, LiDAR-focused implementations in\ncollaborative perception remain underdeveloped, with performance still trailing\nstate-of-the-art approaches. To bridge this gap, we propose INSTINCT\n(INSTance-level INteraCtion ArchiTecture), a novel collaborative perception\nframework featuring three core components: 1) a quality-aware filtering\nmechanism for high-quality instance feature selection; 2) a dual-branch\ndetection routing scheme to decouple collaboration-irrelevant and\ncollaboration-relevant instances; and 3) a Cross Agent Local Instance Fusion\nmodule to aggregate local hybrid instance features. Additionally, we enhance\nthe ground truth (GT) sampling technique to facilitate training with diverse\nhybrid instance features. Extensive experiments across multiple datasets\ndemonstrate that INSTINCT achieves superior performance. Specifically, our\nmethod achieves an improvement in accuracy 13.23%/33.08% in DAIR-V2X and\nV2V4Real while reducing the communication bandwidth to 1/281 and 1/264 compared\nto state-of-the-art methods. The code is available at\nhttps://github.com/CrazyShout/INSTINCT.",
        "url": "http://arxiv.org/abs/2509.23700v1",
        "published_date": "2025-09-28T07:16:32+00:00",
        "updated_date": "2025-09-28T07:16:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yunjiang Xu",
            "Lingzhi Li",
            "Jin Wang",
            "Yupeng Ouyang",
            "Benyuan Yang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "INSTINCT proposes a collaborative perception framework for LiDAR-focused implementations, achieving superior performance and reducing communication bandwidth significantly.",
        "tldr_zh": "INSTINCT提出了一种针对LiDAR的协作感知框架，性能优越且显著减少通信带宽。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "EfficientMIL: Efficient Linear-Complexity MIL Method for WSI Classification",
        "summary": "Whole slide images (WSIs) classification represents a fundamental challenge\nin computational pathology, where multiple instance learning (MIL) has emerged\nas the dominant paradigm. Current state-of-the-art (SOTA) MIL methods rely on\nattention mechanisms, achieving good performance but requiring substantial\ncomputational resources due to quadratic complexity when processing hundreds of\nthousands of patches. To address this computational bottleneck, we introduce\nEfficientMIL, a novel linear-complexity MIL approach for WSIs classification\nwith the patches selection module Adaptive Patch Selector (APS) that we\ndesigned, replacing the quadratic-complexity self-attention mechanisms in\nTransformer-based MIL methods with efficient sequence models including\nRNN-based GRU, LSTM, and State Space Model (SSM) Mamba. EfficientMIL achieves\nsignificant computational efficiency improvements while outperforming other MIL\nmethods across multiple histopathology datasets. On TCGA-Lung dataset,\nEfficientMIL-Mamba achieved AUC of 0.976 and accuracy of 0.933, while on\nCAMELYON16 dataset, EfficientMIL-GRU achieved AUC of 0.990 and accuracy of\n0.975, surpassing previous state-of-the-art methods. Extensive experiments\ndemonstrate that APS is also more effective for patches selection than\nconventional selection strategies.",
        "url": "http://arxiv.org/abs/2509.23640v1",
        "published_date": "2025-09-28T04:47:11+00:00",
        "updated_date": "2025-09-28T04:47:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chengying She",
            "Ben Wang",
            "Xinran Zhang",
            "Dongjie Fan",
            "Jialu Zhang",
            "Chengwei Chen",
            "Lizhuang Liu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "EfficientMIL introduces a linear-complexity MIL approach with a patch selection module for WSI classification, outperforming other MIL methods on histopathology datasets.",
        "tldr_zh": "EfficientMIL 提出了一种线性复杂度的MIL方法，配备了用于WSI分类的补丁选择模块，在组织病理学数据集上优于其他MIL方法。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "OVSeg3R: Learn Open-vocabulary Instance Segmentation from 2D via 3D Reconstruction",
        "summary": "In this paper, we propose a training scheme called OVSeg3R to learn\nopen-vocabulary 3D instance segmentation from well-studied 2D perception models\nwith the aid of 3D reconstruction. OVSeg3R directly adopts reconstructed scenes\nfrom 2D videos as input, avoiding costly manual adjustment while aligning input\nwith real-world applications. By exploiting the 2D to 3D correspondences\nprovided by 3D reconstruction models, OVSeg3R projects each view's 2D instance\nmask predictions, obtained from an open-vocabulary 2D model, onto 3D to\ngenerate annotations for the view's corresponding sub-scene. To avoid\nincorrectly introduced false positives as supervision due to partial\nannotations from 2D to 3D, we propose a View-wise Instance Partition algorithm,\nwhich partitions predictions to their respective views for supervision,\nstabilizing the training process. Furthermore, since 3D reconstruction models\ntend to over-smooth geometric details, clustering reconstructed points into\nrepresentative super-points based solely on geometry, as commonly done in\nmainstream 3D segmentation methods, may overlook geometrically non-salient\nobjects. We therefore introduce 2D Instance Boundary-aware Superpoint, which\nleverages 2D masks to constrain the superpoint clustering, preventing\nsuperpoints from violating instance boundaries. With these designs, OVSeg3R not\nonly extends a state-of-the-art closed-vocabulary 3D instance segmentation\nmodel to open-vocabulary, but also substantially narrows the performance gap\nbetween tail and head classes, ultimately leading to an overall improvement of\n+2.3 mAP on the ScanNet200 benchmark. Furthermore, under the standard\nopen-vocabulary setting, OVSeg3R surpasses previous methods by about +7.1 mAP\non the novel classes, further validating its effectiveness.",
        "url": "http://arxiv.org/abs/2509.23541v1",
        "published_date": "2025-09-28T00:41:22+00:00",
        "updated_date": "2025-09-28T00:41:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hongyang Li",
            "Jinyuan Qu",
            "Lei Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a training scheme called OVSeg3R to learn open-vocabulary 3D instance segmentation from 2D perception models with the aid of 3D reconstruction, achieving improved performance on benchmark datasets.",
        "tldr_zh": "本文提出了一种名为OVSeg3R的训练方案，通过3D重建帮助学习从2D感知模型到开放词汇的3D实例分割，提高了基准数据集上的性能。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7.5
    },
    {
        "title": "Evaluating point-light biological motion in multimodal large language models",
        "summary": "Humans can extract rich semantic information from minimal visual cues, as\ndemonstrated by point-light displays (PLDs), which consist of sparse sets of\ndots localized to key joints of the human body. This ability emerges early in\ndevelopment and is largely attributed to human embodied experience. Since PLDs\nisolate body motion as the sole source of meaning, they represent key stimuli\nfor testing the constraints of action understanding in these systems. Here we\nintroduce ActPLD, the first benchmark to evaluate action processing in MLLMs\nfrom human PLDs. Tested models include state-of-the-art proprietary and\nopen-source systems on single-actor and socially interacting PLDs. Our results\nreveal consistently low performance across models, introducing fundamental gaps\nin action and spatiotemporal understanding.",
        "url": "http://arxiv.org/abs/2509.23517v1",
        "published_date": "2025-09-27T22:33:05+00:00",
        "updated_date": "2025-09-27T22:33:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Akila Kadambi",
            "Marco Iacoboni",
            "Lisa Aziz-Zadeh",
            "Srini Narayanan"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces ActPLD, a benchmark to evaluate action processing in large language models using point-light displays (PLDs). Results suggest low performance across models, highlighting gaps in action and spatiotemporal understanding.",
        "tldr_zh": "本文介绍了ActPLD，这是一个用于评估大型语言模型中动作处理的基准，使用点光显示（PLDs）。结果显示不同模型表现低，突显了在动作和时空理解方面存在的差距。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7.5
    },
    {
        "title": "3DPCNet: Pose Canonicalization for Robust Viewpoint-Invariant 3D Kinematic Analysis from Monocular RGB cameras",
        "summary": "Monocular 3D pose estimators produce camera-centered skeletons, creating\nview-dependent kinematic signals that complicate comparative analysis in\napplications such as health and sports science. We present 3DPCNet, a compact,\nestimator-agnostic module that operates directly on 3D joint coordinates to\nrectify any input pose into a consistent, body-centered canonical frame. Its\nhybrid encoder fuses local skeletal features from a graph convolutional network\nwith global context from a transformer via a gated cross-attention mechanism.\nFrom this representation, the model predicts a continuous 6D rotation that is\nmapped to an $SO(3)$ matrix to align the pose. We train the model in a\nself-supervised manner on the MM-Fi dataset using synthetically rotated poses,\nguided by a composite loss ensuring both accurate rotation and pose\nreconstruction. On the MM-Fi benchmark, 3DPCNet reduces the mean rotation error\nfrom over 20$^{\\circ}$ to 3.4$^{\\circ}$ and the Mean Per Joint Position Error\nfrom ~64 mm to 47 mm compared to a geometric baseline. Qualitative evaluations\non the TotalCapture dataset further demonstrate that our method produces\nacceleration signals from video that show strong visual correspondence to\nground-truth IMU sensor data, confirming that our module removes viewpoint\nvariability to enable physically plausible motion analysis.",
        "url": "http://arxiv.org/abs/2509.23455v1",
        "published_date": "2025-09-27T18:55:21+00:00",
        "updated_date": "2025-09-27T18:55:21+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Tharindu Ekanayake",
            "Constantino Álvarez Casado",
            "Miguel Bordallo López"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "3DPCNet is a module that rectifies pose into a consistent frame for 3D kinematic analysis, reducing rotation and joint position errors significantly compared to a baseline.",
        "tldr_zh": "3DPCNet是一个模块，将姿势矫正到一个一致的框架中，用于3D运动学分析，在减少旋转和关节位置误差方面与基线相比有显著改善。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "Learning Adaptive Pseudo-Label Selection for Semi-Supervised 3D Object Detection",
        "summary": "Semi-supervised 3D object detection (SS3DOD) aims to reduce costly 3D\nannotations utilizing unlabeled data. Recent studies adopt pseudo-label-based\nteacher-student frameworks and demonstrate impressive performance. The main\nchallenge of these frameworks is in selecting high-quality pseudo-labels from\nthe teacher's predictions. Most previous methods, however, select pseudo-labels\nby comparing confidence scores over thresholds manually set. The latest works\ntackle the challenge either by dynamic thresholding or refining the quality of\npseudo-labels. Such methods still overlook contextual information e.g. object\ndistances, classes, and learning states, and inadequately assess the\npseudo-label quality using partial information available from the networks. In\nthis work, we propose a novel SS3DOD framework featuring a learnable\npseudo-labeling module designed to automatically and adaptively select\nhigh-quality pseudo-labels. Our approach introduces two networks at the teacher\noutput level. These networks reliably assess the quality of pseudo-labels by\nthe score fusion and determine context-adaptive thresholds, which are\nsupervised by the alignment of pseudo-labels over GT bounding boxes.\nAdditionally, we introduce a soft supervision strategy that can learn robustly\nunder pseudo-label noises. This helps the student network prioritize cleaner\nlabels over noisy ones in semi-supervised learning. Extensive experiments on\nthe KITTI and Waymo datasets demonstrate the effectiveness of our method. The\nproposed method selects high-precision pseudo-labels while maintaining a wider\ncoverage of contexts and a higher recall rate, significantly improving relevant\nSS3DOD methods.",
        "url": "http://arxiv.org/abs/2509.23880v1",
        "published_date": "2025-09-28T13:40:48+00:00",
        "updated_date": "2025-09-28T13:40:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Taehun Kong",
            "Tae-Kyun Kim"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel framework for semi-supervised 3D object detection that automatically selects high-quality pseudo-labels, improving performance significantly.",
        "tldr_zh": "本文引入一种新颖的框架，用于半监督三维物体检测，自动选择高质量伪标签，显著提高性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Assessing Visual Privacy Risks in Multimodal AI: A Novel Taxonomy-Grounded Evaluation of Vision-Language Models",
        "summary": "Artificial Intelligence have profoundly transformed the technological\nlandscape in recent years. Large Language Models (LLMs) have demonstrated\nimpressive abilities in reasoning, text comprehension, contextual pattern\nrecognition, and integrating language with visual understanding. While these\nadvances offer significant benefits, they also reveal critical limitations in\nthe models' ability to grasp the notion of privacy. There is hence substantial\ninterest in determining if and how these models can understand and enforce\nprivacy principles, particularly given the lack of supporting resources to test\nsuch a task. In this work, we address these challenges by examining how legal\nframeworks can inform the capabilities of these emerging technologies. To this\nend, we introduce a comprehensive, multi-level Visual Privacy Taxonomy that\ncaptures a wide range of privacy issues, designed to be scalable and adaptable\nto existing and future research needs. Furthermore, we evaluate the\ncapabilities of several state-of-the-art Vision-Language Models (VLMs),\nrevealing significant inconsistencies in their understanding of contextual\nprivacy. Our work contributes both a foundational taxonomy for future research\nand a critical benchmark of current model limitations, demonstrating the urgent\nneed for more robust, privacy-aware AI systems.",
        "url": "http://arxiv.org/abs/2509.23827v1",
        "published_date": "2025-09-28T12:04:54+00:00",
        "updated_date": "2025-09-28T12:04:54+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Efthymios Tsaprazlis",
            "Tiantian Feng",
            "Anil Ramakrishna",
            "Rahul Gupta",
            "Shrikanth Narayanan"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a visual privacy taxonomy to evaluate vision-language models in AI, highlighting their limitations in understanding privacy principles.",
        "tldr_zh": "本文引入了一种视觉隐私分类法，用于评估AI中的视觉-语言模型，在理解隐私原则方面存在一些限制。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Controllable Generation of Large-Scale 3D Urban Layouts with Semantic and Structural Guidance",
        "summary": "Urban modeling is essential for city planning, scene synthesis, and gaming.\nExisting image-based methods generate diverse layouts but often lack geometric\ncontinuity and scalability, while graph-based methods capture structural\nrelations yet overlook parcel semantics. We present a controllable framework\nfor large-scale 3D vector urban layout generation, conditioned on both geometry\nand semantics. By fusing geometric and semantic attributes, introducing edge\nweights, and embedding building height in the graph, our method extends 2D\nlayouts to realistic 3D structures. It also enables users to directly control\nthe output by modifying semantic attributes. Experiments show that it produces\nvalid, large-scale urban models, offering an effective tool for data-driven\nplanning and design.",
        "url": "http://arxiv.org/abs/2509.23804v1",
        "published_date": "2025-09-28T11:08:17+00:00",
        "updated_date": "2025-09-28T11:08:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mengyuan Niu",
            "Xinxin Zhuo",
            "Ruizhe Wang",
            "Yuyue Huang",
            "Junyan Yang",
            "Qiao Wang"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper presents a controllable framework for generating large-scale 3D urban layouts with both geometric and semantic guidance, allowing users to modify the output by changing semantic attributes. It offers an effective tool for data-driven planning and design.",
        "tldr_zh": "本文提出了一种可控制的框架，用于生成具有几何和语义指导的大规模3D城市布局，用户可以通过更改语义属性来修改输出。它为数据驱动的规划和设计提供了有效的工具。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "DFG-PCN: Point Cloud Completion with Degree-Flexible Point Graph",
        "summary": "Point cloud completion is a vital task focused on reconstructing complete\npoint clouds and addressing the incompleteness caused by occlusion and limited\nsensor resolution. Traditional methods relying on fixed local region\npartitioning, such as k-nearest neighbors, which fail to account for the highly\nuneven distribution of geometric complexity across different regions of a\nshape. This limitation leads to inefficient representation and suboptimal\nreconstruction, especially in areas with fine-grained details or structural\ndiscontinuities. This paper proposes a point cloud completion framework called\nDegree-Flexible Point Graph Completion Network (DFG-PCN). It adaptively assigns\nnode degrees using a detail-aware metric that combines feature variation and\ncurvature, focusing on structurally important regions. We further introduce a\ngeometry-aware graph integration module that uses Manhattan distance for edge\naggregation and detail-guided fusion of local and global features to enhance\nrepresentation. Extensive experiments on multiple benchmark datasets\ndemonstrate that our method consistently outperforms state-of-the-art\napproaches.",
        "url": "http://arxiv.org/abs/2509.23703v1",
        "published_date": "2025-09-28T07:28:42+00:00",
        "updated_date": "2025-09-28T07:28:42+00:00",
        "categories": [
            "cs.GR",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Zhenyu Shu",
            "Jian Yao",
            "Shiqing Xin"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a point cloud completion framework that addresses the limitations of traditional methods by adaptively assigning node degrees and utilizing a geometry-aware graph integration module to enhance representation.",
        "tldr_zh": "本文提出了一种点云完成框架，通过自适应分配节点度和利用几何感知图集成模块来增强表示，解决了传统方法的局限性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 7.25
    },
    {
        "title": "HomeSafeBench: A Benchmark for Embodied Vision-Language Models in Free-Exploration Home Safety Inspection",
        "summary": "Embodied agents can identify and report safety hazards in the home\nenvironments. Accurately evaluating their capabilities in home safety\ninspection tasks is curcial, but existing benchmarks suffer from two key\nlimitations. First, they oversimplify safety inspection tasks by using textual\ndescriptions of the environment instead of direct visual information, which\nhinders the accurate evaluation of embodied agents based on Vision-Language\nModels (VLMs). Second, they use a single, static viewpoint for environmental\nobservation, which restricts the agents' free exploration and cause the\nomission of certain safety hazards, especially those that are occluded from a\nfixed viewpoint. To alleviate these issues, we propose HomeSafeBench, a\nbenchmark with 12,900 data points covering five common home safety hazards:\nfire, electric shock, falling object, trips, and child safety. HomeSafeBench\nprovides dynamic first-person perspective images from simulated home\nenvironments, enabling the evaluation of VLM capabilities for home safety\ninspection. By allowing the embodied agents to freely explore the room,\nHomeSafeBench provides multiple dynamic perspectives in complex environments\nfor a more thorough inspection. Our comprehensive evaluation of mainstream VLMs\non HomeSafeBench reveals that even the best-performing model achieves an\nF1-score of only 10.23%, demonstrating significant limitations in current VLMs.\nThe models particularly struggle with identifying safety hazards and selecting\neffective exploration strategies. We hope HomeSafeBench will provide valuable\nreference and support for future research related to home security inspections.\nOur dataset and code will be publicly available soon.",
        "url": "http://arxiv.org/abs/2509.23690v1",
        "published_date": "2025-09-28T07:01:27+00:00",
        "updated_date": "2025-09-28T07:01:27+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Siyuan Gao",
            "Jiashu Yao",
            "Haoyu Wen",
            "Yuhang Guo",
            "Zeming Liu",
            "Heyan Huang"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "HomeSafeBench is a benchmark for evaluating embodied agents in home safety inspection using visual information, revealing limitations in current Vision-Language Models.",
        "tldr_zh": "HomeSafeBench 是一个用于评估家庭安全检查中的具身代理的基准，使用视觉信息揭示了当前视觉-语言模型的局限性。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7.25
    },
    {
        "title": "DiffInk: Glyph- and Style-Aware Latent Diffusion Transformer for Text to Online Handwriting Generation",
        "summary": "Deep generative models have advanced text-to-online handwriting generation\n(TOHG), which aims to synthesize realistic pen trajectories conditioned on\ntextual input and style references. However, most existing methods still\nprimarily focus on character- or word-level generation, resulting in\ninefficiency and a lack of holistic structural modeling when applied to full\ntext lines. To address these issues, we propose DiffInk, the first latent\ndiffusion Transformer framework for full-line handwriting generation. We first\nintroduce InkVAE, a novel sequential variational autoencoder enhanced with two\ncomplementary latent-space regularization losses: (1) an OCR-based loss\nenforcing glyph-level accuracy, and (2) a style-classification loss preserving\nwriting style. This dual regularization yields a semantically structured latent\nspace where character content and writer styles are effectively disentangled.\nWe then introduce InkDiT, a novel latent diffusion Transformer that integrates\ntarget text and reference styles to generate coherent pen trajectories.\nExperimental results demonstrate that DiffInk outperforms existing\nstate-of-the-art methods in both glyph accuracy and style fidelity, while\nsignificantly improving generation efficiency. Code will be made publicly\navailable.",
        "url": "http://arxiv.org/abs/2509.23624v1",
        "published_date": "2025-09-28T03:58:15+00:00",
        "updated_date": "2025-09-28T03:58:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wei Pan",
            "Huiguo He",
            "Hiuyi Cheng",
            "Yilin Shi",
            "Lianwen Jin"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "GAN"
        ],
        "tldr": "DiffInk proposes a new framework for full-line handwriting generation by disentangling character content and writer styles in a latent space using a latent diffusion Transformer.",
        "tldr_zh": "DiffInk提出了一个新的框架，通过在潜在空间中解开字符内容和作者风格，使用latent diffusion Transformer实现完整行手写生成。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Multi-Level Heterogeneous Knowledge Transfer Network on Forward Scattering Center Model for Limited Samples SAR ATR",
        "summary": "Simulated data-assisted SAR target recognition methods are the research\nhotspot currently, devoted to solving the problem of limited samples. Existing\nworks revolve around simulated images, but the large amount of irrelevant\ninformation embedded in the images, such as background, noise, etc., seriously\naffects the quality of the migrated information. Our work explores a new\nsimulated data to migrate purer and key target knowledge, i.e., forward\nscattering center model (FSCM) which models the actual local structure of the\ntarget with strong physical meaning and interpretability. To achieve this\npurpose, multi-level heterogeneous knowledge transfer (MHKT) network is\nproposed, which fully migrates FSCM knowledge from the feature, distribution\nand category levels, respectively. Specifically, we permit the more suitable\nfeature representations for the heterogeneous data and separate non-informative\nknowledge by task-associated information selector (TAIS), to complete purer\ntarget feature migration. In the distribution alignment, the new metric\nfunction maximum discrimination divergence (MDD) in target generic knowledge\ntransfer (TGKT) module perceives transferable knowledge efficiently while\npreserving discriminative structure about classes. Moreover, category relation\nknowledge transfer (CRKT) module leverages the category relation consistency\nconstraint to break the dilemma of optimization bias towards simulation data\ndue to imbalance between simulated and measured data. Such stepwise knowledge\nselection and migration will ensure the integrity of the migrated FSCM\nknowledge. Notably, extensive experiments on two new datasets formed by FSCM\ndata and measured SAR images demonstrate the superior performance of our\nmethod.",
        "url": "http://arxiv.org/abs/2509.23596v1",
        "published_date": "2025-09-28T03:04:04+00:00",
        "updated_date": "2025-09-28T03:04:04+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Chenxi Zhao",
            "Daochang Wang",
            "Siqian Zhang",
            "Gangyao Kuang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a new method using a Multi-Level Heterogeneous Knowledge Transfer Network to extract key target information for SAR target recognition from limited samples.",
        "tldr_zh": "本文提出了一种新方法，使用多层异构知识传输网络，从有限样本中提取 SAR 目标识别的关键信息。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Enhancing Polyp Segmentation via Encoder Attention and Dynamic Kernel Update",
        "summary": "Polyp segmentation is a critical step in colorectal cancer detection, yet it\nremains challenging due to the diverse shapes, sizes, and low contrast\nboundaries of polyps in medical imaging. In this work, we propose a novel\nframework that improves segmentation accuracy and efficiency by integrating a\nDynamic Kernel (DK) mechanism with a global Encoder Attention module. The DK\nmechanism, initialized by a global context vector from the EA module,\niteratively refines segmentation predictions across decoding stages, enabling\nthe model to focus on and accurately delineate complex polyp boundaries. The EA\nmodule enhances the network's ability to capture critical lesion features by\naggregating multi scale information from all encoder layers. In addition, we\nemploy Unified Channel Adaptation (UCA) in the decoder to standardize feature\ndimensions across stages, ensuring consistent and computationally efficient\ninformation fusion. Our approach extends the lesion-aware kernel framework by\nintroducing a more flexible, attention driven kernel initialization and a\nunified decoder design. Extensive experiments on the KvasirSEG and CVC ClinicDB\nbenchmark datasets demonstrate that our model outperforms several state of the\nart segmentation methods, achieving superior Dice and Intersection over Union\nscores. Moreover, UCA simplifies the decoder structure, reducing computational\ncost without compromising accuracy. Overall, the proposed method provides a\nrobust and adaptable solution for polyp segmentation, with promising\napplications in clinical and automated diagnostic systems.",
        "url": "http://arxiv.org/abs/2509.23502v1",
        "published_date": "2025-09-27T21:16:09+00:00",
        "updated_date": "2025-09-27T21:16:09+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Fatemeh Salahi Chashmi",
            "Roya Sotoudeh"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel framework for polyp segmentation using Dynamic Kernel and Encoder Attention, outperforming state-of-the-art methods.",
        "tldr_zh": "本文提出了一种新颖的框架，使用动态内核和编码器注意力进行息肉分割，优于现有技术。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "VFSI: Validity First Spatial Intelligence for Constraint-Guided Traffic Diffusion",
        "summary": "Modern diffusion models generate realistic traffic simulations but\nsystematically violate physical constraints. In a large-scale evaluation of\nSceneDiffuser++, a state-of-the-art traffic simulator, we find that 50% of\ngenerated trajectories violate basic physical laws - vehicles collide, drive\noff roads, and spawn inside buildings. This reveals a fundamental limitation:\ncurrent models treat physical validity as an emergent property rather than an\narchitectural requirement. We propose Validity-First Spatial Intelligence\n(VFSI), which enforces constraints through energy-based guidance during\ndiffusion sampling, without model retraining. By incorporating collision\navoidance and kinematic constraints as energy functions, we guide the denoising\nprocess toward physically valid trajectories. Across 200 urban scenarios from\nthe Waymo Open Motion Dataset, VFSI reduces collision rates by 67% (24.6% to\n8.1%) and improves overall validity by 87% (50.3% to 94.2%), while\nsimultaneously improving realism metrics (ADE: 1.34m to 1.21m). Our\nmodel-agnostic approach demonstrates that explicit constraint enforcement\nduring inference is both necessary and sufficient for physically valid traffic\nsimulation.",
        "url": "http://arxiv.org/abs/2509.23971v1",
        "published_date": "2025-09-28T16:48:49+00:00",
        "updated_date": "2025-09-28T16:48:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kargi Chauhan",
            "Leilani H. Gilpin"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper introduces VFSI, a method to enforce physical constraints during traffic simulation, significantly reducing collisions and improving realism without model retraining.",
        "tldr_zh": "该论文介绍了VFSI，一种能在交通模拟期间强制实施物理约束的方法，显著降低碰撞率并增强真实感而无需重新训练模型。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "A Novel Hybrid Deep Learning and Chaotic Dynamics Approach for Thyroid Cancer Classification",
        "summary": "Timely and accurate diagnosis is crucial in addressing the global rise in\nthyroid cancer, ensuring effective treatment strategies and improved patient\noutcomes. We present an intelligent classification method that couples an\nAdaptive Convolutional Neural Network (CNN) with Cohen-Daubechies-Feauveau\n(CDF9/7) wavelets whose detail coefficients are modulated by an n-scroll\nchaotic system to enrich discriminative features. We evaluate on the public\nDDTI thyroid ultrasound dataset (n = 1,638 images; 819 malignant / 819 benign)\nusing 5-fold cross-validation, where the proposed method attains 98.17%\naccuracy, 98.76% sensitivity, 97.58% specificity, 97.55% F1-score, and an AUC\nof 0.9912. A controlled ablation shows that adding chaotic modulation to CDF9/7\nimproves accuracy by +8.79 percentage points over a CDF9/7-only CNN (from\n89.38% to 98.17%). To objectively position our approach, we trained\nstate-of-the-art backbones on the same data and splits: EfficientNetV2-S\n(96.58% accuracy; AUC 0.987), Swin-T (96.41%; 0.986), ViT-B/16 (95.72%; 0.983),\nand ConvNeXt-T (96.94%; 0.987). Our method outperforms the best of these by\n+1.23 points in accuracy and +0.0042 in AUC, while remaining computationally\nefficient (28.7 ms per image; 1,125 MB peak VRAM). Robustness is further\nsupported by cross-dataset testing on TCIA (accuracy 95.82%) and transfer to an\nISIC skin-lesion subset (n = 28 unique images, augmented to 2,048; accuracy\n97.31%). Explainability analyses (Grad-CAM, SHAP, LIME) highlight clinically\nrelevant regions. Altogether, the wavelet-chaos-CNN pipeline delivers\nstate-of-the-art thyroid ultrasound classification with strong generalization\nand practical runtime characteristics suitable for clinical integration.",
        "url": "http://arxiv.org/abs/2509.23968v1",
        "published_date": "2025-09-28T16:46:31+00:00",
        "updated_date": "2025-09-28T16:46:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nada Bouchekout",
            "Abdelkrim Boukabou",
            "Morad Grimes",
            "Yassine Habchi",
            "Yassine Himeur",
            "Hamzah Ali Alkhazaleh",
            "Shadi Atalla",
            "Wathiq Mansoor"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper presents a novel method combining deep learning with chaotic dynamics for thyroid cancer classification, achieving high accuracy and outperforming state-of-the-art models.",
        "tldr_zh": "本文介绍了一种将深度学习与混沌动力学相结合的新方法，用于甲状腺癌分类，取得了较高的准确性，并优于现有的先进模型。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "LifeCLEF Plant Identification Task 2014",
        "summary": "The LifeCLEFs plant identification task provides a testbed for a\nsystem-oriented evaluation of plant identification about 500 species trees and\nherbaceous plants. Seven types of image content are considered: scan and\nscan-like pictures of leaf, and 6 kinds of detailed views with un- constrained\nconditions, directly photographed on the plant: flower, fruit, stem & bark,\nbranch, leaf and entire view. The main originality of this data is that it was\nspecifically built through a citizen sciences initiative conducted by Tela\nBotanica, a French social network of amateur and expert botanists. This makes\nthe task closer to the conditions of a real- world application. This overview\npresents more precisely the resources and assessments of task, summarizes the\nretrieval approaches employed by the participating groups, and provides an\nanalysis of the main eval- uation results. With a total of ten groups from six\ncountries and with a total of twenty seven submitted runs, involving distinct\nand original methods, this fourth year task confirms Image & Multimedia\nRetrieval community interest for biodiversity and botany, and highlights\nfurther challenging studies in plant identification.",
        "url": "http://arxiv.org/abs/2509.23900v1",
        "published_date": "2025-09-28T14:16:15+00:00",
        "updated_date": "2025-09-28T14:16:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Herve Goeau",
            "Alexis Joly",
            "Pierre Bonnet",
            "Souheil Selmi",
            "Jean-Francois Molino",
            "Daniel Barthelemy",
            "Nozha Boujemaa"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper discusses the LifeCLEF plant identification task, which evaluates plant identification using images of various plant parts. The dataset was created through a citizen sciences initiative, making it closer to real-world conditions.",
        "tldr_zh": "该论文讨论了LifeCLEF植物识别任务，通过使用各种植物部分的图像来评估植物识别。该数据集是通过公民科学倡议创建的，使其更接近实际应用条件。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "AssemblyHands-X: Modeling 3D Hand-Body Coordination for Understanding Bimanual Human Activities",
        "summary": "Bimanual human activities inherently involve coordinated movements of both\nhands and body. However, the impact of this coordination in activity\nunderstanding has not been systematically evaluated due to the lack of suitable\ndatasets. Such evaluation demands kinematic-level annotations (e.g., 3D pose)\nfor the hands and body, yet existing 3D activity datasets typically annotate\neither hand or body pose. Another line of work employs marker-based motion\ncapture to provide full-body pose, but the physical markers introduce visual\nartifacts, thereby limiting models' generalization to natural, markerless\nvideos. To address these limitations, we present AssemblyHands-X, the first\nmarkerless 3D hand-body benchmark for bimanual activities, designed to study\nthe effect of hand-body coordination for action recognition. We begin by\nconstructing a pipeline for 3D pose annotation from synchronized multi-view\nvideos. Our approach combines multi-view triangulation with SMPL-X mesh\nfitting, yielding reliable 3D registration of hands and upper body. We then\nvalidate different input representations (e.g., video, hand pose, body pose, or\nhand-body pose) across recent action recognition models based on graph\nconvolution or spatio-temporal attention. Our extensive experiments show that\npose-based action inference is more efficient and accurate than video\nbaselines. Moreover, joint modeling of hand and body cues improves action\nrecognition over using hands or upper body alone, highlighting the importance\nof modeling interdependent hand-body dynamics for a holistic understanding of\nbimanual activities.",
        "url": "http://arxiv.org/abs/2509.23888v1",
        "published_date": "2025-09-28T13:52:14+00:00",
        "updated_date": "2025-09-28T13:52:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tatsuro Banno",
            "Takehiko Ohkawa",
            "Ruicong Liu",
            "Ryosuke Furuta",
            "Yoichi Sato"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces AssemblyHands-X, a markerless 3D hand-body benchmark for studying bimanual activities. It shows that modeling hand-body coordination improves action recognition for bimanual activities.",
        "tldr_zh": "本文介绍了AssemblyHands-X，一个用于研究双手活动的无标记3D手体基准。研究表明，模拟手体协调对于双手活动的动作识别具有改进作用。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Taught Well Learned Ill: Towards Distillation-conditional Backdoor Attack",
        "summary": "Knowledge distillation (KD) is a vital technique for deploying deep neural\nnetworks (DNNs) on resource-constrained devices by transferring knowledge from\nlarge teacher models to lightweight student models. While teacher models from\nthird-party platforms may undergo security verification (\\eg, backdoor\ndetection), we uncover a novel and critical threat: distillation-conditional\nbackdoor attacks (DCBAs). DCBA injects dormant and undetectable backdoors into\nteacher models, which become activated in student models via the KD process,\neven with clean distillation datasets. While the direct extension of existing\nmethods is ineffective for DCBA, we implement this attack by formulating it as\na bilevel optimization problem and proposing a simple yet effective method\n(\\ie, SCAR). Specifically, the inner optimization simulates the KD process by\noptimizing a surrogate student model, while the outer optimization leverages\noutputs from this surrogate to optimize the teacher model for implanting the\nconditional backdoor. Our SCAR addresses this complex optimization utilizing an\nimplicit differentiation algorithm with a pre-optimized trigger injection\nfunction. Extensive experiments across diverse datasets, model architectures,\nand KD techniques validate the effectiveness of our SCAR and its resistance\nagainst existing backdoor detection, highlighting a significant yet previously\noverlooked vulnerability in the KD process. Our code is available at\nhttps://github.com/WhitolfChen/SCAR.",
        "url": "http://arxiv.org/abs/2509.23871v1",
        "published_date": "2025-09-28T13:24:46+00:00",
        "updated_date": "2025-09-28T13:24:46+00:00",
        "categories": [
            "cs.CR",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yukun Chen",
            "Boheng Li",
            "Yu Yuan",
            "Leyi Qi",
            "Yiming Li",
            "Tianwei Zhang",
            "Zhan Qin",
            "Kui Ren"
        ],
        "ai_categories": [
            "GAN"
        ],
        "tldr": "The paper introduces a new threat called distillation-conditional backdoor attacks, injecting hidden backdoors into teacher models that activate in student models during the knowledge distillation process, posing a significant security risk.",
        "tldr_zh": "这篇论文介绍了一种称为distillation-conditional backdoor attacks的新威胁，将隐藏后门注入到教师模型中，在知识蒸馏过程中激活学生模型，构成重大安全风险。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Efficient Multi-turn RL for GUI Agents via Decoupled Training and Adaptive Data Curation",
        "summary": "Vision-language model (VLM) based GUI agents show promise for automating\ncomplex desktop and mobile tasks, but face significant challenges in applying\nreinforcement learning (RL): (1) slow multi-turn interactions with GUI\nenvironments for policy rollout, and (2) insufficient high-quality\nagent-environment interactions for policy learning. To address these\nchallenges, we propose DART, a Decoupled Agentic RL Training framework for GUI\nagents, which coordinates heterogeneous modules in a highly decoupled manner.\nDART separates the training system into four asynchronous modules: environment\ncluster, rollout service, data manager, and trainer. This design enables\nnon-blocking communication, asynchronous training, rollout-wise trajectory\nsampling, and per-worker model synchronization, significantly improving the\nsystem efficiency: 1.6*GPU utilization for rollout, 1.9* training throughput,\nand 5.5* environment utilization. To facilitate effective learning from\nabundant samples, we introduce an adaptive data curation scheme: (1)\npre-collecting successful trajectories for challenging tasks to supplement\nsparse success in online sampling; (2) dynamically adjusting rollout numbers\nand trajectory lengths based on task difficulty; (3) training selectively on\nhigh-entropy steps to prioritize critical decisions; (4) stabilizing learning\nvia truncated importance sampling for policy mismatch between policy rollout\nand updating. On the OSWorld benchmark, DART-GUI-7B achieves a 42.13% task\nsuccess rate, a 14.61% absolute gain over the base model, and 7.34% higher than\nopen-source SOTA. We will fully open-source our training framework, data, and\nmodel checkpoints via computer-use-agents.github.io/dart-gui, which we believe\nis a timely contribution to the open-source community of agentic RL training.",
        "url": "http://arxiv.org/abs/2509.23866v1",
        "published_date": "2025-09-28T13:19:20+00:00",
        "updated_date": "2025-09-28T13:19:20+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Pengxiang Li",
            "Zechen Hu",
            "Zirui Shang",
            "Jingrong Wu",
            "Yang Liu",
            "Hui Liu",
            "Zhi Gao",
            "Chenrui Shi",
            "Bofei Zhang",
            "Zihao Zhang",
            "Xiaochuan Shi",
            "Zedong YU",
            "Yuwei Wu",
            "Xinxiao Wu",
            "Yunde Jia",
            "Liuyu Xiang",
            "Zhaofeng He",
            "Qing Li"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes a framework, DART, for efficiently training GUI agents using reinforcement learning. It achieves significant improvements in training efficiency and task success rates.",
        "tldr_zh": "该论文提出了一个名为DART的框架，用于通过强化学习高效地训练GUI代理。它在训练效率和任务成功率方面取得了显著的改进。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "2nd Place Report of MOSEv2 Challenge 2025: Concept Guided Video Object Segmentation via SeC",
        "summary": "Semi-supervised Video Object Segmentation aims to segment a specified target\nthroughout a video sequence, initialized by a first-frame mask. Previous\nmethods rely heavily on appearance-based pattern matching and thus exhibit\nlimited robustness against challenges such as drastic visual changes,\nocclusions, and scene shifts. This failure is often attributed to a lack of\nhigh-level conceptual understanding of the target. The recently proposed\nSegment Concept (SeC) framework mitigated this limitation by using a Large\nVision-Language Model (LVLM) to establish a deep semantic understanding of the\nobject for more persistent segmentation. In this work, we evaluate its\nzero-shot performance on the challenging coMplex video Object SEgmentation v2\n(MOSEv2) dataset. Without any fine-tuning on the training set, SeC achieved\n39.7 \\JFn on the test set and ranked 2nd place in the Complex VOS track of the\n7th Large-scale Video Object Segmentation Challenge.",
        "url": "http://arxiv.org/abs/2509.23838v1",
        "published_date": "2025-09-28T12:26:03+00:00",
        "updated_date": "2025-09-28T12:26:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhixiong Zhang",
            "Shuangrui Ding",
            "Xiaoyi Dong",
            "Yuhang Zang",
            "Yuhang Cao",
            "Jiaqi Wang"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper presents a method called SeC for video object segmentation that uses a Large Vision-Language Model for better understanding of the target object, achieving good results on a challenging dataset without fine-tuning.",
        "tldr_zh": "本文提出了一种名为SeC的视频对象分割方法，利用大型视觉语言模型更好地理解目标对象，在挑战性数据集上取得了良好的结果，无需对训练集进行微调。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 7.0
    },
    {
        "title": "GRS-SLAM3R: Real-Time Dense SLAM with Gated Recurrent State",
        "summary": "DUSt3R-based end-to-end scene reconstruction has recently shown promising\nresults in dense visual SLAM. However, most existing methods only use image\npairs to estimate pointmaps, overlooking spatial memory and global\nconsistency.To this end, we introduce GRS-SLAM3R, an end-to-end SLAM framework\nfor dense scene reconstruction and pose estimation from RGB images without any\nprior knowledge of the scene or camera parameters. Unlike existing DUSt3R-based\nframeworks, which operate on all image pairs and predict per-pair point maps in\nlocal coordinate frames, our method supports sequentialized input and\nincrementally estimates metric-scale point clouds in the global coordinate. In\norder to improve consistent spatial correlation, we use a latent state for\nspatial memory and design a transformer-based gated update module to reset and\nupdate the spatial memory that continuously aggregates and tracks relevant 3D\ninformation across frames. Furthermore, we partition the scene into submaps,\napply local alignment within each submap, and register all submaps into a\ncommon world frame using relative constraints, producing a globally consistent\nmap. Experiments on various datasets show that our framework achieves superior\nreconstruction accuracy while maintaining real-time performance.",
        "url": "http://arxiv.org/abs/2509.23737v1",
        "published_date": "2025-09-28T08:33:34+00:00",
        "updated_date": "2025-09-28T08:33:34+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Guole Shen",
            "Tianchen Deng",
            "Yanbo Wang",
            "Yongtao Chen",
            "Yilin Shen",
            "Jiuming Liu",
            "Jingchuan Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "GRS-SLAM3R is an end-to-end SLAM framework for dense scene reconstruction and pose estimation from RGB images, achieving superior accuracy and real-time performance.",
        "tldr_zh": "GRS-SLAM3R 是一种端到端 SLAM 框架，用于从 RGB 图像实现密集场景重建和姿态估计，实现了卓越的准确性和实时性。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Confidence Aware SSD Ensemble with Weighted Boxes Fusion for Weapon Detection",
        "summary": "The safety and security of public spaces is of vital importance, driving the\nneed for sophisticated surveillance systems capable of accurately detecting\nweapons, which are often hampered by issues like partial occlusion, varying\nlighting, and cluttered backgrounds. While single-model detectors are advanced,\nthey often lack robustness in these challenging conditions. This paper presents\nthe hypothesis that ensemble of Single Shot Multibox Detector (SSD) models with\ndiverse feature extraction backbones can significantly enhance detection\nrobustness. To leverage diverse feature representations, individual SSD models\nwere trained using a selection of backbone networks: VGG16, ResNet50,\nEfficientNet, and MobileNetV3. The study is conducted on a dataset consisting\nof images of three distinct weapon classes: guns, heavy weapons and knives. The\npredictions from these models are combined using the Weighted Boxes Fusion\n(WBF) method, an ensemble technique designed to optimize bounding box accuracy.\nOur key finding is that the fusion strategy is as critical as the ensemble's\ndiversity, a WBF approach using a 'max' confidence scoring strategy achieved a\nmean Average Precision (mAP) of 0.838. This represents a 2.948% relative\nimprovement over the best-performing single model and consistently outperforms\nother fusion heuristics. This research offers a robust approach to enhancing\nreal-time weapon detection capabilities in surveillance applications by\ndemonstrating that confidence-aware fusion is a key mechanism for improving\naccuracy metrics of ensembles.",
        "url": "http://arxiv.org/abs/2509.23697v1",
        "published_date": "2025-09-28T07:08:48+00:00",
        "updated_date": "2025-09-28T07:08:48+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Atharva Jadhav",
            "Arush Karekar",
            "Manas Divekar",
            "Shachi Natu"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer",
            "Other"
        ],
        "tldr": "The paper proposes using an ensemble of SSD models with different backbones and a weighted boxes fusion method to improve weapon detection accuracy.",
        "tldr_zh": "本文提出使用不同骨干网络的SSD模型集合和加权框融合方法来提高武器检测准确性。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Color-Pair Guided Robust Zero-Shot 6D Pose Estimation and Tracking of Cluttered Objects on Edge Devices",
        "summary": "Robust 6D pose estimation of novel objects under challenging illumination\nremains a significant challenge, often requiring a trade-off between accurate\ninitial pose estimation and efficient real-time tracking. We present a unified\nframework explicitly designed for efficient execution on edge devices, which\nsynergizes a robust initial estimation module with a fast motion-based tracker.\nThe key to our approach is a shared, lighting-invariant color-pair feature\nrepresentation that forms a consistent foundation for both stages. For initial\nestimation, this feature facilitates robust registration between the live RGB-D\nview and the object's 3D mesh. For tracking, the same feature logic validates\ntemporal correspondences, enabling a lightweight model to reliably regress the\nobject's motion. Extensive experiments on benchmark datasets demonstrate that\nour integrated approach is both effective and robust, providing competitive\npose estimation accuracy while maintaining high-fidelity tracking even through\nabrupt pose changes.",
        "url": "http://arxiv.org/abs/2509.23647v1",
        "published_date": "2025-09-28T05:07:49+00:00",
        "updated_date": "2025-09-28T05:07:49+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Xingjian Yang",
            "Ashis G. Banerjee"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper presents a unified framework for 6D pose estimation and tracking of objects in challenging conditions, designed for efficient execution on edge devices using a color-pair feature representation.",
        "tldr_zh": "该论文提出了一个统一框架，用于在具有挑战性条件下对对象进行6D姿态估计和跟踪，旨在利用边缘设备上的效率执行，并使用颜色对特征表示。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "From Static to Dynamic: a Survey of Topology-Aware Perception in Autonomous Driving",
        "summary": "The key to achieving autonomous driving lies in topology-aware perception,\nthe structured understanding of the driving environment with an emphasis on\nlane topology and road semantics. This survey systematically reviews four core\nresearch directions under this theme: vectorized map construction, topological\nstructure modeling, prior knowledge fusion, and language model-based\nperception. Across these directions, we observe a unifying trend: a paradigm\nshift from static, pre-built maps to dynamic, sensor-driven perception.\nSpecifically, traditional static maps have provided semantic context for\nautonomous systems. However, they are costly to construct, difficult to update\nin real time, and lack generalization across regions, limiting their\nscalability. In contrast, dynamic representations leverage on-board sensor data\nfor real-time map construction and topology reasoning. Each of the four\nresearch directions contributes to this shift through compact spatial modeling,\nsemantic relational reasoning, robust domain knowledge integration, and\nmultimodal scene understanding powered by pre-trained language models.\nTogether, they pave the way for more adaptive, scalable, and explainable\nautonomous driving systems.",
        "url": "http://arxiv.org/abs/2509.23641v1",
        "published_date": "2025-09-28T04:47:33+00:00",
        "updated_date": "2025-09-28T04:47:33+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Yixiao Chen",
            "Ruining Yang",
            "Xin Chen",
            "Jia He",
            "Dongliang Xu",
            "Yue Yao"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper surveys how autonomous driving systems are shifting from static, pre-built maps to dynamic, sensor-driven perception for better adaptability and scalability.",
        "tldr_zh": "本文调查自动驾驶系统如何从静态的预先构建地图转向动态的传感器驱动感知，以实现更好的适应性和可扩展性。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data",
        "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have transformed\nvision model adaptation, enabling the rapid deployment of customized models.\nHowever, the compactness of LoRA adaptations introduces new safety concerns,\nparticularly their vulnerability to model extraction attacks. This paper\nintroduces a new focus of model extraction attacks named LoRA extraction that\nextracts LoRA-adaptive models based on a public pre-trained model. We then\npropose a novel extraction method called StolenLoRA which trains a substitute\nmodel to extract the functionality of a LoRA-adapted model using synthetic\ndata. StolenLoRA leverages a Large Language Model to craft effective prompts\nfor data generation, and it incorporates a Disagreement-based Semi-supervised\nLearning (DSL) strategy to maximize information gain from limited queries. Our\nexperiments demonstrate the effectiveness of StolenLoRA, achieving up to a\n96.60% attack success rate with only 10k queries, even in cross-backbone\nscenarios where the attacker and victim models utilize different pre-trained\nbackbones. These findings reveal the specific vulnerability of LoRA-adapted\nmodels to this type of extraction and underscore the urgent need for robust\ndefense mechanisms tailored to PEFT methods. We also explore a preliminary\ndefense strategy based on diversified LoRA deployments, highlighting its\npotential to mitigate such attacks.",
        "url": "http://arxiv.org/abs/2509.23594v1",
        "published_date": "2025-09-28T02:51:35+00:00",
        "updated_date": "2025-09-28T02:51:35+00:00",
        "categories": [
            "cs.CR",
            "cs.CV"
        ],
        "authors": [
            "Yixu Wang",
            "Yan Teng",
            "Yingchun Wang",
            "Xingjun Ma"
        ],
        "ai_categories": [
            "LoRA",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces StolenLoRA, a method for extracting LoRA-adaptive models by training a substitute model with synthetic data. It highlights the vulnerability of LoRA adaptations to model extraction attacks.",
        "tldr_zh": "本文介绍了StolenLoRA，一种通过用合成数据训练替代模型来提取LoRA适应模型的方法。它突显了LoRA适应性对模型提取攻击的脆弱性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "BridgeDrive: Diffusion Bridge Policy for Closed-Loop Trajectory Planning in Autonomous Driving",
        "summary": "Diffusion-based planners have shown great promise for autonomous driving due\nto their ability to capture multi-modal driving behaviors. However, guiding\nthese models effectively in reactive, closed-loop environments remains a\nsignificant challenge. Simple conditioning often fails to provide sufficient\nguidance in complex and dynamic driving scenarios. Recent work attempts to use\ntypical expert driving behaviors (i.e., anchors) to guide diffusion models but\nrelies on a truncated schedule, which introduces theoretical inconsistencies\nand can compromise performance. To address this, we introduce BridgeDrive, a\nnovel anchor-guided diffusion bridge policy for closed-loop trajectory\nplanning. Our approach provides a principled diffusion framework that\neffectively translates anchors into fine-grained trajectory plans,\nappropriately responding to varying traffic conditions. Our planner is\ncompatible with efficient ODE solvers, a critical factor for real-time\nautonomous driving deployment. We achieve state-of-the-art performance on the\nBench2Drive benchmark, improving the success rate by 5% over prior arts.",
        "url": "http://arxiv.org/abs/2509.23589v1",
        "published_date": "2025-09-28T02:47:12+00:00",
        "updated_date": "2025-09-28T02:47:12+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Shu Liu",
            "Wenlin Chen",
            "Weihao Li",
            "Zheng Wang",
            "Lijin Yang",
            "Jianing Huang",
            "Yipin Zhang",
            "Zhongzhan Huang",
            "Ze Cheng",
            "Hao Yang"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer"
        ],
        "tldr": "BridgeDrive introduces a novel anchor-guided diffusion bridge policy for closed-loop trajectory planning in autonomous driving, achieving state-of-the-art performance on Bench2Drive benchmark.",
        "tldr_zh": "BridgeDrive引入了一种新颖的锚点引导扩散桥政策，用于自动驾驶中的闭环轨迹规划，在Bench2Drive基准测试中取得了最先进的表现。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Automated design of compound lenses with discrete-continuous optimization",
        "summary": "We introduce a method that automatically and jointly updates both continuous\nand discrete parameters of a compound lens design, to improve its performance\nin terms of sharpness, speed, or both. Previous methods for compound lens\ndesign use gradient-based optimization to update continuous parameters (e.g.,\ncurvature of individual lens elements) of a given lens topology, requiring\nextensive expert intervention to realize topology changes. By contrast, our\nmethod can additionally optimize discrete parameters such as number and type\n(e.g., singlet or doublet) of lens elements. Our method achieves this\ncapability by combining gradient-based optimization with a tailored Markov\nchain Monte Carlo sampling algorithm, using transdimensional mutation and\nparaxial projection operations for efficient global exploration. We show\nexperimentally on a variety of lens design tasks that our method effectively\nexplores an expanded design space of compound lenses, producing better designs\nthan previous methods and pushing the envelope of speed-sharpness tradeoffs\nachievable by automated lens design.",
        "url": "http://arxiv.org/abs/2509.23572v1",
        "published_date": "2025-09-28T02:08:23+00:00",
        "updated_date": "2025-09-28T02:08:23+00:00",
        "categories": [
            "cs.GR",
            "cs.CV",
            "physics.app-ph"
        ],
        "authors": [
            "Arjun Teh",
            "Delio Vicini",
            "Bernd Bickel",
            "Ioannis Gkioulekas",
            "Matthew O'Toole"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a method for automatically updating both continuous and discrete parameters of compound lens designs to improve sharpness and speed, achieving better results than previous methods.",
        "tldr_zh": "本文介绍了一种方法，用于自动更新复合镜头设计的连续和离散参数，以提高清晰度和速度，实现比以往方法更好的结果。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "RAVEN: Resilient Aerial Navigation via Open-Set Semantic Memory and Behavior Adaptation",
        "summary": "Aerial outdoor semantic navigation requires robots to explore large,\nunstructured environments to locate target objects. Recent advances in semantic\nnavigation have demonstrated open-set object-goal navigation in indoor\nsettings, but these methods remain limited by constrained spatial ranges and\nstructured layouts, making them unsuitable for long-range outdoor search. While\noutdoor semantic navigation approaches exist, they either rely on reactive\npolicies based on current observations, which tend to produce short-sighted\nbehaviors, or precompute scene graphs offline for navigation, limiting\nadaptability to online deployment. We present RAVEN, a 3D memory-based,\nbehavior tree framework for aerial semantic navigation in unstructured outdoor\nenvironments. It (1) uses a spatially consistent semantic voxel-ray map as\npersistent memory, enabling long-horizon planning and avoiding purely reactive\nbehaviors, (2) combines short-range voxel search and long-range ray search to\nscale to large environments, (3) leverages a large vision-language model to\nsuggest auxiliary cues, mitigating sparsity of outdoor targets. These\ncomponents are coordinated by a behavior tree, which adaptively switches\nbehaviors for robust operation. We evaluate RAVEN in 10 photorealistic outdoor\nsimulation environments over 100 semantic tasks, encompassing single-object\nsearch, multi-class, multi-instance navigation and sequential task changes.\nResults show RAVEN outperforms baselines by 85.25% in simulation and\ndemonstrate its real-world applicability through deployment on an aerial robot\nin outdoor field tests.",
        "url": "http://arxiv.org/abs/2509.23563v1",
        "published_date": "2025-09-28T01:43:25+00:00",
        "updated_date": "2025-09-28T01:43:25+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Seungchan Kim",
            "Omar Alama",
            "Dmytro Kurdydyk",
            "John Keller",
            "Nikhil Keetha",
            "Wenshan Wang",
            "Yonatan Bisk",
            "Sebastian Scherer"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "RAVEN is a 3D memory-based framework for aerial semantic navigation in unstructured outdoor environments, outperforming baselines in simulation and real-world field tests.",
        "tldr_zh": "RAVEN是一个基于3D记忆的框架，用于在无结构的户外环境中进行空中语义导航，在模拟和实地测试中优于基线。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Pancreas Part Segmentation under Federated Learning Paradigm",
        "summary": "We present the first federated learning (FL) approach for pancreas part(head,\nbody and tail) segmentation in MRI, addressing a critical clinical challenge as\na significant innovation. Pancreatic diseases exhibit marked regional\nheterogeneity cancers predominantly occur in the head region while chronic\npancreatitis causes tissue loss in the tail, making accurate segmentation of\nthe organ into head, body, and tail regions essential for precise diagnosis and\ntreatment planning. This segmentation task remains exceptionally challenging in\nMRI due to variable morphology, poor soft-tissue contrast, and anatomical\nvariations across patients. Our novel contribution tackles two fundamental\nchallenges: first, the technical complexity of pancreas part delineation in\nMRI, and second the data scarcity problem that has hindered prior approaches.\nWe introduce a privacy-preserving FL framework that enables collaborative model\ntraining across seven medical institutions without direct data sharing,\nleveraging a diverse dataset of 711 T1W and 726 T2W MRI scans. Our key\ninnovations include: (1) a systematic evaluation of three state-of-the-art\nsegmentation architectures (U-Net, Attention U-Net,Swin UNETR) paired with two\nFL algorithms (FedAvg, FedProx), revealing Attention U-Net with FedAvg as\noptimal for pancreatic heterogeneity, which was never been done before; (2) a\nnovel anatomically-informed loss function prioritizing region-specific texture\ncontrasts in MRI. Comprehensive evaluation demonstrates that our approach\nachieves clinically viable performance despite training on distributed,\nheterogeneous datasets.",
        "url": "http://arxiv.org/abs/2509.23562v1",
        "published_date": "2025-09-28T01:42:43+00:00",
        "updated_date": "2025-09-28T01:42:43+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ziliang Hong",
            "Halil Ertugrul Aktas",
            "Andrea Mia Bejar",
            "Katherine Wu",
            "Hongyi Pan",
            "Gorkem Durak",
            "Zheyuan Zhang",
            "Sait Kayali",
            "Temel Tirkes",
            "Federica Proietto Salanitri",
            "Concetto Spampinato",
            "Michael Goggins",
            "Tamas Gonda",
            "Candice Bolan",
            "Raj Keswani",
            "Frank Miller",
            "Michael Wallace",
            "Ulas Bagci"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a federated learning approach for segmenting pancreas parts in MRI scans, addressing clinical challenges and data scarcity issues.",
        "tldr_zh": "本文介绍了一种用于在MRI扫描中分割胰腺部位的联邦学习方法，解决了临床挑战和数据稀缺问题。",
        "relevance_score": 2,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Calibrated and Resource-Aware Super-Resolution for Reliable Driver Behavior Analysis",
        "summary": "Driver monitoring systems require not just high accuracy but reliable,\nwell-calibrated confidence scores for safety-critical deployment. While direct\nlow-resolution training yields high overall accuracy, it produces poorly\ncalibrated predictions that can be dangerous in safety-critical scenarios. We\npropose a resource-aware adaptive super-resolution framework that optimizes for\nmodel calibration and high precision-recall on critical events. Our approach\nachieves state-of-the-art performance on safety-centric metrics: best\ncalibration (ECE of 5.8\\% vs 6.2\\% for LR-trained baselines), highest AUPR for\ndrowsiness detection (0.78 vs 0.74), and superior precision-recall for phone\nuse detection (0.74 vs 0.71). A lightweight artifact detector (0.3M parameters,\n5.2ms overhead) provides additional safety by filtering SR-induced\nhallucinations. While LR-trained video models serve as strong general-purpose\nbaselines, our adaptive framework represents the state-of-the-art solution for\nsafety-critical applications where reliability is paramount.",
        "url": "http://arxiv.org/abs/2509.23535v1",
        "published_date": "2025-09-28T00:08:44+00:00",
        "updated_date": "2025-09-28T00:08:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ibne Farabi Shihab",
            "Weiheng Chai",
            "Jiyang Wang",
            "Sanjeda Akter",
            "Senem Velipasalar Gursoy",
            "Anuj Sharma"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a resource-aware super-resolution framework for driver behavior analysis to provide reliable and well-calibrated confidence scores for safety-critical scenarios.",
        "tldr_zh": "该论文介绍了一种资源感知的超分辨率框架，用于司机行为分析，以提供安全关键场景下可靠和校准的置信度分数。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Accuracy-Robustness Trade Off via Spiking Neural Network Gradient Sparsity Trail",
        "summary": "Spiking Neural Networks (SNNs) have attracted growing interest in both\ncomputational neuroscience and artificial intelligence, primarily due to their\ninherent energy efficiency and compact memory footprint. However, achieving\nadversarial robustness in SNNs, particularly for vision-related tasks, remains\na nascent and underexplored challenge. Recent studies have proposed leveraging\nsparse gradients as a form of regularization to enhance robustness against\nadversarial perturbations. In this work, we present a surprising finding: under\nspecific architectural configurations, SNNs exhibit natural gradient sparsity\nand can achieve state-of-the-art adversarial defense performance without the\nneed for any explicit regularization. Further analysis reveals a trade-off\nbetween robustness and generalization: while sparse gradients contribute to\nimproved adversarial resilience, they can impair the model's ability to\ngeneralize; conversely, denser gradients support better generalization but\nincrease vulnerability to attacks.",
        "url": "http://arxiv.org/abs/2509.23762v1",
        "published_date": "2025-09-28T09:15:33+00:00",
        "updated_date": "2025-09-28T09:15:33+00:00",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Nhan T. Luu"
        ],
        "ai_categories": [
            "AIGC"
        ],
        "tldr": "Spiking Neural Networks (SNNs) can achieve state-of-the-art adversarial defense performance without explicit regularization by leveraging natural gradient sparsity, but this may lead to a trade-off between robustness and generalization.",
        "tldr_zh": "尖峰神经网络（SNNs）可以通过利用自然梯度稀疏性在无需显式正则化的情况下实现最先进的对抗防御性能，但这可能导致稳健性和泛化之间的权衡。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 6.75
    },
    {
        "title": "RPG360: Robust 360 Depth Estimation with Perspective Foundation Models and Graph Optimization",
        "summary": "The increasing use of 360 images across various domains has emphasized the\nneed for robust depth estimation techniques tailored for omnidirectional\nimages. However, obtaining large-scale labeled datasets for 360 depth\nestimation remains a significant challenge. In this paper, we propose RPG360, a\ntraining-free robust 360 monocular depth estimation method that leverages\nperspective foundation models and graph optimization. Our approach converts 360\nimages into six-face cubemap representations, where a perspective foundation\nmodel is employed to estimate depth and surface normals. To address depth scale\ninconsistencies across different faces of the cubemap, we introduce a novel\ndepth scale alignment technique using graph-based optimization, which\nparameterizes the predicted depth and normal maps while incorporating an\nadditional per-face scale parameter. This optimization ensures depth scale\nconsistency across the six-face cubemap while preserving 3D structural\nintegrity. Furthermore, as foundation models exhibit inherent robustness in\nzero-shot settings, our method achieves superior performance across diverse\ndatasets, including Matterport3D, Stanford2D3D, and 360Loc. We also demonstrate\nthe versatility of our depth estimation approach by validating its benefits in\ndownstream tasks such as feature matching 3.2 ~ 5.4% and Structure from Motion\n0.2 ~ 9.7% in AUC@5.",
        "url": "http://arxiv.org/abs/2509.23991v1",
        "published_date": "2025-09-28T17:33:12+00:00",
        "updated_date": "2025-09-28T17:33:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dongki Jung",
            "Jaehoon Choi",
            "Yonghan Lee",
            "Dinesh Manocha"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes RPG360, a robust 360 monocular depth estimation method using perspective foundation models and graph optimization.",
        "tldr_zh": "该论文提出了一种利用透视基础模型和图优化的稳健360单眼深度估计方法RPG360。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Interpreting deep learning-based stellar mass estimation via causal analysis and mutual information decomposition",
        "summary": "End-to-end deep learning models fed with multi-band galaxy images are\npowerful data-driven tools used to estimate galaxy physical properties in the\nabsence of spectroscopy. However, due to a lack of interpretability and the\nassociational nature of such models, it is difficult to understand how the\ninformation additional to integrated photometry (e.g., morphology) contributes\nto the estimation task. Improving our understanding in this field would enable\nfurther advances into unraveling the physical connections among galaxy\nproperties and optimizing data exploitation. Therefore, our work is aimed at\ninterpreting the deep learning-based estimation of stellar mass via two\ninterpretability techniques: causal analysis and mutual information\ndecomposition. The former reveals the causal paths between multiple variables\nbeyond nondirectional statistical associations, while the latter quantifies the\nmulticomponent contributions (i.e., redundant, unique, and synergistic) of\ndifferent input data to the stellar mass estimation. Using data from the Sloan\nDigital Sky Survey (SDSS) and the Wide-field Infrared Survey Explorer (WISE),\nwe obtained meaningful results that provide physical interpretations for\nimage-based models. Our work demonstrates the gains from combining deep\nlearning with interpretability techniques, and holds promise in promoting more\ndata-driven astrophysical research (e.g., astrophysical parameter estimations\nand investigations on complex multivariate physical processes).",
        "url": "http://arxiv.org/abs/2509.23901v1",
        "published_date": "2025-09-28T14:17:25+00:00",
        "updated_date": "2025-09-28T14:17:25+00:00",
        "categories": [
            "astro-ph.IM",
            "astro-ph.GA",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Wei Zhang",
            "Qiufan Lin",
            "Yuan-Sen Ting",
            "Shupei Chen",
            "Hengxin Ruan",
            "Song Li",
            "Yifan Wang"
        ],
        "ai_categories": [
            "Other"
        ],
        "tldr": "The paper interprets deep learning-based stellar mass estimation using causal analysis and mutual information decomposition, providing physical interpretations for image-based models.",
        "tldr_zh": "该论文通过因果分析和互信息分解解释基于深度学习的恒星质量估计，为基于图像的模型提供物理解释。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "LifeCLEF Plant Identification Task 2015",
        "summary": "The LifeCLEF plant identification challenge aims at eval- uating plant\nidentification methods and systems at a very large scale, close to the\nconditions of a real-world biodiversity monitoring scenario. The 2015\nevaluation was actually conducted on a set of more than 100K images\nillustrating 1000 plant species living in West Europe. The main originality of\nthis dataset is that it was built through a large-scale partic- ipatory sensing\nplateform initiated in 2011 and which now involves tens of thousands of\ncontributors. This overview presents more precisely the resources and\nassessments of the challenge, summarizes the approaches and systems employed by\nthe participating research groups, and provides an analysis of the main\noutcomes.",
        "url": "http://arxiv.org/abs/2509.23891v1",
        "published_date": "2025-09-28T13:53:35+00:00",
        "updated_date": "2025-09-28T13:53:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Herve Goeau",
            "Pierre Bonnet",
            "Alexis Joly"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper discusses the evaluation of plant identification methods on a large dataset of plant images captured in West Europe, involving thousands of contributors.",
        "tldr_zh": "该论文讨论了植物识别方法在一个大型数据集上的评估，该数据集包含了在西欧采集的植物图像，并涉及数千名参与者。",
        "relevance_score": 5,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6.5
    },
    {
        "title": "FlowLUT: Efficient Image Enhancement via Differentiable LUTs and Iterative Flow Matching",
        "summary": "Deep learning-based image enhancement methods face a fundamental trade-off\nbetween computational efficiency and representational capacity. For example,\nalthough a conventional three-dimensional Look-Up Table (3D LUT) can process a\ndegraded image in real time, it lacks representational flexibility and depends\nsolely on a fixed prior. To address this problem, we introduce FlowLUT, a novel\nend-to-end model that integrates the efficiency of LUTs, multiple priors, and\nthe parameter-independent characteristic of flow-matched reconstructed images.\nSpecifically, firstly, the input image is transformed in color space by a\ncollection of differentiable 3D LUTs (containing a large number of 3D LUTs with\ndifferent priors). Subsequently, a lightweight content-aware dynamically\npredicts fusion weights, enabling scene-adaptive color correction with\n$\\mathcal{O}(1)$ complexity. Next, a lightweight fusion prediction network runs\non multiple 3D LUTs, with $\\mathcal{O}(1)$ complexity for scene-adaptive color\ncorrection.Furthermore, to address the inherent representation limitations of\nLUTs, we design an innovative iterative flow matching method to restore local\nstructural details and eliminate artifacts. Finally, the entire model is\njointly optimized under a composite loss function enforcing perceptual and\nstructural fidelity. Extensive experimental results demonstrate the\neffectiveness of our method on three benchmarks.",
        "url": "http://arxiv.org/abs/2509.23608v1",
        "published_date": "2025-09-28T03:22:01+00:00",
        "updated_date": "2025-09-28T03:22:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Liubing Hu",
            "Chen Wu",
            "Anrui Wang",
            "Dianjie Lu",
            "Guijuan Zhang",
            "Zhuoran Zheng"
        ],
        "ai_categories": [
            "Other"
        ],
        "tldr": "FlowLUT is an end-to-end model for image enhancement that combines the efficiency of Look-Up Tables (LUTs) with flow matching to improve representational flexibility and eliminate artifacts.",
        "tldr_zh": "FlowLUT是一种端到端的图像增强模型，结合了查找表（Look-Up Tables，LUTs）的效率和流匹配技术，以提高表现灵活性并消除伪影。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6.25
    },
    {
        "title": "Gaze Estimation for Human-Robot Interaction: Analysis Using the NICO Platform",
        "summary": "This paper evaluates the current gaze estimation methods within an HRI\ncontext of a shared workspace scenario. We introduce a new, annotated dataset\ncollected with the NICO robotic platform. We evaluate four state-of-the-art\ngaze estimation models. The evaluation shows that the angular errors are close\nto those reported on general-purpose benchmarks. However, when expressed in\nterms of distance in the shared workspace the best median error is 16.48 cm\nquantifying the practical limitations of current methods. We conclude by\ndiscussing these limitations and offering recommendations on how to best\nintegrate gaze estimation as a modality in HRI systems.",
        "url": "http://arxiv.org/abs/2509.24001v1",
        "published_date": "2025-09-28T17:49:27+00:00",
        "updated_date": "2025-09-28T17:49:27+00:00",
        "categories": [
            "cs.CV",
            "cs.RO",
            "I.4.9"
        ],
        "authors": [
            "Matej Palider",
            "Omar Eldardeer",
            "Viktor Kocur"
        ],
        "ai_categories": [
            "Dataset",
            "Multimodality"
        ],
        "tldr": "The paper evaluates gaze estimation methods in a shared workspace scenario using the NICO platform, highlighting practical limitations and offering recommendations for improvement in HRI systems.",
        "tldr_zh": "本文使用NICO平台在共享工作空间场景中评估凝视估计方法，突出实际限制并提出改进人机交互系统的建议。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "EWC-Guided Diffusion Replay for Exemplar-Free Continual Learning in Medical Imaging",
        "summary": "Medical imaging foundation models must adapt over time, yet full retraining\nis often blocked by privacy constraints and cost. We present a continual\nlearning framework that avoids storing patient exemplars by pairing class\nconditional diffusion replay with Elastic Weight Consolidation. Using a compact\nVision Transformer backbone, we evaluate across eight MedMNIST v2 tasks and\nCheXpert. On CheXpert our approach attains 0.851 AUROC, reduces forgetting by\nmore than 30\\% relative to DER\\texttt{++}, and approaches joint training at\n0.869 AUROC, while remaining efficient and privacy preserving. Analyses connect\nforgetting to two measurable factors: fidelity of replay and Fisher weighted\nparameter drift, highlighting the complementary roles of replay diffusion and\nsynaptic stability. The results indicate a practical route for scalable,\nprivacy aware continual adaptation of clinical imaging models.",
        "url": "http://arxiv.org/abs/2509.23906v1",
        "published_date": "2025-09-28T14:23:46+00:00",
        "updated_date": "2025-09-28T14:23:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Anoushka Harit",
            "William Prew",
            "Zhongtian Sun",
            "Florian Markowetz"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a method for continual learning in medical imaging without storing patient data by combining diffusion replay and Elastic Weight Consolidation, achieving competitive results on various tasks.",
        "tldr_zh": "该论文提出了一种在医学影像中进行持续学习的方法，通过结合扩散回放和弹性权重整合，在各种任务上取得了竞争性结果。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "Poivre: Self-Refining Visual Pointing with Reinforcement Learning",
        "summary": "Visual pointing, which aims to localize a target by predicting its\ncoordinates on an image, has emerged as an important problem in the realm of\nvision-language models (VLMs). Despite its broad applicability, recent\nbenchmarks show that current VLMs still fall far behind human performance on\nthis task. A key limitation is that VLMs are typically required to complete the\npointing task in a single step, akin to asking humans to point at an object\nwithout seeing their own fingers. To address this issue, we propose a simple\nyet effective self-refining procedure: Point, Visualize, then Refine (Poivre).\nThis procedure enables a VLM to first mark its estimated point, then\niteratively refine the coordinates if necessary. Inspired by advances of\nreasoning models in the natural language domain, we employ reinforcement\nlearning (RL) to incentivize this self-refining ability. For the RL training,\nwe design a neat process reward that is not only empirically effective but also\ngrounded in appealing properties. Our trained model, Poivre-7B, sets a new\nstate of the art on Point-Bench, outperforming both proprietary models such as\nGemini-2.5-Pro and large open-source models such as Molmo-72B by over 3%. To\nsupport future research, we release our training and inference code, dataset,\nand the Poivre-7B checkpoint.",
        "url": "http://arxiv.org/abs/2509.23746v1",
        "published_date": "2025-09-28T08:51:47+00:00",
        "updated_date": "2025-09-28T08:51:47+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Wenjie Yang",
            "Zengfeng Huang"
        ],
        "ai_categories": [
            "Transformer",
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces Poivre, a self-refining visual pointing model using reinforcement learning, outperforming existing models by 3% on a benchmark task.",
        "tldr_zh": "本文引入了Poivre，一种使用强化学习的自我优化视觉指向模型，在基准任务上比现有模型提高了3%。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "Temporal Generalization: A Reality Check",
        "summary": "Machine learning (ML) models often struggle to maintain performance under\ndistribution shifts, leading to inaccurate predictions on unseen future data.\nIn this work, we investigate whether and under what conditions models can\nachieve such a generalization when relying solely on past data. We explore two\nprimary approaches: convex combinations of past model parameters\n(\\emph{parameter interpolation}) and explicit extrapolation beyond the convex\nhull of past parameters (\\emph{parameter extrapolation}). We benchmark several\nmethods within these categories on a diverse set of temporal tasks, including\nlanguage modeling, news summarization, news tag prediction, academic paper\ncategorization, satellite image-based land use classification over time, and\nhistorical yearbook photo gender prediction. Our empirical findings show that\nnone of the evaluated methods consistently outperforms the simple baseline of\nusing the latest available model parameters in all scenarios. In the absence of\naccess to future data or robust assumptions about the underlying\ndata-generating process, these results underscore the inherent difficulties of\ngeneralizing and extrapolating to future data and warrant caution when\nevaluating claims of such generalization.",
        "url": "http://arxiv.org/abs/2509.23487v1",
        "published_date": "2025-09-27T20:20:44+00:00",
        "updated_date": "2025-09-27T20:20:44+00:00",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Divyam Madaan",
            "Sumit Chopra",
            "Kyunghyun Cho"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper examines the challenge of maintaining performance of machine learning models under distribution shifts for future data, exploring two approaches for achieving generalization solely on past data, but finds that none consistently outperforms using the latest model parameters.",
        "tldr_zh": "本文探讨了机器学习模型在面对未来数据分布变化时维持性能的挑战，探索了两种在过去数据上实现泛化的方法，但发现没有一种方法能在所有场景中始终优于使用最新模型参数。",
        "relevance_score": 2,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    }
]