[
    {
        "title": "CoMo: Compositional Motion Customization for Text-to-Video Generation",
        "summary": "While recent text-to-video models excel at generating diverse scenes, they\nstruggle with precise motion control, particularly for complex, multi-subject\nmotions. Although methods for single-motion customization have been developed\nto address this gap, they fail in compositional scenarios due to two primary\nchallenges: motion-appearance entanglement and ineffective multi-motion\nblending. This paper introduces CoMo, a novel framework for\n$\\textbf{compositional motion customization}$ in text-to-video generation,\nenabling the synthesis of multiple, distinct motions within a single video.\nCoMo addresses these issues through a two-phase approach. First, in the\nsingle-motion learning phase, a static-dynamic decoupled tuning paradigm\ndisentangles motion from appearance to learn a motion-specific module. Second,\nin the multi-motion composition phase, a plug-and-play divide-and-merge\nstrategy composes these learned motions without additional training by\nspatially isolating their influence during the denoising process. To facilitate\nresearch in this new domain, we also introduce a new benchmark and a novel\nevaluation metric designed to assess multi-motion fidelity and blending.\nExtensive experiments demonstrate that CoMo achieves state-of-the-art\nperformance, significantly advancing the capabilities of controllable video\ngeneration. Our project page is at https://como6.github.io/.",
        "url": "http://arxiv.org/abs/2510.23007v1",
        "published_date": "2025-10-27T04:57:09+00:00",
        "updated_date": "2025-10-27T04:57:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Youcan Xu",
            "Zhen Wang",
            "Jiaxin Shi",
            "Kexin Li",
            "Feifei Shao",
            "Jun Xiao",
            "Yi Yang",
            "Jun Yu",
            "Long Chen"
        ],
        "ai_categories": [
            "GAN",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces CoMo, a framework for compositional motion customization in text-to-video generation, achieving state-of-the-art performance in generating multiple distinct motions in a single video.",
        "tldr_zh": "本文介绍了CoMo，这是一个用于文本到视频生成中的组合运动定制的框架，在单个视频中生成多个不同的动作方面取得了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 10,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "MAGIC-Talk: Motion-aware Audio-Driven Talking Face Generation with Customizable Identity Control",
        "summary": "Audio-driven talking face generation has gained significant attention for\napplications in digital media and virtual avatars. While recent methods improve\naudio-lip synchronization, they often struggle with temporal consistency,\nidentity preservation, and customization, especially in long video generation.\nTo address these issues, we propose MAGIC-Talk, a one-shot diffusion-based\nframework for customizable and temporally stable talking face generation.\nMAGIC-Talk consists of ReferenceNet, which preserves identity and enables\nfine-grained facial editing via text prompts, and AnimateNet, which enhances\nmotion coherence using structured motion priors. Unlike previous methods\nrequiring multiple reference images or fine-tuning, MAGIC-Talk maintains\nidentity from a single image while ensuring smooth transitions across frames.\nAdditionally, a progressive latent fusion strategy is introduced to improve\nlong-form video quality by reducing motion inconsistencies and flickering.\nExtensive experiments demonstrate that MAGIC-Talk outperforms state-of-the-art\nmethods in visual quality, identity preservation, and synchronization accuracy,\noffering a robust solution for talking face generation.",
        "url": "http://arxiv.org/abs/2510.22810v1",
        "published_date": "2025-10-26T19:49:31+00:00",
        "updated_date": "2025-10-26T19:49:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fatemeh Nazarieh",
            "Zhenhua Feng",
            "Diptesh Kanojia",
            "Muhammad Awais",
            "Josef Kittler"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Dataset"
        ],
        "tldr": "MAGIC-Talk is a framework for generating talking faces from audio with customizable identity control and motion coherence, outperforming existing methods in visual quality and synchronization accuracy.",
        "tldr_zh": "MAGIC-Talk 是一个从音频生成具有可定制身份控制和动作连贯性的说话人脸的框架，其在视觉质量和同步准确性方面优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Video-Thinker: Sparking \"Thinking with Videos\" via Reinforcement Learning",
        "summary": "Recent advances in image reasoning methods, particularly \"Thinking with\nImages\", have demonstrated remarkable success in Multimodal Large Language\nModels (MLLMs); however, this dynamic reasoning paradigm has not yet been\nextended to video reasoning tasks. In this paper, we propose Video-Thinker,\nwhich empowers MLLMs to think with videos by autonomously leveraging their\nintrinsic \"grounding\" and \"captioning\" capabilities to generate reasoning clues\nthroughout the inference process. To spark this capability, we construct\nVideo-Thinker-10K, a curated dataset featuring autonomous tool usage within\nchain-of-thought reasoning sequences. Our training strategy begins with\nSupervised Fine-Tuning (SFT) to learn the reasoning format, followed by Group\nRelative Policy Optimization (GRPO) to strengthen this reasoning capability.\nThrough this approach, Video-Thinker enables MLLMs to autonomously navigate\ngrounding and captioning tasks for video reasoning, eliminating the need for\nconstructing and calling external tools. Extensive experiments demonstrate that\nVideo-Thinker achieves significant performance gains on both in-domain tasks\nand challenging out-of-domain video reasoning benchmarks, including\nVideo-Holmes, CG-Bench-Reasoning, and VRBench. Our Video-Thinker-7B\nsubstantially outperforms existing baselines such as Video-R1 and establishes\nstate-of-the-art performance among 7B-sized MLLMs.",
        "url": "http://arxiv.org/abs/2510.23473v1",
        "published_date": "2025-10-27T16:10:45+00:00",
        "updated_date": "2025-10-27T16:10:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shijian Wang",
            "Jiarui Jin",
            "Xingjian Wang",
            "Linxin Song",
            "Runhao Fu",
            "Hecheng Wang",
            "Zongyuan Ge",
            "Yuan Lu",
            "Xuelian Cheng"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "Video-Thinker proposes a method to enable Multimodal Large Language Models to reason with videos autonomously, achieving impressive performance gains on various tasks.",
        "tldr_zh": "Video-Thinker提出了一种方法，使多模态大语言模型能够自主地理解视频，并在各种任务上取得了令人印象深刻的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "MDReID: Modality-Decoupled Learning for Any-to-Any Multi-Modal Object Re-Identification",
        "summary": "Real-world object re-identification (ReID) systems often face modality\ninconsistencies, where query and gallery images come from different sensors\n(e.g., RGB, NIR, TIR). However, most existing methods assume modality-matched\nconditions, which limits their robustness and scalability in practical\napplications. To address this challenge, we propose MDReID, a flexible\nany-to-any image-level ReID framework designed to operate under both\nmodality-matched and modality-mismatched scenarios. MDReID builds on the\ninsight that modality information can be decomposed into two components:\nmodality-shared features that are predictable and transferable, and\nmodality-specific features that capture unique, modality-dependent\ncharacteristics. To effectively leverage this, MDReID introduces two key\ncomponents: the Modality Decoupling Learning (MDL) and Modality-aware Metric\nLearning (MML). Specifically, MDL explicitly decomposes modality features into\nmodality-shared and modality-specific representations, enabling effective\nretrieval in both modality-aligned and mismatched scenarios. MML, a tailored\nmetric learning strategy, further enforces orthogonality and complementarity\nbetween the two components to enhance discriminative power across modalities.\nExtensive experiments conducted on three challenging multi-modality ReID\nbenchmarks (RGBNT201, RGBNT100, MSVR310) consistently demonstrate the\nsuperiority of MDReID. Notably, MDReID achieves significant mAP improvements of\n9.8\\%, 3.0\\%, and 11.5\\% in general modality-matched scenarios, and average\ngains of 3.4\\%, 11.8\\%, and 10.9\\% in modality-mismatched scenarios,\nrespectively. The code is available at:\n\\textcolor{magenta}{https://github.com/stone96123/MDReID}.",
        "url": "http://arxiv.org/abs/2510.23301v1",
        "published_date": "2025-10-27T13:08:46+00:00",
        "updated_date": "2025-10-27T13:08:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yingying Feng",
            "Jie Li",
            "Jie Hu",
            "Yukang Zhang",
            "Lei Tan",
            "Jiayi Ji"
        ],
        "ai_categories": [
            "Multimodality"
        ],
        "tldr": "MDReID proposes a modality-decoupled learning framework for multi-modal object re-identification, achieving significant performance improvements in both modality-matched and modality-mismatched scenarios.",
        "tldr_zh": "MDReID提出了一种针对多模态对象重识别的模态解耦学习框架，在模态匹配和模态不匹配场景下均取得了显著性能改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "SceneDecorator: Towards Scene-Oriented Story Generation with Scene Planning and Scene Consistency",
        "summary": "Recent text-to-image models have revolutionized image generation, but they\nstill struggle with maintaining concept consistency across generated images.\nWhile existing works focus on character consistency, they often overlook the\ncrucial role of scenes in storytelling, which restricts their creativity in\npractice. This paper introduces scene-oriented story generation, addressing two\nkey challenges: (i) scene planning, where current methods fail to ensure\nscene-level narrative coherence by relying solely on text descriptions, and\n(ii) scene consistency, which remains largely unexplored in terms of\nmaintaining scene consistency across multiple stories. We propose\nSceneDecorator, a training-free framework that employs VLM-Guided Scene\nPlanning to ensure narrative coherence across different scenes in a\n``global-to-local'' manner, and Long-Term Scene-Sharing Attention to maintain\nlong-term scene consistency and subject diversity across generated stories.\nExtensive experiments demonstrate the superior performance of SceneDecorator,\nhighlighting its potential to unleash creativity in the fields of arts, films,\nand games.",
        "url": "http://arxiv.org/abs/2510.22994v1",
        "published_date": "2025-10-27T04:19:22+00:00",
        "updated_date": "2025-10-27T04:19:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Quanjian Song",
            "Donghao Zhou",
            "Jingyu Lin",
            "Fei Shen",
            "Jiaze Wang",
            "Xiaowei Hu",
            "Cunjian Chen",
            "Pheng-Ann Heng"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "SceneDecorator introduces a scene-oriented story generation approach that addresses scene planning and scene consistency challenges in maintaining narrative coherence across generated stories.",
        "tldr_zh": "SceneDecorator引入了一种场景导向的故事生成方法，解决了在生成故事时维持连贯性的场景规划和场景一致性挑战。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling",
        "summary": "Current 3D/4D generation methods are usually optimized for photorealism,\nefficiency, and aesthetics. However, they often fail to preserve the semantic\nidentity of the subject across different viewpoints. Adapting generation\nmethods with one or few images of a specific subject (also known as\nPersonalization or Subject-driven generation) allows generating visual content\nthat align with the identity of the subject. However, personalized 3D/4D\ngeneration is still largely underexplored. In this work, we introduce TIRE\n(Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation.\nIt takes an initial 3D asset produced by an existing 3D generative model as\ninput and uses video tracking to identify the regions that need to be modified.\nThen, we adopt a subject-driven 2D inpainting model for progressively infilling\nthe identified regions. Finally, we resplat the modified 2D multi-view\nobservations back to 3D while still maintaining consistency. Extensive\nexperiments demonstrate that our approach significantly improves identity\npreservation in 3D/4D generation compared to state-of-the-art methods. Our\nproject website is available at\nhttps://zsh2000.github.io/track-inpaint-resplat.github.io/.",
        "url": "http://arxiv.org/abs/2510.23605v1",
        "published_date": "2025-10-27T17:59:51+00:00",
        "updated_date": "2025-10-27T17:59:51+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.GR",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Shuhong Zheng",
            "Ashkan Mirzaei",
            "Igor Gilitschenski"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel method, TIRE, for subject-driven 3D/4D generation that improves identity preservation compared to existing methods.",
        "tldr_zh": "该论文介绍了一种新颖的方法，TIRE，用于主体驱动的3D/4D生成，相比现有方法改善了身份保留。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human Animation",
        "summary": "Audio-driven human animation models often suffer from identity drift during\ntemporal autoregressive generation, where characters gradually lose their\nidentity over time. One solution is to generate keyframes as intermediate\ntemporal anchors that prevent degradation, but this requires an additional\nkeyframe generation stage and can restrict natural motion dynamics. To address\nthis, we propose Lookahead Anchoring, which leverages keyframes from future\ntimesteps ahead of the current generation window, rather than within it. This\ntransforms keyframes from fixed boundaries into directional beacons: the model\ncontinuously pursues these future anchors while responding to immediate audio\ncues, maintaining consistent identity through persistent guidance. This also\nenables self-keyframing, where the reference image serves as the lookahead\ntarget, eliminating the need for keyframe generation entirely. We find that the\ntemporal lookahead distance naturally controls the balance between expressivity\nand consistency: larger distances allow for greater motion freedom, while\nsmaller ones strengthen identity adherence. When applied to three recent human\nanimation models, Lookahead Anchoring achieves superior lip synchronization,\nidentity preservation, and visual quality, demonstrating improved temporal\nconditioning across several different architectures. Video results are\navailable at the following link: https://lookahead-anchoring.github.io.",
        "url": "http://arxiv.org/abs/2510.23581v1",
        "published_date": "2025-10-27T17:50:19+00:00",
        "updated_date": "2025-10-27T17:50:19+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Junyoung Seo",
            "Rodrigo Mira",
            "Alexandros Haliassos",
            "Stella Bounareli",
            "Honglie Chen",
            "Linh Tran",
            "Seungryong Kim",
            "Zoe Landgraf",
            "Jie Shen"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality"
        ],
        "tldr": "The paper proposes Lookahead Anchoring to maintain character identity in audio-driven human animation by leveraging keyframes from future timestamps.",
        "tldr_zh": "本文提出了预见性锚定，通过利用未来时间戳的关键帧来保持音频驱动的人类动画中的角色身份。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "Task-Agnostic Fusion of Time Series and Imagery for Earth Observation",
        "summary": "We propose a task-agnostic framework for multimodal fusion of time series and\nsingle timestamp images, enabling cross-modal generation and robust downstream\nperformance. Our approach explores deterministic and learned strategies for\ntime series quantization and then leverages a masked correlation learning\nobjective, aligning discrete image and time series tokens in a unified\nrepresentation space. Instantiated in the Earth observation domain, the\npretrained model generates consistent global temperature profiles from\nsatellite imagery and is validated through counterfactual experiments. Across\ndownstream tasks, our task-agnostic pretraining outperforms task-specific\nfusion by 6\\% in R$^2$ and 2\\% in RMSE on average, and exceeds baseline methods\nby 50\\% in R$^2$ and 12\\% in RMSE. Finally, we analyze gradient sensitivity\nacross modalities, providing insights into model robustness. Code, data, and\nweights will be released under a permissive license.",
        "url": "http://arxiv.org/abs/2510.23118v1",
        "published_date": "2025-10-27T08:38:52+00:00",
        "updated_date": "2025-10-27T08:38:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Gianfranco Basile",
            "Johannes Jakubik",
            "Benedikt Blumenstiel",
            "Thomas Brunschwiler",
            "Juan Bernabe Moreno"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a framework for fusing time series data and images to generate global temperature profiles from satellite imagery, outperforming task-specific fusion methods across various evaluation metrics.",
        "tldr_zh": "该论文介绍了一种融合时间序列数据和图像的框架，可以从卫星图像生成全球温度曲线，并在各种评估指标上优于特定任务的融合方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "EndoWave: Rational-Wavelet 4D Gaussian Splatting for Endoscopic Reconstruction",
        "summary": "In robot-assisted minimally invasive surgery, accurate 3D reconstruction from\nendoscopic video is vital for downstream tasks and improved outcomes. However,\nendoscopic scenarios present unique challenges, including photometric\ninconsistencies, non-rigid tissue motion, and view-dependent highlights. Most\n3DGS-based methods that rely solely on appearance constraints for optimizing\n3DGS are often insufficient in this context, as these dynamic visual artifacts\ncan mislead the optimization process and lead to inaccurate reconstructions. To\naddress these limitations, we present EndoWave, a unified spatio-temporal\nGaussian Splatting framework by incorporating an optical flow-based geometric\nconstraint and a multi-resolution rational wavelet supervision. First, we adopt\na unified spatio-temporal Gaussian representation that directly optimizes\nprimitives in a 4D domain. Second, we propose a geometric constraint derived\nfrom optical flow to enhance temporal coherence and effectively constrain the\n3D structure of the scene. Third, we propose a multi-resolution rational\northogonal wavelet as a constraint, which can effectively separate the details\nof the endoscope and enhance the rendering performance. Extensive evaluations\non two real surgical datasets, EndoNeRF and StereoMIS, demonstrate that our\nmethod EndoWave achieves state-of-the-art reconstruction quality and visual\naccuracy compared to the baseline method.",
        "url": "http://arxiv.org/abs/2510.23087v1",
        "published_date": "2025-10-27T07:45:17+00:00",
        "updated_date": "2025-10-27T07:45:17+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Taoyu Wu",
            "Yiyi Miao",
            "Jiaxin Guo",
            "Ziyan Chen",
            "Sihang Zhao",
            "Zhuoxiao Li",
            "Zhe Tang",
            "Baoru Huang",
            "Limin Yu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a new method, EndoWave, for accurate 3D reconstruction in robot-assisted minimally invasive surgery using endoscopic video, achieving state-of-the-art results compared to existing methods.",
        "tldr_zh": "该论文介绍了一种新的方法，EndoWave，用于在机器辅助微创手术中使用内窥镜视频进行准确的三维重建，与现有方法相比取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "Scaling Up Occupancy-centric Driving Scene Generation: Dataset and Method",
        "summary": "Driving scene generation is a critical domain for autonomous driving,\nenabling downstream applications, including perception and planning evaluation.\nOccupancy-centric methods have recently achieved state-of-the-art results by\noffering consistent conditioning across frames and modalities; however, their\nperformance heavily depends on annotated occupancy data, which still remains\nscarce. To overcome this limitation, we curate Nuplan-Occ, the largest semantic\noccupancy dataset to date, constructed from the widely used Nuplan benchmark.\nIts scale and diversity facilitate not only large-scale generative modeling but\nalso autonomous driving downstream applications. Based on this dataset, we\ndevelop a unified framework that jointly synthesizes high-quality semantic\noccupancy, multi-view videos, and LiDAR point clouds. Our approach incorporates\na spatio-temporal disentangled architecture to support high-fidelity spatial\nexpansion and temporal forecasting of 4D dynamic occupancy. To bridge modal\ngaps, we further propose two novel techniques: a Gaussian splatting-based\nsparse point map rendering strategy that enhances multi-view video generation,\nand a sensor-aware embedding strategy that explicitly models LiDAR sensor\nproperties for realistic multi-LiDAR simulation. Extensive experiments\ndemonstrate that our method achieves superior generation fidelity and\nscalability compared to existing approaches, and validates its practical value\nin downstream tasks. Repo:\nhttps://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation/tree/v2",
        "url": "http://arxiv.org/abs/2510.22973v1",
        "published_date": "2025-10-27T03:52:45+00:00",
        "updated_date": "2025-10-27T03:52:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bohan Li",
            "Xin Jin",
            "Hu Zhu",
            "Hongsi Liu",
            "Ruikai Li",
            "Jiazhe Guo",
            "Kaiwen Cai",
            "Chao Ma",
            "Yueming Jin",
            "Hao Zhao",
            "Xiaokang Yang",
            "Wenjun Zeng"
        ],
        "ai_categories": [
            "Dataset",
            "GAN",
            "Multimodality"
        ],
        "tldr": "The paper introduces a new dataset and method for generating driving scenes in autonomous vehicles, achieving superior results in terms of fidelity and scalability compared to existing approaches.",
        "tldr_zh": "本文介绍了一种用于生成自动驾驶车辆驾驶场景的新数据集和方法，与现有方法相比，在保真度和可扩展性方面取得了卓越的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "FastJAM: a Fast Joint Alignment Model for Images",
        "summary": "Joint Alignment (JA) of images aims to align a collection of images into a\nunified coordinate frame, such that semantically-similar features appear at\ncorresponding spatial locations. Most existing approaches often require long\ntraining times, large-capacity models, and extensive hyperparameter tuning. We\nintroduce FastJAM, a rapid, graph-based method that drastically reduces the\ncomputational complexity of joint alignment tasks. FastJAM leverages pairwise\nmatches computed by an off-the-shelf image matcher, together with a rapid\nnonparametric clustering, to construct a graph representing intra- and\ninter-image keypoint relations. A graph neural network propagates and\naggregates these correspondences, efficiently predicting per-image homography\nparameters via image-level pooling. Utilizing an inverse-compositional loss,\nthat eliminates the need for a regularization term over the predicted\ntransformations (and thus also obviates the hyperparameter tuning associated\nwith such terms), FastJAM performs image JA quickly and effectively.\nExperimental results on several benchmarks demonstrate that FastJAM achieves\nresults better than existing modern JA methods in terms of alignment quality,\nwhile reducing computation time from hours or minutes to mere seconds. Our code\nis available at our project webpage, https://bgu-cs-vil.github.io/FastJAM/",
        "url": "http://arxiv.org/abs/2510.22842v1",
        "published_date": "2025-10-26T21:21:27+00:00",
        "updated_date": "2025-10-26T21:21:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Omri Hirsch",
            "Ron Shapira Weber",
            "Shira Ifergane",
            "Oren Freifeld"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "FastJAM is a rapid graph-based method for image Joint Alignment that outperforms existing methods in alignment quality while significantly reducing computation time.",
        "tldr_zh": "FastJAM是一种快速的基于图的方法，用于图像联合对齐，在对齐质量方面优于现有方法，同时显著减少计算时间。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Self-Calibrated Consistency can Fight Back for Adversarial Robustness in Vision-Language Models",
        "summary": "Pre-trained vision-language models (VLMs) such as CLIP have demonstrated\nstrong zero-shot capabilities across diverse domains, yet remain highly\nvulnerable to adversarial perturbations that disrupt image-text alignment and\ncompromise reliability. Existing defenses typically rely on adversarial\nfine-tuning with labeled data, limiting their applicability in zero-shot\nsettings. In this work, we identify two key weaknesses of current CLIP\nadversarial attacks -- lack of semantic guidance and vulnerability to view\nvariations -- collectively termed semantic and viewpoint fragility. To address\nthese challenges, we propose Self-Calibrated Consistency (SCC), an effective\ntest-time defense. SCC consists of two complementary modules: Semantic\nconsistency, which leverages soft pseudo-labels from counterattack warm-up and\nmulti-view predictions to regularize cross-modal alignment and separate the\ntarget embedding from confusable negatives; and Spatial consistency, aligning\nperturbed visual predictions via augmented views to stabilize inference under\nadversarial perturbations. Together, these modules form a plug-and-play\ninference strategy. Extensive experiments on 22 benchmarks under diverse attack\nsettings show that SCC consistently improves the zero-shot robustness of CLIP\nwhile maintaining accuracy, and can be seamlessly integrated with other VLMs\nfor further gains. These findings highlight the great potential of establishing\nan adversarially robust paradigm from CLIP, with implications extending to\nbroader vision-language domains such as BioMedCLIP.",
        "url": "http://arxiv.org/abs/2510.22785v1",
        "published_date": "2025-10-26T18:37:12+00:00",
        "updated_date": "2025-10-26T18:37:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaxiang Liu",
            "Jiawei Du",
            "Xiao Liu",
            "Prayag Tiwari",
            "Mingkun Xu"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "AIGC"
        ],
        "tldr": "The paper proposes a defense mechanism called Self-Calibrated Consistency to improve the adversarial robustness of vision-language models like CLIP by addressing semantic and viewpoint fragility.",
        "tldr_zh": "本文提出了一种被称为自我校准一致性的防御机制，以提高像CLIP这样的视觉语言模型的对抗性稳健性，同时解决语义和视角的脆弱性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence",
        "summary": "The scope of neural code intelligence is rapidly expanding beyond text-based\nsource code to encompass the rich visual outputs that programs generate. This\nvisual dimension is critical for advanced applications like flexible content\ngeneration and precise, program-driven editing of visualizations. However,\nprogress has been impeded by the scarcity of high-quality multimodal code data,\na bottleneck stemming from challenges in synthesis and quality assessment. To\naddress these challenges, we make contributions from both a data and modeling\nperspective. We first introduce a complete synthesis toolkit that leverages\nreciprocal synergies between data modalities to efficiently produce a\nlarge-scale, high-quality corpus spanning from standard charts to complex\ninteractive web UIs and code-driven animations. Leveraging this toolkit, we\nconstruct JanusCode-800K, the largest multimodal code corpus to date. This\npowers the training of our models, JanusCoder and JanusCoderV, which establish\na visual-programmatic interface for generating code from textual instructions,\nvisual inputs, or a combination of both. Our unified model is a departure from\nexisting approaches that build specialized models for isolated tasks. Extensive\nexperiments on both text-centric and vision-centric coding tasks demonstrate\nthe superior performance of the JanusCoder series, with our 7B to 14B scale\nmodels approaching or even exceeding the performance of commercial models.\nFurthermore, extensive analysis provides key insights into harmonizing\nprogrammatic logic with its visual expression. Our code and checkpoints will\nare available at https://github.com/InternLM/JanusCoder.",
        "url": "http://arxiv.org/abs/2510.23538v1",
        "published_date": "2025-10-27T17:13:49+00:00",
        "updated_date": "2025-10-27T17:13:49+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.SE"
        ],
        "authors": [
            "Qiushi Sun",
            "Jingyang Gong",
            "Yang Liu",
            "Qiaosheng Chen",
            "Lei Li",
            "Kai Chen",
            "Qipeng Guo",
            "Ben Kao",
            "Fei Yuan"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces JanusCoder, a visual-programmatic interface for generating code from textual instructions, visual inputs, or a combination of both. They have built a large multimodal code corpus and models that outperform commercial models in text-centric and vision-centric coding tasks.",
        "tldr_zh": "本文介绍了JanusCoder，一个可以从文本指令、视觉输入或二者结合生成代码的视觉程序界面。他们构建了一个大型的多模态代码语料库和模型，在文本中心和视觉中心编码任务中优于商业模型。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "USF-MAE: Ultrasound Self-Supervised Foundation Model with Masked Autoencoding",
        "summary": "Ultrasound imaging is one of the most widely used diagnostic modalities,\noffering real-time, radiation-free assessment across diverse clinical domains.\nHowever, interpretation of ultrasound images remains challenging due to high\nnoise levels, operator dependence, and limited field of view, resulting in\nsubstantial inter-observer variability. Current Deep Learning approaches are\nhindered by the scarcity of large labeled datasets and the domain gap between\ngeneral and sonographic images, which limits the transferability of models\npretrained on non-medical data. To address these challenges, we introduce the\nUltrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE),\nthe first large-scale self-supervised MAE framework pretrained exclusively on\nultrasound data. The model was pre-trained on 370,000 2D and 3D ultrasound\nimages curated from 46 open-source datasets, collectively termed OpenUS-46,\nspanning over twenty anatomical regions. This curated dataset has been made\npublicly available to facilitate further research and reproducibility. Using a\nVision Transformer encoder-decoder architecture, USF-MAE reconstructs masked\nimage patches, enabling it to learn rich, modality-specific representations\ndirectly from unlabeled data. The pretrained encoder was fine-tuned on three\npublic downstream classification benchmarks: BUS-BRA (breast cancer), MMOTU-2D\n(ovarian tumors), and GIST514-DB (gastrointestinal stromal tumors). Across all\ntasks, USF-MAE consistently outperformed conventional CNN and ViT baselines,\nachieving F1-scores of 81.6%, 79.6%, and 82.4%, respectively. Despite not using\nlabels during pretraining, USF-MAE approached the performance of the supervised\nfoundation model UltraSam on breast cancer classification and surpassed it on\nthe other tasks, demonstrating strong cross-anatomical generalization.",
        "url": "http://arxiv.org/abs/2510.22990v1",
        "published_date": "2025-10-27T04:16:43+00:00",
        "updated_date": "2025-10-27T04:16:43+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Youssef Megahed",
            "Robin Ducharme",
            "Mark Walker",
            "Steven Hawken",
            "Adrian D. C. Chan"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces USF-MAE, a self-supervised model pretrained on ultrasound data that outperforms conventional models in medical image classification tasks.",
        "tldr_zh": "该论文介绍了USF-MAE，这是一个在超声波数据上进行预训练的自监督模型，比传统模型在医学图像分类任务中表现更好。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations",
        "summary": "Humans learn abstract concepts through multisensory synergy, and once formed,\nsuch representations can often be recalled from a single modality. Inspired by\nthis principle, we introduce Concerto, a minimalist simulation of human concept\nlearning for spatial cognition, combining 3D intra-modal self-distillation with\n2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more\ncoherent and informative spatial features, as demonstrated by zero-shot\nvisualizations. It outperforms both standalone SOTA 2D and 3D self-supervised\nmodels by 14.2% and 4.8%, respectively, as well as their feature concatenation,\nin linear probing for 3D scene perception. With full fine-tuning, Concerto sets\nnew SOTA results across multiple scene understanding benchmarks (e.g., 80.7%\nmIoU on ScanNet). We further present a variant of Concerto tailored for\nvideo-lifted point cloud spatial understanding, and a translator that linearly\nprojects Concerto representations into CLIP's language space, enabling\nopen-world perception. These results highlight that Concerto emerges spatial\nrepresentations with superior fine-grained geometric and semantic consistency.",
        "url": "http://arxiv.org/abs/2510.23607v1",
        "published_date": "2025-10-27T17:59:59+00:00",
        "updated_date": "2025-10-27T17:59:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yujia Zhang",
            "Xiaoyang Wu",
            "Yixing Lao",
            "Chengyao Wang",
            "Zhuotao Tian",
            "Naiyan Wang",
            "Hengshuang Zhao"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "Concerto introduces a joint 2D-3D self-supervised learning approach for spatial representation learning, outperforming existing models in 3D scene perception tasks and achieving state-of-the-art results on various benchmarks.",
        "tldr_zh": "Concerto引入了一种联合的2D-3D自监督学习方法，用于空间表示学习，在3D场景感知任务中胜过现有模型，并在各种基准测试中取得了最新成果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "PixelRefer: A Unified Framework for Spatio-Temporal Object Referring with Arbitrary Granularity",
        "summary": "Multimodal large language models (MLLMs) have demonstrated strong\ngeneral-purpose capabilities in open-world visual comprehension. However, most\nexisting MLLMs primarily focus on holistic, scene-level understanding, often\noverlooking the need for fine-grained, object-centric reasoning. In this paper,\nwe present PixelRefer, a unified region-level MLLM framework that enables\nadvanced fine-grained understanding over user-specified regions across both\nimages and videos. Motivated by the observation that LLM attention\npredominantly focuses on object-level tokens, we propose a Scale-Adaptive\nObject Tokenizer (SAOT) to generate compact and semantically rich object\nrepresentations from free-form regions. Our analysis reveals that global visual\ntokens contribute mainly in early LLM layers, inspiring the design of\nPixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion\nmodule to pre-fuse global context into object tokens. This yields a lightweight\nObject-Only Framework that substantially reduces computational cost while\nmaintaining high semantic fidelity. To facilitate fine-grained instruction\ntuning, we curate PixelRefer-2.2M, a high-quality object-centric instruction\ndataset. Extensive experiments across a range of benchmarks validate that\nPixelRefer achieves leading performance with fewer training samples, while\nPixelRefer-Lite offers competitive accuracy with notable gains in efficiency.",
        "url": "http://arxiv.org/abs/2510.23603v1",
        "published_date": "2025-10-27T17:59:32+00:00",
        "updated_date": "2025-10-27T17:59:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuqian Yuan",
            "Wenqiao Zhang",
            "Xin Li",
            "Shihao Wang",
            "Kehan Li",
            "Wentong Li",
            "Jun Xiao",
            "Lei Zhang",
            "Beng Chin Ooi"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "PixelRefer is a framework for fine-grained object-centric reasoning in images and videos, achieving high performance with fewer training samples and increased efficiency.",
        "tldr_zh": "PixelRefer 是一个用于图像和视频的细粒度对象中心推理的框架，能够在较少的训练样本和增强的效率下取得高性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error Detection",
        "summary": "We introduce \\textbf{PRISM-Bench}, a benchmark of puzzle-based visual\nchallenges designed to evaluate not only whether models can solve problems, but\nhow their reasoning unfolds. Unlike prior evaluations that measure only\nfinal-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual\npuzzle and a step-by-step chain-of-thought (CoT) containing exactly one error,\nmodels must identify the first incorrect step. This setting enables\nfine-grained assessment of logical consistency, error detection, and visual\nreasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric,\nand analogical reasoning, resisting shortcuts based on superficial pattern\nmatching. Evaluations across state-of-the-art MLLMs reveal a persistent gap\nbetween fluent generation and faithful reasoning: models that produce plausible\nCoTs often fail to locate simple logical faults. By disentangling answer\ngeneration from reasoning verification, PRISM-Bench offers a sharper lens on\nmultimodal reasoning competence and underscores the need for diagnostic\nevaluation protocols in the development of trustworthy MLLMs.",
        "url": "http://arxiv.org/abs/2510.23594v1",
        "published_date": "2025-10-27T17:57:52+00:00",
        "updated_date": "2025-10-27T17:57:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yusu Qian",
            "Cheng Wan",
            "Chao Jia",
            "Yinfei Yang",
            "Qingyu Zhao",
            "Zhe Gan"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "PRISM-Bench is a benchmark for evaluating models' reasoning abilities on puzzle-based visual tasks, focusing on logic, error detection, and visual reasoning.",
        "tldr_zh": "PRISM-Bench 是一个用于评估模型在基于谜题的视觉任务上的推理能力的基准，重点关注逻辑、错误检测和视觉推理。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FARMER: Flow AutoRegressive Transformer over Pixels",
        "summary": "Directly modeling the explicit likelihood of the raw data distribution is key\ntopic in the machine learning area, which achieves the scaling successes in\nLarge Language Models by autoregressive modeling. However, continuous AR\nmodeling over visual pixel data suffer from extremely long sequences and\nhigh-dimensional spaces. In this paper, we present FARMER, a novel end-to-end\ngenerative framework that unifies Normalizing Flows (NF) and Autoregressive\n(AR) models for tractable likelihood estimation and high-quality image\nsynthesis directly from raw pixels. FARMER employs an invertible autoregressive\nflow to transform images into latent sequences, whose distribution is modeled\nimplicitly by an autoregressive model. To address the redundancy and complexity\nin pixel-level modeling, we propose a self-supervised dimension reduction\nscheme that partitions NF latent channels into informative and redundant\ngroups, enabling more effective and efficient AR modeling. Furthermore, we\ndesign a one-step distillation scheme to significantly accelerate inference\nspeed and introduce a resampling-based classifier-free guidance algorithm to\nboost image generation quality. Extensive experiments demonstrate that FARMER\nachieves competitive performance compared to existing pixel-based generative\nmodels while providing exact likelihoods and scalable training.",
        "url": "http://arxiv.org/abs/2510.23588v1",
        "published_date": "2025-10-27T17:54:08+00:00",
        "updated_date": "2025-10-27T17:54:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guangting Zheng",
            "Qinyu Zhao",
            "Tao Yang",
            "Fei Xiao",
            "Zhijie Lin",
            "Jie Wu",
            "Jiajun Deng",
            "Yanyong Zhang",
            "Rui Zhu"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "FARMER is a novel framework combining Normalizing Flows and Autoregressive models for image generation from raw pixels, achieving competitive performance and exact likelihood estimation.",
        "tldr_zh": "FARMER是一个结合了正则化流和自回归模型的新框架，用于从原始像素生成图像，表现出竞争性能和精确似然估计。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UrbanVLA: A Vision-Language-Action Model for Urban Micromobility",
        "summary": "Urban micromobility applications, such as delivery robots, demand reliable\nnavigation across large-scale urban environments while following long-horizon\nroute instructions. This task is particularly challenging due to the dynamic\nand unstructured nature of real-world city areas, yet most existing navigation\nmethods remain tailored to short-scale and controllable scenarios. Effective\nurban micromobility requires two complementary levels of navigation skills:\nlow-level capabilities such as point-goal reaching and obstacle avoidance, and\nhigh-level capabilities, such as route-visual alignment. To this end, we\npropose UrbanVLA, a route-conditioned Vision-Language-Action (VLA) framework\ndesigned for scalable urban navigation. Our method explicitly aligns noisy\nroute waypoints with visual observations during execution, and subsequently\nplans trajectories to drive the robot. To enable UrbanVLA to master both levels\nof navigation, we employ a two-stage training pipeline. The process begins with\nSupervised Fine-Tuning (SFT) using simulated environments and trajectories\nparsed from web videos. This is followed by Reinforcement Fine-Tuning (RFT) on\na mixture of simulation and real-world data, which enhances the model's safety\nand adaptability in real-world settings. Experiments demonstrate that UrbanVLA\nsurpasses strong baselines by more than 55% in the SocialNav task on MetaUrban.\nFurthermore, UrbanVLA achieves reliable real-world navigation, showcasing both\nscalability to large-scale urban environments and robustness against real-world\nuncertainties.",
        "url": "http://arxiv.org/abs/2510.23576v1",
        "published_date": "2025-10-27T17:46:43+00:00",
        "updated_date": "2025-10-27T17:46:43+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Anqi Li",
            "Zhiyong Wang",
            "Jiazhao Zhang",
            "Minghan Li",
            "Yunpeng Qi",
            "Zhibo Chen",
            "Zhizheng Zhang",
            "He Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces UrbanVLA, a model for urban micromobility that aligns route instructions with visual observations to navigate large urban environments effectively. It outperforms existing methods by 55% on the SocialNav task and shows scalability and robustness in real-world settings.",
        "tldr_zh": "该论文介绍了UrbanVLA，这是一个针对城市微移动的模型，通过将路线指示与视觉观察对齐，在大城市环境中有效导航。它在SocialNav任务上表现比现有方法提高了55%，并展示了在现实环境中的可伸缩性和鲁棒性。",
        "relevance_score": 1,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "More Than Generation: Unifying Generation and Depth Estimation via Text-to-Image Diffusion Models",
        "summary": "Generative depth estimation methods leverage the rich visual priors stored in\npre-trained text-to-image diffusion models, demonstrating astonishing zero-shot\ncapability. However, parameter updates during training lead to catastrophic\ndegra- dation in the image generation capability of the pre-trained model. We\nintroduce MERGE, a unified model for image generation and depth estimation,\nstarting from a fixed pre-trained text-to-image model. MERGE demonstrates that\nthe pre-trained text-to-image model can do more than image generation, but also\nexpand to depth estimation effortlessly. Specifically, MERGE introduces a play-\nand-plug framework that enables seamless switching between image generation and\ndepth estimation modes through simple and pluggable converters. Meanwhile, we\npropose a Group Reuse Mechanism to encourage parameter reuse and im- prove the\nutilization of the additional learnable parameters. MERGE unleashes the\npowerful depth estimation capability of the pre-trained text-to-image model\nwhile preserving its original image generation ability. Compared to other\nunified models for image generation and depth estimation, MERGE achieves\nstate-of- the-art performance across multiple depth estimation benchmarks. The\ncode will be made available at https://github.com/H-EmbodVis/MERGE",
        "url": "http://arxiv.org/abs/2510.23574v1",
        "published_date": "2025-10-27T17:44:56+00:00",
        "updated_date": "2025-10-27T17:44:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hongkai Lin",
            "Dingkang Liang",
            "Mingyang Du",
            "Xin Zhou",
            "Xiang Bai"
        ],
        "ai_categories": [
            "Diffusion"
        ],
        "tldr": "The paper introduces a model called MERGE that unifies image generation and depth estimation using a pre-trained text-to-image model, achieving state-of-the-art performance in depth estimation benchmarks.",
        "tldr_zh": "该论文介绍了一种名为MERGE的模型，它利用预训练的文本到图像模型统一了图像生成和深度估计，达到了深度估计基准测试的最新性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Revising Second Order Terms in Deep Animation Video Coding",
        "summary": "First Order Motion Model is a generative model that animates human heads\nbased on very little motion information derived from keypoints. It is a\npromising solution for video communication because first it operates at very\nlow bitrate and second its computational complexity is moderate compared to\nother learning based video codecs. However, it has strong limitations by\ndesign. Since it generates facial animations by warping source-images, it fails\nto recreate videos with strong head movements. This works concentrates on one\nspecific kind of head movements, namely head rotations. We show that replacing\nthe Jacobian transformations in FOMM by a global rotation helps the system to\nperform better on items with head-rotations while saving 40% to 80% of bitrate\non P-frames. Moreover, we apply state-of-the-art normalization techniques to\nthe discriminator to stabilize the adversarial training which is essential for\ngenerating visually appealing videos. We evaluate the performance by the\nlearned metics LPIPS and DISTS to show the success our optimizations.",
        "url": "http://arxiv.org/abs/2510.23561v1",
        "published_date": "2025-10-27T17:32:08+00:00",
        "updated_date": "2025-10-27T17:32:08+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Konstantin Schmidt",
            "Thomas Richter"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper proposes a method to improve facial animation in videos by replacing Jacobian transformations with global rotations, resulting in better performance on head-rotations with bitrate savings on P-frames.",
        "tldr_zh": "本文提出一种方法，通过用全局旋转替换Jacobian转换来改进视频中的面部动画，在头部旋转方面表现更好，并在P帧上节省比特率。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A U-Net and Transformer Pipeline for Multilingual Image Translation",
        "summary": "This paper presents an end-to-end multilingual translation pipeline that\nintegrates a custom U-Net for text detection, the Tesseract engine for text\nrecognition, and a from-scratch sequence-to-sequence (Seq2Seq) Transformer for\nNeural Machine Translation (NMT). Our approach first utilizes a U-Net model,\ntrained on a synthetic dataset , to accurately segment and detect text regions\nfrom an image. These detected regions are then processed by Tesseract to\nextract the source text. This extracted text is fed into a custom Transformer\nmodel trained from scratch on a multilingual parallel corpus spanning 5\nlanguages. Unlike systems reliant on monolithic pre-trained models, our\narchitecture emphasizes full customization and adaptability. The system is\nevaluated on its text detection accuracy, text recognition quality, and\ntranslation performance via BLEU scores. The complete pipeline demonstrates\npromising results, validating the viability of a custom-built system for\ntranslating text directly from images.",
        "url": "http://arxiv.org/abs/2510.23554v1",
        "published_date": "2025-10-27T17:28:55+00:00",
        "updated_date": "2025-10-27T17:28:55+00:00",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Siddharth Sahay",
            "Radhika Agarwal"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a multilingual image translation pipeline using U-Net, Transformer, and Tesseract for text detection, recognition, and translation, showing promising results.",
        "tldr_zh": "该论文介绍了一种使用U-Net、Transformer和Tesseract进行文本检测、识别和翻译的多语言图像翻译流水线，展现了令人期待的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FreeFuse: Multi-Subject LoRA Fusion via Auto Masking at Test Time",
        "summary": "This paper proposes FreeFuse, a novel training-free approach for\nmulti-subject text-to-image generation through automatic fusion of multiple\nsubject LoRAs. In contrast to existing methods that either focus on\npre-inference LoRA weight merging or rely on segmentation models and complex\ntechniques like noise blending to isolate LoRA outputs, our key insight is that\ncontext-aware dynamic subject masks can be automatically derived from\ncross-attention layer weights. Mathematical analysis shows that directly\napplying these masks to LoRA outputs during inference well approximates the\ncase where the subject LoRA is integrated into the diffusion model and used\nindividually for the masked region. FreeFuse demonstrates superior practicality\nand efficiency as it requires no additional training, no modification to LoRAs,\nno auxiliary models, and no user-defined prompt templates or region\nspecifications. Alternatively, it only requires users to provide the LoRA\nactivation words for seamless integration into standard workflows. Extensive\nexperiments validate that FreeFuse outperforms existing approaches in both\ngeneration quality and usability under the multi-subject generation tasks. The\nproject page is at https://future-item.github.io/FreeFuse/",
        "url": "http://arxiv.org/abs/2510.23515v1",
        "published_date": "2025-10-27T16:54:08+00:00",
        "updated_date": "2025-10-27T16:54:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yaoli Liu",
            "Yao-Xiang Ding",
            "Kun Zhou"
        ],
        "ai_categories": [
            "LoRA",
            "Transformer",
            "GAN"
        ],
        "tldr": "FreeFuse proposes a training-free approach for multi-subject text-to-image generation by automatically fusing multiple subject LoRAs, outperforming existing methods in generation quality and usability.",
        "tldr_zh": "FreeFuse提出了一种无需训练的方法，通过自动融合多个主题LoRA，实现多主体文本到图像的生成，优于现有方法在生成质量和易用性方面表现。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Yesnt: Are Diffusion Relighting Models Ready for Capture Stage Compositing? A Hybrid Alternative to Bridge the Gap",
        "summary": "Volumetric video relighting is essential for bringing captured performances\ninto virtual worlds, but current approaches struggle to deliver temporally\nstable, production-ready results. Diffusion-based intrinsic decomposition\nmethods show promise for single frames, yet suffer from stochastic noise and\ninstability when extended to sequences, while video diffusion models remain\nconstrained by memory and scale. We propose a hybrid relighting framework that\ncombines diffusion-derived material priors with temporal regularization and\nphysically motivated rendering. Our method aggregates multiple stochastic\nestimates of per-frame material properties into temporally consistent shading\ncomponents, using optical-flow-guided regularization. For indirect effects such\nas shadows and reflections, we extract a mesh proxy from Gaussian Opacity\nFields and render it within a standard graphics pipeline. Experiments on real\nand synthetic captures show that this hybrid strategy achieves substantially\nmore stable relighting across sequences than diffusion-only baselines, while\nscaling beyond the clip lengths feasible for video diffusion. These results\nindicate that hybrid approaches, which balance learned priors with physically\ngrounded constraints, are a practical step toward production-ready volumetric\nvideo relighting.",
        "url": "http://arxiv.org/abs/2510.23494v1",
        "published_date": "2025-10-27T16:28:55+00:00",
        "updated_date": "2025-10-27T16:28:55+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Elisabeth Jüttner",
            "Leona Krath",
            "Stefan Korfhage",
            "Hannah Dröge",
            "Matthias B. Hullin",
            "Markus Plack"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer",
            "Other"
        ],
        "tldr": "The paper proposes a hybrid relighting framework combining material priors with temporal regularization and physically motivated rendering for stable volumetric video relighting.",
        "tldr_zh": "本文提出了一种混合照明框架，将材料先验与时间正则化和物理驱动的渲染结合起来，实现稳定的体积视频照明。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "T-REGS: Minimum Spanning Tree Regularization for Self-Supervised Learning",
        "summary": "Self-supervised learning (SSL) has emerged as a powerful paradigm for\nlearning representations without labeled data, often by enforcing invariance to\ninput transformations such as rotations or blurring. Recent studies have\nhighlighted two pivotal properties for effective representations: (i) avoiding\ndimensional collapse-where the learned features occupy only a low-dimensional\nsubspace, and (ii) enhancing uniformity of the induced distribution. In this\nwork, we introduce T-REGS, a simple regularization framework for SSL based on\nthe length of the Minimum Spanning Tree (MST) over the learned representation.\nWe provide theoretical analysis demonstrating that T-REGS simultaneously\nmitigates dimensional collapse and promotes distribution uniformity on\narbitrary compact Riemannian manifolds. Several experiments on synthetic data\nand on classical SSL benchmarks validate the effectiveness of our approach at\nenhancing representation quality.",
        "url": "http://arxiv.org/abs/2510.23484v1",
        "published_date": "2025-10-27T16:16:40+00:00",
        "updated_date": "2025-10-27T16:16:40+00:00",
        "categories": [
            "cs.LG",
            "cs.CG",
            "cs.CV"
        ],
        "authors": [
            "Julie Mordacq",
            "David Loiseaux",
            "Vicky Kalogeiton",
            "Steve Oudot"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "T-REGS is a regularization framework for Self-Supervised Learning that aims to prevent dimensional collapse and promote distribution uniformity in learned representations.",
        "tldr_zh": "T-REGS是一种用于自监督学习的正则化框架，旨在防止维度坍缩并促进学习表示的分布均匀性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "On the Faithfulness of Visual Thinking: Measurement and Enhancement",
        "summary": "Recent large vision-language models (LVLMs) can generate vision-text\nmultimodal chain-of-thought (MCoT) traces after reinforcement fine-tuning\n(RFT). However, we observe that the visual information incorporated in MCoT is\noften inaccurate, though still yield correct answers, indicating a lack of\nfaithfulness in the MCoT reasoning process. We attribute this unfaithfulness to\nthe RL reward in RFT, which solely incentivizes the format of interleaved\nvision-text cues, ie, it encourages the model to incorporate visual information\ninto its text reasoning steps without considering the correctness of the visual\ninformation. In this paper, we first probe the faithfulness of MCoT by\nmeasuring how much the prediction changes when its visual and textual thoughts\nare intervened. Surprisingly, the model's predictions remain nearly unchanged\nunder visual intervention but change significantly under textual intervention,\nindicating that the visual evidence is largely ignored. To further analyze\nvisual information, we introduce an automated LVLM-based evaluation metric that\nquantifies the faithfulness of visual cues from two perspectives: reliability\nand sufficiency. Our evaluation reveals that the visual information in current\nMCoT traces is simultaneously unreliable and insufficient. To address this\nissue, we propose a novel MCoT learning strategy termed Sufficient-Component\nCause Model (SCCM) learning. This approach encourages the MCoT to generate\nsufficient yet minimal visual components that are independently capable of\nleading to correct answers. We note that the proposed SCCM is annotation-free\nand compatible with various RFT for MCoT in a plug-and-play manner. Empirical\nresults demonstrate that SCCM consistently improves the visual faithfulness\nacross a suite of fine-grained perception and reasoning benchmarks. Code is\navailable at https://github.com/EugeneLiu01/Faithful_Thinking_with_Image.",
        "url": "http://arxiv.org/abs/2510.23482v1",
        "published_date": "2025-10-27T16:15:54+00:00",
        "updated_date": "2025-10-27T16:15:54+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zujing Liu",
            "Junwen Pan",
            "Qi She",
            "Yuan Gao",
            "Guisong Xia"
        ],
        "ai_categories": [
            "Multimodality",
            "Other"
        ],
        "tldr": "The paper evaluates the faithfulness of visual cues in vision-language models and proposes a new learning strategy to address the issue.",
        "tldr_zh": "本文评估视觉语言模型中视觉线索的忠实度，并提出了一种新的学习策略来解决这个问题。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MergeMix: A Unified Augmentation Paradigm for Visual and Multi-Modal Understanding",
        "summary": "Vision-language alignment in multi-modal large language models (MLLMs)\ntypically relies on supervised fine-tuning (SFT) or reinforcement learning\n(RL). SFT is stable and efficient but requires large-scale human annotations\nand cannot capture subtle preferences, while RL brings in a reward signal for\ntraining, but suffers from overhead and instability. These limitations\nhighlight a trade-off between scalability, robustness, and alignment quality.\nTo address this, we propose MergeMix, a training-time augmentation paradigm\nthat bridges SFT and RL. It first applies an attention-aware image mixing via\ntoken merge with more cluster representation and spatial context, and then\npresents a preference-driven training paradigm for MLLMs by building preference\npairs with mixed images and raw images, and optimizing via SimPO loss. As a\nmixup augmentation, MergeMix enhances attention consistency and efficiency,\nsurpassing other heuristic-based methods in classification. Extensive\nexperiments demonstrate that MergeMix achieves competitive accuracy with\nimproved efficiency, providing a scalable approach to preference alignment in\nclassification and MLLMs.",
        "url": "http://arxiv.org/abs/2510.23479v1",
        "published_date": "2025-10-27T16:12:40+00:00",
        "updated_date": "2025-10-27T16:12:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xin Jin",
            "Siyuan Li",
            "Siyong Jian",
            "Kai Yu",
            "Huan Wang"
        ],
        "ai_categories": [
            "Multimodality",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces MergeMix, a novel training-time augmentation paradigm for multi-modal large language models, aiming to improve alignment and efficiency in classification.",
        "tldr_zh": "该论文介绍了MergeMix，一种新颖的训练时间增强范式，用于多模态大语言模型，旨在提高分类中的对齐和效率。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UrbanIng-V2X: A Large-Scale Multi-Vehicle, Multi-Infrastructure Dataset Across Multiple Intersections for Cooperative Perception",
        "summary": "Recent cooperative perception datasets have played a crucial role in\nadvancing smart mobility applications by enabling information exchange between\nintelligent agents, helping to overcome challenges such as occlusions and\nimproving overall scene understanding. While some existing real-world datasets\nincorporate both vehicle-to-vehicle and vehicle-to-infrastructure interactions,\nthey are typically limited to a single intersection or a single vehicle. A\ncomprehensive perception dataset featuring multiple connected vehicles and\ninfrastructure sensors across several intersections remains unavailable,\nlimiting the benchmarking of algorithms in diverse traffic environments.\nConsequently, overfitting can occur, and models may demonstrate misleadingly\nhigh performance due to similar intersection layouts and traffic participant\nbehavior. To address this gap, we introduce UrbanIng-V2X, the first\nlarge-scale, multi-modal dataset supporting cooperative perception involving\nvehicles and infrastructure sensors deployed across three urban intersections\nin Ingolstadt, Germany. UrbanIng-V2X consists of 34 temporally aligned and\nspatially calibrated sensor sequences, each lasting 20 seconds. All sequences\ncontain recordings from one of three intersections, involving two vehicles and\nup to three infrastructure-mounted sensor poles operating in coordinated\nscenarios. In total, UrbanIng-V2X provides data from 12 vehicle-mounted RGB\ncameras, 2 vehicle LiDARs, 17 infrastructure thermal cameras, and 12\ninfrastructure LiDARs. All sequences are annotated at a frequency of 10 Hz with\n3D bounding boxes spanning 13 object classes, resulting in approximately 712k\nannotated instances across the dataset. We provide comprehensive evaluations\nusing state-of-the-art cooperative perception methods and publicly release the\ncodebase, dataset, HD map, and a digital twin of the complete data collection\nenvironment.",
        "url": "http://arxiv.org/abs/2510.23478v1",
        "published_date": "2025-10-27T16:12:12+00:00",
        "updated_date": "2025-10-27T16:12:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Karthikeyan Chandra Sekaran",
            "Markus Geisler",
            "Dominik Rößle",
            "Adithya Mohan",
            "Daniel Cremers",
            "Wolfgang Utschick",
            "Michael Botsch",
            "Werner Huber",
            "Torsten Schön"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper introduces UrbanIng-V2X, a large-scale dataset for cooperative perception involving vehicles and infrastructure sensors across multiple intersections, enabling benchmarking of algorithms in diverse traffic environments.",
        "tldr_zh": "本文介绍了UrbanIng-V2X，这是一个大规模数据集，用于涉及多个十字路口的车辆和基础设施传感器之间的协同感知，可以在不同交通环境中对算法进行基准测试。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences",
        "summary": "Reward models (RMs) play a critical role in aligning AI behaviors with human\npreferences, yet they face two fundamental challenges: (1) Modality Imbalance,\nwhere most RMs are mainly focused on text and image modalities, offering\nlimited support for video, audio, and other modalities; and (2) Preference\nRigidity, where training on fixed binary preference pairs fails to capture the\ncomplexity and diversity of personalized preferences. To address the above\nchallenges, we propose Omni-Reward, a step toward generalist omni-modal reward\nmodeling with support for free-form preferences, consisting of: (1) Evaluation:\nWe introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form\npreferences, covering nine tasks across five modalities including text, image,\nvideo, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal\npreference dataset comprising 248K general preference pairs and 69K\ninstruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We\npropose Omni-RewardModel, which includes both discriminative and generative\nRMs, and achieves strong performance on Omni-RewardBench as well as other\nwidely used reward modeling benchmarks.",
        "url": "http://arxiv.org/abs/2510.23451v1",
        "published_date": "2025-10-27T15:53:20+00:00",
        "updated_date": "2025-10-27T15:53:20+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Zhuoran Jin",
            "Hongbang Yuan",
            "Kejian Zhu",
            "Jiachun Li",
            "Pengfei Cao",
            "Yubo Chen",
            "Kang Liu",
            "Jun Zhao"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper proposes Omni-Reward, a generalist omni-modal reward modeling approach with free-form preferences to address challenges in modality imbalance and preference rigidity.",
        "tldr_zh": "本文提出了Omni-Reward，这是一个通用的全模态奖励建模方法，支持自由形式的偏好，以解决模态不平衡和偏好刚性等挑战。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "FRBNet: Revisiting Low-Light Vision through Frequency-Domain Radial Basis Network",
        "summary": "Low-light vision remains a fundamental challenge in computer vision due to\nsevere illumination degradation, which significantly affects the performance of\ndownstream tasks such as detection and segmentation. While recent\nstate-of-the-art methods have improved performance through invariant feature\nlearning modules, they still fall short due to incomplete modeling of low-light\nconditions. Therefore, we revisit low-light image formation and extend the\nclassical Lambertian model to better characterize low-light conditions. By\nshifting our analysis to the frequency domain, we theoretically prove that the\nfrequency-domain channel ratio can be leveraged to extract\nillumination-invariant features via a structured filtering process. We then\npropose a novel and end-to-end trainable module named \\textbf{F}requency-domain\n\\textbf{R}adial \\textbf{B}asis \\textbf{Net}work (\\textbf{FRBNet}), which\nintegrates the frequency-domain channel ratio operation with a learnable\nfrequency domain filter for the overall illumination-invariant feature\nenhancement. As a plug-and-play module, FRBNet can be integrated into existing\nnetworks for low-light downstream tasks without modifying loss functions.\nExtensive experiments across various downstream tasks demonstrate that FRBNet\nachieves superior performance, including +2.2 mAP for dark object detection and\n+2.9 mIoU for nighttime segmentation. Code is available at:\nhttps://github.com/Sing-Forevet/FRBNet.",
        "url": "http://arxiv.org/abs/2510.23444v1",
        "published_date": "2025-10-27T15:46:07+00:00",
        "updated_date": "2025-10-27T15:46:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Fangtong Sun",
            "Congyu Li",
            "Ke Yang",
            "Yuchen Pan",
            "Hanwen Yu",
            "Xichuan Zhang",
            "Yiying Li"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces FRBNet, a module for low-light vision that enhances illumination-invariant features, leading to superior performance in dark object detection and nighttime segmentation tasks.",
        "tldr_zh": "本文介绍了FRBNet，这是一个用于增强光照不变特征的模块，可在暗物体检测和夜间分割任务中实现卓越性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CURVETE: Curriculum Learning and Progressive Self-supervised Training for Medical Image Classification",
        "summary": "Identifying high-quality and easily accessible annotated samples poses a\nnotable challenge in medical image analysis. Transfer learning techniques,\nleveraging pre-training data, offer a flexible solution to this issue. However,\nthe impact of fine-tuning diminishes when the dataset exhibits an irregular\ndistribution between classes. This paper introduces a novel deep convolutional\nneural network, named Curriculum Learning and Progressive Self-supervised\nTraining (CURVETE). CURVETE addresses challenges related to limited samples,\nenhances model generalisability, and improves overall classification\nperformance. It achieves this by employing a curriculum learning strategy based\non the granularity of sample decomposition during the training of generic\nunlabelled samples. Moreover, CURVETE address the challenge of irregular class\ndistribution by incorporating a class decomposition approach in the downstream\ntask. The proposed method undergoes evaluation on three distinct medical image\ndatasets: brain tumour, digital knee x-ray, and Mini-DDSM datasets. We\ninvestigate the classification performance using a generic self-supervised\nsample decomposition approach with and without the curriculum learning\ncomponent in training the pretext task. Experimental results demonstrate that\nthe CURVETE model achieves superior performance on test sets with an accuracy\nof 96.60% on the brain tumour dataset, 75.60% on the digital knee x-ray\ndataset, and 93.35% on the Mini-DDSM dataset using the baseline ResNet-50.\nFurthermore, with the baseline DenseNet-121, it achieved accuracies of 95.77%,\n80.36%, and 93.22% on the brain tumour, digital knee x-ray, and Mini-DDSM\ndatasets, respectively, outperforming other training strategies.",
        "url": "http://arxiv.org/abs/2510.23442v1",
        "published_date": "2025-10-27T15:46:02+00:00",
        "updated_date": "2025-10-27T15:46:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Asmaa Abbas",
            "Mohamed Gaber",
            "Mohammed M. Abdelsamea"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper introduces a novel deep convolutional neural network named CURVETE for medical image classification, addressing challenges related to limited samples and irregular class distribution.",
        "tldr_zh": "该论文引入了一种名为CURVETE的新型深度卷积神经网络，用于医学图像分类，解决了有限样本和不规则类分布的挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Generalisable Foundation Models for 3D Brain MRI",
        "summary": "Foundation models in artificial intelligence (AI) are transforming medical\nimaging by enabling general-purpose feature learning from large-scale,\nunlabeled datasets. In this work, we introduce BrainFound, a self-supervised\nfoundation model for brain MRI, built by extending DINO-v2, a vision\ntransformer originally designed for 2D natural images. BrainFound adapts\nDINO-v2 to model full 3D brain anatomy by incorporating volumetric information\nfrom sequential MRI slices, moving beyond conventional single-slice paradigms.\nIt supports both single- and multimodal inputs, enabling a broad range of\ndownstream tasks, including disease detection and image segmentation, while\ngeneralising across varied imaging protocols and clinical scenarios. We show\nthat BrainFound consistently outperforms existing self-supervised pretraining\nstrategies and supervised baselines, particularly in label-scarce and\nmulti-contrast settings. By integrating information from diverse 3D MRI\nmodalities (e.g., T1, T2, FLAIR), it enhances diagnostic accuracy and reduces\ndependency on extensive expert annotations. This flexibility makes BrainFound a\nscalable and practical solution for 3D neuroimaging pipelines, with significant\npotential for clinical deployment and research innovation.",
        "url": "http://arxiv.org/abs/2510.23415v1",
        "published_date": "2025-10-27T15:19:46+00:00",
        "updated_date": "2025-10-27T15:19:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Moona Mazher",
            "Geoff J. M. Parker",
            "Daniel C. Alexander"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a self-supervised foundation model for 3D brain MRI that outperforms existing strategies in various scenarios, enhancing diagnostic accuracy and reducing dependency on expert annotations.",
        "tldr_zh": "该论文介绍了一种用于3D脑MRI的自监督基础模型，该模型在各种情况下的表现均优于现有策略，提高了诊断准确性并减少了对专家标注的依赖。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "VideoTG-R1: Boosting Video Temporal Grounding via Curriculum Reinforcement Learning on Reflected Boundary Annotations",
        "summary": "Video temporal grounding (VTG) aims to locate precise segments in videos\nbased on language queries, which is a fundamental challenge in video\nunderstanding. While recent Multimodal Large Language Models (MLLMs) have shown\npromise in tackling VTG through reinforcement learning (RL), they overlook the\nchallenges arising from both the quality and difficulty of training samples.\n(1) Partially annotated samples. Many samples contain relevant segments beyond\nthe annotated interval, introducing ambiguous supervision. (2) Hard-to-ground\nsamples. Samples with poor zero-shot performance produce consistently low and\nindistinguishable rewards during RL training, exhibiting no clear preference\namong multiple outputs and thus hindering learning efficiency. To address these\nchallenges, we propose VideoTG-R1, a novel curriculum RL framework with\nreflected boundary annotations, enabling data-efficient training. Specifically,\nwe propose a Boundary Reflection Agent that utilizes MLLMs to predict\nquery-relevant timestamps outside the annotated intervals, allowing us to\nidentify and filter out partially annotated samples, thereby reducing\nambiguity. Furthermore, we introduce a Difficulty Estimation Agent to assess\nthe training difficulty of each sample and design a curriculum RL strategy that\ndynamically masks the videos of hard-to-ground samples according to the\ntraining steps, easing the training difficulty and providing clearer\npreference. Experiments on the VTG and grounded VideoQA tasks demonstrate the\neffectiveness of our method. Remarkably, with only 10% of the training samples\nand 21% of the computational budget, VideoTG-R1 outperforms full-data\ncounterparts under both group relative policy optimization (GRPO) and\nsupervised fine-tuning (SFT). The code is available at\nhttps://github.com/ldong1111/VideoTG-R1.",
        "url": "http://arxiv.org/abs/2510.23397v1",
        "published_date": "2025-10-27T14:55:38+00:00",
        "updated_date": "2025-10-27T14:55:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lu Dong",
            "Haiyu Zhang",
            "Han Lin",
            "Ziang Yan",
            "Xiangyu Zeng",
            "Hongjie Zhang",
            "Yifei Huang",
            "Yi Wang",
            "Zhen-Hua Ling",
            "Limin Wang",
            "Yali Wang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces VideoTG-R1, a novel framework for video temporal grounding using reflected boundary annotations and curriculum reinforcement learning. It aims to address challenges like partially annotated samples and hard-to-ground samples for more efficient training and better performance in video understanding tasks.",
        "tldr_zh": "本文介绍了VideoTG-R1，一种利用反射边界标注和课程强化学习的视频时间定位框架。旨在解决部分注释样本和难以定位样本等挑战，以实现更高效的训练和更好的视频理解任务性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "An Efficient Remote Sensing Super Resolution Method Exploring Diffusion Priors and Multi-Modal Constraints for Crop Type Mapping",
        "summary": "Super resolution offers a way to harness medium even lowresolution but\nhistorically valuable remote sensing image archives. Generative models,\nespecially diffusion models, have recently been applied to remote sensing super\nresolution (RSSR), yet several challenges exist. First, diffusion models are\neffective but require expensive training from scratch resources and have slow\ninference speeds. Second, current methods have limited utilization of auxiliary\ninformation as real-world constraints to reconstruct scientifically realistic\nimages. Finally, most current methods lack evaluation on downstream tasks. In\nthis study, we present a efficient LSSR framework for RSSR, supported by a new\nmultimodal dataset of paired 30 m Landsat 8 and 10 m Sentinel 2 imagery. Built\non frozen pretrained Stable Diffusion, LSSR integrates crossmodal attention\nwith auxiliary knowledge (Digital Elevation Model, land cover, month) and\nSynthetic Aperture Radar guidance, enhanced by adapters and a tailored Fourier\nNDVI loss to balance spatial details and spectral fidelity. Extensive\nexperiments demonstrate that LSSR significantly improves crop boundary\ndelineation and recovery, achieving state-of-the-art performance with Peak\nSignal-to-Noise Ratio/Structural Similarity Index Measure of 32.63/0.84 (RGB)\nand 23.99/0.78 (IR), and the lowest NDVI Mean Squared Error (0.042), while\nmaintaining efficient inference (0.39 sec/image). Moreover, LSSR transfers\neffectively to NASA Harmonized Landsat and Sentinel (HLS) super resolution,\nyielding more reliable crop classification (F1: 0.86) than Sentinel-2 (F1:\n0.85). These results highlight the potential of RSSR to advance precision\nagriculture.",
        "url": "http://arxiv.org/abs/2510.23382v1",
        "published_date": "2025-10-27T14:34:52+00:00",
        "updated_date": "2025-10-27T14:34:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Songxi Yang",
            "Tang Sui",
            "Qunying Huang"
        ],
        "ai_categories": [
            "Diffusion",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces an efficient super resolution method for remote sensing images using diffusion priors and multi-modal constraints to improve crop type mapping, achieving state-of-the-art performance in crop boundary delineation and classification.",
        "tldr_zh": "本文介绍了一种利用扩散先验和多模态约束的高效遥感图像超分辨率方法，用于改善作物类型映射，在作物边界划分和分类方面表现出色。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PlanarTrack: A high-quality and challenging benchmark for large-scale planar object tracking",
        "summary": "Planar tracking has drawn increasing interest owing to its key roles in\nrobotics and augmented reality. Despite recent great advancement, further\ndevelopment of planar tracking, particularly in the deep learning era, is\nlargely limited compared to generic tracking due to the lack of large-scale\nplatforms. To mitigate this, we propose PlanarTrack, a large-scale high-quality\nand challenging benchmark for planar tracking. Specifically, PlanarTrack\nconsists of 1,150 sequences with over 733K frames, including 1,000 short-term\nand 150 new long-term videos, which enables comprehensive evaluation of short-\nand long-term tracking performance. All videos in PlanarTrack are recorded in\nunconstrained conditions from the wild, which makes PlanarTrack challenging but\nmore realistic for real-world applications. To ensure high-quality annotations,\neach video frame is manually annotated by four corner points with multi-round\nmeticulous inspection and refinement. To enhance target diversity of\nPlanarTrack, we only capture a unique target in one sequence, which is\ndifferent from existing benchmarks. To our best knowledge, PlanarTrack is by\nfar the largest and most diverse and challenging dataset dedicated to planar\ntracking. To understand performance of existing methods on PlanarTrack and to\nprovide a comparison for future research, we evaluate 10 representative planar\ntrackers with extensive comparison and in-depth analysis. Our evaluation\nreveals that, unsurprisingly, the top planar trackers heavily degrade on the\nchallenging PlanarTrack, which indicates more efforts are required for\nimproving planar tracking. Our data and results will be released at\nhttps://github.com/HengLan/PlanarTrack",
        "url": "http://arxiv.org/abs/2510.23368v1",
        "published_date": "2025-10-27T14:18:13+00:00",
        "updated_date": "2025-10-27T14:18:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yifan Jiao",
            "Xinran Liu",
            "Xiaoqiong Liu",
            "Xiaohui Yuan",
            "Heng Fan",
            "Libo Zhang"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "PlanarTrack proposes a new benchmark for planar object tracking with a large-scale dataset for comprehensive evaluation. Existing top planar trackers perform poorly on this challenging benchmark, indicating a need for improvement in planar tracking.",
        "tldr_zh": "PlanarTrack提出了一个新的基准测试，用于评估平面物体追踪。现有的顶级平面追踪器在这一具有挑战性的基准上表现不佳，这表明需要改进平面追踪。",
        "relevance_score": 1,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Multitask Multimodal Self-Supervised Learning for Medical Images",
        "summary": "This thesis works to address a pivotal challenge in medical image analysis:\nthe reliance on extensive labeled datasets, which are often limited due to the\nneed for expert annotation and constrained by privacy and legal issues. By\nfocusing on the development of self-supervised learning techniques and domain\nadaptation methods, this research aims to circumvent these limitations,\npresenting a novel approach to enhance the utility and efficacy of deep\nlearning in medical imaging.\n  Central to this thesis is the development of the Medformer, an innovative\nneural network architecture designed for multitask learning and deep domain\nadaptation. This model is adept at pre-training on diverse medical image\ndatasets, handling varying sizes and modalities, and is equipped with a dynamic\ninput-output adaptation mechanism. This enables efficient processing and\nintegration of a wide range of medical image types, from 2D X-rays to complex\n3D MRIs, thus mitigating the dependency on large labeled datasets.\n  Further, the thesis explores the current state of self-supervised learning in\nmedical imaging. It introduces novel pretext tasks that are capable of\nextracting meaningful information from unlabeled data, significantly advancing\nthe model's interpretative abilities. This approach is validated through\nrigorous experimentation, including the use of the MedMNIST dataset,\ndemonstrating the model's proficiency in learning generalized features\napplicable to various downstream tasks.\n  In summary, this thesis contributes to the advancement of medical image\nanalysis by offering a scalable, adaptable framework that reduces reliance on\nlabeled data. It paves the way for more accurate, efficient diagnostic tools in\nhealthcare, signifying a major step forward in the application of deep learning\nin medical imaging.",
        "url": "http://arxiv.org/abs/2510.23325v1",
        "published_date": "2025-10-27T13:42:16+00:00",
        "updated_date": "2025-10-27T13:42:16+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Cristian Simionescu"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper proposes a multitask multimodal self-supervised learning approach for medical images, aiming to reduce reliance on labeled datasets and improve deep learning in medical imaging.",
        "tldr_zh": "本文提出了一种用于医学图像的多任务多模态自监督学习方法，旨在减少对标记数据集的依赖，并改善医学成像中的深度学习。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "ReconViaGen: Towards Accurate Multi-view 3D Object Reconstruction via Generation",
        "summary": "Existing multi-view 3D object reconstruction methods heavily rely on\nsufficient overlap between input views, where occlusions and sparse coverage in\npractice frequently yield severe reconstruction incompleteness. Recent\nadvancements in diffusion-based 3D generative techniques offer the potential to\naddress these limitations by leveraging learned generative priors to\nhallucinate invisible parts of objects, thereby generating plausible 3D\nstructures. However, the stochastic nature of the inference process limits the\naccuracy and reliability of generation results, preventing existing\nreconstruction frameworks from integrating such 3D generative priors. In this\nwork, we comprehensively analyze the reasons why diffusion-based 3D generative\nmethods fail to achieve high consistency, including (a) the insufficiency in\nconstructing and leveraging cross-view connections when extracting multi-view\nimage features as conditions, and (b) the poor controllability of iterative\ndenoising during local detail generation, which easily leads to plausible but\ninconsistent fine geometric and texture details with inputs. Accordingly, we\npropose ReconViaGen to innovatively integrate reconstruction priors into the\ngenerative framework and devise several strategies that effectively address\nthese issues. Extensive experiments demonstrate that our ReconViaGen can\nreconstruct complete and accurate 3D models consistent with input views in both\nglobal structure and local details.Project page:\nhttps://jiahao620.github.io/reconviagen.",
        "url": "http://arxiv.org/abs/2510.23306v1",
        "published_date": "2025-10-27T13:15:06+00:00",
        "updated_date": "2025-10-27T13:15:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jiahao Chang",
            "Chongjie Ye",
            "Yushuang Wu",
            "Yuantao Chen",
            "Yidan Zhang",
            "Zhongjin Luo",
            "Chenghong Li",
            "Yihao Zhi",
            "Xiaoguang Han"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper proposes ReconViaGen, a method that integrates reconstruction priors into generative frameworks to address limitations of existing multi-view 3D object reconstruction methods.",
        "tldr_zh": "本文提出了ReconViaGen方法，将重建先验融合到生成框架中，以解决现有多视角3D物体重建方法的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MMSD3.0: A Multi-Image Benchmark for Real-World Multimodal Sarcasm Detection",
        "summary": "Despite progress in multimodal sarcasm detection, existing datasets and\nmethods predominantly focus on single-image scenarios, overlooking potential\nsemantic and affective relations across multiple images. This leaves a gap in\nmodeling cases where sarcasm is triggered by multi-image cues in real-world\nsettings. To bridge this gap, we introduce MMSD3.0, a new benchmark composed\nentirely of multi-image samples curated from tweets and Amazon reviews. We\nfurther propose the Cross-Image Reasoning Model (CIRM), which performs targeted\ncross-image sequence modeling to capture latent inter-image connections. In\naddition, we introduce a relevance-guided, fine-grained cross-modal fusion\nmechanism based on text-image correspondence to reduce information loss during\nintegration. We establish a comprehensive suite of strong and representative\nbaselines and conduct extensive experiments, showing that MMSD3.0 is an\neffective and reliable benchmark that better reflects real-world conditions.\nMoreover, CIRM demonstrates state-of-the-art performance across MMSD, MMSD2.0\nand MMSD3.0, validating its effectiveness in both single-image and multi-image\nscenarios.",
        "url": "http://arxiv.org/abs/2510.23299v1",
        "published_date": "2025-10-27T13:05:27+00:00",
        "updated_date": "2025-10-27T13:05:27+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Haochen Zhao",
            "Yuyao Kong",
            "Yongxiu Xu",
            "Gaopeng Gou",
            "Hongbo Xu",
            "Yubin Wang",
            "Haoliang Zhang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Transformer"
        ],
        "tldr": "MMSD3.0 introduces a new benchmark for multimodal sarcasm detection using multiple images and a Cross-Image Reasoning Model (CIRM) for improved performance.",
        "tldr_zh": "MMSD3.0引入了一个新的基准，用于利用多个图像进行多模态讽刺检测，并提出了一种交叉图像推理模型（CIRM），以提高性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Adaptive Stochastic Coefficients for Accelerating Diffusion Sampling",
        "summary": "Diffusion-based generative processes, formulated as differential equation\nsolving, frequently balance computational speed with sample quality. Our\ntheoretical investigation of ODE- and SDE-based solvers reveals complementary\nweaknesses: ODE solvers accumulate irreducible gradient error along\ndeterministic trajectories, while SDE methods suffer from amplified\ndiscretization errors when the step budget is limited. Building upon this\ninsight, we introduce AdaSDE, a novel single-step SDE solver that aims to unify\nthe efficiency of ODEs with the error resilience of SDEs. Specifically, we\nintroduce a single per-step learnable coefficient, estimated via lightweight\ndistillation, which dynamically regulates the error correction strength to\naccelerate diffusion sampling. Notably, our framework can be integrated with\nexisting solvers to enhance their capabilities. Extensive experiments\ndemonstrate state-of-the-art performance: at 5 NFE, AdaSDE achieves FID scores\nof 4.18 on CIFAR-10, 8.05 on FFHQ and 6.96 on LSUN Bedroom. Codes are available\nin https://github.com/WLU-wry02/AdaSDE.",
        "url": "http://arxiv.org/abs/2510.23285v1",
        "published_date": "2025-10-27T12:53:48+00:00",
        "updated_date": "2025-10-27T12:53:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruoyu Wang",
            "Beier Zhu",
            "Junzhi Li",
            "Liangyu Yuan",
            "Chi Zhang"
        ],
        "ai_categories": [
            "AIGC",
            "Diffusion",
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces AdaSDE, a novel single-step SDE solver that aims to combine the efficiency of ODEs with the error resilience of SDEs for accelerating diffusion sampling.",
        "tldr_zh": "本文引入AdaSDE，一种新型的单步SDE求解器，旨在将ODE的效率与SDE的误差鲁棒性相结合，加速扩散采样。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "hYOLO Model: Enhancing Object Classification with Hierarchical Context in YOLOv8",
        "summary": "Current convolution neural network (CNN) classification methods are\npredominantly focused on flat classification which aims solely to identify a\nspecified object within an image. However, real-world objects often possess a\nnatural hierarchical organization that can significantly help classification\ntasks. Capturing the presence of relations between objects enables better\ncontextual understanding as well as control over the severity of mistakes.\nConsidering these aspects, this paper proposes an end-to-end hierarchical model\nfor image detection and classification built upon the YOLO model family. A\nnovel hierarchical architecture, a modified loss function, and a performance\nmetric tailored to the hierarchical nature of the model are introduced. The\nproposed model is trained and evaluated on two different hierarchical\ncategorizations of the same dataset: a systematic categorization that\ndisregards visual similarities between objects and a categorization accounting\nfor common visual characteristics across classes. The results illustrate how\nthe suggested methodology addresses the inherent hierarchical structure present\nin real-world objects, which conventional flat classification algorithms often\noverlook.",
        "url": "http://arxiv.org/abs/2510.23278v1",
        "published_date": "2025-10-27T12:39:50+00:00",
        "updated_date": "2025-10-27T12:39:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Veska Tsenkova",
            "Peter Stanchev",
            "Daniel Petrov",
            "Deyan Lazarov"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a hierarchical model for image detection and classification using the YOLO model family, focusing on capturing relations between objects in a hierarchical manner.",
        "tldr_zh": "本文提出了一种利用YOLO模型家族进行图像检测和分类的分层模型，重点是以分层方式捕捉对象之间的关系。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Video Is Not Worth a Thousand Words",
        "summary": "As we become increasingly dependent on vision language models (VLMs) to\nanswer questions about the world around us, there is a significant amount of\nresearch devoted to increasing both the difficulty of video question answering\n(VQA) datasets, and the context lengths of the models that they evaluate. The\nreliance on large language models as backbones has lead to concerns about\npotential text dominance, and the exploration of interactions between\nmodalities is underdeveloped. How do we measure whether we're heading in the\nright direction, with the complexity that multi-modal models introduce? We\npropose a joint method of computing both feature attributions and modality\nscores based on Shapley values, where both the features and modalities are\narbitrarily definable. Using these metrics, we compare $6$ VLM models of\nvarying context lengths on $4$ representative datasets, focusing on\nmultiple-choice VQA. In particular, we consider video frames and whole textual\nelements as equal features in the hierarchy, and the multiple-choice VQA task\nas an interaction between three modalities: video, question and answer. Our\nresults demonstrate a dependence on text and show that the multiple-choice VQA\ntask devolves into a model's ability to ignore distractors. Code available at\nhttps://github.com/sjpollard/a-video-is-not-worth-a-thousand-words.",
        "url": "http://arxiv.org/abs/2510.23253v1",
        "published_date": "2025-10-27T12:15:02+00:00",
        "updated_date": "2025-10-27T12:15:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sam Pollard",
            "Michael Wray"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a method to evaluate the effectiveness of vision language models in video question answering tasks, highlighting the importance of multi-modal interactions.",
        "tldr_zh": "本文提出了一种评估视觉语言模型在视频问答任务中有效性的方法，强调了多模态交互的重要性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Autoregressive Styled Text Image Generation, but Make it Reliable",
        "summary": "Generating faithful and readable styled text images (especially for Styled\nHandwritten Text generation - HTG) is an open problem with several possible\napplications across graphic design, document understanding, and image editing.\nA lot of research effort in this task is dedicated to developing strategies\nthat reproduce the stylistic characteristics of a given writer, with promising\nresults in terms of style fidelity and generalization achieved by the recently\nproposed Autoregressive Transformer paradigm for HTG. However, this method\nrequires additional inputs, lacks a proper stop mechanism, and might end up in\nrepetition loops, generating visual artifacts. In this work, we rethink the\nautoregressive formulation by framing HTG as a multimodal prompt-conditioned\ngeneration task, and tackle the content controllability issues by introducing\nspecial textual input tokens for better alignment with the visual ones.\nMoreover, we devise a Classifier-Free-Guidance-based strategy for our\nautoregressive model. Through extensive experimental validation, we demonstrate\nthat our approach, dubbed Eruku, compared to previous solutions requires fewer\ninputs, generalizes better to unseen styles, and follows more faithfully the\ntextual prompt, improving content adherence.",
        "url": "http://arxiv.org/abs/2510.23240v1",
        "published_date": "2025-10-27T11:54:23+00:00",
        "updated_date": "2025-10-27T11:54:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Carmine Zaccagnino",
            "Fabio Quattrini",
            "Vittorio Pippi",
            "Silvia Cascianelli",
            "Alessio Tonioni",
            "Rita Cucchiara"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Multimodality"
        ],
        "tldr": "The paper introduces a new method, Eruku, for generating faithful and readable styled text images using a multimodal prompt-conditioned generation approach, improving content adherence and generalization to unseen styles.",
        "tldr_zh": "该论文提出了一种新方法，Eruku，使用多模态提示条件生成方法生成忠实而可读的样式文本图像，改进了内容的依从性和对未知风格的泛化。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Through the Lens: Benchmarking Deepfake Detectors Against Moiré-Induced Distortions",
        "summary": "Deepfake detection remains a pressing challenge, particularly in real-world\nsettings where smartphone-captured media from digital screens often introduces\nMoir\\'e artifacts that can distort detection outcomes. This study\nsystematically evaluates state-of-the-art (SOTA) deepfake detectors on\nMoir\\'e-affected videos, an issue that has received little attention. We\ncollected a dataset of 12,832 videos, spanning 35.64 hours, from the Celeb-DF,\nDFD, DFDC, UADFV, and FF++ datasets, capturing footage under diverse real-world\nconditions, including varying screens, smartphones, lighting setups, and camera\nangles. To further examine the influence of Moir\\'e patterns on deepfake\ndetection, we conducted additional experiments using our DeepMoir\\'eFake,\nreferred to as (DMF) dataset and two synthetic Moir\\'e generation techniques.\nAcross 15 top-performing detectors, our results show that Moir\\'e artifacts\ndegrade performance by as much as 25.4%, while synthetically generated Moir\\'e\npatterns lead to a 21.4% drop in accuracy. Surprisingly, demoir\\'eing methods,\nintended as a mitigation approach, instead worsened the problem, reducing\naccuracy by up to 17.2%. These findings underscore the urgent need for\ndetection models that can robustly handle Moir\\'e distortions alongside other\nrealworld challenges, such as compression, sharpening, and blurring. By\nintroducing the DMF dataset, we aim to drive future research toward closing the\ngap between controlled experiments and practical deepfake detection.",
        "url": "http://arxiv.org/abs/2510.23225v1",
        "published_date": "2025-10-27T11:23:04+00:00",
        "updated_date": "2025-10-27T11:23:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Razaib Tariq",
            "Minji Heo",
            "Simon S. Woo",
            "Shahroz Tariq"
        ],
        "ai_categories": [
            "Dataset",
            "GAN"
        ],
        "tldr": "This paper evaluates the performance of deepfake detectors against Moiré-induced distortions in real-world settings, highlighting the challenges and the need for robust detection models.",
        "tldr_zh": "本文评估了深度伪造检测器在现实世界环境中对抗Moiré诱发的失真的性能，突出了挑战和对稳健检测模型的需求。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Accurate and Scalable Multimodal Pathology Retrieval via Attentive Vision-Language Alignment",
        "summary": "The rapid digitization of histopathology slides has opened up new\npossibilities for computational tools in clinical and research workflows. Among\nthese, content-based slide retrieval stands out, enabling pathologists to\nidentify morphologically and semantically similar cases, thereby supporting\nprecise diagnoses, enhancing consistency across observers, and assisting\nexample-based education. However, effective retrieval of whole slide images\n(WSIs) remains challenging due to their gigapixel scale and the difficulty of\ncapturing subtle semantic differences amid abundant irrelevant content. To\novercome these challenges, we present PathSearch, a retrieval framework that\nunifies fine-grained attentive mosaic representations with global-wise slide\nembeddings aligned through vision-language contrastive learning. Trained on a\ncorpus of 6,926 slide-report pairs, PathSearch captures both fine-grained\nmorphological cues and high-level semantic patterns to enable accurate and\nflexible retrieval. The framework supports two key functionalities: (1)\nmosaic-based image-to-image retrieval, ensuring accurate and efficient slide\nresearch; and (2) multi-modal retrieval, where text queries can directly\nretrieve relevant slides. PathSearch was rigorously evaluated on four public\npathology datasets and three in-house cohorts, covering tasks including\nanatomical site retrieval, tumor subtyping, tumor vs. non-tumor discrimination,\nand grading across diverse organs such as breast, lung, kidney, liver, and\nstomach. External results show that PathSearch outperforms traditional\nimage-to-image retrieval frameworks. A multi-center reader study further\ndemonstrates that PathSearch improves diagnostic accuracy, boosts confidence,\nand enhances inter-observer agreement among pathologists in real clinical\nscenarios. These results establish PathSearch as a scalable and generalizable\nretrieval solution for digital pathology.",
        "url": "http://arxiv.org/abs/2510.23224v1",
        "published_date": "2025-10-27T11:22:28+00:00",
        "updated_date": "2025-10-27T11:22:28+00:00",
        "categories": [
            "cs.CV",
            "cs.IR"
        ],
        "authors": [
            "Hongyi Wang",
            "Zhengjie Zhu",
            "Jiabo Ma",
            "Fang Wang",
            "Yue Shi",
            "Bo Luo",
            "Jili Wang",
            "Qiuyu Cai",
            "Xiuming Zhang",
            "Yen-Wei Chen",
            "Lanfen Lin",
            "Hao Chen"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces PathSearch, a framework for accurate and scalable multimodal pathology retrieval using vision-language alignment, showing improved diagnostic accuracy and inter-observer agreement among pathologists.",
        "tldr_zh": "本文介绍了PathSearch，这是一个利用视觉语言对齐进行准确和可扩展的多模式病理学检索的框架，显示在真实临床环境中提高了诊断准确性和观察者间一致性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "VR-Drive: Viewpoint-Robust End-to-End Driving with Feed-Forward 3D Gaussian Splatting",
        "summary": "End-to-end autonomous driving (E2E-AD) has emerged as a promising paradigm\nthat unifies perception, prediction, and planning into a holistic, data-driven\nframework. However, achieving robustness to varying camera viewpoints, a common\nreal-world challenge due to diverse vehicle configurations, remains an open\nproblem. In this work, we propose VR-Drive, a novel E2E-AD framework that\naddresses viewpoint generalization by jointly learning 3D scene reconstruction\nas an auxiliary task to enable planning-aware view synthesis. Unlike prior\nscene-specific synthesis approaches, VR-Drive adopts a feed-forward inference\nstrategy that supports online training-time augmentation from sparse views\nwithout additional annotations. To further improve viewpoint consistency, we\nintroduce a viewpoint-mixed memory bank that facilitates temporal interaction\nacross multiple viewpoints and a viewpoint-consistent distillation strategy\nthat transfers knowledge from original to synthesized views. Trained in a fully\nend-to-end manner, VR-Drive effectively mitigates synthesis-induced noise and\nimproves planning under viewpoint shifts. In addition, we release a new\nbenchmark dataset to evaluate E2E-AD performance under novel camera viewpoints,\nenabling comprehensive analysis. Our results demonstrate that VR-Drive is a\nscalable and robust solution for the real-world deployment of end-to-end\nautonomous driving systems.",
        "url": "http://arxiv.org/abs/2510.23205v1",
        "published_date": "2025-10-27T10:49:39+00:00",
        "updated_date": "2025-10-27T10:49:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hoonhee Cho",
            "Jae-Young Kang",
            "Giwon Lee",
            "Hyemin Yang",
            "Heejun Park",
            "Seokwoo Jung",
            "Kuk-Jin Yoon"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces VR-Drive, an end-to-end autonomous driving framework that addresses viewpoint generalization using 3D scene reconstruction and planning-aware view synthesis.",
        "tldr_zh": "本文介绍了VR-Drive，一个通过3D场景重建和规划感知视图合成来解决视角泛化问题的端到端自动驾驶框架。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Evaluation of Vision-LLMs in Surveillance Video",
        "summary": "The widespread use of cameras in our society has created an overwhelming\namount of video data, far exceeding the capacity for human monitoring. This\npresents a critical challenge for public safety and security, as the timely\ndetection of anomalous or criminal events is crucial for effective response and\nprevention. The ability for an embodied agent to recognize unexpected events is\nfundamentally tied to its capacity for spatial reasoning. This paper\ninvestigates the spatial reasoning of vision-language models (VLMs) by framing\nanomalous action recognition as a zero-shot, language-grounded task, addressing\nthe embodied perception challenge of interpreting dynamic 3D scenes from sparse\n2D video. Specifically, we investigate whether small, pre-trained vision--LLMs\ncan act as spatially-grounded, zero-shot anomaly detectors by converting video\ninto text descriptions and scoring labels via textual entailment. We evaluate\nfour open models on UCF-Crime and RWF-2000 under prompting and\nprivacy-preserving conditions. Few-shot exemplars can improve accuracy for some\nmodels, but may increase false positives, and privacy filters -- especially\nfull-body GAN transforms -- introduce inconsistencies that degrade accuracy.\nThese results chart where current vision--LLMs succeed (simple, spatially\nsalient events) and where they falter (noisy spatial cues, identity\nobfuscation). Looking forward, we outline concrete paths to strengthen spatial\ngrounding without task-specific training: structure-aware prompts, lightweight\nspatial memory across clips, scene-graph or 3D-pose priors during description,\nand privacy methods that preserve action-relevant geometry. This positions\nzero-shot, language-grounded pipelines as adaptable building blocks for\nembodied, real-world video understanding. Our implementation for evaluating\nVLMs is publicly available at:\nhttps://github.com/pascalbenschopTU/VLLM_AnomalyRecognition",
        "url": "http://arxiv.org/abs/2510.23190v1",
        "published_date": "2025-10-27T10:27:02+00:00",
        "updated_date": "2025-10-27T10:27:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pascal Benschop",
            "Cristian Meo",
            "Justin Dauwels",
            "Jelte P. Mense"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper evaluates the use of vision-language models for anomaly detection in surveillance videos, highlighting successes and challenges in spatial reasoning and privacy-preserving conditions.",
        "tldr_zh": "该论文评估了在监控视频中使用视觉-语言模型进行异常检测，突出了在空间推理和隐私保护条件下取得的成功和挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Finding 3D Scene Analogies with Multimodal Foundation Models",
        "summary": "Connecting current observations with prior experiences helps robots adapt and\nplan in new, unseen 3D environments. Recently, 3D scene analogies have been\nproposed to connect two 3D scenes, which are smooth maps that align scene\nregions with common spatial relationships. These maps enable detailed transfer\nof trajectories or waypoints, potentially supporting demonstration transfer for\nimitation learning or task plan transfer across scenes. However, existing\nmethods for the task require additional training and fixed object vocabularies.\nIn this work, we propose to use multimodal foundation models for finding 3D\nscene analogies in a zero-shot, open-vocabulary setting. Central to our\napproach is a hybrid neural representation of scenes that consists of a sparse\ngraph based on vision-language model features and a feature field derived from\n3D shape foundation models. 3D scene analogies are then found in a\ncoarse-to-fine manner, by first aligning the graph and refining the\ncorrespondence with feature fields. Our method can establish accurate\ncorrespondences between complex scenes, and we showcase applications in\ntrajectory and waypoint transfer.",
        "url": "http://arxiv.org/abs/2510.23184v1",
        "published_date": "2025-10-27T10:23:31+00:00",
        "updated_date": "2025-10-27T10:23:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junho Kim",
            "Young Min Kim"
        ],
        "ai_categories": [
            "Multimodality"
        ],
        "tldr": "The paper proposes a method using multimodal foundation models to find 3D scene analogies in a zero-shot, open-vocabulary setting to support trajectory and waypoint transfer across scenes.",
        "tldr_zh": "本文提出了一种使用多模态基础模型在零样本、开放词汇设置中找到3D场景类比的方法，支持跨场景的轨迹和航点转移。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AG-Fusion: adaptive gated multimodal fusion for 3d object detection in complex scenes",
        "summary": "Multimodal camera-LiDAR fusion technology has found extensive application in\n3D object detection, demonstrating encouraging performance. However, existing\nmethods exhibit significant performance degradation in challenging scenarios\ncharacterized by sensor degradation or environmental disturbances. We propose a\nnovel Adaptive Gated Fusion (AG-Fusion) approach that selectively integrates\ncross-modal knowledge by identifying reliable patterns for robust detection in\ncomplex scenes. Specifically, we first project features from each modality into\na unified BEV space and enhance them using a window-based attention mechanism.\nSubsequently, an adaptive gated fusion module based on cross-modal attention is\ndesigned to integrate these features into reliable BEV representations robust\nto challenging environments. Furthermore, we construct a new dataset named\nExcavator3D (E3D) focusing on challenging excavator operation scenarios to\nbenchmark performance in complex conditions. Our method not only achieves\ncompetitive performance on the standard KITTI dataset with 93.92% accuracy, but\nalso significantly outperforms the baseline by 24.88% on the challenging E3D\ndataset, demonstrating superior robustness to unreliable modal information in\ncomplex industrial scenes.",
        "url": "http://arxiv.org/abs/2510.23151v1",
        "published_date": "2025-10-27T09:26:27+00:00",
        "updated_date": "2025-10-27T09:26:27+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Sixian Liu",
            "Chen Xu",
            "Qiang Wang",
            "Donghai Shi",
            "Yiwen Li"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces AG-Fusion, an adaptive gated multimodal fusion approach for 3D object detection in challenging environments, achieving superior performance in complex industrial scenes.",
        "tldr_zh": "该论文提出了AG-Fusion，一种自适应门控多模态融合方法，用于在具有挑战性的环境中进行3D目标检测，在复杂工业场景中表现出卓越的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Fast Voxel-Wise Kinetic Modeling in Dynamic PET using a Physics-Informed CycleGAN",
        "summary": "Tracer kinetic modeling serves a vital role in diagnosis, treatment planning,\ntracer development and oncology, but burdens practitioners with complex and\ninvasive arterial input function estimation (AIF). We adopt a physics-informed\nCycleGAN showing promise in DCE-MRI quantification to dynamic PET\nquantification. Our experiments demonstrate sound AIF predictions and parameter\nmaps closely resembling the reference.",
        "url": "http://arxiv.org/abs/2510.23140v1",
        "published_date": "2025-10-27T09:17:02+00:00",
        "updated_date": "2025-10-27T09:17:02+00:00",
        "categories": [
            "cs.CV",
            "q-bio.OT"
        ],
        "authors": [
            "Christian Salomonsen",
            "Samuel Kuttner",
            "Michael Kampffmeyer",
            "Robert Jenssen",
            "Kristoffer Wickstrøm",
            "Jong Chul Ye",
            "Elisabeth Wetzer"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a method using a physics-informed CycleGAN to predict arterial input function for dynamic PET quantification with promising results.",
        "tldr_zh": "本文介绍了一种使用物理信息的CycleGAN方法来预测动态PET定量的动脉输入功能，取得了令人满意的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Residual Diffusion Bridge Model for Image Restoration",
        "summary": "Diffusion bridge models establish probabilistic paths between arbitrary\npaired distributions and exhibit great potential for universal image\nrestoration. Most existing methods merely treat them as simple variants of\nstochastic interpolants, lacking a unified analytical perspective. Besides,\nthey indiscriminately reconstruct images through global noise injection and\nremoval, inevitably distorting undegraded regions due to imperfect\nreconstruction. To address these challenges, we propose the Residual Diffusion\nBridge Model (RDBM). Specifically, we theoretically reformulate the stochastic\ndifferential equations of generalized diffusion bridge and derive the\nanalytical formulas of its forward and reverse processes. Crucially, we\nleverage the residuals from given distributions to modulate the noise injection\nand removal, enabling adaptive restoration of degraded regions while preserving\nintact others. Moreover, we unravel the fundamental mathematical essence of\nexisting bridge models, all of which are special cases of RDBM and empirically\ndemonstrate the optimality of our proposed models. Extensive experiments are\nconducted to demonstrate the state-of-the-art performance of our method both\nqualitatively and quantitatively across diverse image restoration tasks. Code\nis publicly available at https://github.com/MiliLab/RDBM.",
        "url": "http://arxiv.org/abs/2510.23116v1",
        "published_date": "2025-10-27T08:35:49+00:00",
        "updated_date": "2025-10-27T08:35:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hebaixu Wang",
            "Jing Zhang",
            "Haoyang Chen",
            "Haonan Guo",
            "Di Wang",
            "Jiayi Ma",
            "Bo Du"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces the Residual Diffusion Bridge Model for image restoration, which adaptively restores degraded regions while preserving undegraded ones.",
        "tldr_zh": "该论文提出了一种残余扩散桥模型，用于图像恢复，可以自适应地恢复受损区域而保留未受损的区域。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Revisiting Multimodal Positional Encoding in Vision-Language Models",
        "summary": "Multimodal position encoding is essential for vision-language models, yet\nthere has been little systematic investigation into multimodal position\nencoding. We conduct a comprehensive analysis of multimodal Rotary Positional\nEmbedding (RoPE) by examining its two core components: position design and\nfrequency allocation. Through extensive experiments, we identify three key\nguidelines: positional coherence, full frequency utilization, and preservation\nof textual priors-ensuring unambiguous layout, rich representation, and\nfaithful transfer from the pre-trained LLM. Based on these insights, we propose\nMulti-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), two simple and\nplug-and-play variants that require no architectural changes. Our methods\nconsistently outperform existing approaches across diverse benchmarks, with\nsignificant improvements in both general and fine-grained multimodal\nunderstanding. Code will be avaliable at\nhttps://github.com/JJJYmmm/Multimodal-RoPEs.",
        "url": "http://arxiv.org/abs/2510.23095v1",
        "published_date": "2025-10-27T08:00:46+00:00",
        "updated_date": "2025-10-27T08:00:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jie Huang",
            "Xuejing Liu",
            "Sibo Song",
            "Ruibing Hou",
            "Hong Chang",
            "Junyang Lin",
            "Shuai Bai"
        ],
        "ai_categories": [
            "Multimodality",
            "LoRA",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper investigates multimodal positional encoding in vision-language models, proposing Multi-Head RoPE and MRoPE-Interleave methods that outperform existing approaches in multimodal understanding.",
        "tldr_zh": "本文研究了视觉-语言模型中的多模态位置编码，提出了Multi-Head RoPE和MRoPE-Interleave方法，这些方法在多模态理解方面优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Nested AutoRegressive Models",
        "summary": "AutoRegressive (AR) models have demonstrated competitive performance in image\ngeneration, achieving results comparable to those of diffusion models. However,\ntheir token-by-token image generation mechanism remains computationally\nintensive and existing solutions such as VAR often lead to limited sample\ndiversity. In this work, we propose a Nested AutoRegressive~(NestAR) model,\nwhich proposes nested AutoRegressive architectures in generating images. NestAR\ndesigns multi-scale modules in a hierarchical order. These different scaled\nmodules are constructed in an AR architecture, where one larger-scale module is\nconditioned on outputs from its previous smaller-scale module. Within each\nmodule, NestAR uses another AR structure to generate ``patches'' of tokens. The\nproposed nested AR architecture reduces the overall complexity from\n$\\mathcal{O}(n)$ to $\\mathcal{O}(\\log n)$ in generating $n$ image tokens, as\nwell as increases image diversities. NestAR further incorporates flow matching\nloss to use continuous tokens, and develops objectives to coordinate these\nmulti-scale modules in model training. NestAR achieves competitive image\ngeneration performance while significantly lowering computational cost.",
        "url": "http://arxiv.org/abs/2510.23028v1",
        "published_date": "2025-10-27T05:49:02+00:00",
        "updated_date": "2025-10-27T05:49:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hongyu Wu",
            "Xuhui Fan",
            "Zhangkai Wu",
            "Longbing Cao"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper introduces Nested AutoRegressive (NestAR) models for image generation, reducing computational complexity and increasing sample diversity.",
        "tldr_zh": "本文介绍了Nested AutoRegressive（NestAR）模型，用于图像生成，降低计算复杂性并增加样本多样性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniAIDet: A Unified and Universal Benchmark for AI-Generated Image Content Detection and Localization",
        "summary": "With the rapid proliferation of image generative models, the authenticity of\ndigital images has become a significant concern. While existing studies have\nproposed various methods for detecting AI-generated content, current benchmarks\nare limited in their coverage of diverse generative models and image\ncategories, often overlooking end-to-end image editing and artistic images. To\naddress these limitations, we introduce UniAIDet, a unified and comprehensive\nbenchmark that includes both photographic and artistic images. UniAIDet covers\na wide range of generative models, including text-to-image, image-to-image,\nimage inpainting, image editing, and deepfake models. Using UniAIDet, we\nconduct a comprehensive evaluation of various detection methods and answer\nthree key research questions regarding generalization capability and the\nrelation between detection and localization. Our benchmark and analysis provide\na robust foundation for future research.",
        "url": "http://arxiv.org/abs/2510.23023v1",
        "published_date": "2025-10-27T05:37:23+00:00",
        "updated_date": "2025-10-27T05:37:23+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Huixuan Zhang",
            "Xiaojun Wan"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "UniAIDet introduces a unified benchmark for AI-generated image content detection and localization, covering diverse generative models and image categories.",
        "tldr_zh": "UniAIDet引入了一个统一的基准测试，用于检测和定位AI生成的图像内容，涵盖了多种生成模型和图像类别。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "M$^{3}$T2IBench: A Large-Scale Multi-Category, Multi-Instance, Multi-Relation Text-to-Image Benchmark",
        "summary": "Text-to-image models are known to struggle with generating images that\nperfectly align with textual prompts. Several previous studies have focused on\nevaluating image-text alignment in text-to-image generation. However, these\nevaluations either address overly simple scenarios, especially overlooking the\ndifficulty of prompts with multiple different instances belonging to the same\ncategory, or they introduce metrics that do not correlate well with human\nevaluation. In this study, we introduce M$^3$T2IBench, a large-scale,\nmulti-category, multi-instance, multi-relation along with an\nobject-detection-based evaluation metric, $AlignScore$, which aligns closely\nwith human evaluation. Our findings reveal that current open-source\ntext-to-image models perform poorly on this challenging benchmark.\nAdditionally, we propose the Revise-Then-Enforce approach to enhance image-text\nalignment. This training-free post-editing method demonstrates improvements in\nimage-text alignment across a broad range of diffusion models. \\footnote{Our\ncode and data has been released in supplementary material and will be made\npublicly available after the paper is accepted.}",
        "url": "http://arxiv.org/abs/2510.23020v1",
        "published_date": "2025-10-27T05:32:50+00:00",
        "updated_date": "2025-10-27T05:32:50+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Huixuan Zhang",
            "Xiaojun Wan"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a new benchmark for text-to-image models that focuses on difficult prompts with multiple instances belonging to the same category. It also proposes a training-free post-editing method to improve image-text alignment.",
        "tldr_zh": "该论文介绍了一个针对文本到图像模型的新基准，侧重于具有相同类别多个实例的复杂提示。它还提出了一种无需训练的后编辑方法来改善图像文本对齐。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UGAE: Unified Geometry and Attribute Enhancement for G-PCC Compressed Point Clouds",
        "summary": "Lossy compression of point clouds reduces storage and transmission costs;\nhowever, it inevitably leads to irreversible distortion in geometry structure\nand attribute information. To address these issues, we propose a unified\ngeometry and attribute enhancement (UGAE) framework, which consists of three\ncore components: post-geometry enhancement (PoGE), pre-attribute enhancement\n(PAE), and post-attribute enhancement (PoAE). In PoGE, a Transformer-based\nsparse convolutional U-Net is used to reconstruct the geometry structure with\nhigh precision by predicting voxel occupancy probabilities. Building on the\nrefined geometry structure, PAE introduces an innovative enhanced\ngeometry-guided recoloring strategy, which uses a detail-aware K-Nearest\nNeighbors (DA-KNN) method to achieve accurate recoloring and effectively\npreserve high-frequency details before attribute compression. Finally, at the\ndecoder side, PoAE uses an attribute residual prediction network with a\nweighted mean squared error (W-MSE) loss to enhance the quality of\nhigh-frequency regions while maintaining the fidelity of low-frequency regions.\nUGAE significantly outperformed existing methods on three benchmark datasets:\n8iVFB, Owlii, and MVUB. Compared to the latest G-PCC test model (TMC13v29),\nUGAE achieved an average BD-PSNR gain of 9.98 dB and 90.98% BD-bitrate savings\nfor geometry under the D1 metric, as well as a 3.67 dB BD-PSNR improvement with\n56.88% BD-bitrate savings for attributes on the Y component. Additionally, it\nimproved perceptual quality significantly.",
        "url": "http://arxiv.org/abs/2510.23009v1",
        "published_date": "2025-10-27T05:01:57+00:00",
        "updated_date": "2025-10-27T05:01:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pan Zhao",
            "Hui Yuan",
            "Chongzhen Tian",
            "Tian Guo",
            "Raouf Hamzaoui",
            "Zhigeng Pan"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "UGAE proposes a framework to enhance geometry and attributes in compressed point clouds, outperforming existing methods on benchmark datasets.",
        "tldr_zh": "UGAE提出了一个框架来增强压缩点云中的几何和属性，在基准数据集上表现优异。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Exploring Semantic-constrained Adversarial Example with Instruction Uncertainty Reduction",
        "summary": "Recently, semantically constrained adversarial examples (SemanticAE), which\nare directly generated from natural language instructions, have become a\npromising avenue for future research due to their flexible attacking forms. To\ngenerate SemanticAEs, current methods fall short of satisfactory attacking\nability as the key underlying factors of semantic uncertainty in human\ninstructions, such as referring diversity, descriptive incompleteness, and\nboundary ambiguity, have not been fully investigated. To tackle the issues,\nthis paper develops a multi-dimensional instruction uncertainty reduction\n(InSUR) framework to generate more satisfactory SemanticAE, i.e., transferable,\nadaptive, and effective. Specifically, in the dimension of the sampling method,\nwe propose the residual-driven attacking direction stabilization to alleviate\nthe unstable adversarial optimization caused by the diversity of language\nreferences. By coarsely predicting the language-guided sampling process, the\noptimization process will be stabilized by the designed ResAdv-DDIM sampler,\ntherefore releasing the transferable and robust adversarial capability of\nmulti-step diffusion models. In task modeling, we propose the context-encoded\nattacking scenario constraint to supplement the missing knowledge from\nincomplete human instructions. Guidance masking and renderer integration are\nproposed to regulate the constraints of 2D/3D SemanticAE, activating stronger\nscenario-adapted attacks. Moreover, in the dimension of generator evaluation,\nwe propose the semantic-abstracted attacking evaluation enhancement by\nclarifying the evaluation boundary, facilitating the development of more\neffective SemanticAE generators. Extensive experiments demonstrate the\nsuperiority of the transfer attack performance of InSUR. Moreover, we realize\nthe reference-free generation of semantically constrained 3D adversarial\nexamples for the first time.",
        "url": "http://arxiv.org/abs/2510.22981v1",
        "published_date": "2025-10-27T04:02:52+00:00",
        "updated_date": "2025-10-27T04:02:52+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Jin Hu",
            "Jiakai Wang",
            "Linna Jing",
            "Haolin Li",
            "Haodong Liu",
            "Haotong Qin",
            "Aishan Liu",
            "Ke Xu",
            "Xianglong Liu"
        ],
        "ai_categories": [
            "GAN",
            "AIGC",
            "Transformer"
        ],
        "tldr": "The paper introduces a framework for generating semantically constrained adversarial examples to improve attacking ability based on reducing instruction uncertainty.",
        "tldr_zh": "本文介绍了一个框架，用于生成语义约束对抗例子，以改善攻击能力，基于减少指令不确定性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VALA: Learning Latent Anchors for Training-Free and Temporally Consistent",
        "summary": "Recent advances in training-free video editing have enabled lightweight and\nprecise cross-frame generation by leveraging pre-trained text-to-image\ndiffusion models. However, existing methods often rely on heuristic frame\nselection to maintain temporal consistency during DDIM inversion, which\nintroduces manual bias and reduces the scalability of end-to-end inference. In\nthis paper, we propose~\\textbf{VALA} (\\textbf{V}ariational \\textbf{A}lignment\nfor \\textbf{L}atent \\textbf{A}nchors), a variational alignment module that\nadaptively selects key frames and compresses their latent features into\nsemantic anchors for consistent video editing. To learn meaningful assignments,\nVALA propose a variational framework with a contrastive learning objective.\nTherefore, it can transform cross-frame latent representations into compressed\nlatent anchors that preserve both content and temporal coherence. Our method\ncan be fully integrated into training-free text-to-image based video editing\nmodels. Extensive experiments on real-world video editing benchmarks show that\nVALA achieves state-of-the-art performance in inversion fidelity, editing\nquality, and temporal consistency, while offering improved efficiency over\nprior methods.",
        "url": "http://arxiv.org/abs/2510.22970v1",
        "published_date": "2025-10-27T03:44:11+00:00",
        "updated_date": "2025-10-27T03:44:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhangkai Wu",
            "Xuhui Fan",
            "Zhongyuan Xie",
            "Kaize Shi",
            "Longbing Cao"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper introduces VALA, a variational alignment module for consistent video editing without training. It compresses latent features of key frames into anchors for content and temporal coherence.",
        "tldr_zh": "本文介绍了VALA，一种用于视频编辑的变分对齐模块，无需训练。它将关键帧的潜在特征压缩为锚点，以实现内容和时间上的一致性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Survey of Multimodal Geospatial Foundation Models: Techniques, Applications, and Challenges",
        "summary": "Foundation models have transformed natural language processing and computer\nvision, and their impact is now reshaping remote sensing image analysis. With\npowerful generalization and transfer learning capabilities, they align\nnaturally with the multimodal, multi-resolution, and multi-temporal\ncharacteristics of remote sensing data. To address unique challenges in the\nfield, multimodal geospatial foundation models (GFMs) have emerged as a\ndedicated research frontier. This survey delivers a comprehensive review of\nmultimodal GFMs from a modality-driven perspective, covering five core visual\nand vision-language modalities. We examine how differences in imaging physics\nand data representation shape interaction design, and we analyze key techniques\nfor alignment, integration, and knowledge transfer to tackle modality\nheterogeneity, distribution shifts, and semantic gaps. Advances in training\nparadigms, architectures, and task-specific adaptation strategies are\nsystematically assessed alongside a wealth of emerging benchmarks.\nRepresentative multimodal visual and vision-language GFMs are evaluated across\nten downstream tasks, with insights into their architectures, performance, and\napplication scenarios. Real-world case studies, spanning land cover mapping,\nagricultural monitoring, disaster response, climate studies, and geospatial\nintelligence, demonstrate the practical potential of GFMs. Finally, we outline\npressing challenges in domain generalization, interpretability, efficiency, and\nprivacy, and chart promising avenues for future research.",
        "url": "http://arxiv.org/abs/2510.22964v1",
        "published_date": "2025-10-27T03:40:00+00:00",
        "updated_date": "2025-10-27T03:40:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Liling Yang",
            "Ning Chen",
            "Jun Yue",
            "Yidan Liu",
            "Jiayi Ma",
            "Pedram Ghamisi",
            "Antonio Plaza",
            "Leyuan Fang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper surveys multimodal geospatial foundation models for remote sensing applications, covering different modalities and challenges in alignment and integration.",
        "tldr_zh": "本文调查了用于遥感应用的多模态地理空间基础模型，涵盖了不同的模态和在对齐和集成方面的挑战。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FAME: Fairness-aware Attention-modulated Video Editing",
        "summary": "Training-free video editing (VE) models tend to fall back on gender\nstereotypes when rendering profession-related prompts. We propose \\textbf{FAME}\nfor \\textit{Fairness-aware Attention-modulated Video Editing} that mitigates\nprofession-related gender biases while preserving prompt alignment and temporal\nconsistency for coherent VE. We derive fairness embeddings from existing\nminority representations by softly injecting debiasing tokens into the text\nencoder. Simultaneously, FAME integrates fairness modulation into both temporal\nself attention and prompt-to-region cross attention to mitigate the motion\ncorruption and temporal inconsistency caused by directly introducing fairness\ncues. For temporal self attention, FAME introduces a region constrained\nattention mask combined with time decay weighting, which enhances intra-region\ncoherence while suppressing irrelevant inter-region interactions. For cross\nattention, it reweights tokens to region matching scores by incorporating\nfairness sensitive similarity masks derived from debiasing prompt embeddings.\nTogether, these modulations keep fairness-sensitive semantics tied to the right\nvisual regions and prevent temporal drift across frames. Extensive experiments\non new VE fairness-oriented benchmark \\textit{FairVE} demonstrate that FAME\nachieves stronger fairness alignment and semantic fidelity, surpassing existing\nVE baselines.",
        "url": "http://arxiv.org/abs/2510.22960v1",
        "published_date": "2025-10-27T03:34:15+00:00",
        "updated_date": "2025-10-27T03:34:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhangkai Wu",
            "Xuhui Fan",
            "Zhongyuan Xie",
            "Kaize Shi",
            "Zhidong Li",
            "Longbing Cao"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer"
        ],
        "tldr": "FAME proposes a fairness-aware video editing model to mitigate gender biases while maintaining coherence and consistency in video editing tasks.",
        "tldr_zh": "FAME提出了一个关注公平性的视频编辑模型，以减少性别偏见，同时保持视频编辑任务的连贯性和一致性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LightBagel: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation",
        "summary": "Unified multimodal models have recently shown remarkable gains in both\ncapability and versatility, yet most leading systems are still trained from\nscratch and require substantial computational resources. In this paper, we show\nthat competitive performance can be obtained far more efficiently by\nstrategically fusing publicly available models specialized for either\ngeneration or understanding. Our key design is to retain the original blocks\nwhile additionally interleaving multimodal self-attention blocks throughout the\nnetworks. This double fusion mechanism (1) effectively enables rich multi-modal\nfusion while largely preserving the original strengths of the base models, and\n(2) catalyzes synergistic fusion of high-level semantic representations from\nthe understanding encoder with low-level spatial signals from the generation\nencoder. By training with only ~ 35B tokens, this approach achieves strong\nresults across multiple benchmarks: 0.91 on GenEval for compositional\ntext-to-image generation, 82.16 on DPG-Bench for complex text-to-image\ngeneration, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By\nfully releasing the entire suite of code, model weights, and datasets, we hope\nto support future research on unified multimodal modeling.",
        "url": "http://arxiv.org/abs/2510.22946v1",
        "published_date": "2025-10-27T02:59:57+00:00",
        "updated_date": "2025-10-27T02:59:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zeyu Wang",
            "Zilong Chen",
            "Chenhui Gou",
            "Feng Li",
            "Chaorui Deng",
            "Deyao Zhu",
            "Kunchang Li",
            "Weihao Yu",
            "Haoqin Tu",
            "Haoqi Fan",
            "Cihang Xie"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a lightweight double fusion framework for unified multimodal understanding and generation, achieving competitive results with minimal computational resources.",
        "tldr_zh": "本文介绍了一个轻量级双重融合框架，用于统一的多模态理解和生成，在资源消耗较少的情况下取得竞争力结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Switchable Token-Specific Codebook Quantization For Face Image Compression",
        "summary": "With the ever-increasing volume of visual data, the efficient and lossless\ntransmission, along with its subsequent interpretation and understanding, has\nbecome a critical bottleneck in modern information systems. The emerged\ncodebook-based solution utilize a globally shared codebook to quantize and\ndequantize each token, controlling the bpp by adjusting the number of tokens or\nthe codebook size. However, for facial images, which are rich in attributes,\nsuch global codebook strategies overlook both the category-specific\ncorrelations within images and the semantic differences among tokens, resulting\nin suboptimal performance, especially at low bpp. Motivated by these\nobservations, we propose a Switchable Token-Specific Codebook Quantization for\nface image compression, which learns distinct codebook groups for different\nimage categories and assigns an independent codebook to each token. By\nrecording the codebook group to which each token belongs with a small number of\nbits, our method can reduce the loss incurred when decreasing the size of each\ncodebook group. This enables a larger total number of codebooks under a lower\noverall bpp, thereby enhancing the expressive capability and improving\nreconstruction performance. Owing to its generalizable design, our method can\nbe integrated into any existing codebook-based representation learning approach\nand has demonstrated its effectiveness on face recognition datasets, achieving\nan average accuracy of 93.51% for reconstructed images at 0.05 bpp.",
        "url": "http://arxiv.org/abs/2510.22943v1",
        "published_date": "2025-10-27T02:56:17+00:00",
        "updated_date": "2025-10-27T02:56:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yongbo Wang",
            "Haonan Wang",
            "Guodong Mu",
            "Ruixin Zhang",
            "Jiaqi Chen",
            "Jingyun Zhang",
            "Jun Wang",
            "Yuan Xie",
            "Zhizhong Zhang",
            "Shouhong Ding"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "Proposes a Switchable Token-Specific Codebook Quantization for face image compression, improving performance by considering category-specific correlations and semantic differences among tokens.",
        "tldr_zh": "提出了一种用于人脸图像压缩的Switchable Token-Specific Codebook Quantization方法，通过考虑图像类别间的相关性和标记之间的语义差异来提高性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Positional Preservation Embedding for Multimodal Large Language Models",
        "summary": "Multimodal large language models (MLLMs) have achieved strong performance on\nvision-language tasks, yet often suffer from inefficiencies due to redundant\nvisual tokens. Existing token merging methods reduce sequence length but\nfrequently disrupt spatial layouts and temporal continuity by disregarding\npositional relationships. In this work, we propose a novel encoding operator\ndubbed as \\textbf{P}ositional \\textbf{P}reservation \\textbf{E}mbedding\n(\\textbf{PPE}), which has the main hallmark of preservation of spatiotemporal\nstructure during visual token compression. PPE explicitly introduces the\ndisentangled encoding of 3D positions in the token dimension, enabling each\ncompressed token to encapsulate different positions from multiple original\ntokens. Furthermore, we show that PPE can effectively support cascade\nclustering -- a progressive token compression strategy that leads to better\nperformance retention. PPE is a parameter-free and generic operator that can be\nseamlessly integrated into existing token merging methods without any\nadjustments. Applied to state-of-the-art token merging framework, PPE achieves\nconsistent improvements of $2\\%\\sim5\\%$ across multiple vision-language\nbenchmarks, including MMBench (general vision understanding), TextVQA (layout\nunderstanding) and VideoMME (temporal understanding). These results demonstrate\nthat preserving positional cues is critical for efficient and effective MLLM\nreasoning.",
        "url": "http://arxiv.org/abs/2510.22936v1",
        "published_date": "2025-10-27T02:40:02+00:00",
        "updated_date": "2025-10-27T02:40:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mouxiao Huang",
            "Borui Jiang",
            "Dehua Zheng",
            "Hailin Hu",
            "Kai Han",
            "Xinghao Chen"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality"
        ],
        "tldr": "The paper introduces a novel encoding operator called Positional Preservation Embedding (PPE) for multimodal large language models. PPE preserves spatiotemporal structure during token compression, leading to improved performance on vision-language tasks.",
        "tldr_zh": "本文介绍了一种名为位置保留嵌入（PPE）的新编码算子，用于多模态大语言模型。PPE在标记压缩过程中保持了时空结构，从而在视觉-语言任务中取得了更好的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Gen-LangSplat: Generalized Language Gaussian Splatting with Pre-Trained Feature Compression",
        "summary": "Modeling open-vocabulary language fields in 3D is essential for intuitive\nhuman-AI interaction and querying within physical environments.\nState-of-the-art approaches, such as LangSplat, leverage 3D Gaussian Splatting\nto efficiently construct these language fields, encoding features distilled\nfrom high-dimensional models like CLIP. However, this efficiency is currently\noffset by the requirement to train a scene-specific language autoencoder for\nfeature compression, introducing a costly, per-scene optimization bottleneck\nthat hinders deployment scalability. In this work, we introduce Gen-LangSplat,\nthat eliminates this requirement by replacing the scene-wise autoencoder with a\ngeneralized autoencoder, pre-trained extensively on the large-scale ScanNet\ndataset. This architectural shift enables the use of a fixed, compact latent\nspace for language features across any new scene without any scene-specific\ntraining. By removing this dependency, our entire language field construction\nprocess achieves a efficiency boost while delivering querying performance\ncomparable to, or exceeding, the original LangSplat method. To validate our\ndesign choice, we perform a thorough ablation study empirically determining the\noptimal latent embedding dimension and quantifying representational fidelity\nusing Mean Squared Error and cosine similarity between the original and\nreprojected 512-dimensional CLIP embeddings. Our results demonstrate that\ngeneralized embeddings can efficiently and accurately support open-vocabulary\nquerying in novel 3D scenes, paving the way for scalable, real-time interactive\n3D AI applications.",
        "url": "http://arxiv.org/abs/2510.22930v1",
        "published_date": "2025-10-27T02:13:38+00:00",
        "updated_date": "2025-10-27T02:13:38+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Pranav Saxena"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "Gen-LangSplat introduces a generalized autoencoder to efficiently construct language fields in 3D without scene-specific training, improving querying performance in novel 3D scenes.",
        "tldr_zh": "Gen-LangSplat引入广义自动编码器，在3D中有效构建语言场，无需特定场景训练，提高了在新颖3D场景中的查询性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Seeing the Unseen: Towards Zero-Shot Inspection for Wind Turbine Blades using Knowledge-Augmented Vision Language Models",
        "summary": "Wind turbine blades operate in harsh environments, making timely damage\ndetection essential for preventing failures and optimizing maintenance.\nDrone-based inspection and deep learning are promising, but typically depend on\nlarge, labeled datasets, which limit their ability to detect rare or evolving\ndamage types. To address this, we propose a zero-shot-oriented inspection\nframework that integrates Retrieval-Augmented Generation (RAG) with\nVision-Language Models (VLM). A multimodal knowledge base is constructed,\ncomprising technical documentation, representative reference images, and\ndomain-specific guidelines. A hybrid text-image retriever with keyword-aware\nreranking assembles the most relevant context to condition the VLM at\ninference, injecting domain knowledge without task-specific training. We\nevaluate the framework on 30 labeled blade images covering diverse damage\ncategories. Although the dataset is small due to the difficulty of acquiring\nverified blade imagery, it covers multiple representative defect types. On this\ntest set, the RAG-grounded VLM correctly classified all samples, whereas the\nsame VLM without retrieval performed worse in both accuracy and precision. We\nfurther compare against open-vocabulary baselines and incorporate uncertainty\nClopper-Pearson confidence intervals to account for the small-sample setting.\nAblation studies indicate that the key advantage of the framework lies in\nexplainability and generalizability: retrieved references ground the reasoning\nprocess and enable the detection of previously unseen defects by leveraging\ndomain knowledge rather than relying solely on visual cues. This research\ncontributes a data-efficient solution for industrial inspection that reduces\ndependence on extensive labeled datasets.",
        "url": "http://arxiv.org/abs/2510.22868v1",
        "published_date": "2025-10-26T23:19:28+00:00",
        "updated_date": "2025-10-26T23:19:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yang Zhang",
            "Qianyu Zhou",
            "Farhad Imani",
            "Jiong Tang"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes a zero-shot inspection framework for wind turbine blades using Retrieval-Augmented Generation and Vision-Language Models, leveraging domain knowledge to improve defect detection without relying solely on visual cues.",
        "tldr_zh": "本文提出了一种零-shot检测框架，用于风力涡轮叶片的检测，利用检索增强生成和视觉语言模型，借助领域知识提高缺陷检测能力，而不仅依赖视觉线索。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Semantic Surgery: Zero-Shot Concept Erasure in Diffusion Models",
        "summary": "Concept erasure in text-to-image diffusion models is crucial for mitigating\nharmful content, yet existing methods often compromise generative quality. We\nintroduce Semantic Surgery, a novel training-free, zero-shot framework for\nconcept erasure that operates directly on text embeddings before the diffusion\nprocess. It dynamically estimates the presence of target concepts in a prompt\nand performs a calibrated vector subtraction to neutralize their influence at\nthe source, enhancing both erasure completeness and locality. The framework\nincludes a Co-Occurrence Encoding module for robust multi-concept erasure and a\nvisual feedback loop to address latent concept persistence. As a training-free\nmethod, Semantic Surgery adapts dynamically to each prompt, ensuring precise\ninterventions. Extensive experiments on object, explicit content, artistic\nstyle, and multi-celebrity erasure tasks show our method significantly\noutperforms state-of-the-art approaches. We achieve superior completeness and\nrobustness while preserving locality and image quality (e.g., 93.58 H-score in\nobject erasure, reducing explicit content to just 1 instance, and 8.09 H_a in\nstyle erasure with no quality degradation). This robustness also allows our\nframework to function as a built-in threat detection system, offering a\npractical solution for safer text-to-image generation.",
        "url": "http://arxiv.org/abs/2510.22851v1",
        "published_date": "2025-10-26T22:04:17+00:00",
        "updated_date": "2025-10-26T22:04:17+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Lexiang Xiong",
            "Chengyu Liu",
            "Jingwen Ye",
            "Yan Liu",
            "Yuecong Xu"
        ],
        "ai_categories": [
            "Diffusion"
        ],
        "tldr": "The paper introduces Semantic Surgery, a novel zero-shot framework for concept erasure in text-to-image diffusion models, outperforming existing methods in erasure tasks while maintaining image quality and robustness.",
        "tldr_zh": "本文介绍了语义手术，一种新的零射击框架，用于文本到图像扩散模型中的概念擦除，在擦除任务中优于现有方法，同时保持图像质量和稳健性。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Semantic-Preserving Cross-Style Visual Reasoning for Robust Multi-Modal Understanding in Large Vision-Language Models",
        "summary": "The \"style trap\" poses a significant challenge for Large Vision-Language\nModels (LVLMs), hindering robust semantic understanding across diverse visual\nstyles, especially in in-context learning (ICL). Existing methods often fail to\neffectively decouple style from content, hindering generalization. To address\nthis, we propose the Semantic-Preserving Cross-Style Visual Reasoner (SP-CSVR),\na novel framework for stable semantic understanding and adaptive cross-style\nvisual reasoning. SP-CSVR integrates a Cross-Style Feature Encoder (CSFE) for\nstyle-content disentanglement, a Semantic-Aligned In-Context Decoder (SAICD)\nfor efficient few-shot style adaptation, and an Adaptive Semantic Consistency\nModule (ASCM) employing multi-task contrastive learning to enforce cross-style\nsemantic invariance. Extensive experiments on a challenging multi-style dataset\ndemonstrate SP-CSVR's state-of-the-art performance across visual captioning,\nvisual question answering, and in-context style adaptation. Comprehensive\nevaluations, including ablation studies and generalization analysis, confirm\nSP-CSVR's efficacy in enhancing robustness, generalization, and efficiency\nacross diverse visual styles.",
        "url": "http://arxiv.org/abs/2510.22838v1",
        "published_date": "2025-10-26T21:11:46+00:00",
        "updated_date": "2025-10-26T21:11:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aya Nakayama",
            "Brian Wong",
            "Yuji Nishimura",
            "Kaito Tanaka"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel framework called SP-CSVR for robust multi-modal understanding in large vision-language models by addressing the 'style trap' challenge.",
        "tldr_zh": "本文提出了一种名为SP-CSVR的新框架，用于解决大型视觉-语言模型中的鲁棒多模态理解问题，通过应对“风格陷阱”挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FairJudge: MLLM Judging for Social Attributes and Prompt Image Alignment",
        "summary": "Text-to-image (T2I) systems lack simple, reproducible ways to evaluate how\nwell images match prompts and how models treat social attributes. Common\nproxies -- face classifiers and contrastive similarity -- reward surface cues,\nlack calibrated abstention, and miss attributes only weakly visible (for\nexample, religion, culture, disability). We present FairJudge, a lightweight\nprotocol that treats instruction-following multimodal LLMs as fair judges. It\nscores alignment with an explanation-oriented rubric mapped to [-1, 1];\nconstrains judgments to a closed label set; requires evidence grounded in the\nvisible content; and mandates abstention when cues are insufficient. Unlike\nCLIP-only pipelines, FairJudge yields accountable, evidence-aware decisions;\nunlike mitigation that alters generators, it targets evaluation fairness. We\nevaluate gender, race, and age on FairFace, PaTA, and FairCoT; extend to\nreligion, culture, and disability; and assess profession correctness and\nalignment on IdenProf, FairCoT-Professions, and our new DIVERSIFY-Professions.\nWe also release DIVERSIFY, a 469-image corpus of diverse, non-iconic scenes.\nAcross datasets, judge models outperform contrastive and face-centric baselines\non demographic prediction and improve mean alignment while maintaining high\nprofession accuracy, enabling more reliable, reproducible fairness audits.",
        "url": "http://arxiv.org/abs/2510.22827v1",
        "published_date": "2025-10-26T20:43:48+00:00",
        "updated_date": "2025-10-26T20:43:48+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Zahraa Al Sahili",
            "Maryam Fetanat",
            "Maimuna Nowaz",
            "Ioannis Patras",
            "Matthew Purver"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces FairJudge, a protocol for evaluating text-to-image systems that considers social attributes and prompt alignment, aiming for fair and evidence-aware decisions.",
        "tldr_zh": "本论文介绍了一种名为FairJudge的协议，用于评估文本到图像系统的社会属性和提示对齐，旨在做出公正且具备证据意识的决策。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MedXplain-VQA: Multi-Component Explainable Medical Visual Question Answering",
        "summary": "Explainability is critical for the clinical adoption of medical visual\nquestion answering (VQA) systems, as physicians require transparent reasoning\nto trust AI-generated diagnoses. We present MedXplain-VQA, a comprehensive\nframework integrating five explainable AI components to deliver interpretable\nmedical image analysis. The framework leverages a fine-tuned BLIP-2 backbone,\nmedical query reformulation, enhanced Grad-CAM attention, precise region\nextraction, and structured chain-of-thought reasoning via multi-modal language\nmodels. To evaluate the system, we introduce a medical-domain-specific\nframework replacing traditional NLP metrics with clinically relevant\nassessments, including terminology coverage, clinical structure quality, and\nattention region relevance. Experiments on 500 PathVQA histopathology samples\ndemonstrate substantial improvements, with the enhanced system achieving a\ncomposite score of 0.683 compared to 0.378 for baseline methods, while\nmaintaining high reasoning confidence (0.890). Our system identifies 3-5\ndiagnostically relevant regions per sample and generates structured\nexplanations averaging 57 words with appropriate clinical terminology. Ablation\nstudies reveal that query reformulation provides the most significant initial\nimprovement, while chain-of-thought reasoning enables systematic diagnostic\nprocesses. These findings underscore the potential of MedXplain-VQA as a\nrobust, explainable medical VQA system. Future work will focus on validation\nwith medical experts and large-scale clinical datasets to ensure clinical\nreadiness.",
        "url": "http://arxiv.org/abs/2510.22803v1",
        "published_date": "2025-10-26T19:23:20+00:00",
        "updated_date": "2025-10-26T19:23:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hai-Dang Nguyen",
            "Minh-Anh Dang",
            "Minh-Tan Le",
            "Minh-Tuan Le"
        ],
        "ai_categories": [
            "Multimodality",
            "AI"
        ],
        "tldr": "MedXplain-VQA is a framework for explainable medical visual question answering, showing significant improvements in diagnostic processes and interpretability.",
        "tldr_zh": "MedXplain-VQA是一个解释性医学视觉问答框架，显示出显著改进的诊断流程和可解释性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MiCADangelo: Fine-Grained Reconstruction of Constrained CAD Models from 3D Scans",
        "summary": "Computer-Aided Design (CAD) plays a foundational role in modern manufacturing\nand product development, often requiring designers to modify or build upon\nexisting models. Converting 3D scans into parametric CAD representations--a\nprocess known as CAD reverse engineering--remains a significant challenge due\nto the high precision and structural complexity of CAD models. Existing deep\nlearning-based approaches typically fall into two categories: bottom-up,\ngeometry-driven methods, which often fail to produce fully parametric outputs,\nand top-down strategies, which tend to overlook fine-grained geometric details.\nMoreover, current methods neglect an essential aspect of CAD modeling:\nsketch-level constraints. In this work, we introduce a novel approach to CAD\nreverse engineering inspired by how human designers manually perform the task.\nOur method leverages multi-plane cross-sections to extract 2D patterns and\ncapture fine parametric details more effectively. It enables the reconstruction\nof detailed and editable CAD models, outperforming state-of-the-art methods\nand, for the first time, incorporating sketch constraints directly into the\nreconstruction process.",
        "url": "http://arxiv.org/abs/2510.23429v1",
        "published_date": "2025-10-27T15:33:51+00:00",
        "updated_date": "2025-10-27T15:33:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ahmet Serdar Karadeniz",
            "Dimitrios Mallis",
            "Danila Rukhovich",
            "Kseniya Cherenkova",
            "Anis Kacem",
            "Djamila Aouada"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel approach to reverse engineering CAD models from 3D scans, incorporating sketch-level constraints for better reconstruction.",
        "tldr_zh": "本文介绍了一种新颖的方法，从3D扫描中逆向工程CAD模型，将草图级约束融入重建过程中。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Symmetria: A Synthetic Dataset for Learning in Point Clouds",
        "summary": "Unlike image or text domains that benefit from an abundance of large-scale\ndatasets, point cloud learning techniques frequently encounter limitations due\nto the scarcity of extensive datasets. To overcome this limitation, we present\nSymmetria, a formula-driven dataset that can be generated at any arbitrary\nscale. By construction, it ensures the absolute availability of precise ground\ntruth, promotes data-efficient experimentation by requiring fewer samples,\nenables broad generalization across diverse geometric settings, and offers easy\nextensibility to new tasks and modalities. Using the concept of symmetry, we\ncreate shapes with known structure and high variability, enabling neural\nnetworks to learn point cloud features effectively. Our results demonstrate\nthat this dataset is highly effective for point cloud self-supervised\npre-training, yielding models with strong performance in downstream tasks such\nas classification and segmentation, which also show good few-shot learning\ncapabilities. Additionally, our dataset can support fine-tuning models to\nclassify real-world objects, highlighting our approach's practical utility and\napplication. We also introduce a challenging task for symmetry detection and\nprovide a benchmark for baseline comparisons. A significant advantage of our\napproach is the public availability of the dataset, the accompanying code, and\nthe ability to generate very large collections, promoting further research and\ninnovation in point cloud learning.",
        "url": "http://arxiv.org/abs/2510.23414v1",
        "published_date": "2025-10-27T15:18:26+00:00",
        "updated_date": "2025-10-27T15:18:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ivan Sipiran",
            "Gustavo Santelices",
            "Lucas Oyarzún",
            "Andrea Ranieri",
            "Chiara Romanengo",
            "Silvia Biasotti",
            "Bianca Falcidieno"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "Symmetria is a synthetic dataset designed for point cloud learning, promoting data-efficient experimentation, generalization, and few-shot learning capabilities.",
        "tldr_zh": "Symmetria是一个为点云学习设计的合成数据集，促进了数据高效实验、泛化和少样本学习能力。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Implicit Modeling for Transferability Estimation of Vision Foundation Models",
        "summary": "Transferability estimation identifies the best pre-trained models for\ndownstream tasks without incurring the high computational cost of full\nfine-tuning. This capability facilitates deployment and advances the\npre-training and fine-tuning paradigm. However, existing methods often struggle\nto accurately assess transferability for emerging pre-trained models with\ndiverse architectures, training strategies, and task alignments. In this work,\nwe propose Implicit Transferability Modeling (ITM), a novel framework that\nimplicitly models each model's intrinsic transferability, coupled with a\nDivide-and-Conquer Variational Approximation (DVA) strategy to efficiently\napproximate embedding space evolution. This design enables generalization\nacross a broader range of models and downstream tasks. Extensive experiments on\na comprehensive benchmark--spanning extensive training regimes and a wider\nvariety of model types--demonstrate that ITM consistently outperforms existing\nmethods in terms of stability, effectiveness, and efficiency.",
        "url": "http://arxiv.org/abs/2510.23145v1",
        "published_date": "2025-10-27T09:21:19+00:00",
        "updated_date": "2025-10-27T09:21:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yaoyan Zheng",
            "Huiqun Wang",
            "Nan Zhou",
            "Di Huang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel framework called Implicit Transferability Modeling (ITM) for efficiently estimating transferability of pre-trained vision models, outperforming existing methods across various model types and downstream tasks.",
        "tldr_zh": "本文提出了一种名为Implicit Transferability Modeling（ITM）的新框架，用于高效估计预先训练视觉模型的可转移性，在各种模型类型和下游任务中胜过现有方法。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "DeepSalt: Bridging Laboratory and Satellite Spectra through Domain Adaptation and Knowledge Distillation for Large-Scale Soil Salinity Estimation",
        "summary": "Soil salinization poses a significant threat to both ecosystems and\nagriculture because it limits plants' ability to absorb water and, in doing so,\nreduces crop productivity. This phenomenon alters the soil's spectral\nproperties, creating a measurable relationship between salinity and light\nreflectance that enables remote monitoring. While laboratory spectroscopy\nprovides precise measurements, its reliance on in-situ sampling limits\nscalability to regional or global levels. Conversely, hyperspectral satellite\nimagery enables wide-area observation but lacks the fine-grained\ninterpretability of laboratory instruments. To bridge this gap, we introduce\nDeepSalt, a deep-learning-based spectral transfer framework that leverages\nknowledge distillation and a novel Spectral Adaptation Unit to transfer\nhigh-resolution spectral insights from laboratory-based spectroscopy to\nsatellite-based hyperspectral sensing. Our approach eliminates the need for\nextensive ground sampling while enabling accurate, large-scale salinity\nestimation, as demonstrated through comprehensive empirical benchmarks.\nDeepSalt achieves significant performance gains over methods without explicit\ndomain adaptation, underscoring the impact of the proposed Spectral Adaptation\nUnit and the knowledge distillation strategy. The model also effectively\ngeneralized to unseen geographic regions, explaining a substantial portion of\nthe salinity variance.",
        "url": "http://arxiv.org/abs/2510.23124v1",
        "published_date": "2025-10-27T08:57:59+00:00",
        "updated_date": "2025-10-27T08:57:59+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Rupasree Dey",
            "Abdul Matin",
            "Everett Lewark",
            "Tanjim Bin Faruk",
            "Andrei Bachinin",
            "Sam Leuthold",
            "M. Francesca Cotrufo",
            "Shrideep Pallickara",
            "Sangmi Lee Pallickara"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "DeepSalt introduces a deep-learning framework to transfer high-resolution spectral insights from laboratory-based spectroscopy to satellite-based hyperspectral sensing for accurate large-scale soil salinity estimation.",
        "tldr_zh": "DeepSalt引入了一个深度学习框架，将实验室光谱学的高分辨率光谱洞察力转移到卫星超光谱感测中，以实现准确的大规模土壤盐度估计。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "HieraMamba: Video Temporal Grounding via Hierarchical Anchor-Mamba Pooling",
        "summary": "Video temporal grounding, the task of localizing the start and end times of a\nnatural language query in untrimmed video, requires capturing both global\ncontext and fine-grained temporal detail. This challenge is particularly\npronounced in long videos, where existing methods often compromise temporal\nfidelity by over-downsampling or relying on fixed windows. We present\nHieraMamba, a hierarchical architecture that preserves temporal structure and\nsemantic richness across scales. At its core are Anchor-MambaPooling (AMP)\nblocks, which utilize Mamba's selective scanning to produce compact anchor\ntokens that summarize video content at multiple granularities. Two\ncomplementary objectives, anchor-conditioned and segment-pooled contrastive\nlosses, encourage anchors to retain local detail while remaining globally\ndiscriminative. HieraMamba sets a new state-of-the-art on Ego4D-NLQ, MAD, and\nTACoS, demonstrating precise, temporally faithful localization in long,\nuntrimmed videos.",
        "url": "http://arxiv.org/abs/2510.23043v1",
        "published_date": "2025-10-27T06:13:07+00:00",
        "updated_date": "2025-10-27T06:13:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Joungbin An",
            "Kristen Grauman"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "HieraMamba is a hierarchical architecture for precise temporal grounding in videos, setting a new state-of-the-art in localization tasks.",
        "tldr_zh": "HieraMamba是一个用于视频中精确时间定位的分层架构，在定位任务中取得了新的技术水平。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "RobotArena $\\infty$: Scalable Robot Benchmarking via Real-to-Sim Translation",
        "summary": "The pursuit of robot generalists - instructable agents capable of performing\ndiverse tasks across diverse environments - demands rigorous and scalable\nevaluation. Yet real-world testing of robot policies remains fundamentally\nconstrained: it is labor-intensive, slow, unsafe at scale, and difficult to\nreproduce. Existing simulation benchmarks are similarly limited, as they train\nand test policies within the same synthetic domains and cannot assess models\ntrained from real-world demonstrations or alternative simulation environments.\nAs policies expand in scope and complexity, these barriers only intensify,\nsince defining \"success\" in robotics often hinges on nuanced human judgments of\nexecution quality. In this paper, we introduce a new benchmarking framework\nthat overcomes these challenges by shifting VLA evaluation into large-scale\nsimulated environments augmented with online human feedback. Leveraging\nadvances in vision-language models, 2D-to-3D generative modeling, and\ndifferentiable rendering, our approach automatically converts video\ndemonstrations from widely used robot datasets into simulated counterparts.\nWithin these digital twins, we assess VLA policies using both automated\nVLM-guided scoring and scalable human preference judgments collected from\ncrowdworkers, transforming human involvement from tedious scene setup,\nresetting, and safety supervision into lightweight preference comparisons. To\nmeasure robustness, we systematically perturb simulated environments along\nmultiple axes, such as textures and object placements, stress-testing policy\ngeneralization under controlled variation. The result is a continuously\nevolving, reproducible, and scalable benchmark for real-world trained robot\nmanipulation policies, addressing a critical missing capability in today's\nrobotics landscape.",
        "url": "http://arxiv.org/abs/2510.23571v1",
        "published_date": "2025-10-27T17:41:38+00:00",
        "updated_date": "2025-10-27T17:41:38+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yash Jangir",
            "Yidi Zhang",
            "Kashu Yamazaki",
            "Chenyu Zhang",
            "Kuan-Hsun Tu",
            "Tsung-Wei Ke",
            "Lei Ke",
            "Yonatan Bisk",
            "Katerina Fragkiadaki"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a new benchmarking framework for evaluating robot policies in scalable simulated environments augmented with online human feedback.",
        "tldr_zh": "本文介绍了一种新的基准测试框架，用于在扩展的模拟环境中评估机器人政策，加上在线人类反馈。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT",
        "summary": "Egocentric video reasoning centers on an unobservable agent behind the camera\nwho dynamically shapes the environment, requiring inference of hidden\nintentions and recognition of fine-grained interactions. This core challenge\nlimits current multimodal large language models MLLMs, which excel at visible\nevent reasoning but lack embodied, first-person understanding. To bridge this\ngap, we introduce EgoThinker, a novel framework that endows MLLMs with robust\negocentric reasoning capabilities through spatio-temporal chain-of-thought\nsupervision and a two-stage learning curriculum. First, we introduce EgoRe-5M,\na large-scale egocentric QA dataset constructed from 13M diverse egocentric\nvideo clips. This dataset features multi-minute segments annotated with\ndetailed CoT rationales and dense hand-object grounding. Second, we employ SFT\non EgoRe-5M to instill reasoning skills, followed by reinforcement fine-tuning\nRFT to further enhance spatio-temporal localization. Experimental results show\nthat EgoThinker outperforms existing methods across multiple egocentric\nbenchmarks, while achieving substantial improvements in fine-grained\nspatio-temporal localization tasks. Full code and data are released at\nhttps://github.com/InternRobotics/EgoThinker.",
        "url": "http://arxiv.org/abs/2510.23569v1",
        "published_date": "2025-10-27T17:38:17+00:00",
        "updated_date": "2025-10-27T17:38:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Baoqi Pei",
            "Yifei Huang",
            "Jilan Xu",
            "Yuping He",
            "Guo Chen",
            "Fei Wu",
            "Yu Qiao",
            "Jiangmiao Pang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "EgoThinker introduces a framework for egocentric reasoning using spatio-temporal supervision and a two-stage learning curriculum, outperforming existing methods in egocentric benchmarks.",
        "tldr_zh": "EgoThinker通过时空监督和两阶段学习课程引入了一种以自我为中心的推理框架，在自我为中心的基准测试中表现优异。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "VOLD: Reasoning Transfer from LLMs to Vision-Language Models via On-Policy Distillation",
        "summary": "Training vision-language models (VLMs) for complex reasoning remains a\nchallenging task, i.a. due to the scarcity of high-quality image-text reasoning\ndata. Conversely, text-based reasoning resources are abundant and scalable, but\nit is still an open question how to leveraging them for VLM reasoning. To\naddress this problem, we propose VOLD, a framework to transfer reasoning\ncapabilities from text-only teacher models to VLM student models. To this end,\nVOLD combines reinforcement learning via Group Relative Policy Optimization\n(GRPO) with on-policy distillation, which allows the student reasoning traces\nto be guided by the teacher model, resulting in a significant gain over using\nGRPO alone. We further show that a cold-start alignment is essential for an\neffective transfer during the online training phase in this scenario and that\nwithout sufficient distributional alignment between teacher and student,\non-policy distillation fails to provide meaningful guidance. We evaluate VOLD\nacross diverse benchmarks including MMMU-Pro, MathVision, MathVista, and\nLogicVista, showing that VOLD outperforms the baseline model significantly and\nimproves over the state of the art by a margin. Our ablation shows the\nimportance of a cold-start alignment via SFT for on-policy distillation with a\ntext-only teacher.",
        "url": "http://arxiv.org/abs/2510.23497v1",
        "published_date": "2025-10-27T16:32:12+00:00",
        "updated_date": "2025-10-27T16:32:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Walid Bousselham",
            "Hilde Kuehne",
            "Cordelia Schmid"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality"
        ],
        "tldr": "The paper proposes a framework, VOLD, to transfer reasoning capabilities from text-only models to vision-language models using reinforcement learning and on-policy distillation, showing significant improvements over baseline models across multiple benchmarks.",
        "tldr_zh": "本文提出了一个框架VOLD，通过强化学习和政策迭代来将文本模型的推理能力转移到视觉-语言模型，在多个基准测试中显著优于基线模型。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Interpretable Tile-Based Classification of Paclitaxel Exposure",
        "summary": "Medical image analysis is central to drug discovery and preclinical\nevaluation, where scalable, objective readouts can accelerate decision-making.\nWe address classification of paclitaxel (Taxol) exposure from phase-contrast\nmicroscopy of C6 glioma cells -- a task with subtle dose differences that\nchallenges full-image models. We propose a simple tiling-and-aggregation\npipeline that operates on local patches and combines tile outputs into an image\nlabel, achieving state-of-the-art accuracy on the benchmark dataset and\nimproving over the published baseline by around 20 percentage points, with\ntrends confirmed by cross-validation. To understand why tiling is effective, we\nfurther apply Grad-CAM and Score-CAM and attention analyses, which enhance\nmodel interpretability and point toward robustness-oriented directions for\nfuture medical image research. Code is released to facilitate reproduction and\nextension.",
        "url": "http://arxiv.org/abs/2510.23363v1",
        "published_date": "2025-10-27T14:13:51+00:00",
        "updated_date": "2025-10-27T14:13:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sean Fletcher",
            "Gabby Scott",
            "Douglas Currie",
            "Xin Zhang",
            "Yuqi Song",
            "Bruce MacLeod"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper introduces a new method for classifying paclitaxel exposure in medical images of C6 glioma cells, achieving state-of-the-art accuracy by using a tiling-and-aggregation approach. It also enhances model interpretability through attention analyses.",
        "tldr_zh": "该论文介绍了一种新的方法来分类C6神经胶质瘤细胞中紫杉醇暴露的医学图像，通过使用瓷砖和聚集方法实现了最先进的准确性。它还通过注意力分析增强了模型的可解释性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Progressive Growing of Patch Size: Curriculum Learning for Accelerated and Improved Medical Image Segmentation",
        "summary": "In this work, we introduce Progressive Growing of Patch Size, an automatic\ncurriculum learning approach for 3D medical image segmentation. Our approach\nprogressively increases the patch size during model training, resulting in an\nimproved class balance for smaller patch sizes and accelerated convergence of\nthe training process. We evaluate our curriculum approach in two settings: a\nresource-efficient mode and a performance mode, both regarding Dice score\nperformance and computational costs across 15 diverse and popular 3D medical\nimage segmentation tasks. The resource-efficient mode matches the Dice score\nperformance of the conventional constant patch size sampling baseline with a\nnotable reduction in training time to only 44%. The performance mode improves\nupon constant patch size segmentation results, achieving a statistically\nsignificant relative mean performance gain of 1.28% in Dice Score. Remarkably,\nacross all 15 tasks, our proposed performance mode manages to surpass the\nconstant patch size baseline in Dice Score performance, while simultaneously\nreducing training time to only 89%. The benefits are particularly pronounced\nfor highly imbalanced tasks such as lesion segmentation tasks. Rigorous\nexperiments demonstrate that our performance mode not only improves mean\nsegmentation performance but also reduces performance variance, yielding more\ntrustworthy model comparison. Furthermore, our findings reveal that the\nproposed curriculum sampling is not tied to a specific architecture but\nrepresents a broadly applicable strategy that consistently boosts performance\nacross diverse segmentation models, including UNet, UNETR, and SwinUNETR. In\nsummary, we show that this simple yet elegant transformation on input data\nsubstantially improves both Dice Score performance and training runtime, while\nbeing compatible across diverse segmentation backbones.",
        "url": "http://arxiv.org/abs/2510.23241v1",
        "published_date": "2025-10-27T11:55:12+00:00",
        "updated_date": "2025-10-27T11:55:12+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Stefan M. Fischer",
            "Johannes Kiechle",
            "Laura Daza",
            "Lina Felsner",
            "Richard Osuala",
            "Daniel M. Lang",
            "Karim Lekadir",
            "Jan C. Peeken",
            "Julia A. Schnabel"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a curriculum learning approach for 3D medical image segmentation, progressively increasing patch size for improved results and faster training.",
        "tldr_zh": "该论文引入了一种用于3D医学图像分割的课程学习方法，逐渐增大补丁尺寸以改善结果并加快训练速度。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7.25
    },
    {
        "title": "LoMix: Learnable Weighted Multi-Scale Logits Mixing for Medical Image Segmentation",
        "summary": "U-shaped networks output logits at multiple spatial scales, each capturing a\ndifferent blend of coarse context and fine detail. Yet, training still treats\nthese logits in isolation - either supervising only the final,\nhighest-resolution logits or applying deep supervision with identical loss\nweights at every scale - without exploring mixed-scale combinations.\nConsequently, the decoder output misses the complementary cues that arise only\nwhen coarse and fine predictions are fused. To address this issue, we introduce\nLoMix (Logits Mixing), a NAS-inspired, differentiable plug-and-play module that\ngenerates new mixed-scale outputs and learns how exactly each of them should\nguide the training process. More precisely, LoMix mixes the multi-scale decoder\nlogits with four lightweight fusion operators: addition, multiplication,\nconcatenation, and attention-based weighted fusion, yielding a rich set of\nsynthetic mutant maps. Every original or mutant map is given a softplus loss\nweight that is co-optimized with network parameters, mimicking a one-step\narchitecture search that automatically discovers the most useful scales,\nmixtures, and operators. Plugging LoMix into recent U-shaped architectures\n(i.e., PVT-V2-B2 backbone with EMCAD decoder) on Synapse 8-organ dataset\nimproves DICE by +4.2% over single-output supervision, +2.2% over deep\nsupervision, and +1.5% over equally weighted additive fusion, all with zero\ninference overhead. When training data are scarce (e.g., one or two labeled\nscans), the advantage grows to +9.23%, underscoring LoMix's data efficiency.\nAcross four benchmarks and diverse U-shaped networks, LoMiX improves DICE by up\nto +13.5% over single-output supervision, confirming that learnable weighted\nmixed-scale fusion generalizes broadly while remaining data efficient, fully\ninterpretable, and overhead-free at inference. Our code is available at\nhttps://github.com/SLDGroup/LoMix.",
        "url": "http://arxiv.org/abs/2510.22995v1",
        "published_date": "2025-10-27T04:25:57+00:00",
        "updated_date": "2025-10-27T04:25:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Md Mostafijur Rahman",
            "Radu Marculescu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces LoMix, a module for medical image segmentation that combines multi-scale logits using different fusion operators. It significantly improves segmentation accuracy over existing methods.",
        "tldr_zh": "本文介绍了LoMix，一种用于医学图像分割的模块，利用不同的融合算子将多尺度logits相结合。相较于现有方法，它显著提高了分割准确性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Estimating Pasture Biomass from Top-View Images: A Dataset for Precision Agriculture",
        "summary": "Accurate estimation of pasture biomass is important for decision-making in\nlivestock production systems. Estimates of pasture biomass can be used to\nmanage stocking rates to maximise pasture utilisation, while minimising the\nrisk of overgrazing and promoting overall system health. We present a\ncomprehensive dataset of 1,162 annotated top-view images of pastures collected\nacross 19 locations in Australia. The images were taken across multiple seasons\nand include a range of temperate pasture species. Each image captures a 70cm *\n30cm quadrat and is paired with on-ground measurements including biomass sorted\nby component (green, dead, and legume fraction), vegetation height, and\nNormalized Difference Vegetation Index (NDVI) from Active Optical Sensors\n(AOS). The multidimensional nature of the data, which combines visual,\nspectral, and structural information, opens up new possibilities for advancing\nthe use of precision grazing management. The dataset is released and hosted in\na Kaggle competition that challenges the international Machine Learning\ncommunity with the task of pasture biomass estimation. The dataset is available\non the official Kaggle webpage:\nhttps://www.kaggle.com/competitions/csiro-biomass",
        "url": "http://arxiv.org/abs/2510.22916v1",
        "published_date": "2025-10-27T01:35:00+00:00",
        "updated_date": "2025-10-27T01:35:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qiyu Liao",
            "Dadong Wang",
            "Rebecca Haling",
            "Jiajun Liu",
            "Xun Li",
            "Martyna Plomecka",
            "Andrew Robson",
            "Matthew Pringle",
            "Rhys Pirie",
            "Megan Walker",
            "Joshua Whelan"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper presents a dataset of top-view images of pastures for estimating biomass, with the goal of advancing precision grazing management.",
        "tldr_zh": "该论文提供了一个用于估计生物量的从顶部视图图像数据集，旨在推动精准放牧管理。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "InFlux: A Benchmark for Self-Calibration of Dynamic Intrinsics of Video Cameras",
        "summary": "Accurately tracking camera intrinsics is crucial for achieving 3D\nunderstanding from 2D video. However, most 3D algorithms assume that camera\nintrinsics stay constant throughout a video, which is often not true for many\nreal-world in-the-wild videos. A major obstacle in this field is a lack of\ndynamic camera intrinsics benchmarks--existing benchmarks typically offer\nlimited diversity in scene content and intrinsics variation, and none provide\nper-frame intrinsic changes for consecutive video frames. In this paper, we\npresent Intrinsics in Flux (InFlux), a real-world benchmark that provides\nper-frame ground truth intrinsics annotations for videos with dynamic\nintrinsics. Compared to prior benchmarks, InFlux captures a wider range of\nintrinsic variations and scene diversity, featuring 143K+ annotated frames from\n386 high-resolution indoor and outdoor videos with dynamic camera intrinsics.\nTo ensure accurate per-frame intrinsics, we build a comprehensive lookup table\nof calibration experiments and extend the Kalibr toolbox to improve its\naccuracy and robustness. Using our benchmark, we evaluate existing baseline\nmethods for predicting camera intrinsics and find that most struggle to achieve\naccurate predictions on videos with dynamic intrinsics. For the dataset, code,\nvideos, and submission, please visit https://influx.cs.princeton.edu/.",
        "url": "http://arxiv.org/abs/2510.23589v1",
        "published_date": "2025-10-27T17:54:57+00:00",
        "updated_date": "2025-10-27T17:54:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Erich Liang",
            "Roma Bhattacharjee",
            "Sreemanti Dey",
            "Rafael Moschopoulos",
            "Caitlin Wang",
            "Michel Liao",
            "Grace Tan",
            "Andrew Wang",
            "Karhan Kayan",
            "Stamatis Alexandropoulos",
            "Jia Deng"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper introduces a benchmark called InFlux for self-calibration of dynamic intrinsics of video cameras, providing per-frame ground truth intrinsics annotations for videos with dynamic camera intrinsics.",
        "tldr_zh": "本文介绍了一个名为InFlux的基准，用于视频相机动态固有参数的自校准，为具有动态相机固有参数的视频提供了每帧真实固有参数注释。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Localising under the drape: proprioception in the era of distributed surgical robotic system",
        "summary": "Despite their mechanical sophistication, surgical robots remain blind to\ntheir surroundings. This lack of spatial awareness causes collisions, system\nrecoveries, and workflow disruptions, issues that will intensify with the\nintroduction of distributed robots with independent interacting arms. Existing\ntracking systems rely on bulky infrared cameras and reflective markers,\nproviding only limited views of the surgical scene and adding hardware burden\nin crowded operating rooms. We present a marker-free proprioception method that\nenables precise localisation of surgical robots under their sterile draping\ndespite associated obstruction of visual cues. Our method solely relies on\nlightweight stereo-RGB cameras and novel transformer-based deep learning\nmodels. It builds on the largest multi-centre spatial robotic surgery dataset\nto date (1.4M self-annotated images from human cadaveric and preclinical in\nvivo studies). By tracking the entire robot and surgical scene, rather than\nindividual markers, our approach provides a holistic view robust to occlusions,\nsupporting surgical scene understanding and context-aware control. We\ndemonstrate an example of potential clinical benefits during in vivo breathing\ncompensation with access to tissue dynamics, unobservable under state of the\nart tracking, and accurately locate in multi-robot systems for future\nintelligent interaction. In addition, and compared with existing systems, our\nmethod eliminates markers and improves tracking visibility by 25%. To our\nknowledge, this is the first demonstration of marker-free proprioception for\nfully draped surgical robots, reducing setup complexity, enhancing safety, and\npaving the way toward modular and autonomous robotic surgery.",
        "url": "http://arxiv.org/abs/2510.23512v1",
        "published_date": "2025-10-27T16:50:12+00:00",
        "updated_date": "2025-10-27T16:50:12+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Martin Huber",
            "Nicola A. Cavalcanti",
            "Ayoob Davoodi",
            "Ruixuan Li",
            "Christopher E. Mower",
            "Fabio Carrillo",
            "Christoph J. Laux",
            "Francois Teyssere",
            "Thibault Chandanson",
            "Antoine Harlé",
            "Elie Saghbiny",
            "Mazda Farshad",
            "Guillaume Morel",
            "Emmanuel Vander Poorten",
            "Philipp Fürnstahl",
            "Sébastien Ourselin",
            "Christos Bergeles",
            "Tom Vercauteren"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a marker-free proprioception method for precise localization of surgical robots under sterile draping using cameras and deep learning, improving tracking visibility by 25% and paving the way for autonomous robotic surgery.",
        "tldr_zh": "本文介绍一种无标记感知方法，通过摄像头和深度学习精确定位手术机器人，提高了跟踪的可见性，为自主机器人手术铺平道路。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "iPac: Incorporating Intra-image Patch Context into Graph Neural Networks for Medical Image Classification",
        "summary": "Graph neural networks have emerged as a promising paradigm for image\nprocessing, yet their performance in image classification tasks is hindered by\na limited consideration of the underlying structure and relationships among\nvisual entities. This work presents iPac, a novel approach to introduce a new\ngraph representation of images to enhance graph neural network image\nclassification by recognizing the importance of underlying structure and\nrelationships in medical image classification. iPac integrates various stages,\nincluding patch partitioning, feature extraction, clustering, graph\nconstruction, and graph-based learning, into a unified network to advance graph\nneural network image classification. By capturing relevant features and\norganising them into clusters, we construct a meaningful graph representation\nthat effectively encapsulates the semantics of the image. Experimental\nevaluation on diverse medical image datasets demonstrates the efficacy of iPac,\nexhibiting an average accuracy improvement of up to 5% over baseline methods.\nOur approach offers a versatile and generic solution for image classification,\nparticularly in the realm of medical images, by leveraging the graph\nrepresentation and accounting for the inherent structure and relationships\namong visual entities.",
        "url": "http://arxiv.org/abs/2510.23504v1",
        "published_date": "2025-10-27T16:37:16+00:00",
        "updated_date": "2025-10-27T16:37:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Usama Zidan",
            "Mohamed Gaber",
            "Mohammed M. Abdelsamea"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces iPac, a novel approach that incorporates intra-image patch context into graph neural networks for medical image classification, showing a 5% accuracy improvement over baseline methods.",
        "tldr_zh": "本文引入了iPac，一种将图神经网络与医学图像分类相结合的新方法，相比基准方法提高了5%的准确率。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    },
    {
        "title": "Quality-controlled registration of urban MLS point clouds reducing drift effects by adaptive fragmentation",
        "summary": "This study presents a novel workflow designed to efficiently and accurately\nregister large-scale mobile laser scanning (MLS) point clouds to a target model\npoint cloud in urban street scenarios. This workflow specifically targets the\ncomplexities inherent in urban environments and adeptly addresses the\nchallenges of integrating point clouds that vary in density, noise\ncharacteristics, and occlusion scenarios, which are common in bustling city\ncenters. Two methodological advancements are introduced. First, the proposed\nSemi-sphere Check (SSC) preprocessing technique optimally fragments MLS\ntrajectory data by identifying mutually orthogonal planar surfaces. This step\nreduces the impact of MLS drift on the accuracy of the entire point cloud\nregistration, while ensuring sufficient geometric features within each fragment\nto avoid local minima. Second, we propose Planar Voxel-based Generalized\nIterative Closest Point (PV-GICP), a fine registration method that selectively\nutilizes planar surfaces within voxel partitions. This pre-process strategy not\nonly improves registration accuracy but also reduces computation time by more\nthan 50% compared to conventional point-to-plane ICP methods. Experiments on\nreal-world datasets from Munich's inner city demonstrate that our workflow\nachieves sub-0.01 m average registration accuracy while significantly\nshortening processing times. The results underscore the potential of the\nproposed methods to advance automated 3D urban modeling and updating, with\ndirect applications in urban planning, infrastructure management, and dynamic\ncity monitoring.",
        "url": "http://arxiv.org/abs/2510.23416v1",
        "published_date": "2025-10-27T15:21:39+00:00",
        "updated_date": "2025-10-27T15:21:39+00:00",
        "categories": [
            "cs.CV",
            "eess.SP"
        ],
        "authors": [
            "Marco Antonio Ortiz Rincon",
            "Yihui Yang",
            "Christoph Holst"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a novel workflow for efficiently registering large-scale MLS point clouds in urban environments, addressing challenges like varying densities and noise, with promising results for automated 3D urban modeling.",
        "tldr_zh": "本文介绍了一种在城市环境中高效注册大规模MLS点云的新工作流程，解决了密度和噪声变化等挑战，对于自动化3D城市建模具有潜在应用。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Color and Frequency Correction for Image Colorization",
        "summary": "The project has carried out the re-optimization of image coloring in\naccordance with the existing Autocolorization direction model DDColor. For the\nexperiments on the existing weights of DDColor, we found that it has\nlimitations in some frequency bands and the color cast problem caused by\ninsufficient input dimension. We construct two optimization schemes and combine\nthem, which achieves the performance improvement of indicators such as PSNR and\nSSIM of the images after DDColor.",
        "url": "http://arxiv.org/abs/2510.23399v1",
        "published_date": "2025-10-27T14:57:14+00:00",
        "updated_date": "2025-10-27T14:57:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yun Kai Zhuang"
        ],
        "ai_categories": [
            "GAN",
            "Transformer"
        ],
        "tldr": "The paper proposes two optimization schemes to improve image colorization performance based on the existing Autocolorization model DDColor.",
        "tldr_zh": "本文提出了两种优化方案，以改善基于现有自动着色模型DDColor的图像着色性能。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7.0
    },
    {
        "title": "DecoDINO: 3D Human-Scene Contact Prediction with Semantic Classification",
        "summary": "Accurate vertex-level contact prediction between humans and surrounding\nobjects is a prerequisite for high fidelity human object interaction models\nused in robotics, AR/VR, and behavioral simulation. DECO was the first in the\nwild estimator for this task but is limited to binary contact maps and\nstruggles with soft surfaces, occlusions, children, and false-positive foot\ncontacts. We address these issues and introduce DecoDINO, a three-branch\nnetwork based on DECO's framework. It uses two DINOv2 ViT-g/14 encoders,\nclass-balanced loss weighting to reduce bias, and patch-level cross-attention\nfor improved local reasoning. Vertex features are finally passed through a\nlightweight MLP with a softmax to assign semantic contact labels. We also\ntested a vision-language model (VLM) to integrate text features, but the\nsimpler architecture performed better and was used instead. On the DAMON\nbenchmark, DecoDINO (i) raises the binary-contact F1 score by 7$\\%$, (ii)\nhalves the geodesic error, and (iii) augments predictions with object-level\nsemantic labels. Ablation studies show that LoRA fine-tuning and the dual\nencoders are key to these improvements. DecoDINO outperformed the challenge\nbaseline in both tasks of the DAMON Challenge. Our code is available at\nhttps://github.com/DavidePasero/deco/tree/main.",
        "url": "http://arxiv.org/abs/2510.23203v1",
        "published_date": "2025-10-27T10:46:22+00:00",
        "updated_date": "2025-10-27T10:46:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lukas Bierling",
            "Davide Pasero",
            "Fleur Dolmans",
            "Helia Ghasemi",
            "Angelo Broere"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "DecoDINO is a network for predicting human-scene contacts with semantic labels, outperforming previous methods and improving accuracy.",
        "tldr_zh": "DecoDINO是一个用于预测人物与场景接触的网络，具有语义标签，优于先前的方法并提高了准确性。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "DQ3D: Depth-guided Query for Transformer-Based 3D Object Detection in Traffic Scenarios",
        "summary": "3D object detection from multi-view images in traffic scenarios has garnered\nsignificant attention in recent years. Many existing approaches rely on object\nqueries that are generated from 3D reference points to localize objects.\nHowever, a limitation of these methods is that some reference points are often\nfar from the target object, which can lead to false positive detections. In\nthis paper, we propose a depth-guided query generator for 3D object detection\n(DQ3D) that leverages depth information and 2D detections to ensure that\nreference points are sampled from the surface or interior of the object.\nFurthermore, to address partially occluded objects in current frame, we\nintroduce a hybrid attention mechanism that fuses historical detection results\nwith depth-guided queries, thereby forming hybrid queries. Evaluation on the\nnuScenes dataset demonstrates that our method outperforms the baseline by 6.3\\%\nin terms of mean Average Precision (mAP) and 4.3\\% in the NuScenes Detection\nScore (NDS).",
        "url": "http://arxiv.org/abs/2510.23144v1",
        "published_date": "2025-10-27T09:20:59+00:00",
        "updated_date": "2025-10-27T09:20:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziyu Wang",
            "Wenhao Li",
            "Ji Wu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a depth-guided query generator for 3D object detection in traffic scenarios, outperforming existing methods by 6.3% in mean Average Precision and 4.3% in NuScenes Detection Score.",
        "tldr_zh": "本文提出了一种深度引导查询生成器，用于交通场景下的3D物体检测，优于现有方法6.3%的平均精度和4.3%的NuScenes检测得分。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Seeing Structural Failure Before it Happens: An Image-Based Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction",
        "summary": "Physics Informed Neural Networks (PINNs) are gaining attention for their\nability to embed physical laws into deep learning models, which is particularly\nuseful in structural engineering tasks with limited data. This paper aims to\nexplore the use of PINNs to predict the weight of small scale spaghetti\nbridges, a task relevant to understanding load limits and potential failure\nmodes in simplified structural models. Our proposed framework incorporates\nphysics-based constraints to the prediction model for improved performance. In\naddition to standard PINNs, we introduce a novel architecture named Physics\nInformed Kolmogorov Arnold Network (PIKAN), which blends universal function\napproximation theory with physical insights. The structural parameters provided\nas input to the model are collected either manually or through computer vision\nmethods. Our dataset includes 15 real bridges, augmented to 100 samples, and\nour best model achieves an $R^2$ score of 0.9603 and a mean absolute error\n(MAE) of 10.50 units. From applied perspective, we also provide a web based\ninterface for parameter entry and prediction. These results show that PINNs can\noffer reliable estimates of structural weight, even with limited data, and may\nhelp inform early stage failure analysis in lightweight bridge designs.\n  The complete data and code are available at\nhttps://github.com/OmerJauhar/PINNS-For-Spaghetti-Bridges.",
        "url": "http://arxiv.org/abs/2510.23117v1",
        "published_date": "2025-10-27T08:38:17+00:00",
        "updated_date": "2025-10-27T08:38:17+00:00",
        "categories": [
            "cs.LG",
            "cs.CV",
            "65M70 (Primary), 68T07 (Secondary)",
            "I.2.6; I.4.8; G.1.8"
        ],
        "authors": [
            "Omer Jauhar Khan",
            "Sudais Khan",
            "Hafeez Anwar"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper proposes a novel Physics-Informed Neural Network (PINN) for predicting the weight of spaghetti bridges, incorporating physics-based constraints and achieving high accuracy. They also introduce a new architecture called Physics Informed Kolmogorov Arnold Network (PIKAN) for improved performance.",
        "tldr_zh": "本文提出了一种新颖的物理信息神经网络(PINN)，用于预测意大利面桥的重量，结合物理约束并取得了高准确性。他们还介绍了一种名为Physics Informed Kolmogorov Arnold Network (PIKAN)的新架构，用于提高性能。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Strategies for Robust Deep Learning Based Deformable Registration",
        "summary": "Deep learning based deformable registration methods have become popular in\nrecent years. However, their ability to generalize beyond training data\ndistribution can be poor, significantly hindering their usability. LUMIR brain\nregistration challenge for Learn2Reg 2025 aims to advance the field by\nevaluating the performance of the registration on contrasts and modalities\ndifferent from those included in the training set. Here we describe our\nsubmission to the challenge, which proposes a very simple idea for\nsignificantly improving robustness by transforming the images into MIND feature\nspace before feeding them into the model. In addition, a special ensembling\nstrategy is proposed that shows a small but consistent improvement.",
        "url": "http://arxiv.org/abs/2510.23079v1",
        "published_date": "2025-10-27T07:29:28+00:00",
        "updated_date": "2025-10-27T07:29:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Joel Honkamaa",
            "Pekka Marttinen"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces strategies to improve the robustness of deep learning based deformable registration methods for brain imaging by transforming images into MIND feature space and using a special ensembling strategy.",
        "tldr_zh": "本文提出了一种策略，通过将图像转换成 MIND 特征空间并使用特殊的集成策略，来提高基于深度学习的可变形配准方法在脑成像中的鲁棒性。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7.0
    },
    {
        "title": "Seq-DeepIPC: Sequential Sensing for End-to-End Control in Legged Robot Navigation",
        "summary": "We present Seq-DeepIPC, a sequential end-to-end perception-to-control model\nfor legged robot navigation in realworld environments. Seq-DeepIPC advances\nintelligent sensing for autonomous legged navigation by tightly integrating\nmulti-modal perception (RGB-D + GNSS) with temporal fusion and control. The\nmodel jointly predicts semantic segmentation and depth estimation, giving\nricher spatial features for planning and control. For efficient deployment on\nedge devices, we use EfficientNet-B0 as the encoder, reducing computation while\nmaintaining accuracy. Heading estimation is simplified by removing the noisy\nIMU and instead computing the bearing angle directly from consecutive GNSS\npositions. We collected a larger and more diverse dataset that includes both\nroad and grass terrains, and validated Seq-DeepIPC on a robot dog. Comparative\nand ablation studies show that sequential inputs improve perception and control\nin our models, while other baselines do not benefit. Seq-DeepIPC achieves\ncompetitive or better results with reasonable model size; although GNSS-only\nheading is less reliable near tall buildings, it is robust in open areas.\nOverall, Seq-DeepIPC extends end-to-end navigation beyond wheeled robots to\nmore versatile and temporally-aware systems. To support future research, we\nwill release the codes to our GitHub repository at\nhttps://github.com/oskarnatan/Seq-DeepIPC.",
        "url": "http://arxiv.org/abs/2510.23057v1",
        "published_date": "2025-10-27T06:39:57+00:00",
        "updated_date": "2025-10-27T06:39:57+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.SY",
            "eess.IV",
            "eess.SY"
        ],
        "authors": [
            "Oskar Natan",
            "Jun Miura"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "Seq-DeepIPC is a sequential end-to-end perception-to-control model for legged robot navigation, integrating multi-modal perception with temporal fusion and control, achieving competitive results with reasonable model size.",
        "tldr_zh": "Seq-DeepIPC是一种用于四肢机器人导航的序列端到端感知-控制模型，将多模态感知与时间融合和控制紧密集成，在保持合理模型大小的同时取得竞争性结果。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "An Intelligent Water-Saving Irrigation System Based on Multi-Sensor Fusion and Visual Servoing Control",
        "summary": "This paper introduces an intelligent water-saving irrigation system designed\nto address critical challenges in precision agriculture, such as inefficient\nwater use and poor terrain adaptability. The system integrates advanced\ncomputer vision, robotic control, and real-time stabilization technologies via\na multi-sensor fusion approach. A lightweight YOLO model, deployed on an\nembedded vision processor (K210), enables real-time plant container detection\nwith over 96% accuracy under varying lighting conditions. A simplified hand-eye\ncalibration algorithm-designed for 'handheld camera' robot arm\nconfigurations-ensures that the end effector can be precisely positioned, with\na success rate exceeding 90%. The active leveling system, driven by the\nSTM32F103ZET6 main control chip and JY901S inertial measurement data, can\nstabilize the irrigation platform on slopes up to 10 degrees, with a response\ntime of 1.8 seconds. Experimental results across three simulated agricultural\nenvironments (standard greenhouse, hilly terrain, complex lighting) demonstrate\na 30-50% reduction in water consumption compared to conventional flood\nirrigation, with water use efficiency exceeding 92% in all test cases.",
        "url": "http://arxiv.org/abs/2510.23003v1",
        "published_date": "2025-10-27T04:43:20+00:00",
        "updated_date": "2025-10-27T04:43:20+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.SY",
            "eess.SY"
        ],
        "authors": [
            "ZhengKai Huang",
            "YiKun Wang",
            "ChenYu Hui",
            "XiaoCheng"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper presents an intelligent water-saving irrigation system that integrates computer vision, robotic control, and real-time stabilization technologies to reduce water consumption in precision agriculture.",
        "tldr_zh": "本文介绍了一种智能节水灌溉系统，结合了计算机视觉、机器人控制和实时稳定技术，以减少精准农业中的用水量。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "VoMP: Predicting Volumetric Mechanical Property Fields",
        "summary": "Physical simulation relies on spatially-varying mechanical properties, often\nlaboriously hand-crafted. VoMP is a feed-forward method trained to predict\nYoung's modulus ($E$), Poisson's ratio ($\\nu$), and density ($\\rho$) throughout\nthe volume of 3D objects, in any representation that can be rendered and\nvoxelized. VoMP aggregates per-voxel multi-view features and passes them to our\ntrained Geometry Transformer to predict per-voxel material latent codes. These\nlatents reside on a manifold of physically plausible materials, which we learn\nfrom a real-world dataset, guaranteeing the validity of decoded per-voxel\nmaterials. To obtain object-level training data, we propose an annotation\npipeline combining knowledge from segmented 3D datasets, material databases,\nand a vision-language model, along with a new benchmark. Experiments show that\nVoMP estimates accurate volumetric properties, far outperforming prior art in\naccuracy and speed.",
        "url": "http://arxiv.org/abs/2510.22975v1",
        "published_date": "2025-10-27T03:56:25+00:00",
        "updated_date": "2025-10-27T03:56:25+00:00",
        "categories": [
            "cs.CV",
            "cs.GR",
            "cs.LG"
        ],
        "authors": [
            "Rishit Dagli",
            "Donglai Xiang",
            "Vismay Modi",
            "Charles Loop",
            "Clement Fuji Tsang",
            "Anka He Chen",
            "Anita Hu",
            "Gavriel State",
            "David I. W. Levin",
            "Maria Shugrina"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "VoMP is a method that predicts mechanical properties throughout 3D objects using a feed-forward approach, outperforming previous methods in accuracy and speed.",
        "tldr_zh": "VoMP是一种使用前馈方法在3D对象中预测机械性能的方法，其准确性和速度均优于先前的方法。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "DPGLA: Bridging the Gap between Synthetic and Real Data for Unsupervised Domain Adaptation in 3D LiDAR Semantic Segmentation",
        "summary": "Annotating real-world LiDAR point clouds for use in intelligent autonomous\nsystems is costly. To overcome this limitation, self-training-based\nUnsupervised Domain Adaptation (UDA) has been widely used to improve point\ncloud semantic segmentation by leveraging synthetic point cloud data. However,\nwe argue that existing methods do not effectively utilize unlabeled data, as\nthey either rely on predefined or fixed confidence thresholds, resulting in\nsuboptimal performance. In this paper, we propose a Dynamic Pseudo-Label\nFiltering (DPLF) scheme to enhance real data utilization in point cloud UDA\nsemantic segmentation. Additionally, we design a simple and efficient\nPrior-Guided Data Augmentation Pipeline (PG-DAP) to mitigate domain shift\nbetween synthetic and real-world point clouds. Finally, we utilize data mixing\nconsistency loss to push the model to learn context-free representations. We\nimplement and thoroughly evaluate our approach through extensive comparisons\nwith state-of-the-art methods. Experiments on two challenging synthetic-to-real\npoint cloud semantic segmentation tasks demonstrate that our approach achieves\nsuperior performance. Ablation studies confirm the effectiveness of the DPLF\nand PG-DAP modules. We release the code of our method in this paper.",
        "url": "http://arxiv.org/abs/2510.23525v1",
        "published_date": "2025-10-27T17:05:59+00:00",
        "updated_date": "2025-10-27T17:05:59+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Wanmeng Li",
            "Simone Mosco",
            "Daniel Fusaro",
            "Alberto Pretto"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel method, DPGLA, for unsupervised domain adaptation in 3D LiDAR semantic segmentation by enhancing real data utilization through dynamic pseudo-label filtering and data augmentation techniques.",
        "tldr_zh": "本文提出了一种新颖的方法，DPGLA，在3D LiDAR语义分割的无监督域适应中通过动态伪标签过滤和数据增强技术增强实际数据利用。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Note on the Construction of Structure Tensor",
        "summary": "This note presents a theoretical discussion of two structure tensor\nconstructions: one proposed by Bigun and Granlund 1987, and the other by\nGranlund and Knutsson 1995. At first glance, these approaches may appear quite\ndifferent--the former is implemented by averaging outer products of gradient\nfilter responses, while the latter constructs the tensor from weighted outer\nproducts of tune-in frequency vectors of quadrature filters. We argue that when\nboth constructions are viewed through the common lens of Total Least Squares\n(TLS) line fitting to the power spectrum, they can be reconciled to a large\nextent, and additional benefits emerge. From this perspective, the correction\nterm introduced in Granlund and Knutsson 1995 becomes unnecessary. Omitting it\nensures that the resulting tensor remains positive semi-definite, thereby\nsimplifying the interpretation of its eigenvalues. Furthermore, this\ninterpretation allows fitting more than a single 0rientation to the input by\nreinterpreting quadrature filter responses without relying on a structure\ntensor. It also removes the constraint that responses must originate strictly\nfrom quadrature filters, allowing the use of alternative filter types and\nnon-angular tessellations. These alternatives include Gabor filters--which,\nalthough not strictly quadrature, are still suitable for structure tensor\nconstruction--even when they tessellate the spectrum in a Cartesian fashion,\nprovided they are sufficiently concentrated.",
        "url": "http://arxiv.org/abs/2510.23137v1",
        "published_date": "2025-10-27T09:16:34+00:00",
        "updated_date": "2025-10-27T09:16:34+00:00",
        "categories": [
            "cs.CV",
            "math.SP"
        ],
        "authors": [
            "Josef Bigun",
            "Fernado Alonso-Fernandez"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "The paper discusses two different constructions of a structure tensor used in image processing and argues that they can be reconciled to simplify interpretation and allow for more flexible use of filter types.",
        "tldr_zh": "本文讨论了两种不同的结构张量构造方法，用于图像处理，并认为它们可以被调和以简化解释和允许更灵活地使用滤波器类型。",
        "relevance_score": 5,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6.5
    },
    {
        "title": "LLM-based Fusion of Multi-modal Features for Commercial Memorability Prediction",
        "summary": "This paper addresses the prediction of commercial (brand) memorability as\npart of \"Subtask 2: Commercial/Ad Memorability\" within the \"Memorability:\nPredicting movie and commercial memorability\" task at the MediaEval 2025\nworkshop competition. We propose a multimodal fusion system with a Gemma-3 LLM\nbackbone that integrates pre-computed visual (ViT) and textual (E5) features by\nmulti-modal projections. The model is adapted using Low-Rank Adaptation (LoRA).\nA heavily-tuned ensemble of gradient boosted trees serves as a baseline. A key\ncontribution is the use of LLM-generated rationale prompts, grounded in\nexpert-derived aspects of memorability, to guide the fusion model. The results\ndemonstrate that the LLM-based system exhibits greater robustness and\ngeneralization performance on the final test set, compared to the baseline.\n  The paper's codebase can be found at\nhttps://github.com/dsgt-arc/mediaeval-2025-memorability",
        "url": "http://arxiv.org/abs/2510.22829v1",
        "published_date": "2025-10-26T20:51:52+00:00",
        "updated_date": "2025-10-26T20:51:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "authors": [
            "Aleksandar Pramov"
        ],
        "ai_categories": [
            "Multimodality",
            "LoRA",
            "Transformer"
        ],
        "tldr": "This paper presents a multimodal fusion system using LLM to predict commercial memorability with improved performance compared to baseline models.",
        "tldr_zh": "本文提出了一种使用LLM进行多模态融合的系统，用于预测商业记忆度，性能优于基准模型。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Bi-Encoder Contrastive Learning for Fingerprint and Iris Biometrics",
        "summary": "There has been a historic assumption that the biometrics of an individual are\nstatistically uncorrelated. We test this assumption by training Bi-Encoder\nnetworks on three verification tasks, including fingerprint-to-fingerprint\nmatching, iris-to-iris matching, and cross-modal fingerprint-to-iris matching\nusing 274 subjects with $\\sim$100k fingerprints and 7k iris images. We trained\nResNet-50 and Vision Transformer backbones in Bi-Encoder architectures such\nthat the contrastive loss between images sampled from the same individual is\nminimized. The iris ResNet architecture reaches 91 ROC AUC score for\niris-to-iris matching, providing clear evidence that the left and right irises\nof an individual are correlated. Fingerprint models reproduce the positive\nintra-subject suggested by prior work in this space. This is the first work\nattempting to use Vision Transformers for this matching. Cross-modal matching\nrises only slightly above chance, which suggests that more data and a more\nsophisticated pipeline is needed to obtain compelling results. These findings\ncontinue challenge independence assumptions of biometrics and we plan to extend\nthis work to other biometrics in the future. Code available:\nhttps://github.com/MatthewSo/bio_fingerprints_iris.",
        "url": "http://arxiv.org/abs/2510.22937v1",
        "published_date": "2025-10-27T02:41:43+00:00",
        "updated_date": "2025-10-27T02:41:43+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Matthew So",
            "Judah Goldfeder",
            "Mark Lis",
            "Hod Lipson"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper explores the correlation between fingerprint and iris biometrics through bi-encoder contrastive learning, finding evidence of correlation between iris images of the same individual and positive intra-subject fingerprint matching.",
        "tldr_zh": "本文通过生物编码器对比学习探讨了指纹和虹膜生物特征之间的相关性，发现同一个个体的虹膜图像之间存在相关性，并找到了正的同主题指纹匹配。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]