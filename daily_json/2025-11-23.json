[
    {
        "title": "Native 3D Editing with Full Attention",
        "summary": "Instruction-guided 3D editing is a rapidly emerging field with the potential to broaden access to 3D content creation. However, existing methods face critical limitations: optimization-based approaches are prohibitively slow, while feed-forward approaches relying on multi-view 2D editing often suffer from inconsistent geometry and degraded visual quality. To address these issues, we propose a novel native 3D editing framework that directly manipulates 3D representations in a single, efficient feed-forward pass. Specifically, we create a large-scale, multi-modal dataset for instruction-guided 3D editing, covering diverse addition, deletion, and modification tasks. This dataset is meticulously curated to ensure that edited objects faithfully adhere to the instructional changes while preserving the consistency of unedited regions with the source object. Building upon this dataset, we explore two distinct conditioning strategies for our model: a conventional cross-attention mechanism and a novel 3D token concatenation approach. Our results demonstrate that token concatenation is more parameter-efficient and achieves superior performance. Extensive evaluations show that our method outperforms existing 2D-lifting approaches, setting a new benchmark in generation quality, 3D consistency, and instruction fidelity.",
        "url": "http://arxiv.org/abs/2511.17501v1",
        "published_date": "2025-11-21T18:59:26+00:00",
        "updated_date": "2025-11-21T18:59:26+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Weiwei Cai",
            "Shuangkang Fang",
            "Weicai Ye",
            "Xin Dong",
            "Yunhan Yang",
            "Xuanyang Zhang",
            "Wei Cheng",
            "Yanpei Cao",
            "Gang Yu",
            "Tao Chen"
        ],
        "ai_categories": []
    },
    {
        "title": "EvDiff: High Quality Video with an Event Camera",
        "summary": "As neuromorphic sensors, event cameras asynchronously record changes in brightness as streams of sparse events with the advantages of high temporal resolution and high dynamic range. Reconstructing intensity images from events is a highly ill-posed task due to the inherent ambiguity of absolute brightness. Early methods generally follow an end-to-end regression paradigm, directly mapping events to intensity frames in a deterministic manner. While effective to some extent, these approaches often yield perceptually inferior results and struggle to scale up in model capacity and training data. In this work, we propose EvDiff, an event-based diffusion model that follows a surrogate training framework to produce high-quality videos. To reduce the heavy computational cost of high-frame-rate video generation, we design an event-based diffusion model that performs only a single forward diffusion step, equipped with a temporally consistent EvEncoder. Furthermore, our novel Surrogate Training Framework eliminates the dependence on paired event-image datasets, allowing the model to leverage large-scale image datasets for higher capacity. The proposed EvDiff is capable of generating high-quality colorful videos solely from monochromatic event streams. Experiments on real-world datasets demonstrate that our method strikes a sweet spot between fidelity and realism, outperforming existing approaches on both pixel-level and perceptual metrics.",
        "url": "http://arxiv.org/abs/2511.17492v1",
        "published_date": "2025-11-21T18:49:18+00:00",
        "updated_date": "2025-11-21T18:49:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weilun Li",
            "Lei Sun",
            "Ruixi Gao",
            "Qi Jiang",
            "Yuqin Ma",
            "Kaiwei Wang",
            "Ming-Hsuan Yang",
            "Luc Van Gool",
            "Danda Pani Paudel"
        ],
        "ai_categories": []
    },
    {
        "title": "Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination",
        "summary": "Understanding text-rich videos requires reading small, transient textual cues that often demand repeated inspection. Yet most video QA models rely on single-pass perception over fixed frames, leading to hallucinations and failures on fine-grained evidence. Inspired by how humans pause, zoom, and re-read critical regions, we introduce Video-R4 (Reinforcing Text-Rich Video Reasoning with Visual Rumination), a video reasoning LMM that performs visual rumination: iteratively selecting frames, zooming into informative regions, re-encoding retrieved pixels, and updating its reasoning state. We construct two datasets with executable rumination trajectories: Video-R4-CoT-17k for supervised practice and Video-R4-RL-30k for reinforcement learning. We propose a multi-stage rumination learning framework that progressively finetunes a 7B LMM to learn atomic and mixing visual operations via SFT and GRPO-based RL. Video-R4-7B achieves state-of-the-art results on M4-ViteVQA and further generalizes to multi-page document QA, slides QA, and generic video QA, demonstrating that iterative rumination is an effective paradigm for pixel-grounded multimodal reasoning.",
        "url": "http://arxiv.org/abs/2511.17490v1",
        "published_date": "2025-11-21T18:47:09+00:00",
        "updated_date": "2025-11-21T18:47:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yolo Yunlong Tang",
            "Daiki Shimada",
            "Hang Hua",
            "Chao Huang",
            "Jing Bi",
            "Rogerio Feris",
            "Chenliang Xu"
        ],
        "ai_categories": []
    },
    {
        "title": "Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models",
        "summary": "Scaling up multimodal models has enabled remarkable advances in visual understanding and reasoning, but practical demands call for smaller, efficient systems. In this work, we conduct a principled analysis of downscaling intelligence in multimodal models, examining how reduced large language model (LLM) capacity affects multimodal capabilities. Our initial findings reveal an interesting trend: LLM downscaling disproportionately affects visual capabilities, rather than abilities inherited from the LLM. We then examine whether this drop mainly reflects the expected decline in visual reasoning or a more fundamental loss of perceptual abilities. Isolating the effect of LLM downscaling on perception, we find performance still drops sharply, often matching or exceeding the impact on reasoning. To address this bottleneck, we introduce visual extraction tuning, which explicitly trains the model to extract instruction-relevant visual details consistently across tasks. With these extracted visual details, we then apply step-by-step reasoning to generate answers. Together, these components form our Extract+Think approach, setting a new standard for efficiency and performance in this space.",
        "url": "http://arxiv.org/abs/2511.17487v1",
        "published_date": "2025-11-21T18:43:01+00:00",
        "updated_date": "2025-11-21T18:43:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mark Endo",
            "Serena Yeung-Levy"
        ],
        "ai_categories": []
    },
    {
        "title": "An Artificial Intelligence Framework for Measuring Human Spine Aging Using MRI",
        "summary": "The human spine is a complex structure composed of 33 vertebrae. It holds the body and is important for leading a healthy life. The spine is vulnerable to age-related degenerations that can be identified through magnetic resonance imaging (MRI). In this paper we propose a novel computer-vison-based deep learning method to estimate spine age using images from over 18,000 MRI series. Data are restricted to subjects with only age-related spine degeneration. Eligibility criteria are created by identifying common age-based clusters of degenerative spine conditions using uniform manifold approximation and projection (UMAP) and hierarchical density-based spatial clustering of applications with noise (HDBSCAN). Model selection is determined using a detailed ablation study on data size, loss, and the effect of different spine regions. We evaluate the clinical utility of our model by calculating the difference between actual spine age and model-predicted age, the spine age gap (SAG), and examining the association between these differences and spine degenerative conditions and lifestyle factors. We find that SAG is associated with conditions including disc bulges, disc osteophytes, spinal stenosis, and fractures, as well as lifestyle factors like smoking and physically demanding work, and thus may be a useful biomarker for measuring overall spine health.",
        "url": "http://arxiv.org/abs/2511.17485v1",
        "published_date": "2025-11-21T18:40:21+00:00",
        "updated_date": "2025-11-21T18:40:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Roozbeh Bazargani",
            "Saqib Abdullah Basar",
            "Daniel Daly-Grafstein",
            "Rodrigo Solis Pompa",
            "Soojin Lee",
            "Saurabh Garg",
            "Yuntong Ma",
            "John A. Carrino",
            "Siavash Khallaghi",
            "Sam Hashemi"
        ],
        "ai_categories": []
    },
    {
        "title": "Radar2Shape: 3D Shape Reconstruction from High-Frequency Radar using Multiresolution Signed Distance Functions",
        "summary": "Determining the shape of 3D objects from high-frequency radar signals is analytically complex but critical for commercial and aerospace applications. Previous deep learning methods have been applied to radar modeling; however, they often fail to represent arbitrary shapes or have difficulty with real-world radar signals which are collected over limited viewing angles. Existing methods in optical 3D reconstruction can generate arbitrary shapes from limited camera views, but struggle when they naively treat the radar signal as a camera view. In this work, we present Radar2Shape, a denoising diffusion model that handles a partially observable radar signal for 3D reconstruction by correlating its frequencies with multiresolution shape features. Our method consists of a two-stage approach: first, Radar2Shape learns a regularized latent space with hierarchical resolutions of shape features, and second, it diffuses into this latent space by conditioning on the frequencies of the radar signal in an analogous coarse-to-fine manner. We demonstrate that Radar2Shape can successfully reconstruct arbitrary 3D shapes even from partially-observed radar signals, and we show robust generalization to two different simulation methods and real-world data. Additionally, we release two synthetic benchmark datasets to encourage future research in the high-frequency radar domain so that models like Radar2Shape can safely be adapted into real-world radar systems.",
        "url": "http://arxiv.org/abs/2511.17484v1",
        "published_date": "2025-11-21T18:40:03+00:00",
        "updated_date": "2025-11-21T18:40:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Neel Sortur",
            "Justin Goodwin",
            "Purvik Patel",
            "Luis Enrique Martinez",
            "Tzofi Klinghoffer",
            "Rajmonda S. Caceres",
            "Robin Walters"
        ],
        "ai_categories": []
    },
    {
        "title": "Counterfactual World Models via Digital Twin-conditioned Video Diffusion",
        "summary": "World models learn to predict the temporal evolution of visual observations given a control signal, potentially enabling agents to reason about environments through forward simulation. Because of the focus on forward simulation, current world models generate predictions based on factual observations. For many emerging applications, such as comprehensive evaluations of physical AI behavior under varying conditions, the ability of world models to answer counterfactual queries, such as \"what would happen if this object was removed?\", is of increasing importance. We formalize counterfactual world models that additionally take interventions as explicit inputs, predicting temporal sequences under hypothetical modifications to observed scene properties. Traditional world models operate directly on entangled pixel-space representations where object properties and relationships cannot be selectively modified. This modeling choice prevents targeted interventions on specific scene properties. We introduce CWMDT, a framework to overcome those limitations, turning standard video diffusion models into effective counterfactual world models. First, CWMDT constructs digital twins of observed scenes to explicitly encode objects and their relationships, represented as structured text. Second, CWMDT applies large language models to reason over these representations and predict how a counterfactual intervention propagates through time to alter the observed scene. Third, CWMDT conditions a video diffusion model with the modified representation to generate counterfactual visual sequences. Evaluations on two benchmarks show that the CWMDT approach achieves state-of-the-art performance, suggesting that alternative representations of videos, such as the digital twins considered here, offer powerful control signals for video forward simulation-based world models.",
        "url": "http://arxiv.org/abs/2511.17481v1",
        "published_date": "2025-11-21T18:37:23+00:00",
        "updated_date": "2025-11-21T18:37:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiqing Shen",
            "Aiza Maksutova",
            "Chenjia Li",
            "Mathias Unberath"
        ],
        "ai_categories": []
    }
]