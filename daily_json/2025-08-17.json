[
    {
        "title": "SafeCtrl: Region-Based Safety Control for Text-to-Image Diffusion via Detect-Then-Suppress",
        "summary": "The widespread deployment of text-to-image models is challenged by their\npotential to generate harmful content. While existing safety methods, such as\nprompt rewriting or model fine-tuning, provide valuable interventions, they\noften introduce a trade-off between safety and fidelity. Recent\nlocalization-based approaches have shown promise, yet their reliance on\nexplicit ``concept replacement\" can sometimes lead to semantic incongruity. To\naddress these limitations, we explore a more flexible detect-then-suppress\nparadigm. We introduce SafeCtrl, a lightweight, non-intrusive plugin that first\nprecisely localizes unsafe content. Instead of performing a hard A-to-B\nsubstitution, SafeCtrl then suppresses the harmful semantics, allowing the\ngenerative process to naturally and coherently resolve into a safe,\ncontext-aware alternative. A key aspect of our work is a novel training\nstrategy using Direct Preference Optimization (DPO). We leverage readily\navailable, image-level preference data to train our module, enabling it to\nlearn nuanced suppression behaviors and perform region-guided interventions at\ninference without requiring costly, pixel-level annotations. Extensive\nexperiments show that SafeCtrl significantly outperforms state-of-the-art\nmethods in both safety efficacy and fidelity preservation. Our findings suggest\nthat decoupled, suppression-based control is a highly effective and scalable\ndirection for building more responsible generative models.",
        "url": "http://arxiv.org/abs/2508.11904v1",
        "published_date": "2025-08-16T04:28:52+00:00",
        "updated_date": "2025-08-16T04:28:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lingyun Zhang",
            "Yu Xie",
            "Yanwei Fu",
            "Ping Chen"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper introduces SafeCtrl, a region-based safety control method for text-to-image diffusion, improving safety without sacrificing fidelity.",
        "tldr_zh": "该论文介绍了SafeCtrl，一种基于区域的安全控制方法，用于文本到图像的扩散，提高了安全性而不牺牲准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "AdaRing: Towards Ultra-Light Vision-Language Adaptation via Cross-Layer Tensor Ring Decomposition",
        "summary": "Adapter-based fine-tuning has gained remarkable attention in adapting large\npre-trained vision language models (VLMs) for a wide range of downstream tasks\nefficiently. In this paradigm, only the inserted adapters are fine-tuned,\nwithout the need for training the original VLM backbone. Existing works scale\nadapters by integrating them into every layer of VLMs to increase the capacity\nof adapters. However, these methods face two primary limitations: 1) limited\ncompression rate due to ignoring cross-layer redundancy, and 2) limited\nrepresentational capacity across homogeneous adapters. In this paper, we\npropose a novel vision-language fine-tuning framework based on cross-layer\ntensor ring decomposition (TRD) with the integration and collaboration of\ndiverse adapters, called AdaRing, achieving ultra-light parameter-efficient\nadaptation of VLMs on various tasks. To remove the high redundancy that exists\namong adapters across layers, we exploit the tensor-level low-rankness to\nformulate adapters as layer-shared tensor cores and layer-specific slices.\nMoreover, guided by generalization-aware fine-tuning, diverse rank-driven\nadapters cooperate to handle tasks that require different representations. Our\nexperiments show that the proposed AdaRing achieves the state-of-the-art\nperformance while reducing average training parameters by 90%.",
        "url": "http://arxiv.org/abs/2508.11870v1",
        "published_date": "2025-08-16T01:56:27+00:00",
        "updated_date": "2025-08-16T01:56:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ying Huang",
            "Yuanbin Man",
            "Wenqi Jia",
            "Zhengzhong Tu",
            "Junzhou Huang",
            "Miao Yin"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality"
        ],
        "tldr": "AdaRing proposes a vision-language fine-tuning framework based on cross-layer tensor ring decomposition for ultra-light parameter-efficient adaptation of VLMs on various tasks, achieving state-of-the-art performance with 90% parameter reduction.",
        "tldr_zh": "AdaRing提出了一种基于跨层张量环分解的视觉语言微调框架，实现了在各种任务上对VLM进行超轻量参数高效适应，性能达到了最新水平，同时降低了90%的参数。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "ComplicitSplat: Downstream Models are Vulnerable to Blackbox Attacks by 3D Gaussian Splat Camouflages",
        "summary": "As 3D Gaussian Splatting (3DGS) gains rapid adoption in safety-critical tasks\nfor efficient novel-view synthesis from static images, how might an adversary\ntamper images to cause harm? We introduce ComplicitSplat, the first attack that\nexploits standard 3DGS shading methods to create viewpoint-specific camouflage\n- colors and textures that change with viewing angle - to embed adversarial\ncontent in scene objects that are visible only from specific viewpoints and\nwithout requiring access to model architecture or weights. Our extensive\nexperiments show that ComplicitSplat generalizes to successfully attack a\nvariety of popular detector - both single-stage, multi-stage, and\ntransformer-based models on both real-world capture of physical objects and\nsynthetic scenes. To our knowledge, this is the first black-box attack on\ndownstream object detectors using 3DGS, exposing a novel safety risk for\napplications like autonomous navigation and other mission-critical robotic\nsystems.",
        "url": "http://arxiv.org/abs/2508.11854v1",
        "published_date": "2025-08-16T00:38:34+00:00",
        "updated_date": "2025-08-16T00:38:34+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Matthew Hull",
            "Haoyang Yang",
            "Pratham Mehta",
            "Mansi Phute",
            "Aeree Cho",
            "Haorang Wang",
            "Matthew Lau",
            "Wenke Lee",
            "Wilian Lunardi",
            "Martin Andreoni",
            "Polo Chau"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Other"
        ],
        "tldr": "ComplicitSplat introduces a black-box attack using 3D Gaussian Splatting to embed adversarial content in scene objects visible from specific viewpoints, posing a safety risk for downstream object detectors.",
        "tldr_zh": "ComplicitSplat利用3D高斯喷溅技术进行黑盒攻击，将对手内容嵌入场景物体中，只能从特定视角看到，为下游目标检测器带来安全风险。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Simple o3: Towards Interleaved Vision-Language Reasoning",
        "summary": "Multimodal Large Language Models (MLLMs) have shown impressive performance on\nvision-language tasks, but their long Chain-of-Thought (CoT) capabilities in\nmultimodal scenarios remain underexplored. Inspired by OpenAI's o3 model, which\nemulates human-like ''thinking with image'' through iterative visual\ntransformations and linguistic reasoning, we propose Simple o3, an end-to-end\nframework that integrates dynamic tool interactions (e.g., cropping, zooming,\nand reusing) into interleaved vision-language reasoning via supervised\nfine-tuning (SFT). Our approach features a scalable data synthesis pipeline\nthat generates high-quality interleaved vision-language reasoning chains via an\n''observe-reason-act'' cycle, complete with executable visual operations and\nrigorous verification, yielding the open-source TWI-Tools-146K dataset.\nExperimental results demonstrate Simple o3's superior performance on diverse\nbenchmarks, outperforming existing approaches. By combining enhanced reasoning\ncapabilities, Simple o3 establishes a powerful yet computationally affordable\nparadigm for advancing multimodal reasoning. Remarkably, we provide the first\nin-depth analysis of different interleaved reasoning strategies, offering\ninsights into their impact on model performance. We found that by introducing\nadditional visual tokens for interleaved vision-language reasoning, reusing and\nmagnifying the original image significantly improves the model's visual\nreasoning and fine-grained perception, while image cropping based on precise\nvisual grounding allows the model to effectively focus on key entities or\nregions, further enhancing its capabilities.",
        "url": "http://arxiv.org/abs/2508.12109v1",
        "published_date": "2025-08-16T17:15:39+00:00",
        "updated_date": "2025-08-16T17:15:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ye Wang",
            "Qianglong Chen",
            "Zejun Li",
            "Siyuan Wang",
            "Shijie Guo",
            "Zhirui Zhang",
            "Zhongyu Wei"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces Simple o3, an end-to-end framework for interleaved vision-language reasoning that outperforms existing approaches by incorporating dynamic tool interactions and generating high-quality reasoning chains.",
        "tldr_zh": "本文介绍了Simple o3，这是一个端到端框架，用于交错式视觉-语言推理，通过整合动态工具交互和生成高质量的推理链，胜过现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VELVET-Med: Vision and Efficient Language Pre-training for Volumetric Imaging Tasks in Medicine",
        "summary": "Vision-and-language models (VLMs) have been increasingly explored in the\nmedical domain, particularly following the success of CLIP in general domain.\nHowever, unlike the relatively straightforward pairing of 2D images and text,\ncurating large-scale paired data in the medical field for volumetric modalities\nsuch as CT scans remains a challenging and time-intensive process. This\ndifficulty often limits the performance on downstream tasks. To address these\nchallenges, we propose a novel vision-language pre-training (VLP) framework,\ntermed as \\textbf{VELVET-Med}, specifically designed for limited volumetric\ndata such as 3D CT and associated radiology reports. Instead of relying on\nlarge-scale data collection, our method focuses on the development of effective\npre-training objectives and model architectures. The key contributions are: 1)\nWe incorporate uni-modal self-supervised learning into VLP framework, which are\noften underexplored in the existing literature. 2) We propose a novel language\nencoder, termed as \\textbf{TriBERT}, for learning multi-level textual\nsemantics. 3) We devise the hierarchical contrastive learning to capture\nmulti-level vision-language correspondence. Using only 38,875 scan-report\npairs, our approach seeks to uncover rich spatial and semantic relationships\nembedded in volumetric medical images and corresponding clinical narratives,\nthereby enhancing the generalization ability of the learned encoders. The\nresulting encoders exhibit strong transferability, achieving state-of-the-art\nperformance across a wide range of downstream tasks, including 3D segmentation,\ncross-modal retrieval, visual question answering, and report generation.",
        "url": "http://arxiv.org/abs/2508.12108v1",
        "published_date": "2025-08-16T17:08:43+00:00",
        "updated_date": "2025-08-16T17:08:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziyang Zhang",
            "Yang Yu",
            "Xulei Yang",
            "Si Yong Yeo"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces VELVET-Med, a pre-training framework for medical imaging tasks using limited volumetric data, achieving state-of-the-art performance across various downstream tasks.",
        "tldr_zh": "本文介绍了VELVET-Med，一个针对医学成像任务的预训练框架，利用有限的体积数据，在各种下游任务中取得了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion",
        "summary": "Diffusion models have transformed image synthesis by establishing\nunprecedented quality and creativity benchmarks. Nevertheless, their\nlarge-scale deployment faces challenges due to computationally intensive\niterative denoising processes. Although post-training quantization(PTQ)\nprovides an effective pathway for accelerating sampling, the iterative nature\nof diffusion models causes stepwise quantization errors to accumulate\nprogressively during generation, inevitably compromising output fidelity. To\naddress this challenge, we develop a theoretical framework that mathematically\nformulates error propagation in Diffusion Models (DMs), deriving per-step\nquantization error propagation equations and establishing the first closed-form\nsolution for cumulative error. Building on this theoretical foundation, we\npropose a timestep-aware cumulative error compensation scheme. Extensive\nexperiments across multiple image datasets demonstrate that our compensation\nstrategy effectively mitigates error propagation, significantly enhancing\nexisting PTQ methods to achieve state-of-the-art(SOTA) performance on\nlow-precision diffusion models.",
        "url": "http://arxiv.org/abs/2508.12094v1",
        "published_date": "2025-08-16T16:31:00+00:00",
        "updated_date": "2025-08-16T16:31:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Songwei Liu",
            "Hong Liu",
            "Fangmin Chen",
            "Xurui Peng",
            "Chenqian Yan",
            "Lean Fu",
            "Xing Mei"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a compensation strategy to mitigate error propagation in quantized diffusion models, enhancing their performance on low-precision image generation tasks.",
        "tldr_zh": "本文提出了一种补偿策略，以减少量化扩散模型中的误差传播，提升其在低精度图像生成任务中的表现。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Generic Event Boundary Detection via Denoising Diffusion",
        "summary": "Generic event boundary detection (GEBD) aims to identify natural boundaries\nin a video, segmenting it into distinct and meaningful chunks. Despite the\ninherent subjectivity of event boundaries, previous methods have focused on\ndeterministic predictions, overlooking the diversity of plausible solutions. In\nthis paper, we introduce a novel diffusion-based boundary detection model,\ndubbed DiffGEBD, that tackles the problem of GEBD from a generative\nperspective. The proposed model encodes relevant changes across adjacent frames\nvia temporal self-similarity and then iteratively decodes random noise into\nplausible event boundaries being conditioned on the encoded features.\nClassifier-free guidance allows the degree of diversity to be controlled in\ndenoising diffusion. In addition, we introduce a new evaluation metric to\nassess the quality of predictions considering both diversity and fidelity.\nExperiments show that our method achieves strong performance on two standard\nbenchmarks, Kinetics-GEBD and TAPOS, generating diverse and plausible event\nboundaries.",
        "url": "http://arxiv.org/abs/2508.12084v1",
        "published_date": "2025-08-16T15:44:34+00:00",
        "updated_date": "2025-08-16T15:44:34+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jaejun Hwang",
            "Dayoung Gong",
            "Manjin Kim",
            "Minsu Cho"
        ],
        "ai_categories": [
            "Diffusion"
        ],
        "tldr": "The paper introduces a diffusion-based model for detecting event boundaries in videos, focusing on generating diverse and plausible solutions.",
        "tldr_zh": "本文引入了一种基于扩散的模型，用于检测视频中的事件边界，重点在于生成多样且可信的解决方案。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models",
        "summary": "This paper introduces VimoRAG, a novel video-based retrieval-augmented motion\ngeneration framework for motion large language models (LLMs). As motion LLMs\nface severe out-of-domain/out-of-vocabulary issues due to limited annotated\ndata, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D\nmotion generation by retrieving relevant 2D human motion signals. While\nvideo-based motion RAG is nontrivial, we address two key bottlenecks: (1)\ndeveloping an effective motion-centered video retrieval model that\ndistinguishes human poses and actions, and (2) mitigating the issue of error\npropagation caused by suboptimal retrieval results. We design the Gemini Motion\nVideo Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer,\nenabling effective retrieval and generation processes. Experimental results\nshow that VimoRAG significantly boosts the performance of motion LLMs\nconstrained to text-only input.",
        "url": "http://arxiv.org/abs/2508.12081v1",
        "published_date": "2025-08-16T15:31:14+00:00",
        "updated_date": "2025-08-16T15:31:14+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Haidong Xu",
            "Guangwei Xu",
            "Zhedong Zheng",
            "Xiatian Zhu",
            "Wei Ji",
            "Xiangtai Li",
            "Ruijie Guo",
            "Meishan Zhang",
            "Min zhang",
            "Hao Fei"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "GAN"
        ],
        "tldr": "VimoRAG introduces a video-based retrieval-augmented motion generation framework to enhance 3D motion generation for motion language models by leveraging large-scale in-the-wild video databases.",
        "tldr_zh": "VimoRAG引入了一种基于视频检索增强的运动生成框架，通过利用大规模的实景视频数据库来增强运动语言模型的3D运动生成。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Bongard-RWR+: Real-World Representations of Fine-Grained Concepts in Bongard Problems",
        "summary": "Bongard Problems (BPs) provide a challenging testbed for abstract visual\nreasoning (AVR), requiring models to identify visual concepts fromjust a few\nexamples and describe them in natural language. Early BP benchmarks featured\nsynthetic black-and-white drawings, which might not fully capture the\ncomplexity of real-world scenes. Subsequent BP datasets employed real-world\nimages, albeit the represented concepts are identifiable from high-level image\nfeatures, reducing the task complexity. Differently, the recently released\nBongard-RWR dataset aimed at representing abstract concepts formulated in the\noriginal BPs using fine-grained real-world images. Its manual construction,\nhowever, limited the dataset size to just $60$ instances, constraining\nevaluation robustness. In this work, we introduce Bongard-RWR+, a BP dataset\ncomposed of $5\\,400$ instances that represent original BP abstract concepts\nusing real-world-like images generated via a vision language model (VLM)\npipeline. Building on Bongard-RWR, we employ Pixtral-12B to describe manually\ncurated images and generate new descriptions aligned with the underlying\nconcepts, use Flux.1-dev to synthesize images from these descriptions, and\nmanually verify that the generated images faithfully reflect the intended\nconcepts. We evaluate state-of-the-art VLMs across diverse BP formulations,\nincluding binary and multiclass classification, as well as textual answer\ngeneration. Our findings reveal that while VLMs can recognize coarse-grained\nvisual concepts, they consistently struggle with discerning fine-grained\nconcepts, highlighting limitations in their reasoning capabilities.",
        "url": "http://arxiv.org/abs/2508.12026v1",
        "published_date": "2025-08-16T12:26:44+00:00",
        "updated_date": "2025-08-16T12:26:44+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Szymon Pawlonka",
            "Mikołaj Małkiński",
            "Jacek Mańdziuk"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a new dataset called Bongard-RWR+ that represents abstract concepts using real-world-like images for testing artificial intelligence models' reasoning capability. It shows that current models struggle with discerning fine-grained concepts.",
        "tldr_zh": "本文介绍了一个名为Bongard-RWR+的新数据集，用于测试人工智能模型对于细粒度概念的理解能力。研究表明当前模型在识别细粒度概念方面存在困难。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding",
        "summary": "With the rapid advancement of e-commerce, exploring general representations\nrather than task-specific ones has attracted increasing research attention. For\nproduct understanding, although existing discriminative dual-flow architectures\ndrive progress in this field, they inherently struggle to model the many-to-one\nalignment between multiple images and texts of products. Therefore, we argue\nthat generative Multimodal Large Language Models (MLLMs) hold significant\npotential for improving product representation learning. Nevertheless,\nachieving this goal still remains non-trivial due to several key challenges:\nthe lack of multimodal and aspect-aware modeling modules in typical LLMs; the\ncommon presence of background noise in product images; and the absence of a\nstandard benchmark for evaluation. To address these issues, we propose the\nfirst generative MLLM-based model named MOON for product representation\nlearning. Our method (1) employs a guided Mixture-of-Experts (MoE) module for\ntargeted modeling of multimodal and aspect-specific product content; (2)\neffectively detects core semantic regions in product images to mitigate the\ndistraction and interference caused by background noise; and (3) introduces the\nspecialized negative sampling strategy to increase the difficulty and diversity\nof negative samples. In addition, we release a large-scale multimodal benchmark\nMBE for various product understanding tasks. Experimentally, our model\ndemonstrates competitive zero-shot performance on both our benchmark and the\npublic dataset, showcasing strong generalization across various downstream\ntasks, including cross-modal retrieval, product classification, and attribute\nprediction. Furthermore, the case study and visualization illustrate the\neffectiveness of MOON for product understanding.",
        "url": "http://arxiv.org/abs/2508.11999v1",
        "published_date": "2025-08-16T09:59:25+00:00",
        "updated_date": "2025-08-16T09:59:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.IR",
            "cs.LG"
        ],
        "authors": [
            "Daoze Zhang",
            "Zhanheng Nie",
            "Jianyu Liu",
            "Chenghan Fu",
            "Wanxian Guan",
            "Yuan Gao",
            "Jun Song",
            "Pengjie Wang",
            "Jian Xu",
            "Bo Zheng"
        ],
        "ai_categories": [
            "Multimodality",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces MOON, a generative MLLM-based model for product representation learning in e-commerce. It addresses challenges like multimodal modeling, background noise in images, and lack of standard benchmarks.",
        "tldr_zh": "这篇论文介绍了一个适用于电子商务产品表示学习的基于生成MLLM的模型MOON。它解决了多模态建模、图像背景噪音以及标准基准缺失等挑战。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniUGG: Unified 3D Understanding and Generation via Geometric-Semantic Encoding",
        "summary": "Despite the impressive progress on understanding and generating images shown\nby the recent unified architectures, the integration of 3D tasks remains\nchallenging and largely unexplored. In this paper, we introduce UniUGG, the\nfirst unified understanding and generation framework for 3D modalities. Our\nunified framework employs an LLM to comprehend and decode sentences and 3D\nrepresentations. At its core, we propose a spatial decoder leveraging a latent\ndiffusion model to generate high-quality 3D representations. This allows for\nthe generation and imagination of 3D scenes based on a reference image and an\narbitrary view transformation, while remaining supports for spatial visual\nquestion answering (VQA) tasks. Additionally, we propose a geometric-semantic\nlearning strategy to pretrain the vision encoder. This design jointly captures\nthe input's semantic and geometric cues, enhancing both spatial understanding\nand generation. Extensive experimental results demonstrate the superiority of\nour method in visual representation, spatial understanding, and 3D generation.\nThe source code will be released upon paper acceptance.",
        "url": "http://arxiv.org/abs/2508.11952v1",
        "published_date": "2025-08-16T07:27:31+00:00",
        "updated_date": "2025-08-16T07:27:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yueming Xu",
            "Jiahui Zhang",
            "Ze Huang",
            "Yurui Chen",
            "Yanpeng Zhou",
            "Zhenyu Chen",
            "Yu-Jie Yuan",
            "Pengxiang Xia",
            "Guowei Huang",
            "Xinyue Cai",
            "Zhongang Qi",
            "Xingyue Quan",
            "Jianye Hao",
            "Hang Xu",
            "Li Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "GAN"
        ],
        "tldr": "UniUGG is a unified framework for 3D understanding and generation, integrating spatial decoding and geometric-semantic learning for superior visual representation and generation.",
        "tldr_zh": "UniUGG是一个统一的框架，用于3D理解和生成，整合了空间解码和几何语义学习，以获得卓越的视觉表达和生成能力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "DynamicPose: Real-time and Robust 6D Object Pose Tracking for Fast-Moving Cameras and Objects",
        "summary": "We present DynamicPose, a retraining-free 6D pose tracking framework that\nimproves tracking robustness in fast-moving camera and object scenarios.\nPrevious work is mainly applicable to static or quasi-static scenes, and its\nperformance significantly deteriorates when both the object and the camera move\nrapidly. To overcome these challenges, we propose three synergistic components:\n(1) A visual-inertial odometry compensates for the shift in the Region of\nInterest (ROI) caused by camera motion; (2) A depth-informed 2D tracker\ncorrects ROI deviations caused by large object translation; (3) A VIO-guided\nKalman filter predicts object rotation, generates multiple candidate poses, and\nthen obtains the final pose by hierarchical refinement. The 6D pose tracking\nresults guide subsequent 2D tracking and Kalman filter updates, forming a\nclosed-loop system that ensures accurate pose initialization and precise pose\ntracking. Simulation and real-world experiments demonstrate the effectiveness\nof our method, achieving real-time and robust 6D pose tracking for fast-moving\ncameras and objects.",
        "url": "http://arxiv.org/abs/2508.11950v1",
        "published_date": "2025-08-16T07:25:08+00:00",
        "updated_date": "2025-08-16T07:25:08+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Tingbang Liang",
            "Yixin Zeng",
            "Jiatong Xie",
            "Boyu Zhou"
        ],
        "ai_categories": [
            "Multimodality"
        ],
        "tldr": "DynamicPose is a retraining-free 6D pose tracking framework that improves tracking robustness for fast-moving cameras and objects by using visual-inertial odometry and depth-informed 2D tracking.",
        "tldr_zh": "DynamicPose是一个不需要重新训练的6D姿态跟踪框架，通过使用视觉惯性测距和深度信息的2D跟踪，提高了对快速移动相机和物体的跟踪稳健性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Assessment of Using Synthetic Data in Brain Tumor Segmentation",
        "summary": "Manual brain tumor segmentation from MRI scans is challenging due to tumor\nheterogeneity, scarcity of annotated data, and class imbalance in medical\nimaging datasets. Synthetic data generated by generative models has the\npotential to mitigate these issues by improving dataset diversity. This study\ninvestigates, as a proof of concept, the impact of incorporating synthetic MRI\ndata, generated using a pre-trained GAN model, into training a U-Net\nsegmentation network. Experiments were conducted using real data from the BraTS\n2020 dataset, synthetic data generated with the medigan library, and hybrid\ndatasets combining real and synthetic samples in varying proportions. While\noverall quantitative performance (Dice coefficient, IoU, precision, recall,\naccuracy) was comparable between real-only and hybrid-trained models,\nqualitative inspection suggested that hybrid datasets, particularly with 40%\nreal and 60% synthetic data, improved whole tumor boundary delineation.\nHowever, region-wise accuracy for the tumor core and the enhancing tumor\nremained lower, indicating a persistent class imbalance. The findings support\nthe feasibility of synthetic data as an augmentation strategy for brain tumor\nsegmentation, while highlighting the need for larger-scale experiments,\nvolumetric data consistency, and mitigating class imbalance in future work.",
        "url": "http://arxiv.org/abs/2508.11922v1",
        "published_date": "2025-08-16T05:56:38+00:00",
        "updated_date": "2025-08-16T05:56:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aditi Jahagirdar",
            "Sameer Joshi"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The study explores using synthetic data in brain tumor segmentation to address data scarcity and class imbalance.",
        "tldr_zh": "本研究探讨了在脑肿瘤分割中使用合成数据以解决数据稀缺和类别不平衡的问题。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ENA: Efficient N-dimensional Attention",
        "summary": "Efficient modeling of long sequences of high-order data requires a more\nefficient architecture than Transformer. In this paper, we investigate two key\naspects of extending linear recurrent models, especially those originally\ndesigned for language modeling, to high-order data (1D to ND): scanning\nstrategies and attention-hybrid architectures. Empirical results suggest that\nscanning provides limited benefits, while attention-hybrid models yield\npromising results. Focusing on the latter, we further evaluate types of\nattention and find that tiled high-order sliding window attention (SWA) is\nefficient in both theory and practice. We term the resulting hybrid\narchitecture of linear recurrence and high-order SWA as Efficient N-dimensional\nAttention (ENA). We then conduct several experiments to demonstrate its\neffectiveness. The intuition behind ENA is that linear recurrence compresses\nglobal information into a state, while SWA complements it by enforcing strict\nlocal modeling. Together, they form a simple framework that offers a promising\nand practical solution for ultra-long high-order data modeling.",
        "url": "http://arxiv.org/abs/2508.11921v1",
        "published_date": "2025-08-16T05:55:51+00:00",
        "updated_date": "2025-08-16T05:55:51+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Yibo Zhong"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper proposes Efficient N-dimensional Attention (ENA) for modeling high-order data using a hybrid architecture of linear recurrence and high-order sliding window attention.",
        "tldr_zh": "本文提出了一种名为ENA的方法，通过线性递归和高阶滑动窗口注意力的混合架构来建模高阶数据。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OVG-HQ: Online Video Grounding with Hybrid-modal Queries",
        "summary": "Video grounding (VG) task focuses on locating specific moments in a video\nbased on a query, usually in text form. However, traditional VG struggles with\nsome scenarios like streaming video or queries using visual cues. To fill this\ngap, we present a new task named Online Video Grounding with Hybrid-modal\nQueries (OVG-HQ), which enables online segment localization using text, images,\nvideo segments, and their combinations. This task poses two new challenges:\nlimited context in online settings and modality imbalance during training,\nwhere dominant modalities overshadow weaker ones. To address these, we propose\nOVG-HQ-Unify, a unified framework featuring a Parametric Memory Block (PMB)\nthat retain previously learned knowledge to enhance current decision and a\ncross-modal distillation strategy that guides the learning of non-dominant\nmodalities. This design enables a single model to effectively handle\nhybrid-modal queries. Due to the lack of suitable datasets, we construct\nQVHighlights-Unify, an expanded dataset with multi-modal queries. Besides,\nsince offline metrics overlook prediction timeliness, we adapt them to the\nonline setting, introducing oR@n, IoU=m, and online mean Average Precision\n(omAP) to evaluate both accuracy and efficiency. Experiments show that our\nOVG-HQ-Unify outperforms existing models, offering a robust solution for\nonline, hybrid-modal video grounding. Source code and datasets are available at\nhttps://github.com/maojiaqi2324/OVG-HQ.",
        "url": "http://arxiv.org/abs/2508.11903v1",
        "published_date": "2025-08-16T04:21:45+00:00",
        "updated_date": "2025-08-16T04:21:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Runhao Zeng",
            "Jiaqi Mao",
            "Minghao Lai",
            "Minh Hieu Phan",
            "Yanjie Dong",
            "Wei Wang",
            "Qi Chen",
            "Xiping Hu"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces OVG-HQ, a new task for locating moments in videos using hybrid-modal queries, addressing challenges like limited context and modality imbalance. The proposed framework outperforms existing models, offering a robust solution for online, hybrid-modal video grounding.",
        "tldr_zh": "本文介绍了一项名为OVG-HQ的任务，通过混合模态查询定位视频中的时刻，解决了有限上下文和模态不平衡等挑战。所提出的框架优于现有模型，提供了在线、混合模态视频定位的强大解决方案。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Sobel-Gradient MLP Baseline for Handwritten Character Recognition",
        "summary": "We revisit the classical Sobel operator to ask a simple question: Are\nfirst-order edge maps sufficient to drive an all-dense multilayer perceptron\n(MLP) for handwritten character recognition (HCR), as an alternative to\nconvolutional neural networks (CNNs)? Using only horizontal and vertical Sobel\nderivatives as input, we train an MLP on MNIST and EMNIST Letters. Despite its\nextreme simplicity, the resulting network reaches 98% accuracy on MNIST digits\nand 92% on EMNIST letters -- approaching CNNs while offering a smaller memory\nfootprint and transparent features. Our findings highlight that much of the\nclass-discriminative information in handwritten character images is already\ncaptured by first-order gradients, making edge-aware MLPs a compelling option\nfor HCR.",
        "url": "http://arxiv.org/abs/2508.11902v1",
        "published_date": "2025-08-16T04:17:39+00:00",
        "updated_date": "2025-08-16T04:17:39+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Azam Nouri"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper explores using simple first-order edge maps from the Sobel operator as input to a multilayer perceptron for handwritten character recognition, achieving high accuracy comparable to convolutional neural networks.",
        "tldr_zh": "该论文探讨了将Sobel算子产生的简单一阶边缘图作为输入，用于手写字符识别的多层感知器，在MNIST和EMNIST Letters数据集上取得了98%和92%的准确率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Large Kernel Modulation Network for Efficient Image Super-Resolution",
        "summary": "Image super-resolution (SR) in resource-constrained scenarios demands\nlightweight models balancing performance and latency. Convolutional neural\nnetworks (CNNs) offer low latency but lack non-local feature capture, while\nTransformers excel at non-local modeling yet suffer slow inference. To address\nthis trade-off, we propose the Large Kernel Modulation Network (LKMN), a pure\nCNN-based model. LKMN has two core components: Enhanced Partial Large Kernel\nBlock (EPLKB) and Cross-Gate Feed-Forward Network (CGFN). The EPLKB utilizes\nchannel shuffle to boost inter-channel interaction, incorporates channel\nattention to focus on key information, and applies large kernel strip\nconvolutions on partial channels for non-local feature extraction with reduced\ncomplexity. The CGFN dynamically adjusts discrepancies between input, local,\nand non-local features via a learnable scaling factor, then employs a\ncross-gate strategy to modulate and fuse these features, enhancing their\ncomplementarity. Extensive experiments demonstrate that our method outperforms\nexisting state-of-the-art (SOTA) lightweight SR models while balancing quality\nand efficiency. Specifically, LKMN-L achieves 0.23 dB PSNR improvement over\nDAT-light on the Manga109 dataset at $\\times$4 upscale, with nearly $\\times$4.8\ntimes faster. Codes are in the supplementary materials. The code is available\nat https://github.com/Supereeeee/LKMN.",
        "url": "http://arxiv.org/abs/2508.11893v1",
        "published_date": "2025-08-16T03:43:14+00:00",
        "updated_date": "2025-08-16T03:43:14+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Quanwei Hu",
            "Yinggan Tang",
            "Xuguang Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a Large Kernel Modulation Network (LKMN) for efficient image super-resolution, outperforming existing models in terms of quality and efficiency.",
        "tldr_zh": "该论文介绍了一种用于高效图像超分辨率的大核调制网络（LKMN），在质量和效率方面优于现有模型。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models",
        "summary": "Instructed Visual Segmentation (IVS) tasks require segmenting objects in\nimages or videos based on natural language instructions. While recent\nmultimodal large language models (MLLMs) have achieved strong performance on\nIVS, their inference cost remains a major bottleneck, particularly in video. We\nempirically analyze visual token sampling in MLLMs and observe a strong\ncorrelation between subset token coverage and segmentation performance. This\nmotivates our design of a simple and effective token pruning method that\nselects a compact yet spatially representative subset of tokens to accelerate\ninference. In this paper, we introduce a novel visual token pruning method for\nIVS, called EVTP-IV, which builds upon the k-center by integrating spatial\ninformation to ensure better coverage. We further provide an\ninformation-theoretic analysis to support our design. Experiments on standard\nIVS benchmarks show that our method achieves up to 5X speed-up on video tasks\nand 3.5X on image tasks, while maintaining comparable accuracy using only 20%\nof the tokens. Our method also consistently outperforms state-of-the-art\npruning baselines under varying pruning ratios.",
        "url": "http://arxiv.org/abs/2508.11886v1",
        "published_date": "2025-08-16T03:16:33+00:00",
        "updated_date": "2025-08-16T03:16:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG",
            "eess.IV"
        ],
        "authors": [
            "Wenhui Zhu",
            "Xiwen Chen",
            "Zhipeng Wang",
            "Shao Tang",
            "Sayan Ghosh",
            "Xuanzhao Dong",
            "Rajat Koner",
            "Yalin Wang"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a method called EVTP-IV for effective visual token pruning in multi-modal large language models to accelerate Instructed Visual Segmentation tasks, achieving significant speed-up while maintaining accuracy.",
        "tldr_zh": "该论文提出了一种名为EVTP-IV的方法，用于在多模态大型语言模型中进行有效的视觉标记修剪，以加速指导的视觉分割任务，在保持准确度的同时实现了显着的加速。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Impact of Clinical Image Quality on Efficient Foundation Model Finetuning",
        "summary": "Foundation models in medical imaging have shown promising label efficiency,\nachieving high downstream performance with only a fraction of annotated data.\nHere, we evaluate this in prostate multiparametric MRI using ProFound, a\ndomain-specific vision foundation model pretrained on large-scale prostate MRI\ndatasets. We investigate how variable image quality affects label-efficient\nfinetuning by measuring the generalisability of finetuned models. Experiments\nsystematically vary high-/low-quality image ratios in finetuning and evaluation\nsets. Our findings indicate that image quality distribution and its\nfinetune-and-test mismatch significantly affect model performance. In\nparticular: a) Varying the ratio of high- to low-quality images between\nfinetuning and test sets leads to notable differences in downstream\nperformance; and b) The presence of sufficient high-quality images in the\nfinetuning set is critical for maintaining strong performance, whilst the\nimportance of matched finetuning and testing distribution varies between\ndifferent downstream tasks, such as automated radiology reporting and prostate\ncancer detection.When quality ratios are consistent, finetuning needs far less\nlabeled data than training from scratch, but label efficiency depends on image\nquality distribution. Without enough high-quality finetuning data, pretrained\nmodels may fail to outperform those trained without pretraining. This\nhighlights the importance of assessing and aligning quality distributions\nbetween finetuning and deployment, and the need for quality standards in\nfinetuning data for specific downstream tasks. Using ProFound, we show the\nvalue of quantifying image quality in both finetuning and deployment to fully\nrealise the data and compute efficiency benefits of foundation models.",
        "url": "http://arxiv.org/abs/2508.11864v1",
        "published_date": "2025-08-16T01:33:36+00:00",
        "updated_date": "2025-08-16T01:33:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yucheng Tang",
            "Pawel Rajwa",
            "Alexander Ng",
            "Yipei Wang",
            "Wen Yan",
            "Natasha Thorley",
            "Aqua Asif",
            "Clare Allen",
            "Louise Dickinson",
            "Francesco Giganti",
            "Shonit Punwani",
            "Daniel C. Alexander",
            "Veeru Kasivisvanathan",
            "Yipeng Hu"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "The paper evaluates how image quality affects label-efficient finetuning of foundation models in medical imaging using prostate MRI data, highlighting the importance of quality distribution alignment between finetuning and deployment.",
        "tldr_zh": "本文评估了影像质量如何影响医学影像中基于前列腺MRI数据的标签高效微调基础模型，强调了在微调和部署之间对质量分布的重要性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Labels or Input? Rethinking Augmentation in Multimodal Hate Detection",
        "summary": "The modern web is saturated with multimodal content, intensifying the\nchallenge of detecting hateful memes, where harmful intent is often conveyed\nthrough subtle interactions between text and image under the guise of humor or\nsatire. While recent advances in Vision-Language Models (VLMs) show promise,\nthese models lack support for fine-grained supervision and remain susceptible\nto implicit hate speech. In this paper, we present a dual-pronged approach to\nimprove multimodal hate detection. First, we propose a prompt optimization\nframework that systematically varies prompt structure, supervision granularity,\nand training modality. We show that prompt design and label scaling both\ninfluence performance, with structured prompts improving robustness even in\nsmall models, and InternVL2 achieving the best F1-scores across binary and\nscaled settings. Second, we introduce a multimodal data augmentation pipeline\nthat generates 2,479 counterfactually neutral memes by isolating and rewriting\nthe hateful modality. This pipeline, powered by a multi-agent LLM-VLM setup,\nsuccessfully reduces spurious correlations and improves classifier\ngeneralization. Our approaches inspire new directions for building synthetic\ndata to train robust and fair vision-language models. Our findings demonstrate\nthat prompt structure and data composition are as critical as model size, and\nthat targeted augmentation can support more trustworthy and context-sensitive\nhate detection.",
        "url": "http://arxiv.org/abs/2508.11808v1",
        "published_date": "2025-08-15T21:31:00+00:00",
        "updated_date": "2025-08-15T21:31:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.CY",
            "cs.MM",
            "I.2.7; I.2.10"
        ],
        "authors": [
            "Sahajpreet Singh",
            "Rongxin Ouyang",
            "Subhayan Mukerjee",
            "Kokil Jaidka"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes a dual approach to improve multimodal hate detection by optimizing prompts and introducing a multimodal data augmentation pipeline.",
        "tldr_zh": "本文提出了一种双重方法来改进多模式仇恨检测，通过优化提示和引入多模式数据增强管道。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VideoAVE: A Multi-Attribute Video-to-Text Attribute Value Extraction Dataset and Benchmark Models",
        "summary": "Attribute Value Extraction (AVE) is important for structuring product\ninformation in e-commerce. However, existing AVE datasets are primarily limited\nto text-to-text or image-to-text settings, lacking support for product videos,\ndiverse attribute coverage, and public availability. To address these gaps, we\nintroduce VideoAVE, the first publicly available video-to-text e-commerce AVE\ndataset across 14 different domains and covering 172 unique attributes. To\nensure data quality, we propose a post-hoc CLIP-based Mixture of Experts\nfiltering system (CLIP-MoE) to remove the mismatched video-product pairs,\nresulting in a refined dataset of 224k training data and 25k evaluation data.\nIn order to evaluate the usability of the dataset, we further establish a\ncomprehensive benchmark by evaluating several state-of-the-art video vision\nlanguage models (VLMs) under both attribute-conditioned value prediction and\nopen attribute-value pair extraction tasks. Our results analysis reveals that\nvideo-to-text AVE remains a challenging problem, particularly in open settings,\nand there is still room for developing more advanced VLMs capable of leveraging\neffective temporal information. The dataset and benchmark code for VideoAVE are\navailable at: https://github.com/gjiaying/VideoAVE",
        "url": "http://arxiv.org/abs/2508.11801v1",
        "published_date": "2025-08-15T20:58:47+00:00",
        "updated_date": "2025-08-15T20:58:47+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Ming Cheng",
            "Tong Wu",
            "Jiazhen Hu",
            "Jiaying Gong",
            "Hoda Eldardiry"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "VideoAVE introduces a video-to-text attribute value extraction dataset for e-commerce, covering diverse attributes and domains, with a benchmark for evaluating state-of-the-art models.",
        "tldr_zh": "VideoAVE引入了一个用于电子商务的视频到文本属性值提取数据集，涵盖多样的属性和领域，并提供评估最先进模型的基准。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Exploring Spatial-Temporal Dynamics in Event-based Facial Micro-Expression Analysis",
        "summary": "Micro-expression analysis has applications in domains such as Human-Robot\nInteraction and Driver Monitoring Systems. Accurately capturing subtle and fast\nfacial movements remains difficult when relying solely on RGB cameras, due to\nlimitations in temporal resolution and sensitivity to motion blur. Event\ncameras offer an alternative, with microsecond-level precision, high dynamic\nrange, and low latency. However, public datasets featuring event-based\nrecordings of Action Units are still scarce. In this work, we introduce a\nnovel, preliminary multi-resolution and multi-modal micro-expression dataset\nrecorded with synchronized RGB and event cameras under variable lighting\nconditions. Two baseline tasks are evaluated to explore the spatial-temporal\ndynamics of micro-expressions: Action Unit classification using Spiking Neural\nNetworks (51.23\\% accuracy with events vs. 23.12\\% with RGB), and frame\nreconstruction using Conditional Variational Autoencoders, achieving SSIM =\n0.8513 and PSNR = 26.89 dB with high-resolution event input. These promising\nresults show that event-based data can be used for micro-expression recognition\nand frame reconstruction.",
        "url": "http://arxiv.org/abs/2508.11988v1",
        "published_date": "2025-08-16T09:03:20+00:00",
        "updated_date": "2025-08-16T09:03:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nicolas Mastropasqua",
            "Ignacio Bugueno-Cordova",
            "Rodrigo Verschae",
            "Daniel Acevedo",
            "Pablo Negri",
            "Maria E. Buemi"
        ],
        "ai_categories": [
            "Dataset",
            "Multimodality"
        ],
        "tldr": "The paper introduces a novel dataset for micro-expression analysis using event cameras, showing promising results for micro-expression recognition and frame reconstruction.",
        "tldr_zh": "本文使用事件相机介绍了一种新型的微表情分析数据集，展示了微表情识别和帧重建的有希望结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.75
    },
    {
        "title": "Deep Learning For Point Cloud Denoising: A Survey",
        "summary": "Real-world environment-derived point clouds invariably exhibit noise across\nvarying modalities and intensities. Hence, point cloud denoising (PCD) is\nessential as a preprocessing step to improve downstream task performance. Deep\nlearning (DL)-based PCD models, known for their strong representation\ncapabilities and flexible architectures, have surpassed traditional methods in\ndenoising performance. To our best knowledge, despite recent advances in\nperformance, no comprehensive survey systematically summarizes the developments\nof DL-based PCD. To fill the gap, this paper seeks to identify key challenges\nin DL-based PCD, summarizes the main contributions of existing methods, and\nproposes a taxonomy tailored to denoising tasks. To achieve this goal, we\nformulate PCD as a two-step process: outlier removal and surface noise\nrestoration, encompassing most scenarios and requirements of PCD. Additionally,\nwe compare methods in terms of similarities, differences, and respective\nadvantages. Finally, we discuss research limitations and future directions,\noffering insights for further advancements in PCD.",
        "url": "http://arxiv.org/abs/2508.11932v1",
        "published_date": "2025-08-16T06:25:19+00:00",
        "updated_date": "2025-08-16T06:25:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chengwei Zhang",
            "Xueyi Zhang",
            "Mingrui Lao",
            "Tao Jiang",
            "Xinhao Xu",
            "Wenjie Li",
            "Fubo Zhang",
            "Longyong Chen"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "This paper presents a survey on deep learning-based point cloud denoising, identifying key challenges, summarizing existing methods, and proposing a taxonomy for denoising tasks.",
        "tldr_zh": "本文提出了一项关于基于深度学习的点云去噪的调查，识别了关键挑战，总结了现有方法，并为去噪任务提出了一种分类法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.75
    },
    {
        "title": "Towards Understanding 3D Vision: the Role of Gaussian Curvature",
        "summary": "Recent advances in computer vision have predominantly relied on data-driven\napproaches that leverage deep learning and large-scale datasets. Deep neural\nnetworks have achieved remarkable success in tasks such as stereo matching and\nmonocular depth reconstruction. However, these methods lack explicit models of\n3D geometry that can be directly analyzed, transferred across modalities, or\nsystematically modified for controlled experimentation. We investigate the role\nof Gaussian curvature in 3D surface modeling. Besides Gaussian curvature being\nan invariant quantity under change of observers or coordinate systems, we\ndemonstrate using the Middlebury stereo dataset that it offers: (i) a sparse\nand compact description of 3D surfaces, (ii) state-of-the-art monocular and\nstereo methods seem to implicitly consider it, but no explicit module of such\nuse can be extracted, (iii) a form of geometric prior that can inform and\nimprove 3D surface reconstruction, and (iv) a possible use as an unsupervised\nmetric for stereo methods.",
        "url": "http://arxiv.org/abs/2508.11825v1",
        "published_date": "2025-08-15T22:14:46+00:00",
        "updated_date": "2025-08-15T22:14:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sherlon Almeida da Silva",
            "Davi Geiger",
            "Luiz Velho",
            "Moacir Antonelli Ponti"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper explores the role of Gaussian curvature in 3D surface modeling and its potential impact on improving 3D surface reconstruction and stereo methods.",
        "tldr_zh": "本文探讨了高斯曲率在三维表面建模中的作用，以及它在改善三维表面重建和立体方法中的潜在影响。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "Enhancing 3D point accuracy of laser scanner through multi-stage convolutional neural network for applications in construction",
        "summary": "We propose a multi-stage convolutional neural network (MSCNN) based\nintegrated method for reducing uncertainty of 3D point accuracy of lasar\nscanner (LS) in rough indoor rooms, providing more accurate spatial\nmeasurements for high-precision geometric model creation and renovation. Due to\ndifferent equipment limitations and environmental factors, high-end and low-end\nLS have positional errors. Our approach pairs high-accuracy scanners (HAS) as\nreferences with corresponding low-accuracy scanners (LAS) of measurements in\nidentical environments to quantify specific error patterns. By establishing a\nstatistical relationship between measurement discrepancies and their spatial\ndistribution, we develop a correction framework that combines traditional\ngeometric processing with targeted neural network refinement. This method\ntransforms the quantification of systematic errors into a supervised learning\nproblem, allowing precise correction while preserving critical geometric\nfeatures. Experimental results in our rough indoor rooms dataset show\nsignificant improvements in measurement accuracy, with mean square error (MSE)\nreductions exceeding 70% and peak signal-to-noise ratio (PSNR) improvements of\napproximately 6 decibels. This approach enables low-end devices to achieve\nmeasurement uncertainty levels approaching those of high-end devices without\nhardware modifications.",
        "url": "http://arxiv.org/abs/2508.12089v1",
        "published_date": "2025-08-16T16:02:56+00:00",
        "updated_date": "2025-08-16T16:02:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qinyuan Fan",
            "Clemens Gühmann"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a method using a multi-stage convolutional neural network to improve the accuracy of 3D point measurements from laser scanners in indoor environments, achieving significant error reductions and enabling low-end devices to match high-end ones without hardware modifications.",
        "tldr_zh": "该论文提出了一种使用多阶卷积神经网络的方法，以提高室内环境中激光扫描仪的3D点测量精度，实现了显著的误差降低，并使低端设备在不进行硬件修改的情况下达到与高端设备相匹配的水平。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "WiseLVAM: A Novel Framework For Left Ventricle Automatic Measurements",
        "summary": "Clinical guidelines recommend performing left ventricular (LV) linear\nmeasurements in B-mode echocardiographic images at the basal level -- typically\nat the mitral valve leaflet tips -- and aligned perpendicular to the LV long\naxis along a virtual scanline (SL). However, most automated methods estimate\nlandmarks directly from B-mode images for the measurement task, where even\nsmall shifts in predicted points along the LV walls can lead to significant\nmeasurement errors, reducing their clinical reliability. A recent\nsemi-automatic method, EnLVAM, addresses this limitation by constraining\nlandmark prediction to a clinician-defined SL and training on generated\nAnatomical Motion Mode (AMM) images to predict LV landmarks along the same. To\nenable full automation, a contour-aware SL placement approach is proposed in\nthis work, in which the LV contour is estimated using a weakly supervised\nB-mode landmark detector. SL placement is then performed by inferring the LV\nlong axis and the basal level-mimicking clinical guidelines. Building on this\nfoundation, we introduce \\textit{WiseLVAM} -- a novel, fully automated yet\nmanually adaptable framework for automatically placing the SL and then\nautomatically performing the LV linear measurements in the AMM mode.\n\\textit{WiseLVAM} utilizes the structure-awareness from B-mode images and the\nmotion-awareness from AMM mode to enhance robustness and accuracy with the\npotential to provide a practical solution for the routine clinical application.",
        "url": "http://arxiv.org/abs/2508.12023v1",
        "published_date": "2025-08-16T11:58:01+00:00",
        "updated_date": "2025-08-16T11:58:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Durgesh Kumar Singh",
            "Qing Cao",
            "Sarina Thomas",
            "Ahcène Boubekki",
            "Robert Jenssen",
            "Michael Kampffmeyer"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "WiseLVAM is a fully automated framework for left ventricle measurements that combines structure-awareness from B-mode images and motion-awareness from AMM mode for better accuracy and reliability in clinical applications.",
        "tldr_zh": "WiseLVAM是一个全自动的框架，用于左心室测量，结合了来自B-mode图像的结构意识和来自AMM模式的动作意识，以提高临床应用中的准确性和可靠性。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7.25
    },
    {
        "title": "PEdger++: Practical Edge Detection via Assembling Cross Information",
        "summary": "Edge detection serves as a critical foundation for numerous computer vision\napplications, including object detection, semantic segmentation, and image\nediting, by extracting essential structural cues that define object boundaries\nand salient edges. To be viable for broad deployment across devices with\nvarying computational capacities, edge detectors shall balance high accuracy\nwith low computational complexity. While deep learning has evidently improved\naccuracy, they often suffer from high computational costs, limiting their\napplicability on resource-constrained devices. This paper addresses the\nchallenge of achieving that balance: \\textit{i.e.}, {how to efficiently capture\ndiscriminative features without relying on large-size and sophisticated\nmodels}. We propose PEdger++, a collaborative learning framework designed to\nreduce computational costs and model sizes while improving edge detection\naccuracy. The core principle of our PEdger++ is that cross-information derived\nfrom heterogeneous architectures, diverse training moments, and multiple\nparameter samplings, is beneficial to enhance learning from an ensemble\nperspective. Extensive experimental results on the BSDS500, NYUD and Multicue\ndatasets demonstrate the effectiveness of our approach, both quantitatively and\nqualitatively, showing clear improvements over existing methods. We also\nprovide multiple versions of the model with varying computational requirements,\nhighlighting PEdger++'s adaptability with respect to different resource\nconstraints. Codes are accessible at\nhttps://github.com/ForawardStar/EdgeDetectionviaPEdgerPlus/.",
        "url": "http://arxiv.org/abs/2508.11961v1",
        "published_date": "2025-08-16T07:48:36+00:00",
        "updated_date": "2025-08-16T07:48:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuanbin Fu",
            "Liang Li",
            "Xiaojie Guo"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "PEdger++ is a collaborative learning framework that aims to balance accuracy and computational complexity in edge detection by leveraging cross-information from different sources.",
        "tldr_zh": "PEdger++是一个协作学习框架，旨在通过利用来自不同来源的交叉信息，找到边缘检测中精度和计算复杂性之间的平衡。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Recent Advances in Transformer and Large Language Models for UAV Applications",
        "summary": "The rapid advancement of Transformer-based models has reshaped the landscape\nof uncrewed aerial vehicle (UAV) systems by enhancing perception,\ndecision-making, and autonomy. This review paper systematically categorizes and\nevaluates recent developments in Transformer architectures applied to UAVs,\nincluding attention mechanisms, CNN-Transformer hybrids, reinforcement learning\nTransformers, and large language models (LLMs). Unlike previous surveys, this\nwork presents a unified taxonomy of Transformer-based UAV models, highlights\nemerging applications such as precision agriculture and autonomous navigation,\nand provides comparative analyses through structured tables and performance\nbenchmarks. The paper also reviews key datasets, simulators, and evaluation\nmetrics used in the field. Furthermore, it identifies existing gaps in the\nliterature, outlines critical challenges in computational efficiency and\nreal-time deployment, and offers future research directions. This comprehensive\nsynthesis aims to guide researchers and practitioners in understanding and\nadvancing Transformer-driven UAV technologies.",
        "url": "http://arxiv.org/abs/2508.11834v1",
        "published_date": "2025-08-15T22:56:37+00:00",
        "updated_date": "2025-08-15T22:56:37+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO",
            "cs.SY",
            "eess.IV",
            "eess.SY"
        ],
        "authors": [
            "Hamza Kheddar",
            "Yassine Habchi",
            "Mohamed Chahine Ghanem",
            "Mustapha Hemis",
            "Dusit Niyato"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "A review paper on recent advances of using Transformer-based models in UAV applications, categorizing and evaluating different architectures, applications, datasets, and challenges",
        "tldr_zh": "一篇综述文章，对最近在无人机应用中使用基于Transformer的模型的进展进行了分类和评估，包括不同的架构、应用、数据集和挑战",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Q-FSRU: Quantum-Augmented Frequency-Spectral Fusion for Medical Visual Question Answering",
        "summary": "Solving tough clinical questions that require both image and text\nunderstanding is still a major challenge in healthcare AI. In this work, we\npropose Q-FSRU, a new model that combines Frequency Spectrum Representation and\nFusion (FSRU) with a method called Quantum Retrieval-Augmented Generation\n(Quantum RAG) for medical Visual Question Answering (VQA). The model takes in\nfeatures from medical images and related text, then shifts them into the\nfrequency domain using Fast Fourier Transform (FFT). This helps it focus on\nmore meaningful data and filter out noise or less useful information. To\nimprove accuracy and ensure that answers are based on real knowledge, we add a\nquantum-inspired retrieval system. It fetches useful medical facts from\nexternal sources using quantum-based similarity techniques. These details are\nthen merged with the frequency-based features for stronger reasoning. We\nevaluated our model using the VQA-RAD dataset, which includes real radiology\nimages and questions. The results showed that Q-FSRU outperforms earlier\nmodels, especially on complex cases needing image-text reasoning. The mix of\nfrequency and quantum information improves both performance and explainability.\nOverall, this approach offers a promising way to build smart, clear, and\nhelpful AI tools for doctors.",
        "url": "http://arxiv.org/abs/2508.12036v1",
        "published_date": "2025-08-16T13:21:49+00:00",
        "updated_date": "2025-08-16T13:21:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Rakesh Thakur",
            "Yusra Tariq"
        ],
        "ai_categories": [
            "AIGC"
        ],
        "tldr": "Q-FSRU is a new model for medical Visual Question Answering that combines Frequency Spectrum Representation and Fusion with Quantum Retrieval-Augmented Generation, outperforming earlier models on complex image-text reasoning tasks.",
        "tldr_zh": "Q-FSRU是一个新的模型，用于医学视觉问答，结合了频谱表示与融合以及量子检索增强生成，对复杂的图像文本推理任务表现优异。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes",
        "summary": "Reconstructing dynamic driving scenes from dashcam videos has attracted\nincreasing attention due to its significance in autonomous driving and scene\nunderstanding. While recent advances have made impressive progress, most\nmethods still unify all background elements into a single representation,\nhindering both instance-level understanding and flexible scene editing. Some\napproaches attempt to lift 2D segmentation into 3D space, but often rely on\npre-processed instance IDs or complex pipelines to map continuous features to\ndiscrete identities. Moreover, these methods are typically designed for indoor\nscenes with rich viewpoints, making them less applicable to outdoor driving\nscenarios. In this paper, we present InstDrive, an instance-aware 3D Gaussian\nSplatting framework tailored for the interactive reconstruction of dynamic\ndriving scene. We use masks generated by SAM as pseudo ground-truth to guide 2D\nfeature learning via contrastive loss and pseudo-supervised objectives. At the\n3D level, we introduce regularization to implicitly encode instance identities\nand enforce consistency through a voxel-based loss. A lightweight static\ncodebook further bridges continuous features and discrete identities without\nrequiring data pre-processing or complex optimization. Quantitative and\nqualitative experiments demonstrate the effectiveness of InstDrive, and to the\nbest of our knowledge, it is the first framework to achieve 3D instance\nsegmentation in dynamic, open-world driving scenes.More visualizations are\navailable at our project page.",
        "url": "http://arxiv.org/abs/2508.12015v1",
        "published_date": "2025-08-16T11:17:31+00:00",
        "updated_date": "2025-08-16T11:17:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hongyuan Liu",
            "Haochen Yu",
            "Jianfei Jiang",
            "Qiankun Liu",
            "Jiansheng Chen",
            "Huimin Ma"
        ],
        "ai_categories": [
            "AIGC"
        ],
        "tldr": "InstDrive is a framework for reconstructing dynamic driving scenes in 3D with instance-awareness and Gaussian splatting, demonstrating effectiveness in achieving 3D instance segmentation in open-world driving scenarios.",
        "tldr_zh": "InstDrive是一个用于在3D中重建具有实例感知和高斯分层的动态驾驶场景的框架，展示了在开放世界驾驶场景中实现3D实例分割的有效性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SAMDWICH: Moment-aware Video-text Alignment for Referring Video Object Segmentation",
        "summary": "Referring Video Object Segmentation (RVOS) aims to segment and track objects\nin videos based on natural language expressions, requiring precise alignment\nbetween visual content and textual queries. However, existing methods often\nsuffer from semantic misalignment, largely due to indiscriminate frame sampling\nand supervision of all visible objects during training -- regardless of their\nactual relevance to the expression. To address this, we introduce a\nmoment-aware RVOS framework named SAMDWICH, along with a newly annotated\ndataset, MeViS-M, built upon the challenging MeViS benchmark. We manually\nannotate temporal moments indicating when each object is referred to by the\nexpression, enabling semantically grounded supervision that strengthens\nvideo-text alignment. SAMDWICH leverages these aligned text-to-clip pairs to\nguide training, significantly enhancing referential understanding. Building\nupon this framework, we propose Moment-guided Dual-path Propagation (MDP), a\nmoment-aware propagation strategy that improves both object grounding and\ntracking by training on both relevant and irrelevant frames through a\nmoment-centric memory mechanism. In addition, we introduce Object-level\nSelective Supervision (OSS), an object-level filtering strategy that supervises\nonly the objects temporally aligned with the expression in each training clip.\nThis selective supervision reduces semantic noise and reinforces\nlanguage-conditioned learning. Extensive experiments show that SAMDWICH\nachieves state-of-the-art performance on challenging MeViS benchmark,\nparticularly excelling in complex scenarios involving diverse expressions.",
        "url": "http://arxiv.org/abs/2508.11955v1",
        "published_date": "2025-08-16T07:34:43+00:00",
        "updated_date": "2025-08-16T07:34:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Seunghun Lee",
            "Jiwan Seo",
            "Jeonghoon Kim",
            "Siwon Kim",
            "Haeun Yun",
            "Hyogyeong Jeon",
            "Wonhyeok Choi",
            "Jaehoon Jeong",
            "Zane Durante",
            "Sang Hyun Park",
            "Sunghoon Im"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "SAMDWICH is a moment-aware framework for referring video object segmentation, achieving state-of-the-art performance on challenging benchmarks.",
        "tldr_zh": "SAMDWICH是一个关于视频对象分割的具有时刻感知的框架，在具有挑战性的基准测试中取得了最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "TimeSenCLIP: A Vision-Language Model for Remote Sensing Using Single-Pixel Time Series",
        "summary": "Vision-language models have shown significant promise in remote sensing\napplications, particularly for land-use and land-cover (LULC) via zero-shot\nclassification and retrieval. However, current approaches face two key\nchallenges: reliance on large spatial tiles that increase computational cost,\nand dependence on text-based supervision, which is often not readily available.\nIn this work, we present TimeSenCLIP, a lightweight framework that reevaluate\nthe role of spatial context by evaluating the effectiveness of a single pixel\nby leveraging its temporal and spectral dimensions, for classifying LULC and\necosystem types. By leveraging spectral and temporal information from\nSentinel-2 imagery and cross-view learning with geo-tagged ground-level photos,\nwe minimises the need for caption-based training while preserving semantic\nalignment between overhead (satellite) and ground perspectives. Our approach is\ngrounded in the LUCAS and Sen4Map datasets, and evaluated on classification\ntasks including LULC, crop type, and ecosystem type. We demonstrate that single\npixel inputs, when combined with temporal and spectral cues, are sufficient for\nthematic mapping, offering a scalable and efficient alternative for large-scale\nremote sensing applications. Code is available at\nhttps://github.com/pallavijain-pj/TimeSenCLIP",
        "url": "http://arxiv.org/abs/2508.11919v1",
        "published_date": "2025-08-16T05:44:33+00:00",
        "updated_date": "2025-08-16T05:44:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pallavi Jain",
            "Diego Marcos",
            "Dino Ienco",
            "Roberto Interdonato",
            "Tristan Berchoux"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "TimeSenCLIP is a vision-language model for classifying land-use and ecosystem types in remote sensing using single-pixel time series, offering a scalable and efficient alternative for large-scale applications.",
        "tldr_zh": "TimeSenCLIP是一个视觉语言模型，利用单个像素时间序列对遥感中的土地利用和生态系统类型进行分类，为大规模应用提供了可扩展和高效的替代方案。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Data Shift of Object Detection in Autonomous Driving",
        "summary": "With the widespread adoption of machine learning technologies in autonomous\ndriving systems, their role in addressing complex environmental perception\nchallenges has become increasingly crucial. However, existing machine learning\nmodels exhibit significant vulnerability, as their performance critically\ndepends on the fundamental assumption that training and testing data satisfy\nthe independent and identically distributed condition, which is difficult to\nguarantee in real-world applications. Dynamic variations in data distribution\ncaused by seasonal changes, weather fluctuations lead to data shift problems in\nautonomous driving systems. This study investigates the data shift problem in\nautonomous driving object detection tasks, systematically analyzing its\ncomplexity and diverse manifestations. We conduct a comprehensive review of\ndata shift detection methods and employ shift detection analysis techniques to\nperform dataset categorization and balancing. Building upon this foundation, we\nconstruct an object detection model. To validate our approach, we optimize the\nmodel by integrating CycleGAN-based data augmentation techniques with the\nYOLOv5 framework. Experimental results demonstrate that our method achieves\nsuperior performance compared to baseline models on the BDD100K dataset.",
        "url": "http://arxiv.org/abs/2508.11868v1",
        "published_date": "2025-08-16T01:52:31+00:00",
        "updated_date": "2025-08-16T01:52:31+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Lida Xu"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper investigates data shift problems in autonomous driving object detection tasks, proposes methods for detection and balancing, and achieves superior performance on the BDD100K dataset.",
        "tldr_zh": "本文研究自动驾驶目标检测任务中的数据转移问题，提出了检测和平衡方法，并在BDD100K数据集上取得了优越的性能。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "From Pixels to Graphs: Deep Graph-Level Anomaly Detection on Dermoscopic Images",
        "summary": "Graph Neural Networks (GNNs) have emerged as a powerful approach for\ngraph-based machine learning tasks. Previous work applied GNNs to image-derived\ngraph representations for various downstream tasks such as classification or\nanomaly detection. These transformations include segmenting images, extracting\nfeatures from segments, mapping them to nodes, and connecting them. However, to\nthe best of our knowledge, no study has rigorously compared the effectiveness\nof the numerous potential image-to-graph transformation approaches for\nGNN-based graph-level anomaly detection (GLAD). In this study, we\nsystematically evaluate the efficacy of multiple segmentation schemes, edge\nconstruction strategies, and node feature sets based on color, texture, and\nshape descriptors to produce suitable image-derived graph representations to\nperform graph-level anomaly detection. We conduct extensive experiments on\ndermoscopic images using state-of-the-art GLAD models, examining performance\nand efficiency in purely unsupervised, weakly supervised, and fully supervised\nregimes. Our findings reveal, for example, that color descriptors contribute\nthe best standalone performance, while incorporating shape and texture features\nconsistently enhances detection efficacy. In particular, our best unsupervised\nconfiguration using OCGTL achieves a competitive AUC-ROC score of up to 0.805\nwithout relying on pretrained backbones like comparable image-based approaches.\nWith the inclusion of sparse labels, the performance increases substantially to\n0.872 and with full supervision to 0.914 AUC-ROC.",
        "url": "http://arxiv.org/abs/2508.11826v1",
        "published_date": "2025-08-15T22:16:29+00:00",
        "updated_date": "2025-08-15T22:16:29+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Dehn Xu",
            "Tim Katzke",
            "Emmanuel Müller"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper explores using deep graph-level anomaly detection on dermoscopic images by evaluating different image-to-graph transformation approaches.",
        "tldr_zh": "该论文探讨了在皮肤镜图像上使用深度图级异常检测，通过评估不同的图像转换方法。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "An MLP Baseline for Handwriting Recognition Using Planar Curvature and Gradient Orientation",
        "summary": "This study investigates whether second-order geometric cues - planar\ncurvature magnitude, curvature sign, and gradient orientation - are sufficient\non their own to drive a multilayer perceptron (MLP) classifier for handwritten\ncharacter recognition (HCR), offering an alternative to convolutional neural\nnetworks (CNNs). Using these three handcrafted feature maps as inputs, our\ncurvature-orientation MLP achieves 97 percent accuracy on MNIST digits and 89\npercent on EMNIST letters. These results underscore the discriminative power of\ncurvature-based representations for handwritten character images and\ndemonstrate that the advantages of deep learning can be realized even with\ninterpretable, hand-engineered features.",
        "url": "http://arxiv.org/abs/2508.11803v1",
        "published_date": "2025-08-15T21:18:23+00:00",
        "updated_date": "2025-08-15T21:18:23+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Azam Nouri"
        ],
        "ai_categories": [
            "Other"
        ],
        "tldr": "The paper explores using geometric cues for handwriting recognition with a MLP, achieving high accuracy without convolutional neural networks.",
        "tldr_zh": "本文探讨了使用几何线索进行手写识别与MLP结合，实现高准确度而不使用卷积神经网络。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Statistical analysis of multivariate planar curves and applications to X-ray classification",
        "summary": "Recent developments in computer vision have enabled the availability of\nsegmented images across various domains, such as medicine, where segmented\nradiography images play an important role in diagnosis-making. As prediction\nproblems are common in medical image analysis, this work explores the use of\nsegmented images (through the associated contours they highlight) as predictors\nin a supervised classification context. Consequently, we develop a new approach\nfor image analysis that takes into account the shape of objects within images.\nFor this aim, we introduce a new formalism that extends the study of single\nrandom planar curves to the joint analysis of multiple planar curves-referred\nto here as multivariate planar curves. In this framework, we propose a solution\nto the alignment issue in statistical shape analysis. The obtained multivariate\nshape variables are then used in functional classification methods through\ntangent projections. Detection of cardiomegaly in segmented X-rays and\nnumerical experiments on synthetic data demonstrate the appeal and robustness\nof the proposed method.",
        "url": "http://arxiv.org/abs/2508.11780v1",
        "published_date": "2025-08-15T19:13:27+00:00",
        "updated_date": "2025-08-15T19:13:27+00:00",
        "categories": [
            "stat.ME",
            "cs.CV",
            "stat.ML"
        ],
        "authors": [
            "Moindjié Issam-Ali",
            "Descary Marie-Hélène",
            "Beaulac Cédric"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a new approach for image analysis using multivariate planar curves for classification, focusing on detecting cardiomegaly in segmented X-rays.",
        "tldr_zh": "该论文介绍了一种利用多变量平面曲线进行分类的图像分析新方法，重点是在分割X射线图像中检测心脏肥大。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Automated Model Evaluation for Object Detection via Prediction Consistency and Reliablity",
        "summary": "Recent advances in computer vision have made training object detectors more\nefficient and effective; however, assessing their performance in real-world\napplications still relies on costly manual annotation. To address this\nlimitation, we develop an automated model evaluation (AutoEval) framework for\nobject detection. We propose Prediction Consistency and Reliability (PCR),\nwhich leverages the multiple candidate bounding boxes that conventional\ndetectors generate before non-maximum suppression (NMS). PCR estimates\ndetection performance without ground-truth labels by jointly measuring 1) the\nspatial consistency between boxes before and after NMS, and 2) the reliability\nof the retained boxes via the confidence scores of overlapping boxes. For a\nmore realistic and scalable evaluation, we construct a meta-dataset by applying\nimage corruptions of varying severity. Experimental results demonstrate that\nPCR yields more accurate performance estimates than existing AutoEval methods,\nand the proposed meta-dataset covers a wider range of detection performance.\nThe code is available at https://github.com/YonseiML/autoeval-det.",
        "url": "http://arxiv.org/abs/2508.12082v1",
        "published_date": "2025-08-16T15:39:56+00:00",
        "updated_date": "2025-08-16T15:39:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Seungju Yoo",
            "Hyuk Kwon",
            "Joong-Won Hwang",
            "Kibok Lee"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces an automated model evaluation framework for object detection using Prediction Consistency and Reliability (PCR) to estimate performance without ground-truth labels.",
        "tldr_zh": "该论文介绍了一种利用预测一致性和可靠性（PCR）进行目标检测自动模型评估的框架，以估计性能而无需地面真相标签。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.25
    },
    {
        "title": "Transferable Class Statistics and Multi-scale Feature Approximation for 3D Object Detection",
        "summary": "This paper investigates multi-scale feature approximation and transferable\nfeatures for object detection from point clouds. Multi-scale features are\ncritical for object detection from point clouds. However, multi-scale feature\nlearning usually involves multiple neighborhood searches and scale-aware\nlayers, which can hinder efforts to achieve lightweight models and may not be\nconducive to research constrained by limited computational resources. This\npaper approximates point-based multi-scale features from a single neighborhood\nbased on knowledge distillation. To compensate for the loss of constructive\ndiversity in a single neighborhood, this paper designs a transferable feature\nembedding mechanism. Specifically, class-aware statistics are employed as\ntransferable features given the small computational cost. In addition, this\npaper introduces the central weighted intersection over union for localization\nto alleviate the misalignment brought by the center offset in optimization.\nNote that the method presented in this paper saves computational costs.\nExtensive experiments on public datasets demonstrate the effectiveness of the\nproposed method.",
        "url": "http://arxiv.org/abs/2508.11951v1",
        "published_date": "2025-08-16T07:27:01+00:00",
        "updated_date": "2025-08-16T07:27:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hao Peng",
            "Hong Sang",
            "Yajing Ma",
            "Ping Qiu",
            "Chao Ji"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a method for 3D object detection using transferable class statistics and multi-scale feature approximation from point clouds, saving computational costs and achieving effectiveness.",
        "tldr_zh": "本文提出了一种利用点云的可转移类别统计和多尺度特征逼近进行3D目标检测的方法，节约计算成本并实现有效性。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.0
    }
]