[
    {
        "title": "Semantic and Visual Crop-Guided Diffusion Models for Heterogeneous Tissue Synthesis in Histopathology",
        "summary": "Synthetic data generation in histopathology faces unique challenges:\npreserving tissue heterogeneity, capturing subtle morphological features, and\nscaling to unannotated datasets. We present a latent diffusion model that\ngenerates realistic heterogeneous histopathology images through a novel\ndual-conditioning approach combining semantic segmentation maps with\ntissue-specific visual crops. Unlike existing methods that rely on text prompts\nor abstract visual embeddings, our approach preserves critical morphological\ndetails by directly incorporating raw tissue crops from corresponding semantic\nregions. For annotated datasets (i.e., Camelyon16, Panda), we extract patches\nensuring 20-80% tissue heterogeneity. For unannotated data (i.e., TCGA), we\nintroduce a self-supervised extension that clusters whole-slide images into 100\ntissue types using foundation model embeddings, automatically generating\npseudo-semantic maps for training. Our method synthesizes high-fidelity images\nwith precise region-wise annotations, achieving superior performance on\ndownstream segmentation tasks. When evaluated on annotated datasets, models\ntrained on our synthetic data show competitive performance to those trained on\nreal data, demonstrating the utility of controlled heterogeneous tissue\ngeneration. In quantitative evaluation, prompt-guided synthesis reduces Frechet\nDistance by up to 6X on Camelyon16 (from 430.1 to 72.0) and yields 2-3x lower\nFD across Panda and TCGA. Downstream DeepLabv3+ models trained solely on\nsynthetic data attain test IoU of 0.71 and 0.95 on Camelyon16 and Panda, within\n1-2% of real-data baselines (0.72 and 0.96). By scaling to 11,765 TCGA\nwhole-slide images without manual annotations, our framework offers a practical\nsolution for an urgent need for generating diverse, annotated histopathology\ndata, addressing a critical bottleneck in computational pathology.",
        "url": "http://arxiv.org/abs/2509.17847v1",
        "published_date": "2025-09-22T14:41:43+00:00",
        "updated_date": "2025-09-22T14:41:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Saghir Alfasly",
            "Wataru Uegami",
            "MD Enamul Hoq",
            "Ghazal Alabtah",
            "H. R. Tizhoosh"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper presents a novel approach using semantic segmentation maps and visual crops to generate realistic heterogeneous histopathology images, achieving superior performance on downstream tasks and offering a practical solution for generating diverse annotated histopathology data.",
        "tldr_zh": "本文提出了一种新颖的方法，利用语义分割地图和视觉裁剪生成逼真多样的组织病理图像，在下游任务中表现优异，为生成多样化注释的组织病理数据提供了实用解决方案。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "Event-Based Visual Teach-and-Repeat via Fast Fourier-Domain Cross-Correlation",
        "summary": "Visual teach-and-repeat navigation enables robots to autonomously traverse\npreviously demonstrated paths by comparing current sensory input with recorded\ntrajectories. However, conventional frame-based cameras fundamentally limit\nsystem responsiveness: their fixed frame rates (typically 30-60 Hz) create\ninherent latency between environmental changes and control responses. Here we\npresent the first event-camera-based visual teach-and-repeat system. To achieve\nthis, we develop a frequency-domain cross-correlation framework that transforms\nthe event stream matching problem into computationally efficient Fourier space\nmultiplications, capable of exceeding 300Hz processing rates, an order of\nmagnitude faster than frame-based approaches. By exploiting the binary nature\nof event frames and applying image compression techniques, we further enhance\nthe computational speed of the cross-correlation process without sacrificing\nlocalization accuracy. Extensive experiments using a Prophesee EVK4 HD event\ncamera mounted on an AgileX Scout Mini robot demonstrate successful autonomous\nnavigation across 4000+ meters of indoor and outdoor trajectories. Our system\nachieves ATEs below 24 cm while maintaining consistent high-frequency control\nupdates. Our evaluations show that our approach achieves substantially higher\nupdate rates compared to conventional frame-based systems, underscoring the\npractical viability of event-based perception for real-time robotic navigation.",
        "url": "http://arxiv.org/abs/2509.17287v1",
        "published_date": "2025-09-21T23:53:31+00:00",
        "updated_date": "2025-09-21T23:53:31+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Gokul B. Nair",
            "Alejandro Fontan",
            "Michael Milford",
            "Tobias Fischer"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel event-camera-based visual teach-and-repeat system for autonomous robot navigation with high processing rates and localization accuracy using Fourier domain cross-correlation and image compression.",
        "tldr_zh": "本文引入了一种新颖的基于事件相机的视觉教导-重复系统，利用傅立叶域交叉相关和图像压缩实现高处理速率和定位精度的自主机器人导航。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "Echo-Path: Pathology-Conditioned Echo Video Generation",
        "summary": "Cardiovascular diseases (CVDs) remain the leading cause of mortality\nglobally, and echocardiography is critical for diagnosis of both common and\ncongenital cardiac conditions. However, echocardiographic data for certain\npathologies are scarce, hindering the development of robust automated diagnosis\nmodels. In this work, we propose Echo-Path, a novel generative framework to\nproduce echocardiogram videos conditioned on specific cardiac pathologies.\nEcho-Path can synthesize realistic ultrasound video sequences that exhibit\ntargeted abnormalities, focusing here on atrial septal defect (ASD) and\npulmonary arterial hypertension (PAH). Our approach introduces a\npathology-conditioning mechanism into a state-of-the-art echo video generator,\nallowing the model to learn and control disease-specific structural and motion\npatterns in the heart. Quantitative evaluation demonstrates that the synthetic\nvideos achieve low distribution distances, indicating high visual fidelity.\nClinically, the generated echoes exhibit plausible pathology markers.\nFurthermore, classifiers trained on our synthetic data generalize well to real\ndata and, when used to augment real training sets, it improves downstream\ndiagnosis of ASD and PAH by 7\\% and 8\\% respectively. Code, weights and dataset\nare available here https://github.com/Marshall-mk/EchoPathv1",
        "url": "http://arxiv.org/abs/2509.17190v1",
        "published_date": "2025-09-21T18:31:28+00:00",
        "updated_date": "2025-09-21T18:31:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kabir Hamzah Muhammad",
            "Marawan Elbatel",
            "Yi Qin",
            "Xiaomeng Li"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces Echo-Path, a generative framework that can produce realistic echocardiogram videos conditioned on specific cardiac pathologies, improving automated diagnosis models for cardiovascular diseases.",
        "tldr_zh": "本文引入了Echo-Path，一个生成框架，可以生成受特定心脏病理条件限制的逼真心脏超声视频，提高了心血管疾病的自动诊断模型。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image Generation",
        "summary": "Generating high-fidelity images of humans with fine-grained control over\nattributes such as hairstyle and clothing remains a core challenge in\npersonalized text-to-image synthesis. While prior methods emphasize identity\npreservation from a reference image, they lack modularity and fail to provide\ndisentangled control over specific visual attributes. We introduce a new\nparadigm for attribute-specific image prompting, in which distinct sets of\nreference images are used to guide the generation of individual aspects of\nhuman appearance, such as hair, clothing, and identity. Our method encodes\nthese inputs into attribute-specific tokens, which are injected into a\npre-trained text-to-image diffusion model. This enables compositional and\ndisentangled control over multiple visual factors, even across multiple people\nwithin a single image. To promote natural composition and robust\ndisentanglement, we curate a cross-reference training dataset featuring\nsubjects in diverse poses and expressions, and propose a multi-attribute\ncross-reference training strategy that encourages the model to generate\nfaithful outputs from misaligned attribute inputs while adhering to both\nidentity and textual conditioning. Extensive experiments show that our method\nachieves state-of-the-art performance in accurately following both visual and\ntextual prompts. Our framework paves the way for more configurable human image\nsynthesis by combining visual prompting with text-driven generation. Webpage is\navailable at: https://snap-research.github.io/composeme/.",
        "url": "http://arxiv.org/abs/2509.18092v1",
        "published_date": "2025-09-22T17:59:30+00:00",
        "updated_date": "2025-09-22T17:59:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guocheng Gordon Qian",
            "Daniil Ostashev",
            "Egor Nemchinov",
            "Avihay Assouline",
            "Sergey Tulyakov",
            "Kuan-Chieh Jackson Wang",
            "Kfir Aberman"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a method for controllable human image generation by using attribute-specific image prompts, achieving state-of-the-art performance in following visual and textual prompts.",
        "tldr_zh": "本文介绍了一种通过使用属性特定的图像提示来实现可控人体图像生成的方法，达到了在遵循视觉和文本提示方面的最先进性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs",
        "summary": "This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework\ndesigned to improve the effectiveness of adapting multimodal large language\nmodels (MLLMs) to video temporal grounding tasks. We reveal that existing\nreinforcement learning methods, such as Group Relative Policy Optimization\n(GRPO), rely on on-policy sampling for policy updates. However, in tasks with\nlarge temporal search spaces, this strategy becomes both inefficient and\nlimited in performance, as it often fails to identify temporally accurate\nsolutions. To address this limitation, TempSamp-R1 leverages ground-truth\nannotations as off-policy supervision to provide temporally precise guidance,\neffectively compensating for the sparsity and misalignment in on-policy\nsolutions. To further stabilize training and reduce variance in reward-based\nupdates, TempSamp-R1 provides a non-linear soft advantage computation method\nthat dynamically reshapes the reward feedback via an asymmetric transformation.\nBy employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1\noptimizes a single unified model to support both CoT and non-CoT inference\nmodes, enabling efficient handling of queries with varying reasoning\ncomplexity. Experimental results demonstrate that TempSamp-R1 outperforms\nGRPO-based baselines, establishing new state-of-the-art performance on\nbenchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions\n(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,\nTempSamp-R1 shows robust few-shot generalization capabilities under limited\ndata. Code: https://github.com/HVision-NKU/TempSamp-R1",
        "url": "http://arxiv.org/abs/2509.18056v1",
        "published_date": "2025-09-22T17:30:15+00:00",
        "updated_date": "2025-09-22T17:30:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yunheng Li",
            "Jing Cheng",
            "Shaoyong Jia",
            "Hangyi Kuang",
            "Shaohui Jiao",
            "Qibin Hou",
            "Ming-Ming Cheng"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "TempSamp-R1 introduces a reinforcement fine-tuning framework to improve the performance of adapting multimodal large language models to video temporal grounding tasks, outperforming existing methods on benchmark datasets.",
        "tldr_zh": "TempSamp-R1引入了一种强化微调框架，以提高调整多模态大型语言模型到视频时间定位任务的性能，在基准数据集上优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "I2VWM: Robust Watermarking for Image to Video Generation",
        "summary": "The rapid progress of image-guided video generation (I2V) has raised concerns\nabout its potential misuse in misinformation and fraud, underscoring the urgent\nneed for effective digital watermarking. While existing watermarking methods\ndemonstrate robustness within a single modality, they fail to trace source\nimages in I2V settings. To address this gap, we introduce the concept of Robust\nDiffusion Distance, which measures the temporal persistence of watermark\nsignals in generated videos. Building on this, we propose I2VWM, a cross-modal\nwatermarking framework designed to enhance watermark robustness across time.\nI2VWM leverages a video-simulation noise layer during training and employs an\noptical-flow-based alignment module during inference. Experiments on both\nopen-source and commercial I2V models demonstrate that I2VWM significantly\nimproves robustness while maintaining imperceptibility, establishing a new\nparadigm for cross-modal watermarking in the era of generative video.\n\\href{https://github.com/MrCrims/I2VWM-Robust-Watermarking-for-Image-to-Video-Generation}{Code\nReleased.}",
        "url": "http://arxiv.org/abs/2509.17773v1",
        "published_date": "2025-09-22T13:37:37+00:00",
        "updated_date": "2025-09-22T13:37:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guanjie Wang",
            "Zehua Ma",
            "Han Fang",
            "Weiming Zhang"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Diffusion",
            "Other"
        ],
        "tldr": "The paper introduces I2VWM, a watermarking framework for enhancing watermark robustness in image to video generation, showing significant improvements in robustness without compromising imperceptibility.",
        "tldr_zh": "本文介绍了I2VWM，一个用于增强图像到视频生成中水印稳健性的框架，显示出在增强稳健性方面取得了显著的改进，同时不影响水印不可察觉性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "WISE: Weak-Supervision-Guided Step-by-Step Explanations for Multimodal LLMs in Image Classification",
        "summary": "Multimodal Large Language Models (MLLMs) have shown promise in visual-textual\nreasoning, with Multimodal Chain-of-Thought (MCoT) prompting significantly\nenhancing interpretability. However, existing MCoT methods rely on\nrationale-rich datasets and largely focus on inter-object reasoning,\noverlooking the intra-object understanding crucial for image classification. To\naddress this gap, we propose WISE, a Weak-supervision-guided Step-by-step\nExplanation method that augments any image classification dataset with MCoTs by\nreformulating the concept-based representations from Concept Bottleneck Models\n(CBMs) into concise, interpretable reasoning chains under weak supervision.\nExperiments across ten datasets show that our generated MCoTs not only improve\ninterpretability by 37% but also lead to gains in classification accuracy when\nused to fine-tune MLLMs. Our work bridges concept-based interpretability and\ngenerative MCoT reasoning, providing a generalizable framework for enhancing\nMLLMs in fine-grained visual understanding.",
        "url": "http://arxiv.org/abs/2509.17740v1",
        "published_date": "2025-09-22T13:05:29+00:00",
        "updated_date": "2025-09-22T13:05:29+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Yiwen Jiang",
            "Deval Mehta",
            "Siyuan Yan",
            "Yaling Shen",
            "Zimu Wang",
            "Zongyuan Ge"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces WISE, a method that enhances image classification interpretability by generating concise reasoning chains for Multimodal Large Language Models.",
        "tldr_zh": "本文介绍了一种名为WISE的方法，通过为多模式大语言模型生成简明的推理链，提高了图像分类的可解释性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Unified Multimodal Coherent Field: Synchronous Semantic-Spatial-Vision Fusion for Brain Tumor Segmentation",
        "summary": "Brain tumor segmentation requires accurate identification of hierarchical\nregions including whole tumor (WT), tumor core (TC), and enhancing tumor (ET)\nfrom multi-sequence magnetic resonance imaging (MRI) images. Due to tumor\ntissue heterogeneity, ambiguous boundaries, and contrast variations across MRI\nsequences, methods relying solely on visual information or post-hoc loss\nconstraints show unstable performance in boundary delineation and hierarchy\npreservation. To address this challenge, we propose the Unified Multimodal\nCoherent Field (UMCF) method. This method achieves synchronous interactive\nfusion of visual, semantic, and spatial information within a unified 3D latent\nspace, adaptively adjusting modal contributions through parameter-free\nuncertainty gating, with medical prior knowledge directly participating in\nattention computation, avoiding the traditional \"process-then-concatenate\"\nseparated architecture. On Brain Tumor Segmentation (BraTS) 2020 and 2021\ndatasets, UMCF+nnU-Net achieves average Dice coefficients of 0.8579 and 0.8977\nrespectively, with an average 4.18% improvement across mainstream\narchitectures. By deeply integrating clinical knowledge with imaging features,\nUMCF provides a new technical pathway for multimodal information fusion in\nprecision medicine.",
        "url": "http://arxiv.org/abs/2509.17520v1",
        "published_date": "2025-09-22T08:45:39+00:00",
        "updated_date": "2025-09-22T08:45:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mingda Zhang",
            "Yuyang Zheng",
            "Ruixiang Tang",
            "Jingru Qiu",
            "Haiyan Ding"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a Unified Multimodal Coherent Field method for brain tumor segmentation, achieving high accuracy by integrating visual, semantic, and spatial information.",
        "tldr_zh": "本文提出了一种统一的多模态协同场方法，用于大脑肿瘤分割，通过整合视觉、语义和空间信息实现高准确度。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "Stable Video-Driven Portraits",
        "summary": "Portrait animation aims to generate photo-realistic videos from a single\nsource image by reenacting the expression and pose from a driving video. While\nearly methods relied on 3D morphable models or feature warping techniques, they\noften suffered from limited expressivity, temporal inconsistency, and poor\ngeneralization to unseen identities or large pose variations. Recent advances\nusing diffusion models have demonstrated improved quality but remain\nconstrained by weak control signals and architectural limitations. In this\nwork, we propose a novel diffusion based framework that leverages masked facial\nregions specifically the eyes, nose, and mouth from the driving video as strong\nmotion control cues. To enable robust training without appearance leakage, we\nadopt cross identity supervision. To leverage the strong prior from the\npretrained diffusion model, our novel architecture introduces minimal new\nparameters that converge faster and help in better generalization. We introduce\nspatial temporal attention mechanisms that allow inter frame and intra frame\ninteractions, effectively capturing subtle motions and reducing temporal\nartifacts. Our model uses history frames to ensure continuity across segments.\nAt inference, we propose a novel signal fusion strategy that balances motion\nfidelity with identity preservation. Our approach achieves superior temporal\nconsistency and accurate expression control, enabling high-quality,\ncontrollable portrait animation suitable for real-world applications.",
        "url": "http://arxiv.org/abs/2509.17476v1",
        "published_date": "2025-09-22T08:11:08+00:00",
        "updated_date": "2025-09-22T08:11:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mallikarjun B. R.",
            "Fei Yin",
            "Vikram Voleti",
            "Nikita Drobyshev",
            "Maksim Lapin",
            "Aaryaman Vasishta",
            "Varun Jampani"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel diffusion-based framework for portrait animation using masked facial regions as strong motion control cues, achieving superior temporal consistency and accurate expression control.",
        "tldr_zh": "本文提出了一种基于扩散的新框架，利用面部区域作为强大的动作控制线索进行肖像动画，实现了出色的时间一致性和准确的表情控制。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "Qwen3-Omni Technical Report",
        "summary": "We present Qwen3-Omni, a single multimodal model that, for the first time,\nmaintains state-of-the-art performance across text, image, audio, and video\nwithout any degradation relative to single-modal counterparts. Qwen3-Omni\nmatches the performance of same-sized single-modal models within the Qwen\nseries and excels particularly on audio tasks. Across 36 audio and audio-visual\nbenchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall\nSOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro,\nSeed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE\narchitecture that unifies perception and generation across text, images, audio,\nand video, yielding fluent text and natural real-time speech. It supports text\ninteraction in 119 languages, speech understanding in 19 languages, and speech\ngeneration in 10 languages. To reduce first-packet latency in streaming\nsynthesis, Talker autoregressively predicts discrete speech codecs using a\nmulti-codebook scheme. Leveraging the representational capacity of these\ncodebooks, we replace computationally intensive block-wise diffusion with a\nlightweight causal ConvNet, enabling streaming from the first codec frame. In\ncold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet\nlatency of 234 ms. To further strengthen multimodal reasoning, we introduce a\nThinking model that explicitly reasons over inputs from any modality. Since the\nresearch community currently lacks a general-purpose audio captioning model, we\nfine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which\nproduces detailed, low-hallucination captions for arbitrary audio inputs.\nQwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and\nQwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0\nlicense.",
        "url": "http://arxiv.org/abs/2509.17765v1",
        "published_date": "2025-09-22T13:26:24+00:00",
        "updated_date": "2025-09-22T13:26:24+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "eess.AS"
        ],
        "authors": [
            "Jin Xu",
            "Zhifang Guo",
            "Hangrui Hu",
            "Yunfei Chu",
            "Xiong Wang",
            "Jinzheng He",
            "Yuxuan Wang",
            "Xian Shi",
            "Ting He",
            "Xinfa Zhu",
            "Yuanjun Lv",
            "Yongqi Wang",
            "Dake Guo",
            "He Wang",
            "Linhan Ma",
            "Pei Zhang",
            "Xinyu Zhang",
            "Hongkun Hao",
            "Zishan Guo",
            "Baosong Yang",
            "Bin Zhang",
            "Ziyang Ma",
            "Xipin Wei",
            "Shuai Bai",
            "Keqin Chen",
            "Xuejing Liu",
            "Peng Wang",
            "Mingkun Yang",
            "Dayiheng Liu",
            "Xingzhang Ren",
            "Bo Zheng",
            "Rui Men",
            "Fan Zhou",
            "Bowen Yu",
            "Jianxin Yang",
            "Le Yu",
            "Jingren Zhou",
            "Junyang Lin"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "Qwen3-Omni is a multimodal model that maintains state-of-the-art performance across text, image, audio, and video, outperforming other models in various benchmarks.",
        "tldr_zh": "Qwen3-Omni是一个多模态模型，跨文本、图像、音频和视频保持着最先进的性能，在各种基准测试中表现优异。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Multi-Agent Amodal Completion: Direct Synthesis with Fine-Grained Semantic Guidance",
        "summary": "Amodal completion, generating invisible parts of occluded objects, is vital\nfor applications like image editing and AR. Prior methods face challenges with\ndata needs, generalization, or error accumulation in progressive pipelines. We\npropose a Collaborative Multi-Agent Reasoning Framework based on upfront\ncollaborative reasoning to overcome these issues. Our framework uses multiple\nagents to collaboratively analyze occlusion relationships and determine\nnecessary boundary expansion, yielding a precise mask for inpainting.\nConcurrently, an agent generates fine-grained textual descriptions, enabling\nFine-Grained Semantic Guidance. This ensures accurate object synthesis and\nprevents the regeneration of occluders or other unwanted elements, especially\nwithin large inpainting areas. Furthermore, our method directly produces\nlayered RGBA outputs guided by visible masks and attention maps from a\nDiffusion Transformer, eliminating extra segmentation. Extensive evaluations\ndemonstrate our framework achieves state-of-the-art visual quality.",
        "url": "http://arxiv.org/abs/2509.17757v1",
        "published_date": "2025-09-22T13:20:06+00:00",
        "updated_date": "2025-09-22T13:20:06+00:00",
        "categories": [
            "cs.CV",
            "cs.MA"
        ],
        "authors": [
            "Hongxing Fan",
            "Lipeng Wang",
            "Haohua Chen",
            "Zehuan Huang",
            "Jiangtao Wu",
            "Lu Sheng"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a Collaborative Multi-Agent Reasoning Framework for amodal completion, which achieves state-of-the-art visual quality.",
        "tldr_zh": "本文引入了一种用于amodal完成的协作多智能体推理框架，实现了最先进的视觉质量。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "MirrorSAM2: Segment Mirror in Videos with Depth Perception",
        "summary": "This paper presents MirrorSAM2, the first framework that adapts Segment\nAnything Model 2 (SAM2) to the task of RGB-D video mirror segmentation.\nMirrorSAM2 addresses key challenges in mirror detection, such as reflection\nambiguity and texture confusion, by introducing four tailored modules: a Depth\nWarping Module for RGB and depth alignment, a Depth-guided Multi-Scale Point\nPrompt Generator for automatic prompt generation, a Frequency Detail Attention\nFusion Module to enhance structural boundaries, and a Mirror Mask Decoder with\na learnable mirror token for refined segmentation. By fully leveraging the\ncomplementarity between RGB and depth, MirrorSAM2 extends SAM2's capabilities\nto the prompt-free setting. To our knowledge, this is the first work to enable\nSAM2 for automatic video mirror segmentation. Experiments on the VMD and DVMD\nbenchmark demonstrate that MirrorSAM2 achieves SOTA performance, even under\nchallenging conditions such as small mirrors, weak boundaries, and strong\nreflections.",
        "url": "http://arxiv.org/abs/2509.17220v1",
        "published_date": "2025-09-21T20:00:33+00:00",
        "updated_date": "2025-09-21T20:00:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mingchen Xu",
            "Yukun Lai",
            "Ze Ji",
            "Jing Wu"
        ],
        "ai_categories": [
            "Other"
        ],
        "tldr": "MirrorSAM2 is a framework that extends SAM2 for RGB-D video mirror segmentation, achieving state-of-the-art performance.",
        "tldr_zh": "MirrorSAM2是一个将SAM2扩展到RGB-D视频镜像分割的框架，实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Seg4Diff: Unveiling Open-Vocabulary Segmentation in Text-to-Image Diffusion Transformers",
        "summary": "Text-to-image diffusion models excel at translating language prompts into\nphotorealistic images by implicitly grounding textual concepts through their\ncross-modal attention mechanisms. Recent multi-modal diffusion transformers\nextend this by introducing joint self-attention over concatenated image and\ntext tokens, enabling richer and more scalable cross-modal alignment. However,\na detailed understanding of how and where these attention maps contribute to\nimage generation remains limited. In this paper, we introduce Seg4Diff\n(Segmentation for Diffusion), a systematic framework for analyzing the\nattention structures of MM-DiT, with a focus on how specific layers propagate\nsemantic information from text to image. Through comprehensive analysis, we\nidentify a semantic grounding expert layer, a specific MM-DiT block that\nconsistently aligns text tokens with spatially coherent image regions,\nnaturally producing high-quality semantic segmentation masks. We further\ndemonstrate that applying a lightweight fine-tuning scheme with mask-annotated\nimage data enhances the semantic grouping capabilities of these layers and\nthereby improves both segmentation performance and generated image fidelity.\nOur findings demonstrate that semantic grouping is an emergent property of\ndiffusion transformers and can be selectively amplified to advance both\nsegmentation and generation performance, paving the way for unified models that\nbridge visual perception and generation.",
        "url": "http://arxiv.org/abs/2509.18096v1",
        "published_date": "2025-09-22T17:59:54+00:00",
        "updated_date": "2025-09-22T17:59:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chaehyun Kim",
            "Heeseong Shin",
            "Eunbeen Hong",
            "Heeji Yoon",
            "Anurag Arnab",
            "Paul Hongsuck Seo",
            "Sunghwan Hong",
            "Seungryong Kim"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "GAN"
        ],
        "tldr": "The paper introduces Seg4Diff, a framework for analyzing attention structures in text-to-image diffusion transformers, improving semantic segmentation and image generation.",
        "tldr_zh": "本文介绍了Seg4Diff，这是一个用于分析文本到图像扩散变换器中的注意力结构的框架，以改进语义分割和图像生成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction",
        "summary": "Universal multimodal embedding models have achieved great success in\ncapturing semantic relevance between queries and candidates. However, current\nmethods either condense queries and candidates into a single vector,\npotentially limiting the expressiveness for fine-grained information, or\nproduce too many vectors that are prohibitively expensive for multi-vector\nretrieval. In this work, we introduce MetaEmbed, a new framework for multimodal\nretrieval that rethinks how multimodal embeddings are constructed and\ninteracted with at scale. During training, a fixed number of learnable Meta\nTokens are appended to the input sequence. At test-time, their last-layer\ncontextualized representations serve as compact yet expressive multi-vector\nembeddings. Through the proposed Matryoshka Multi-Vector Retrieval training,\nMetaEmbed learns to organize information by granularity across multiple\nvectors. As a result, we enable test-time scaling in multimodal retrieval,\nwhere users can balance retrieval quality against efficiency demands by\nselecting the number of tokens used for indexing and retrieval interactions.\nExtensive evaluations on the Massive Multimodal Embedding Benchmark (MMEB) and\nthe Visual Document Retrieval Benchmark (ViDoRe) confirm that MetaEmbed\nachieves state-of-the-art retrieval performance while scaling robustly to\nmodels with 32B parameters.",
        "url": "http://arxiv.org/abs/2509.18095v1",
        "published_date": "2025-09-22T17:59:42+00:00",
        "updated_date": "2025-09-22T17:59:42+00:00",
        "categories": [
            "cs.IR",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Zilin Xiao",
            "Qi Ma",
            "Mengting Gu",
            "Chun-cheng Jason Chen",
            "Xintao Chen",
            "Vicente Ordonez",
            "Vijai Mohan"
        ],
        "ai_categories": [
            "Multimodality"
        ],
        "tldr": "MetaEmbed introduces a new framework for multimodal retrieval that improves expressiveness and efficiency at test-time through Meta Tokens and Matryoshka Multi-Vector Retrieval training.",
        "tldr_zh": "MetaEmbed引入了一个新的多模态检索框架，通过Meta Tokens和Matryoshka Multi-Vector Retrieval训练，在测试时间提高了表达性和效率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning",
        "summary": "Recent advances in Large Multi-modal Models (LMMs) have demonstrated their\nremarkable success as general-purpose multi-modal assistants, with particular\nfocuses on holistic image- and video-language understanding. Conversely, less\nattention has been given to scaling fine-grained pixel-level understanding\ncapabilities, where the models are expected to realize pixel-level alignment\nbetween visual signals and language semantics. Some previous studies have\napplied LMMs to related tasks such as region-level captioning and referring\nexpression segmentation. However, these models are limited to performing either\nreferring or segmentation tasks independently and fail to integrate these\nfine-grained perception capabilities into visual reasoning. To bridge this gap,\nwe propose UniPixel, a large multi-modal model capable of flexibly\ncomprehending visual prompt inputs and generating mask-grounded responses. Our\nmodel distinguishes itself by seamlessly integrating pixel-level perception\nwith general visual understanding capabilities. Specifically, UniPixel\nprocesses visual prompts and generates relevant masks on demand, and performs\nsubsequent reasoning conditioning on these intermediate pointers during\ninference, thereby enabling fine-grained pixel-level reasoning. The\neffectiveness of our approach has been verified on 10 benchmarks across a\ndiverse set of tasks, including pixel-level referring/segmentation and\nobject-centric understanding in images/videos. A novel PixelQA task that\njointly requires referring, segmentation, and question answering is also\ndesigned to verify the flexibility of our method.",
        "url": "http://arxiv.org/abs/2509.18094v1",
        "published_date": "2025-09-22T17:59:40+00:00",
        "updated_date": "2025-09-22T17:59:40+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ye Liu",
            "Zongyang Ma",
            "Junfu Pu",
            "Zhongang Qi",
            "Yang Wu",
            "Ying Shan",
            "Chang Wen Chen"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "UniPixel is a large multi-modal model that integrates pixel-level perception with general visual understanding for pixel-level visual reasoning tasks.",
        "tldr_zh": "UniPixel是一个大型多模态模型，将像素级感知与一般视觉理解相结合，用于像素级的视觉推理任务。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "NeuS-QA: Grounding Long-Form Video Understanding in Temporal Logic and Neuro-Symbolic Reasoning",
        "summary": "Long-Form Video Question Answering (LVQA) poses challenges beyond traditional\nvisual question answering (VQA), which is often limited to static images or\nshort video clips. While current vision-language models (VLMs) perform well in\nthose settings, they struggle with complex queries in LVQA over long videos\ninvolving multi-step temporal reasoning and causality. Vanilla approaches,\nwhich sample frames uniformly and feed them to a VLM with the question, incur\nsignificant token overhead, forcing severe downsampling. As a result, the model\noften misses fine-grained visual structure, subtle event transitions, or key\ntemporal cues, ultimately leading to incorrect answers. To address these\nlimitations, recent works have explored query-adaptive frame sampling,\nhierarchical keyframe selection, and agent-based iterative querying. However,\nthese methods remain fundamentally heuristic: they lack explicit temporal\nrepresentations and cannot enforce or verify logical event relationships. As a\nresult, there are no formal guarantees that the sampled context actually\nencodes the compositional or causal logic demanded by the question. To address\nthese foundational gaps, we introduce NeuS-QA, a training-free, plug-and-play\nneuro-symbolic pipeline for LVQA. NeuS-QA translates a natural language\nquestion into a formal temporal logic expression, constructs a video automaton\nfrom frame-level semantic propositions, and applies model checking to\nrigorously identify video segments satisfying the question's logical\nrequirements. Only these logic-verified segments are submitted to the VLM, thus\nimproving interpretability, reducing hallucinations, and enabling compositional\nreasoning without modifying or fine-tuning the model. Experiments on\nLongVideoBench and CinePile show NeuS-QA improves performance by over 10%,\nespecially on questions involving event ordering, causality, and multi-step\ncompositional reasoning.",
        "url": "http://arxiv.org/abs/2509.18041v1",
        "published_date": "2025-09-22T17:15:13+00:00",
        "updated_date": "2025-09-22T17:15:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sahil Shah",
            "S P Sharan",
            "Harsh Goel",
            "Minkyu Choi",
            "Mustafa Munir",
            "Manvik Pasula",
            "Radu Marculescu",
            "Sandeep Chinchali"
        ],
        "ai_categories": [
            "Transformer",
            "AIGC"
        ],
        "tldr": "NeuS-QA is a neuro-symbolic pipeline for Long-Form Video Question Answering that translates questions into formal temporal logic expressions to improve interpretability and reasoning in video understanding.",
        "tldr_zh": "NeuS-QA是一个用于长视频问答的神经符号管道，将问题转化为形式化的时间逻辑表达式，以提高视频理解的可解释性和推理能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "StableGuard: Towards Unified Copyright Protection and Tamper Localization in Latent Diffusion Models",
        "summary": "The advancement of diffusion models has enhanced the realism of AI-generated\ncontent but also raised concerns about misuse, necessitating robust copyright\nprotection and tampering localization. Although recent methods have made\nprogress toward unified solutions, their reliance on post hoc processing\nintroduces considerable application inconvenience and compromises forensic\nreliability. We propose StableGuard, a novel framework that seamlessly\nintegrates a binary watermark into the diffusion generation process, ensuring\ncopyright protection and tampering localization in Latent Diffusion Models\nthrough an end-to-end design. We develop a Multiplexing Watermark VAE (MPW-VAE)\nby equipping a pretrained Variational Autoencoder (VAE) with a lightweight\nlatent residual-based adapter, enabling the generation of paired watermarked\nand watermark-free images. These pairs, fused via random masks, create a\ndiverse dataset for training a tampering-agnostic forensic network. To further\nenhance forensic synergy, we introduce a Mixture-of-Experts Guided Forensic\nNetwork (MoE-GFN) that dynamically integrates holistic watermark patterns,\nlocal tampering traces, and frequency-domain cues for precise watermark\nverification and tampered region detection. The MPW-VAE and MoE-GFN are jointly\noptimized in a self-supervised, end-to-end manner, fostering a reciprocal\ntraining between watermark embedding and forensic accuracy. Extensive\nexperiments demonstrate that StableGuard consistently outperforms\nstate-of-the-art methods in image fidelity, watermark verification, and\ntampering localization.",
        "url": "http://arxiv.org/abs/2509.17993v1",
        "published_date": "2025-09-22T16:35:19+00:00",
        "updated_date": "2025-09-22T16:35:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoxin Yang",
            "Bangzhen Liu",
            "Xuemiao Xu",
            "Cheng Xu",
            "Yuyang Yu",
            "Zikai Huang",
            "Yi Wang",
            "Shengfeng He"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset",
            "Diffusion"
        ],
        "tldr": "StableGuard introduces a novel framework for copyright protection and tampering localization in AI-generated content using diffusion models, outperforming state-of-the-art methods in various aspects.",
        "tldr_zh": "StableGuard 提出了一种新颖的框架，用于在 AI 生成的内容中使用扩散模型进行版权保护和篡改定位，在各个方面均优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Can multimodal representation learning by alignment preserve modality-specific information?",
        "summary": "Combining multimodal data is a key issue in a wide range of machine learning\ntasks, including many remote sensing problems. In Earth observation, early\nmultimodal data fusion methods were based on specific neural network\narchitectures and supervised learning. Ever since, the scarcity of labeled data\nhas motivated self-supervised learning techniques. State-of-the-art multimodal\nrepresentation learning techniques leverage the spatial alignment between\nsatellite data from different modalities acquired over the same geographic area\nin order to foster a semantic alignment in the latent space. In this paper, we\ninvestigate how this methods can preserve task-relevant information that is not\nshared across modalities. First, we show, under simplifying assumptions, when\nalignment strategies fundamentally lead to an information loss. Then, we\nsupport our theoretical insight through numerical experiments in more realistic\nsettings. With those theoretical and empirical evidences, we hope to support\nnew developments in contrastive learning for the combination of multimodal\nsatellite data. Our code and data is publicly available at\nhttps://github.com/Romain3Ch216/alg_maclean_25.",
        "url": "http://arxiv.org/abs/2509.17943v1",
        "published_date": "2025-09-22T16:06:10+00:00",
        "updated_date": "2025-09-22T16:06:10+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Romain Thoreau",
            "Jessie Levillain",
            "Dawa Derksen"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper investigates how multimodal representation learning methods can preserve task-relevant information that is not shared across modalities, supporting new developments in contrastive learning for combining multimodal satellite data.",
        "tldr_zh": "本文研究了多模态表示学习方法如何保留跨模态不共享的任务相关信息，为结合多模态卫星数据的对比学习新发展提供支持。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ProDyG: Progressive Dynamic Scene Reconstruction via Gaussian Splatting from Monocular Videos",
        "summary": "Achieving truly practical dynamic 3D reconstruction requires online\noperation, global pose and map consistency, detailed appearance modeling, and\nthe flexibility to handle both RGB and RGB-D inputs. However, existing SLAM\nmethods typically merely remove the dynamic parts or require RGB-D input, while\noffline methods are not scalable to long video sequences, and current\ntransformer-based feedforward methods lack global consistency and appearance\ndetails. To this end, we achieve online dynamic scene reconstruction by\ndisentangling the static and dynamic parts within a SLAM system. The poses are\ntracked robustly with a novel motion masking strategy, and dynamic parts are\nreconstructed leveraging a progressive adaptation of a Motion Scaffolds graph.\nOur method yields novel view renderings competitive to offline methods and\nachieves on-par tracking with state-of-the-art dynamic SLAM methods.",
        "url": "http://arxiv.org/abs/2509.17864v1",
        "published_date": "2025-09-22T14:58:11+00:00",
        "updated_date": "2025-09-22T14:58:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shi Chen",
            "Erik Sandström",
            "Sandro Lombardi",
            "Siyuan Li",
            "Martin R. Oswald"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces ProDyG, a method for dynamic 3D reconstruction from monocular videos that can handle both RGB and RGB-D inputs.",
        "tldr_zh": "该论文介绍了ProDyG，一种从单目视频中进行动态三维重建的方法，能够处理RGB和RGB-D输入。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment",
        "summary": "Training-free video object editing aims to achieve precise object-level\nmanipulation, including object insertion, swapping, and deletion. However, it\nfaces significant challenges in maintaining fidelity and temporal consistency.\nExisting methods, often designed for U-Net architectures, suffer from two\nprimary limitations: inaccurate inversion due to first-order solvers, and\ncontextual conflicts caused by crude \"hard\" feature replacement. These issues\nare more challenging in Diffusion Transformers (DiTs), where the unsuitability\nof prior layer-selection heuristics makes effective guidance challenging. To\naddress these limitations, we introduce ContextFlow, a novel training-free\nframework for DiT-based video object editing. In detail, we first employ a\nhigh-order Rectified Flow solver to establish a robust editing foundation. The\ncore of our framework is Adaptive Context Enrichment (for specifying what to\nedit), a mechanism that addresses contextual conflicts. Instead of replacing\nfeatures, it enriches the self-attention context by concatenating Key-Value\npairs from parallel reconstruction and editing paths, empowering the model to\ndynamically fuse information. Additionally, to determine where to apply this\nenrichment (for specifying where to edit), we propose a systematic, data-driven\nanalysis to identify task-specific vital layers. Based on a novel Guidance\nResponsiveness Metric, our method pinpoints the most influential DiT blocks for\ndifferent tasks (e.g., insertion, swapping), enabling targeted and highly\neffective guidance. Extensive experiments show that ContextFlow significantly\noutperforms existing training-free methods and even surpasses several\nstate-of-the-art training-based approaches, delivering temporally coherent,\nhigh-fidelity results.",
        "url": "http://arxiv.org/abs/2509.17818v1",
        "published_date": "2025-09-22T14:13:31+00:00",
        "updated_date": "2025-09-22T14:13:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiyang Chen",
            "Xuanhua He",
            "Xiujun Ma",
            "Yue Ma"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Diffusion",
            "Dataset"
        ],
        "tldr": "ContextFlow is a novel training-free framework for video object editing using Diffusion Transformers, addressing accuracy and contextual conflict issues, and outperforming existing methods.",
        "tldr_zh": "ContextFlow是一种新颖的基于Diffusion Transformers的视频对象编辑框架，解决了准确度和上下文冲突问题，并优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing Semantic Segmentation with Continual Self-Supervised Pre-training",
        "summary": "Self-supervised learning (SSL) has emerged as a central paradigm for training\nfoundation models by leveraging large-scale unlabeled datasets, often producing\nrepresentations with strong generalization capabilities. These models are\ntypically pre-trained on general-purpose datasets such as ImageNet and\nsubsequently adapted to various downstream tasks through finetuning. While\nrecent advances have explored parameter-efficient strategies for adapting\npre-trained models, extending SSL pre-training itself to new domains -\nparticularly under limited data regimes and for dense prediction tasks -\nremains underexplored. In this work, we address the problem of adapting vision\nfoundation models to new domains in an unsupervised and data-efficient manner,\nspecifically targeting downstream semantic segmentation. We propose GLARE\n(Global Local and Regional Enforcement), a novel continual self-supervised\npre-training task designed to enhance downstream segmentation performance.\nGLARE introduces patch-level augmentations to encourage local consistency and\nincorporates a regional consistency constraint that leverages spatial semantics\nin the data. For efficient continual pre-training, we initialize Vision\nTransformers (ViTs) with weights from existing SSL models and update only\nlightweight adapter modules - specifically UniAdapter - while keeping the rest\nof the backbone frozen. Experiments across multiple semantic segmentation\nbenchmarks on different domains demonstrate that GLARE consistently improves\ndownstream performance with minimal computational and parameter overhead.",
        "url": "http://arxiv.org/abs/2509.17816v1",
        "published_date": "2025-09-22T14:11:02+00:00",
        "updated_date": "2025-09-22T14:11:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Brown Ebouky",
            "Ajad Chhatkuli",
            "Cristiano Malossi",
            "Christoph Studer",
            "Roy Assaf",
            "Andrea Bartezzaghi"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a new self-supervised pre-training task called GLARE to enhance semantic segmentation performance in vision models by focusing on local and regional consistency.",
        "tldr_zh": "该论文引入了一种名为GLARE的新的自监督预训练任务，通过专注于本地和区域一致性来增强视觉模型中的语义分割性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TS-P$^2$CL: Plug-and-Play Dual Contrastive Learning for Vision-Guided Medical Time Series Classification",
        "summary": "Medical time series (MedTS) classification is pivotal for intelligent\nhealthcare, yet its efficacy is severely limited by poor cross-subject\ngeneration due to the profound cross-individual heterogeneity. Despite advances\nin architectural innovations and transfer learning techniques, current methods\nremain constrained by modality-specific inductive biases that limit their\nability to learn universally invariant representations. To overcome this, we\npropose TS-P$^2$CL, a novel plug-and-play framework that leverages the\nuniversal pattern recognition capabilities of pre-trained vision models. We\nintroduce a vision-guided paradigm that transforms 1D physiological signals\ninto 2D pseudo-images, establishing a bridge to the visual domain. This\ntransformation enables implicit access to rich semantic priors learned from\nnatural images. Within this unified space, we employ a dual-contrastive\nlearning strategy: intra-modal consistency enforces temporal coherence, while\ncross-modal alignment aligns time-series dynamics with visual semantics,\nthereby mitigating individual-specific biases and learning robust,\ndomain-invariant features. Extensive experiments on six MedTS datasets\ndemonstrate that TS-P$^2$CL consistently outperforms fourteen methods in both\nsubject-dependent and subject-independent settings.",
        "url": "http://arxiv.org/abs/2509.17802v1",
        "published_date": "2025-09-22T13:57:58+00:00",
        "updated_date": "2025-09-22T13:57:58+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Qi'ao Xu",
            "Pengfei Wang",
            "Bo Zhong",
            "Tianwen Qian",
            "Xiaoling Wang",
            "Ye Wang",
            "Hong Yu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "TS-P$^2$CL is a novel framework for medical time series classification that leverages pre-trained vision models to improve classification performance.",
        "tldr_zh": "TS-P$^2$CL是一个新颖的框架，利用预训练的视觉模型来改善医学时间序列分类性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Degradation-Aware All-in-One Image Restoration via Latent Prior Encoding",
        "summary": "Real-world images often suffer from spatially diverse degradations such as\nhaze, rain, snow, and low-light, significantly impacting visual quality and\ndownstream vision tasks. Existing all-in-one restoration (AIR) approaches\neither depend on external text prompts or embed hand-crafted architectural\npriors (e.g., frequency heuristics); both impose discrete, brittle assumptions\nthat weaken generalization to unseen or mixed degradations. To address this\nlimitation, we propose to reframe AIR as learned latent prior inference, where\ndegradation-aware representations are automatically inferred from the input\nwithout explicit task cues. Based on latent priors, we formulate AIR as a\nstructured reasoning paradigm: (1) which features to route (adaptive feature\nselection), (2) where to restore (spatial localization), and (3) what to\nrestore (degradation semantics). We design a lightweight decoding module that\nefficiently leverages these latent encoded cues for spatially-adaptive\nrestoration. Extensive experiments across six common degradation tasks, five\ncompound settings, and previously unseen degradations demonstrate that our\nmethod outperforms state-of-the-art (SOTA) approaches, achieving an average\nPSNR improvement of 1.68 dB while being three times more efficient.",
        "url": "http://arxiv.org/abs/2509.17792v1",
        "published_date": "2025-09-22T13:51:09+00:00",
        "updated_date": "2025-09-22T13:51:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "S M A Sharif",
            "Abdur Rehman",
            "Fayaz Ali Dharejo",
            "Radu Timofte",
            "Rizwan Ali Naqvi"
        ],
        "ai_categories": [
            "AIGC"
        ],
        "tldr": "The paper proposes a novel approach for image restoration using learned latent priors to address diverse degradations, outperforming existing methods in efficiency and quality improvement.",
        "tldr_zh": "本文提出一种新颖的图像恢复方法，利用学习的潜在先验来处理多样性退化问题，在效率和质量改进方面优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Accurate and Efficient Low-Rank Model Merging in Core Space",
        "summary": "In this paper, we address the challenges associated with merging low-rank\nadaptations of large neural networks. With the rise of parameter-efficient\nadaptation techniques, such as Low-Rank Adaptation (LoRA), model fine-tuning\nhas become more accessible. While fine-tuning models with LoRA is highly\nefficient, existing merging methods often sacrifice this efficiency by merging\nfully-sized weight matrices. We propose the Core Space merging framework, which\nenables the merging of LoRA-adapted models within a common alignment basis,\nthereby preserving the efficiency of low-rank adaptation while substantially\nimproving accuracy across tasks. We further provide a formal proof that\nprojection into Core Space ensures no loss of information and provide a\ncomplexity analysis showing the efficiency gains. Extensive empirical results\ndemonstrate that Core Space significantly improves existing merging techniques\nand achieves state-of-the-art results on both vision and language tasks while\nutilizing a fraction of the computational resources. Codebase is available at\nhttps://github.com/apanariello4/core-space-merging.",
        "url": "http://arxiv.org/abs/2509.17786v1",
        "published_date": "2025-09-22T13:48:15+00:00",
        "updated_date": "2025-09-22T13:48:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Aniello Panariello",
            "Daniel Marczak",
            "Simone Magistri",
            "Angelo Porrello",
            "Bartłomiej Twardowski",
            "Andrew D. Bagdanov",
            "Simone Calderara",
            "Joost van de Weijer"
        ],
        "ai_categories": [
            "LoRA",
            "Transformer"
        ],
        "tldr": "The paper proposes a Core Space merging framework for efficiently merging low-rank adapted neural networks, improving accuracy across tasks while using fewer computational resources.",
        "tldr_zh": "本文提出了一种核心空间合并框架，用于有效合并低秩调整的神经网络，在使用更少计算资源的同时提高任务准确性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Incorporating the Refractory Period into Spiking Neural Networks through Spike-Triggered Threshold Dynamics",
        "summary": "As the third generation of neural networks, spiking neural networks (SNNs)\nhave recently gained widespread attention for their biological plausibility,\nenergy efficiency, and effectiveness in processing neuromorphic datasets. To\nbetter emulate biological neurons, various models such as Integrate-and-Fire\n(IF) and Leaky Integrate-and-Fire (LIF) have been widely adopted in SNNs.\nHowever, these neuron models overlook the refractory period, a fundamental\ncharacteristic of biological neurons. Research on excitable neurons reveal that\nafter firing, neurons enter a refractory period during which they are\ntemporarily unresponsive to subsequent stimuli. This mechanism is critical for\npreventing over-excitation and mitigating interference from aberrant signals.\nTherefore, we propose a simple yet effective method to incorporate the\nrefractory period into spiking LIF neurons through spike-triggered threshold\ndynamics, termed RPLIF. Our method ensures that each spike accurately encodes\nneural information, effectively preventing neuron over-excitation under\ncontinuous inputs and interference from anomalous inputs. Incorporating the\nrefractory period into LIF neurons is seamless and computationally efficient,\nenhancing robustness and efficiency while yielding better performance with\nnegligible overhead. To the best of our knowledge, RPLIF achieves\nstate-of-the-art performance on Cifar10-DVS(82.40%) and N-Caltech101(83.35%)\nwith fewer timesteps and demonstrates superior performance on DVS128\nGesture(97.22%) at low latency.",
        "url": "http://arxiv.org/abs/2509.17769v1",
        "published_date": "2025-09-22T13:33:31+00:00",
        "updated_date": "2025-09-22T13:33:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yang Li",
            "Xinyi Zeng",
            "Zhe Xue",
            "Pinxian Zeng",
            "Zikai Zhang",
            "Yan Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "This paper proposes a method to incorporate the refractory period into spiking neural networks for better biological neuron emulation and performance improvement.",
        "tldr_zh": "本文提出了一种方法，通过脉冲触发阈值动态将不应期纳入脉冲神经网络中，以更好地模拟生物神经元和提高性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Neural-MMGS: Multi-modal Neural Gaussian Splats for Large-Scale Scene Reconstruction",
        "summary": "This paper proposes Neural-MMGS, a novel neural 3DGS framework for multimodal\nlarge-scale scene reconstruction that fuses multiple sensing modalities in a\nper-gaussian compact, learnable embedding. While recent works focusing on\nlarge-scale scene reconstruction have incorporated LiDAR data to provide more\naccurate geometric constraints, we argue that LiDAR's rich physical properties\nremain underexplored. Similarly, semantic information has been used for object\nretrieval, but could provide valuable high-level context for scene\nreconstruction. Traditional approaches append these properties to Gaussians as\nseparate parameters, increasing memory usage and limiting information exchange\nacross modalities. Instead, our approach fuses all modalities -- image, LiDAR,\nand semantics -- into a compact, learnable embedding that implicitly encodes\noptical, physical, and semantic features in each Gaussian. We then train\nlightweight neural decoders to map these embeddings to Gaussian parameters,\nenabling the reconstruction of each sensing modality with lower memory overhead\nand improved scalability. We evaluate Neural-MMGS on the Oxford Spires and\nKITTI-360 datasets. On Oxford Spires, we achieve higher-quality\nreconstructions, while on KITTI-360, our method reaches competitive results\nwith less storage consumption compared with current approaches in LiDAR-based\nnovel-view synthesis.",
        "url": "http://arxiv.org/abs/2509.17762v1",
        "published_date": "2025-09-22T13:24:58+00:00",
        "updated_date": "2025-09-22T13:24:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sitian Shen",
            "Georgi Pramatarov",
            "Yifu Tao",
            "Daniele De Martini"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces Neural-MMGS, a framework for large-scale scene reconstruction that fuses image, LiDAR, and semantic information into a compact, learnable embedding for more efficient reconstruction.",
        "tldr_zh": "本文介绍了Neural-MMGS，一个用于大规模场景重建的框架，将图像、LiDAR和语义信息融合到一个紧凑的、可学习的嵌入中，以实现更高效的重建。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dual-View Alignment Learning with Hierarchical-Prompt for Class-Imbalance Multi-Label Classification",
        "summary": "Real-world datasets often exhibit class imbalance across multiple categories,\nmanifesting as long-tailed distributions and few-shot scenarios. This is\nespecially challenging in Class-Imbalanced Multi-Label Image Classification\n(CI-MLIC) tasks, where data imbalance and multi-object recognition present\nsignificant obstacles. To address these challenges, we propose a novel method\ntermed Dual-View Alignment Learning with Hierarchical Prompt (HP-DVAL), which\nleverages multi-modal knowledge from vision-language pretrained (VLP) models to\nmitigate the class-imbalance problem in multi-label settings. Specifically,\nHP-DVAL employs dual-view alignment learning to transfer the powerful feature\nrepresentation capabilities from VLP models by extracting complementary\nfeatures for accurate image-text alignment. To better adapt VLP models for\nCI-MLIC tasks, we introduce a hierarchical prompt-tuning strategy that utilizes\nglobal and local prompts to learn task-specific and context-related prior\nknowledge. Additionally, we design a semantic consistency loss during prompt\ntuning to prevent learned prompts from deviating from general knowledge\nembedded in VLP models. The effectiveness of our approach is validated on two\nCI-MLIC benchmarks: MS-COCO and VOC2007. Extensive experimental results\ndemonstrate the superiority of our method over SOTA approaches, achieving mAP\nimprovements of 10.0\\% and 5.2\\% on the long-tailed multi-label image\nclassification task, and 6.8\\% and 2.9\\% on the multi-label few-shot image\nclassification task.",
        "url": "http://arxiv.org/abs/2509.17747v1",
        "published_date": "2025-09-22T13:11:12+00:00",
        "updated_date": "2025-09-22T13:11:12+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sheng Huang",
            "Jiexuan Yan",
            "Beiyan Liu",
            "Bo Liu",
            "Richang Hong"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a method called HP-DVAL for tackling class imbalance in multi-label image classification using dual-view alignment learning and hierarchical prompts with promising results on benchmark datasets.",
        "tldr_zh": "本文介绍了一种名为HP-DVAL的方法，通过双视图对齐学习和分层提示来解决多标签图像分类中的类别不平衡问题，并在基准数据集上取得了令人期待的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Adaptive Fast-and-Slow Visual Program Reasoning for Long-Form VideoQA",
        "summary": "Large language models (LLMs) have shown promise in generating program\nworkflows for visual tasks. However, previous approaches often rely on\nclosed-source models, lack systematic reasoning, and struggle with long-form\nvideo question answering (videoQA). To address these challenges, we introduce\nthe FS-VisPR framework, an adaptive visual program reasoning approach that\nbalances fast reasoning for simple queries with slow reasoning for difficult\nones. First, we design efficient visual modules (e.g., key clip retrieval and\nsubtitle retrieval) to support long-form video tasks. Then, we construct a\ndiverse and high-quality fast-slow reasoning dataset with a strong LLM to align\nopen-source language models' ability to generate visual program workflows as\nFS-LLM. Next, we design a fast-slow reasoning framework with FS-LLM: Simple\nqueries are directly solved by VideoLLMs, while difficult ones invoke visual\nprogram reasoning, motivated by human-like reasoning processes. During this\nprocess, low-confidence fast-thinking answers will trigger a second-stage\nslow-reasoning process, and a fallback mechanism to fast reasoning is activated\nif the program execution fails. Moreover, we improve visual programs through\nparameter search during both training and inference. By adjusting the\nparameters of the visual modules within the program, multiple variants are\ngenerated: during training, programs that yield correct answers are selected,\nwhile during inference, the program with the highest confidence result is\napplied. Experiments show that FS-VisPR improves both efficiency and\nreliability in visual program workflows. It achieves 50.4% accuracy on LVBench,\nsurpassing GPT-4o, matching the performance of Qwen2.5VL-72B on VideoMME.",
        "url": "http://arxiv.org/abs/2509.17743v1",
        "published_date": "2025-09-22T13:06:17+00:00",
        "updated_date": "2025-09-22T13:06:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chenglin Li",
            "Feng Han",
            "FengTao",
            "Ruilin Li",
            "Qianglong Chen",
            "Jingqi Tong",
            "Yin Zhang",
            "Jiaqi Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces FS-VisPR, an adaptive visual program reasoning approach for videoQA that balances fast and slow reasoning to improve efficiency and reliability in visual program workflows.",
        "tldr_zh": "本文介绍了FS-VisPR，一种自适应的视觉程序推理方法，用于视频问答，在视觉程序工作流程中平衡快速和缓慢的推理，以提高效率和可靠性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Automated Labeling of Intracranial Arteries with Uncertainty Quantification Using Deep Learning",
        "summary": "Accurate anatomical labeling of intracranial arteries is essential for\ncerebrovascular diagnosis and hemodynamic analysis but remains time-consuming\nand subject to interoperator variability. We present a deep learning-based\nframework for automated artery labeling from 3D Time-of-Flight Magnetic\nResonance Angiography (3D ToF-MRA) segmentations (n=35), incorporating\nuncertainty quantification to enhance interpretability and reliability. We\nevaluated three convolutional neural network architectures: (1) a UNet with\nresidual encoder blocks, reflecting commonly used baselines in vascular\nlabeling; (2) CS-Net, an attention-augmented UNet incorporating channel and\nspatial attention mechanisms for enhanced curvilinear structure recognition;\nand (3) nnUNet, a self-configuring framework that automates preprocessing,\ntraining, and architectural adaptation based on dataset characteristics. Among\nthese, nnUNet achieved the highest labeling performance (average Dice score:\n0.922; average surface distance: 0.387 mm), with improved robustness in\nanatomically complex vessels. To assess predictive confidence, we implemented\ntest-time augmentation (TTA) and introduced a novel coordinate-guided strategy\nto reduce interpolation errors during augmented inference. The resulting\nuncertainty maps reliably indicated regions of anatomical ambiguity,\npathological variation, or manual labeling inconsistency. We further validated\nclinical utility by comparing flow velocities derived from automated and manual\nlabels in co-registered 4D Flow MRI datasets, observing close agreement with no\nstatistically significant differences. Our framework offers a scalable,\naccurate, and uncertainty-aware solution for automated cerebrovascular\nlabeling, supporting downstream hemodynamic analysis and facilitating clinical\nintegration.",
        "url": "http://arxiv.org/abs/2509.17726v1",
        "published_date": "2025-09-22T12:57:21+00:00",
        "updated_date": "2025-09-22T12:57:21+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "I.4.0"
        ],
        "authors": [
            "Javier Bisbal",
            "Patrick Winter",
            "Sebastian Jofre",
            "Aaron Ponce",
            "Sameer A. Ansari",
            "Ramez Abdalla",
            "Michael Markl",
            "Oliver Welin Odeback",
            "Sergio Uribe",
            "Cristian Tejos",
            "Julio Sotelo",
            "Susanne Schnell",
            "David Marlevi"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper presents a deep learning framework for automated labeling of intracranial arteries with uncertainty quantification, showing promising results for clinical applications.",
        "tldr_zh": "该论文提出了一个基于深度学习的框架，用于自动标记颅内动脉并进行不确定性量化，展示了在临床应用中的良好结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RCTDistill: Cross-Modal Knowledge Distillation Framework for Radar-Camera 3D Object Detection with Temporal Fusion",
        "summary": "Radar-camera fusion methods have emerged as a cost-effective approach for 3D\nobject detection but still lag behind LiDAR-based methods in performance.\nRecent works have focused on employing temporal fusion and Knowledge\nDistillation (KD) strategies to overcome these limitations. However, existing\napproaches have not sufficiently accounted for uncertainties arising from\nobject motion or sensor-specific errors inherent in radar and camera\nmodalities. In this work, we propose RCTDistill, a novel cross-modal KD method\nbased on temporal fusion, comprising three key modules: Range-Azimuth Knowledge\nDistillation (RAKD), Temporal Knowledge Distillation (TKD), and\nRegion-Decoupled Knowledge Distillation (RDKD). RAKD is designed to consider\nthe inherent errors in the range and azimuth directions, enabling effective\nknowledge transfer from LiDAR features to refine inaccurate BEV\nrepresentations. TKD mitigates temporal misalignment caused by dynamic objects\nby aligning historical radar-camera BEV features with current LiDAR\nrepresentations. RDKD enhances feature discrimination by distilling relational\nknowledge from the teacher model, allowing the student to differentiate\nforeground and background features. RCTDistill achieves state-of-the-art\nradar-camera fusion performance on both the nuScenes and View-of-Delft (VoD)\ndatasets, with the fastest inference speed of 26.2 FPS.",
        "url": "http://arxiv.org/abs/2509.17712v1",
        "published_date": "2025-09-22T12:49:49+00:00",
        "updated_date": "2025-09-22T12:49:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Geonho Bang",
            "Minjae Seong",
            "Jisong Kim",
            "Geunju Baek",
            "Daye Oh",
            "Junhyung Kim",
            "Junho Koh",
            "Jun Won Choi"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "RCTDistill introduces a cross-modal knowledge distillation framework for radar-camera 3D object detection with temporal fusion, achieving state-of-the-art performance.",
        "tldr_zh": "RCTDistill引入了一种用于雷达-相机3D物体检测的跨模态知识蒸馏框架，通过时间融合实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Neurodynamics-Driven Coupled Neural P Systems for Multi-Focus Image Fusion",
        "summary": "Multi-focus image fusion (MFIF) is a crucial technique in image processing,\nwith a key challenge being the generation of decision maps with precise\nboundaries. However, traditional methods based on heuristic rules and deep\nlearning methods with black-box mechanisms are difficult to generate\nhigh-quality decision maps. To overcome this challenge, we introduce\nneurodynamics-driven coupled neural P (CNP) systems, which are third-generation\nneural computation models inspired by spiking mechanisms, to enhance the\naccuracy of decision maps. Specifically, we first conduct an in-depth analysis\nof the model's neurodynamics to identify the constraints between the network\nparameters and the input signals. This solid analysis avoids abnormal\ncontinuous firing of neurons and ensures the model accurately distinguishes\nbetween focused and unfocused regions, generating high-quality decision maps\nfor MFIF. Based on this analysis, we propose a\n\\textbf{N}eurodynamics-\\textbf{D}riven \\textbf{CNP} \\textbf{F}usion model\n(\\textbf{ND-CNPFuse}) tailored for the challenging MFIF task. Unlike current\nideas of decision map generation, ND-CNPFuse distinguishes between focused and\nunfocused regions by mapping the source image into interpretable spike\nmatrices. By comparing the number of spikes, an accurate decision map can be\ngenerated directly without any post-processing. Extensive experimental results\nshow that ND-CNPFuse achieves new state-of-the-art performance on four\nclassical MFIF datasets, including Lytro, MFFW, MFI-WHU, and Real-MFF. The code\nis available at https://github.com/MorvanLi/ND-CNPFuse.",
        "url": "http://arxiv.org/abs/2509.17704v1",
        "published_date": "2025-09-22T12:43:19+00:00",
        "updated_date": "2025-09-22T12:43:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bo Li",
            "Yunkuo Lei",
            "Tingting Bao",
            "Yaxian Wang",
            "Lingling Zhang",
            "Jun Liu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a neurodynamics-driven coupled neural system for multi-focus image fusion, achieving state-of-the-art performance on various datasets.",
        "tldr_zh": "本文介绍了一种神经动力学驱动的耦合神经系统，用于多焦点图像融合，在各种数据集上取得了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Predicting Depth Maps from Single RGB Images and Addressing Missing Information in Depth Estimation",
        "summary": "Depth imaging is a crucial area in Autonomous Driving Systems (ADS), as it\nplays a key role in detecting and measuring objects in the vehicle's\nsurroundings. However, a significant challenge in this domain arises from\nmissing information in Depth images, where certain points are not measurable\ndue to gaps or inconsistencies in pixel data. Our research addresses two key\ntasks to overcome this challenge. First, we developed an algorithm using a\nmulti-layered training approach to generate Depth images from a single RGB\nimage. Second, we addressed the issue of missing information in Depth images by\napplying our algorithm to rectify these gaps, resulting in Depth images with\ncomplete and accurate data. We further tested our algorithm on the Cityscapes\ndataset and successfully resolved the missing information in its Depth images,\ndemonstrating the effectiveness of our approach in real-world urban\nenvironments.",
        "url": "http://arxiv.org/abs/2509.17686v1",
        "published_date": "2025-09-22T12:28:29+00:00",
        "updated_date": "2025-09-22T12:28:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mohamad Mofeed Chaar",
            "Jamal Raiyn",
            "Galia Weidl"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper presents an algorithm for generating depth maps from single RGB images and filling in missing information in depth estimation for Autonomous Driving Systems.",
        "tldr_zh": "本文提出了一种算法，用于从单个RGB图像生成深度图，并在自动驾驶系统的深度估计中填补缺失信息。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DINOv3-Diffusion Policy: Self-Supervised Large Visual Model for Visuomotor Diffusion Policy Learning",
        "summary": "This paper evaluates DINOv3, a recent large-scale self-supervised vision\nbackbone, for visuomotor diffusion policy learning in robotic manipulation. We\ninvestigate whether a purely self-supervised encoder can match or surpass\nconventional supervised ImageNet-pretrained backbones (e.g., ResNet-18) under\nthree regimes: training from scratch, frozen, and finetuned. Across four\nbenchmark tasks (Push-T, Lift, Can, Square) using a unified FiLM-conditioned\ndiffusion policy, we find that (i) finetuned DINOv3 matches or exceeds\nResNet-18 on several tasks, (ii) frozen DINOv3 remains competitive, indicating\nstrong transferable priors, and (iii) self-supervised features improve sample\nefficiency and robustness. These results support self-supervised large visual\nmodels as effective, generalizable perceptual front-ends for action diffusion\npolicies, motivating further exploration of scalable label-free pretraining in\nrobotic manipulation. Compared to using ResNet18 as a backbone, our approach\nwith DINOv3 achieves up to a 10% absolute increase in test-time success rates\non challenging tasks such as Can, and on-the-par performance in tasks like\nLift, PushT, and Square.",
        "url": "http://arxiv.org/abs/2509.17684v1",
        "published_date": "2025-09-22T12:27:26+00:00",
        "updated_date": "2025-09-22T12:27:26+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "ThankGod Egbe",
            "Peng Wang",
            "Zhihao Guo",
            "Zidong Chen"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper evaluates the effectiveness of self-supervised DINOv3 model for visuomotor diffusion policy learning in robotic manipulation, showing that it outperforms traditional supervised models in certain tasks.",
        "tldr_zh": "本文评估了自监督的DINOv3模型在机器人操作中的视觉动作扩散策略学习中的有效性，表明在某些任务中优于传统监督模型。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Clothing agnostic Pre-inpainting Virtual Try-ON",
        "summary": "With the development of deep learning technology, virtual try-on technology\nhas become an important application value in the fields of e-commerce, fashion,\nand entertainment. The recently proposed Leffa has improved the texture\ndistortion problem of diffu-sion-based models, but there are limitations in\nthat the bottom detection inaccuracy and the existing clothing silhouette\nremain in the synthesis results. To solve this problem, this study proposes\nCaP-VTON (Clothing agnostic Pre-inpainting Virtual Try-ON). CaP-VTON has\nimproved the naturalness and consistency of whole-body clothing syn-thesis by\nintegrating multi-category masking based on Dress Code and skin inpainting\nbased on Stable Diffusion. In particular, a generate skin module was introduced\nto solve the skin restoration problem that occurs when long-sleeved images are\nconverted into short-sleeved or sleeveless ones, and high-quality restoration\nwas implemented consider-ing the human body posture and color. As a result,\nCaP-VTON recorded 92.5\\%, which is 15.4\\% better than Leffa in short-sleeved\nsynthesis accuracy, and showed the performance of consistently reproducing the\nstyle and shape of reference clothing in visual evaluation. These structures\nmaintain model-agnostic properties and are applicable to various\ndiffu-sion-based virtual inspection systems, and can contribute to applications\nthat require high-precision virtual wearing, such as e-commerce, custom\nstyling, and avatar creation.",
        "url": "http://arxiv.org/abs/2509.17654v1",
        "published_date": "2025-09-22T11:58:20+00:00",
        "updated_date": "2025-09-22T11:58:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sehyun Kim",
            "Hye Jun Lee",
            "Jiwoo Lee",
            "Taemin Lee"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces CaP-VTON, a virtual try-on technology that improves clothing synthesis naturalness and consistency by integrating multi-category masking and skin inpainting techniques. It outperforms previous models in accuracy and style reproduction.",
        "tldr_zh": "本文介绍了CaP-VTON，一种虚拟试穿技术，通过整合多类别遮罩和皮肤修复技术，提高了服装合成的自然性和一致性。在准确性和风格再现方面优于先前模型。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SISMA: Semantic Face Image Synthesis with Mamba",
        "summary": "Diffusion Models have become very popular for Semantic Image Synthesis (SIS)\nof human faces. Nevertheless, their training and inference is computationally\nexpensive and their computational requirements are high due to the quadratic\ncomplexity of attention layers. In this paper, we propose a novel architecture\ncalled SISMA, based on the recently proposed Mamba. SISMA generates high\nquality samples by controlling their shape using a semantic mask at a reduced\ncomputational demand. We validated our approach through comprehensive\nexperiments with CelebAMask-HQ, revealing that our architecture not only\nachieves a better FID score yet also operates at three times the speed of\nstate-of-the-art architectures. This indicates that the proposed design is a\nviable, lightweight substitute to transformer-based models.",
        "url": "http://arxiv.org/abs/2509.17651v1",
        "published_date": "2025-09-22T11:55:04+00:00",
        "updated_date": "2025-09-22T11:55:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Filippo Botti",
            "Alex Ergasti",
            "Tomaso Fontanini",
            "Claudio Ferrari",
            "Massimo Bertozzi",
            "Andrea Prati"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a new architecture, SISMA, for Semantic Face Image Synthesis that achieves better results and is faster than existing models.",
        "tldr_zh": "本文提出了一种新的架构SISMA，用于语义人脸图像合成，实现了比现有模型更好的结果，并且速度更快。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming Visual Geometry Transformers",
        "summary": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical.",
        "url": "http://arxiv.org/abs/2509.17650v1",
        "published_date": "2025-09-22T11:54:58+00:00",
        "updated_date": "2025-09-22T11:54:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Soroush Mahdi",
            "Fardin Ayar",
            "Ehsan Javanmardi",
            "Manabu Tsukada",
            "Mahdi Javanmardi"
        ],
        "ai_categories": [
            "Transformer",
            "Memory-Bounded",
            "Streaming",
            "Token Eviction"
        ],
        "tldr": "The paper proposes a training-free token eviction policy for memory-bounded streaming visual transformers, reducing memory usage while maintaining accuracy, enabling more practical long-horizon streaming inference.",
        "tldr_zh": "该论文提出了一种针对内存受限的流媒体视觉变换器的无需训练的令牌驱逐策略，降低内存使用量同时保持准确性，使得更实用的长时间跨度流媒体推断成为可能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VideoArtGS: Building Digital Twins of Articulated Objects from Monocular Video",
        "summary": "Building digital twins of articulated objects from monocular video presents\nan essential challenge in computer vision, which requires simultaneous\nreconstruction of object geometry, part segmentation, and articulation\nparameters from limited viewpoint inputs. Monocular video offers an attractive\ninput format due to its simplicity and scalability; however, it's challenging\nto disentangle the object geometry and part dynamics with visual supervision\nalone, as the joint movement of the camera and parts leads to ill-posed\nestimation. While motion priors from pre-trained tracking models can alleviate\nthe issue, how to effectively integrate them for articulation learning remains\nlargely unexplored. To address this problem, we introduce VideoArtGS, a novel\napproach that reconstructs high-fidelity digital twins of articulated objects\nfrom monocular video. We propose a motion prior guidance pipeline that analyzes\n3D tracks, filters noise, and provides reliable initialization of articulation\nparameters. We also design a hybrid center-grid part assignment module for\narticulation-based deformation fields that captures accurate part motion.\nVideoArtGS demonstrates state-of-the-art performance in articulation and mesh\nreconstruction, reducing the reconstruction error by about two orders of\nmagnitude compared to existing methods. VideoArtGS enables practical digital\ntwin creation from monocular video, establishing a new benchmark for\nvideo-based articulated object reconstruction. Our work is made publicly\navailable at: https://videoartgs.github.io.",
        "url": "http://arxiv.org/abs/2509.17647v1",
        "published_date": "2025-09-22T11:52:02+00:00",
        "updated_date": "2025-09-22T11:52:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Yu Liu",
            "Baoxiong Jia",
            "Ruijie Lu",
            "Chuyue Gan",
            "Huayu Chen",
            "Junfeng Ni",
            "Song-Chun Zhu",
            "Siyuan Huang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces VideoArtGS, a method for building high-fidelity digital twins of articulated objects from monocular video, achieving state-of-the-art performance in articulation and mesh reconstruction.",
        "tldr_zh": "该论文介绍了VideoArtGS，一种从单目视频中构建高保真度关节物体数字孪生体的方法，实现了在关节和网格重建方面的最新性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Overview of PlantCLEF 2022: Image-based plant identification at global scale",
        "summary": "It is estimated that there are more than 300,000 species of vascular plants\nin the world. Increasing our knowledge of these species is of paramount\nimportance for the development of human civilization (agriculture,\nconstruction, pharmacopoeia, etc.), especially in the context of the\nbiodiversity crisis. However, the burden of systematic plant identification by\nhuman experts strongly penalizes the aggregation of new data and knowledge.\nSince then, automatic identification has made considerable progress in recent\nyears as highlighted during all previous editions of PlantCLEF. Deep learning\ntechniques now seem mature enough to address the ultimate but realistic problem\nof global identification of plant biodiversity in spite of many problems that\nthe data may present (a huge number of classes, very strongly unbalanced\nclasses, partially erroneous identifications, duplications, variable visual\nquality, diversity of visual contents such as photos or herbarium sheets, etc).\nThe PlantCLEF2022 challenge edition proposes to take a step in this direction\nby tackling a multi-image (and metadata) classification problem with a very\nlarge number of classes (80k plant species). This paper presents the resources\nand evaluations of the challenge, summarizes the approaches and systems\nemployed by the participating research groups, and provides an analysis of key\nfindings.",
        "url": "http://arxiv.org/abs/2509.17632v1",
        "published_date": "2025-09-22T11:40:21+00:00",
        "updated_date": "2025-09-22T11:40:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Herve Goeau",
            "Pierre Bonnet",
            "Alexis Joly"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "PlantCLEF 2022 focuses on image-based plant identification at a global scale using deep learning techniques. The challenge addresses the problem of identifying a large number of plant species.",
        "tldr_zh": "PlantCLEF 2022集中在全球范围内基于图像的植物识别，使用深度学习技术。该挑战解决了大量植物物种识别的问题。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models",
        "summary": "Recent advances in video insertion based on diffusion models are impressive.\nHowever, existing methods rely on complex control signals but struggle with\nsubject consistency, limiting their practical applicability. In this paper, we\nfocus on the task of Mask-free Video Insertion and aim to resolve three key\nchallenges: data scarcity, subject-scene equilibrium, and insertion\nharmonization. To address the data scarcity, we propose a new data pipeline\nInsertPipe, constructing diverse cross-pair data automatically. Building upon\nour data pipeline, we develop OmniInsert, a novel unified framework for\nmask-free video insertion from both single and multiple subject references.\nSpecifically, to maintain subject-scene equilibrium, we introduce a simple yet\neffective Condition-Specific Feature Injection mechanism to distinctly inject\nmulti-source conditions and propose a novel Progressive Training strategy that\nenables the model to balance feature injection from subjects and source video.\nMeanwhile, we design the Subject-Focused Loss to improve the detailed\nappearance of the subjects. To further enhance insertion harmonization, we\npropose an Insertive Preference Optimization methodology to optimize the model\nby simulating human preferences, and incorporate a Context-Aware Rephraser\nmodule during reference to seamlessly integrate the subject into the original\nscenes. To address the lack of a benchmark for the field, we introduce\nInsertBench, a comprehensive benchmark comprising diverse scenes with\nmeticulously selected subjects. Evaluation on InsertBench indicates OmniInsert\noutperforms state-of-the-art closed-source commercial solutions. The code will\nbe released.",
        "url": "http://arxiv.org/abs/2509.17627v1",
        "published_date": "2025-09-22T11:35:55+00:00",
        "updated_date": "2025-09-22T11:35:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinshu Chen",
            "Xinghui Li",
            "Xu Bai",
            "Tianxiang Ma",
            "Pengze Zhang",
            "Zhuowei Chen",
            "Gen Li",
            "Lijie Liu",
            "Songtao Zhao",
            "Bingchuan Li",
            "Qian He"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces OmniInsert, a mask-free video insertion framework that addresses key challenges and outperforms existing solutions.",
        "tldr_zh": "本文介绍了OmniInsert，一种无需掩蔽的视频插入框架，解决了关键挑战并优于现有解决方案。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "COLA: Context-aware Language-driven Test-time Adaptation",
        "summary": "Test-time adaptation (TTA) has gained increasing popularity due to its\nefficacy in addressing ``distribution shift'' issue while simultaneously\nprotecting data privacy.\n  However, most prior methods assume that a paired source domain model and\ntarget domain sharing the same label space coexist, heavily limiting their\napplicability.\n  In this paper, we investigate a more general source model capable of\nadaptation to multiple target domains without needing shared labels.\n  This is achieved by using a pre-trained vision-language model (VLM), \\egno,\nCLIP, that can recognize images through matching with class descriptions.\n  While the zero-shot performance of VLMs is impressive, they struggle to\neffectively capture the distinctive attributes of a target domain.\n  To that end, we propose a novel method -- Context-aware Language-driven TTA\n(COLA).\n  The proposed method incorporates a lightweight context-aware module that\nconsists of three key components: a task-aware adapter, a context-aware unit,\nand a residual connection unit for exploring task-specific knowledge,\ndomain-specific knowledge from the VLM and prior knowledge of the VLM,\nrespectively.\n  It is worth noting that the context-aware module can be seamlessly integrated\ninto a frozen VLM, ensuring both minimal effort and parameter efficiency.\n  Additionally, we introduce a Class-Balanced Pseudo-labeling (CBPL) strategy\nto mitigate the adverse effects caused by class imbalance.\n  We demonstrate the effectiveness of our method not only in TTA scenarios but\nalso in class generalisation tasks.\n  The source code is available at https://github.com/NUDT-Bai-Group/COLA-TTA.",
        "url": "http://arxiv.org/abs/2509.17598v1",
        "published_date": "2025-09-22T11:19:17+00:00",
        "updated_date": "2025-09-22T11:19:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aiming Zhang",
            "Tianyuan Yu",
            "Liang Bai",
            "Jun Tang",
            "Yanming Guo",
            "Yirun Ruan",
            "Yun Zhou",
            "Zhihe Lu"
        ],
        "ai_categories": [
            "Transformer",
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces COLA, a method for context-aware language-driven test-time adaptation using a pre-trained vision-language model to adapt to multiple target domains without shared labels, demonstrating effectiveness in TTA scenarios and class generalisation tasks.",
        "tldr_zh": "本文介绍了COLA，一种利用预训练的视觉语言模型进行上下文感知的语言驱动测试时适应的方法，可以适应多个目标域而无需共享标签，在TTA场景和类别泛化任务中表现出有效性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Interpreting Attention Heads for Image-to-Text Information Flow in Large Vision-Language Models",
        "summary": "Large Vision-Language Models (LVLMs) answer visual questions by transferring\ninformation from images to text through a series of attention heads. While this\nimage-to-text information flow is central to visual question answering, its\nunderlying mechanism remains difficult to interpret due to the simultaneous\noperation of numerous attention heads. To address this challenge, we propose\nhead attribution, a technique inspired by component attribution methods, to\nidentify consistent patterns among attention heads that play a key role in\ninformation transfer. Using head attribution, we investigate how LVLMs rely on\nspecific attention heads to identify and answer questions about the main object\nin an image. Our analysis reveals that a distinct subset of attention heads\nfacilitates the image-to-text information flow. Remarkably, we find that the\nselection of these heads is governed by the semantic content of the input image\nrather than its visual appearance. We further examine the flow of information\nat the token level and discover that (1) text information first propagates to\nrole-related tokens and the final token before receiving image information, and\n(2) image information is embedded in both object-related and background tokens.\nOur work provides evidence that image-to-text information flow follows a\nstructured process, and that analysis at the attention-head level offers a\npromising direction toward understanding the mechanisms of LVLMs.",
        "url": "http://arxiv.org/abs/2509.17588v1",
        "published_date": "2025-09-22T11:12:12+00:00",
        "updated_date": "2025-09-22T11:12:12+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Jinyeong Kim",
            "Seil Kang",
            "Jiwoo Park",
            "Junhyeok Kim",
            "Seong Jae Hwang"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper explores how large vision-language models transfer information from images to text through attention heads, identifying key patterns and mechanisms in the process.",
        "tldr_zh": "本文探讨了大规模视觉-语言模型如何通过注意力头将信息从图像传输到文本，识别了这一过程中的关键模式和机制。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Visual Instruction Pretraining for Domain-Specific Foundation Models",
        "summary": "Modern computer vision is converging on a closed loop in which perception,\nreasoning and generation mutually reinforce each other. However, this loop\nremains incomplete: the top-down influence of high-level reasoning on the\nfoundational learning of low-level perceptual features is not yet\nunderexplored. This paper addresses this gap by proposing a new paradigm for\npretraining foundation models in downstream domains. We introduce Visual\ninsTruction Pretraining (ViTP), a novel approach that directly leverages\nreasoning to enhance perception. ViTP embeds a Vision Transformer (ViT)\nbackbone within a Vision-Language Model and pretrains it end-to-end using a\nrich corpus of visual instruction data curated from target downstream domains.\nViTP is powered by our proposed Visual Robustness Learning (VRL), which compels\nthe ViT to learn robust and domain-relevant features from a sparse set of\nvisual tokens. Extensive experiments on 16 challenging remote sensing and\nmedical imaging benchmarks demonstrate that ViTP establishes new\nstate-of-the-art performance across a diverse range of downstream tasks. The\ncode is available at github.com/zcablii/ViTP.",
        "url": "http://arxiv.org/abs/2509.17562v1",
        "published_date": "2025-09-22T10:57:42+00:00",
        "updated_date": "2025-09-22T10:57:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuxuan Li",
            "Yicheng Zhang",
            "Wenhao Tang",
            "Yimian Dai",
            "Ming-Ming Cheng",
            "Xiang Li",
            "Jian Yang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces Visual insTruction Pretraining (ViTP) that enhances perception by leveraging reasoning in pretrained models using a Vision Transformer backbone.",
        "tldr_zh": "本文介绍了Visual insTruction Pretraining (ViTP)，通过在预训练模型中使用Vision Transformer骨干结构来利用推理增强感知。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "An Empirical Study on the Robustness of YOLO Models for Underwater Object Detection",
        "summary": "Underwater object detection (UOD) remains a critical challenge in computer\nvision due to underwater distortions which degrade low-level features and\ncompromise the reliability of even state-of-the-art detectors. While YOLO\nmodels have become the backbone of real-time object detection, little work has\nsystematically examined their robustness under these uniquely challenging\nconditions. This raises a critical question: Are YOLO models genuinely robust\nwhen operating under the chaotic and unpredictable conditions of underwater\nenvironments? In this study, we present one of the first comprehensive\nevaluations of recent YOLO variants (YOLOv8-YOLOv12) across six simulated\nunderwater environments. Using a unified dataset of 10,000 annotated images\nfrom DUO and Roboflow100, we not only benchmark model robustness but also\nanalyze how distortions affect key low-level features such as texture, edges,\nand color. Our findings show that (1) YOLOv12 delivers the strongest overall\nperformance but is highly vulnerable to noise, and (2) noise disrupts edge and\ntexture features, explaining the poor detection performance in noisy images.\nClass imbalance is a persistent challenge in UOD. Experiments revealed that (3)\nimage counts and instance frequency primarily drive detection performance,\nwhile object appearance exerts only a secondary influence. Finally, we\nevaluated lightweight training-aware strategies: noise-aware sample injection,\nwhich improves robustness in both noisy and real-world conditions, and\nfine-tuning with advanced enhancement, which boosts accuracy in enhanced\ndomains but slightly lowers performance in original data, demonstrating strong\npotential for domain adaptation, respectively. Together, these insights provide\npractical guidance for building resilient and cost-efficient UOD systems.",
        "url": "http://arxiv.org/abs/2509.17561v1",
        "published_date": "2025-09-22T10:55:21+00:00",
        "updated_date": "2025-09-22T10:55:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Edwine Nabahirwa",
            "Wei Song",
            "Minghua Zhang",
            "Shufan Chen"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer",
            "AIGC",
            "Other"
        ],
        "tldr": "The paper evaluates the robustness of YOLO models for underwater object detection, finding YOLOv12 performs well but is vulnerable to noise. It also discusses strategies to improve model performance in challenging conditions.",
        "tldr_zh": "该论文评估了 YOLO 模型在水下目标检测中的鲁棒性，发现 YOLOv12 表现良好但容易受到噪音干扰。还讨论了改善模型在挑战性条件下性能的策略。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Is It Certainly a Deepfake? Reliability Analysis in Detection & Generation Ecosystem",
        "summary": "As generative models are advancing in quality and quantity for creating\nsynthetic content, deepfakes begin to cause online mistrust. Deepfake detectors\nare proposed to counter this effect, however, misuse of detectors claiming fake\ncontent as real or vice versa further fuels this misinformation problem. We\npresent the first comprehensive uncertainty analysis of deepfake detectors,\nsystematically investigating how generative artifacts influence prediction\nconfidence. As reflected in detectors' responses, deepfake generators also\ncontribute to this uncertainty as their generative residues vary, so we cross\nthe uncertainty analysis of deepfake detectors and generators. Based on our\nobservations, the uncertainty manifold holds enough consistent information to\nleverage uncertainty for deepfake source detection. Our approach leverages\nBayesian Neural Networks and Monte Carlo dropout to quantify both aleatoric and\nepistemic uncertainties across diverse detector architectures. We evaluate\nuncertainty on two datasets with nine generators, with four blind and two\nbiological detectors, compare different uncertainty methods, explore region-\nand pixel-based uncertainty, and conduct ablation studies. We conduct and\nanalyze binary real/fake, multi-class real/fake, source detection, and\nleave-one-out experiments between the generator/detector combinations to share\ntheir generalization capability, model calibration, uncertainty, and robustness\nagainst adversarial attacks. We further introduce uncertainty maps that\nlocalize prediction confidence at the pixel level, revealing distinct patterns\ncorrelated with generator-specific artifacts. Our analysis provides critical\ninsights for deploying reliable deepfake detection systems and establishes\nuncertainty quantification as a fundamental requirement for trustworthy\nsynthetic media detection.",
        "url": "http://arxiv.org/abs/2509.17550v1",
        "published_date": "2025-09-22T09:09:13+00:00",
        "updated_date": "2025-09-22T09:09:13+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Neslihan Kose",
            "Anthony Rhodes",
            "Umur Aybars Ciftci",
            "Ilke Demir"
        ],
        "ai_categories": [
            "GAN",
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper investigates uncertainty in deepfake detection using Bayesian Neural Networks and Monte Carlo dropout, providing critical insights for reliable detection.",
        "tldr_zh": "本文使用贝叶斯神经网络和蒙特卡洛辍学探讨了深度伪造检测中的不确定性，为可靠检测提供了关键见解。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SimToken: A Simple Baseline for Referring Audio-Visual Segmentation",
        "summary": "Referring Audio-Visual Segmentation (Ref-AVS) aims to segment specific\nobjects in videos based on natural language expressions involving audio,\nvision, and text information. This task poses significant challenges in\ncross-modal reasoning and fine-grained object localization. In this paper, we\npropose a simple framework, SimToken, that integrates a multimodal large\nlanguage model (MLLM) with the Segment Anything Model (SAM). The MLLM is guided\nto generate a special semantic token representing the referred object. This\ncompact token, enriched with contextual information from all modalities, acts\nas a prompt to guide SAM to segment objectsacross video frames. To further\nimprove semantic learning, we introduce a novel target-consistent semantic\nalignment loss that aligns token embeddings from different expressions but\nreferring to the same object. Experiments on the Ref-AVS benchmark demonstrate\nthat our approach achieves superior performance compared to existing\nmethods.Code will be available at https://github.com/DianJin-HFUT/SimToken",
        "url": "http://arxiv.org/abs/2509.17537v1",
        "published_date": "2025-09-22T08:55:04+00:00",
        "updated_date": "2025-09-22T08:55:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dian Jin",
            "Yanghao Zhou",
            "Jinxing Zhou",
            "Jiaqi Ma",
            "Ruohao Guo",
            "Dan Guo"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes SimToken, a framework for Referring Audio-Visual Segmentation using a multimodal language model and a segmentation model, achieving superior performance compared to existing methods.",
        "tldr_zh": "本文提出SimToken，一个利用多模态语言模型和分割模型进行指代音视频分割的框架，相比现有方法取得了更好的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "4DGCPro: Efficient Hierarchical 4D Gaussian Compression for Progressive Volumetric Video Streaming",
        "summary": "Achieving seamless viewing of high-fidelity volumetric video, comparable to\n2D video experiences, remains an open challenge. Existing volumetric video\ncompression methods either lack the flexibility to adjust quality and bitrate\nwithin a single model for efficient streaming across diverse networks and\ndevices, or struggle with real-time decoding and rendering on lightweight\nmobile platforms. To address these challenges, we introduce 4DGCPro, a novel\nhierarchical 4D Gaussian compression framework that facilitates real-time\nmobile decoding and high-quality rendering via progressive volumetric video\nstreaming in a single bitstream. Specifically, we propose a\nperceptually-weighted and compression-friendly hierarchical 4D Gaussian\nrepresentation with motion-aware adaptive grouping to reduce temporal\nredundancy, preserve coherence, and enable scalable multi-level detail\nstreaming. Furthermore, we present an end-to-end entropy-optimized training\nscheme, which incorporates layer-wise rate-distortion (RD) supervision and\nattribute-specific entropy modeling for efficient bitstream generation.\nExtensive experiments show that 4DGCPro enables flexible quality and multiple\nbitrate within a single model, achieving real-time decoding and rendering on\nmobile devices while outperforming existing methods in RD performance across\nmultiple datasets. Project Page: https://mediax-sjtu.github.io/4DGCPro",
        "url": "http://arxiv.org/abs/2509.17513v1",
        "published_date": "2025-09-22T08:38:17+00:00",
        "updated_date": "2025-09-22T08:38:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zihan Zheng",
            "Zhenlong Wu",
            "Houqiang Zhong",
            "Yuan Tian",
            "Ning Cao",
            "Lan Xu",
            "Jiangchao Yao",
            "Xiaoyun Zhang",
            "Qiang Hu",
            "Wenjun Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces 4DGCPro, a hierarchical 4D Gaussian compression framework for progressive volumetric video streaming, enabling real-time decoding and high-quality rendering on mobile devices.",
        "tldr_zh": "本文介绍了4DGCPro，一个用于逐步体积视频流的分层4D高斯压缩框架，可在移动设备上实现实时解码和高质量渲染。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SAMSON: 3rd Place Solution of LSVOS 2025 VOS Challenge",
        "summary": "Large-scale Video Object Segmentation (LSVOS) addresses the challenge of\naccurately tracking and segmenting objects in long video sequences, where\ndifficulties stem from object reappearance, small-scale targets, heavy\nocclusions, and crowded scenes. Existing approaches predominantly adopt\nSAM2-based frameworks with various memory mechanisms for complex video mask\ngeneration. In this report, we proposed Segment Anything with Memory\nStrengthened Object Navigation (SAMSON), the 3rd place solution in the MOSE\ntrack of ICCV 2025, which integrates the strengths of stateof-the-art VOS\nmodels into an effective paradigm. To handle visually similar instances and\nlong-term object disappearance in MOSE, we incorporate a long-term memorymodule\nfor reliable object re-identification. Additionly, we adopt SAM2Long as a\npost-processing strategy to reduce error accumulation and enhance segmentation\nstability in long video sequences. Our method achieved a final performance of\n0.8427 in terms of J &F in the test-set leaderboard.",
        "url": "http://arxiv.org/abs/2509.17500v1",
        "published_date": "2025-09-22T08:30:34+00:00",
        "updated_date": "2025-09-22T08:30:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yujie Xie",
            "Hongyang Zhang",
            "Zhihui Liu",
            "Shihai Ruan"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "SAMSON is the 3rd place solution in a Large-scale Video Object Segmentation challenge, incorporating memory mechanisms for complex video mask generation and achieving high performance.",
        "tldr_zh": "SAMSON是LSVOS挑战赛第三名解决方案，结合了复杂视频蒙版生成的记忆机制，并取得了较高的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multimodal Medical Image Classification via Synergistic Learning Pre-training",
        "summary": "Multimodal pathological images are usually in clinical diagnosis, but\ncomputer vision-based multimodal image-assisted diagnosis faces challenges with\nmodality fusion, especially in the absence of expert-annotated data. To achieve\nthe modality fusion in multimodal images with label scarcity, we propose a\nnovel ``pretraining + fine-tuning\" framework for multimodal semi-supervised\nmedical image classification. Specifically, we propose a synergistic learning\npretraining framework of consistency, reconstructive, and aligned learning. By\ntreating one modality as an augmented sample of another modality, we implement\na self-supervised learning pre-train, enhancing the baseline model's feature\nrepresentation capability. Then, we design a fine-tuning method for multimodal\nfusion. During the fine-tuning stage, we set different encoders to extract\nfeatures from the original modalities and provide a multimodal fusion encoder\nfor fusion modality. In addition, we propose a distribution shift method for\nmultimodal fusion features, which alleviates the prediction uncertainty and\noverfitting risks caused by the lack of labeled samples. We conduct extensive\nexperiments on the publicly available gastroscopy image datasets Kvasir and\nKvasirv2. Quantitative and qualitative results demonstrate that the proposed\nmethod outperforms the current state-of-the-art classification methods. The\ncode will be released at: https://github.com/LQH89757/MICS.",
        "url": "http://arxiv.org/abs/2509.17492v1",
        "published_date": "2025-09-22T08:21:19+00:00",
        "updated_date": "2025-09-22T08:21:19+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Qinghua Lin",
            "Guang-Hai Liu",
            "Zuoyong Li",
            "Yang Li",
            "Yuting Jiang",
            "Xiang Wu"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper presents a novel 'pretraining + fine-tuning' framework for multimodal semi-supervised medical image classification, outperforming state-of-the-art methods.",
        "tldr_zh": "本文提出了一种新颖的“预训练+微调”框架，用于多模态半监督医学图像分类，在性能上胜过了当前最先进的方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CSDformer: A Conversion Method for Fully Spike-Driven Transformer",
        "summary": "Spike-based transformer is a novel architecture aiming to enhance the\nperformance of spiking neural networks while mitigating the energy overhead\ninherent to transformers. However, methods for generating these models suffer\nfrom critical limitations: excessive training costs introduced by direct\ntraining methods, or unavoidably hardware-unfriendly operations in existing\nconversion methods. In this paper, we propose CSDformer, a novel conversion\nmethod for fully spike-driven transformers. We tailor a conversion-oriented\ntransformer-based architecture and propose a new function NReLU to replace\nsoftmax in self-attention. Subsequently, this model is quantized and trained,\nand converted into a fully spike-driven model with temporal decomposition\ntechnique. Also, we propose delayed Integrate-andFire neurons to reduce\nconversion errors and improve the performance of spiking models. We evaluate\nCSDformer on ImageNet, CIFAR-10 and CIFAR-100 datasets and achieve 76.36% top-1\naccuracy under 7 time-steps on ImageNet, demonstrating superiority over\nstate-of-the-art models. Furthermore, CSDformer eliminates the need for\ntraining SNNs, thereby reducing training costs (reducing computational resource\nby 75% and accelerating training speed by 2-3$\\times$). To the best of our\nknowledge, this is the first fully spike-driven transformer-based model\ndeveloped via conversion method, achieving high performance under ultra-low\nlatency, while dramatically reducing both computational complexity and training\noverhead.",
        "url": "http://arxiv.org/abs/2509.17461v1",
        "published_date": "2025-09-22T07:55:03+00:00",
        "updated_date": "2025-09-22T07:55:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuhao Zhang",
            "Chengjun Zhang",
            "Di Wu",
            "Jie Yang",
            "Mohamad Sawan"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces CSDformer, a novel conversion method for fully spike-driven transformers, showcasing high performance and reduced training costs.",
        "tldr_zh": "本文介绍了CSDformer，一种全脉冲驱动变压器的转换方法，展示了高性能和降低的训练成本。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "CARINOX: Inference-time Scaling with Category-Aware Reward-based Initial Noise Optimization and Exploration",
        "summary": "Text-to-image diffusion models, such as Stable Diffusion, can produce\nhigh-quality and diverse images but often fail to achieve compositional\nalignment, particularly when prompts describe complex object relationships,\nattributes, or spatial arrangements. Recent inference-time approaches address\nthis by optimizing or exploring the initial noise under the guidance of reward\nfunctions that score text-image alignment without requiring model fine-tuning.\nWhile promising, each strategy has intrinsic limitations when used alone:\noptimization can stall due to poor initialization or unfavorable search\ntrajectories, whereas exploration may require a prohibitively large number of\nsamples to locate a satisfactory output. Our analysis further shows that\nneither single reward metrics nor ad-hoc combinations reliably capture all\naspects of compositionality, leading to weak or inconsistent guidance. To\novercome these challenges, we present Category-Aware Reward-based Initial Noise\nOptimization and Exploration (CARINOX), a unified framework that combines noise\noptimization and exploration with a principled reward selection procedure\ngrounded in correlation with human judgments. Evaluations on two complementary\nbenchmarks covering diverse compositional challenges show that CARINOX raises\naverage alignment scores by +16% on T2I-CompBench++ and +11% on the HRS\nbenchmark, consistently outperforming state-of-the-art optimization and\nexploration-based methods across all major categories, while preserving image\nquality and diversity. The project page is available at\nhttps://amirkasaei.com/carinox/{this URL}.",
        "url": "http://arxiv.org/abs/2509.17458v1",
        "published_date": "2025-09-22T07:51:28+00:00",
        "updated_date": "2025-09-22T07:51:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Seyed Amir Kasaei",
            "Ali Aghayari",
            "Arash Marioriyad",
            "Niki Sepasian",
            "Shayan Baghayi Nejad",
            "MohammadAmin Fazli",
            "Mahdieh Soleymani Baghshah",
            "Mohammad Hossein Rohban"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Other"
        ],
        "tldr": "The paper introduces CARINOX, a framework that combines noise optimization and exploration with reward selection to improve alignment scores in text-to-image models, outperforming existing methods.",
        "tldr_zh": "本文介绍了CARINOX框架，将噪声优化和探索与奖励选择相结合，以提高文本到图像模型中的对齐分数，优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Explainable AI for Analyzing Person-Specific Patterns in Facial Recognition Tasks",
        "summary": "The proliferation of facial recognition systems presents major privacy risks,\ndriving the need for effective countermeasures. Current adversarial techniques\napply generalized methods rather than adapting to individual facial\ncharacteristics, limiting their effectiveness and inconspicuousness. In this\nwork, we introduce Layer Embedding Activation Mapping (LEAM), a novel technique\nthat identifies which facial areas contribute most to recognition at an\nindividual level. Unlike adversarial attack methods that aim to fool\nrecognition systems, LEAM is an explainability technique designed to understand\nhow these systems work, providing insights that could inform future privacy\nprotection research. We integrate LEAM with a face parser to analyze data from\n1000 individuals across 9 pre-trained facial recognition models.\n  Our analysis reveals that while different layers within facial recognition\nmodels vary significantly in their focus areas, these models generally\nprioritize similar facial regions across architectures when considering their\noverall activation patterns, which show significantly higher similarity between\nimages of the same individual (Bhattacharyya Coefficient: 0.32-0.57) vs.\ndifferent individuals (0.04-0.13), validating the existence of person-specific\nrecognition patterns. Our results show that facial recognition models\nprioritize the central region of face images (with nose areas accounting for\n18.9-29.7% of critical recognition regions), while still distributing attention\nacross multiple facial fragments. Proper selection of relevant facial areas was\nconfirmed using validation occlusions, based on just 1% of the most relevant,\nLEAM-identified, image pixels, which proved to be transferable across different\nmodels. Our findings establish the foundation for future individually tailored\nprivacy protection systems centered around LEAM's choice of areas to be\nperturbed.",
        "url": "http://arxiv.org/abs/2509.17457v1",
        "published_date": "2025-09-22T07:51:11+00:00",
        "updated_date": "2025-09-22T07:51:11+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "68T10",
            "I.2.10; I.4.m"
        ],
        "authors": [
            "Paweł Jakub Borsukiewicz",
            "Jordan Samhi",
            "Jacques Klein",
            "Tegawendé F. Bissyandé"
        ],
        "ai_categories": [
            "AIGC",
            "Other"
        ],
        "tldr": "The paper introduces a novel technique, LEAM, for understanding person-specific facial recognition patterns to improve privacy protection by analyzing activation patterns in facial recognition models.",
        "tldr_zh": "本文介绍了一种新技术LEAM，用于理解个人特定的面部识别模式，通过分析面部识别模型中的激活模式来提高隐私保护。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian Splats from a Mobile Device",
        "summary": "The field of Embodied AI predominantly relies on simulation for training and\nevaluation, often using either fully synthetic environments that lack\nphotorealism or high-fidelity real-world reconstructions captured with\nexpensive hardware. As a result, sim-to-real transfer remains a major\nchallenge. In this paper, we introduce EmbodiedSplat, a novel approach that\npersonalizes policy training by efficiently capturing the deployment\nenvironment and fine-tuning policies within the reconstructed scenes. Our\nmethod leverages 3D Gaussian Splatting (GS) and the Habitat-Sim simulator to\nbridge the gap between realistic scene capture and effective training\nenvironments. Using iPhone-captured deployment scenes, we reconstruct meshes\nvia GS, enabling training in settings that closely approximate real-world\nconditions. We conduct a comprehensive analysis of training strategies,\npre-training datasets, and mesh reconstruction techniques, evaluating their\nimpact on sim-to-real predictivity in real-world scenarios. Experimental\nresults demonstrate that agents fine-tuned with EmbodiedSplat outperform both\nzero-shot baselines pre-trained on large-scale real-world datasets (HM3D) and\nsynthetically generated datasets (HSSD), achieving absolute success rate\nimprovements of 20\\% and 40\\% on real-world Image Navigation task. Moreover,\nour approach yields a high sim-vs-real correlation (0.87--0.97) for the\nreconstructed meshes, underscoring its effectiveness in adapting policies to\ndiverse environments with minimal effort. Project page:\nhttps://gchhablani.github.io/embodied-splat",
        "url": "http://arxiv.org/abs/2509.17430v1",
        "published_date": "2025-09-22T07:22:31+00:00",
        "updated_date": "2025-09-22T07:22:31+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Gunjan Chhablani",
            "Xiaomeng Ye",
            "Muhammad Zubair Irshad",
            "Zsolt Kira"
        ],
        "ai_categories": [
            "Dataset",
            "GAN",
            "Transformer"
        ],
        "tldr": "The paper introduces EmbodiedSplat, a method for personalized navigation training using 3D Gaussian Splatting for bridging simulation and real-world environments. It outperforms existing methods on real-world Image Navigation tasks.",
        "tldr_zh": "该论文介绍了一种名为EmbodiedSplat的个性化导航训练方法，使用3D高斯斑点方法来连接模拟环境和实际环境。它在真实世界图像导航任务中表现优异。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Single-Image Depth from Defocus with Coded Aperture and Diffusion Posterior Sampling",
        "summary": "We propose a single-snapshot depth-from-defocus (DFD) reconstruction method\nfor coded-aperture imaging that replaces hand-crafted priors with a learned\ndiffusion prior used purely as regularization. Our optimization framework\nenforces measurement consistency via a differentiable forward model while\nguiding solutions with the diffusion prior in the denoised image domain,\nyielding higher accuracy and stability than clas- sical optimization. Unlike\nU-Net-style regressors, our approach requires no paired defocus-RGBD training\ndata and does not tie training to a specific camera configuration. Experiments\non comprehensive simulations and a prototype camera demonstrate consistently\nstrong RGBD reconstructions across noise levels, outperforming both U-Net\nbaselines and a classical coded- aperture DFD method.",
        "url": "http://arxiv.org/abs/2509.17427v1",
        "published_date": "2025-09-22T07:20:30+00:00",
        "updated_date": "2025-09-22T07:20:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hodaka Kawachi",
            "Jose Reinaldo Cunha Santos A. V. Silva Neto",
            "Yasushi Yagi",
            "Hajime Nagahara",
            "Tomoya Nakamura"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper proposes a method for reconstructing depth from defocus using coded aperture imaging and diffusion prior, achieving higher accuracy and stability compared to traditional methods.",
        "tldr_zh": "本文提出了一种利用编码孔径成像和扩散先验重建焦深度的方法，与传统方法相比，实现了更高的准确性和稳定性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Vision Language Models Are Not (Yet) Spelling Correctors",
        "summary": "Spelling correction from visual input poses unique challenges for vision\nlanguage models (VLMs), as it requires not only detecting but also correcting\ntextual errors directly within images. We present ReViCo (Real Visual\nCorrection), the first benchmark that systematically evaluates VLMs on\nreal-world visual spelling correction across Chinese and English. ReViCo\ncontains naturally occurring errors collected from real-world image data and\nsupports fine-grained evaluation at both image and token levels. Through\ncomprehensive experiments on representative cascaded (Qwen) and native\n(InternVL) open-source models, as well as closed-source systems (GPT-4o,\nClaude), we show that current VLMs fall significantly short of human\nperformance, particularly in correction. To address these limitations, we\nexplore two solution paradigms: a Joint OCR-Correction pipeline and a\nBackground Information enhanced approach, both of which yield consistent\nperformance gains. Our analysis highlights fundamental limitations of existing\narchitectures and provides actionable insights for advancing multimodal\nspelling correction.",
        "url": "http://arxiv.org/abs/2509.17418v1",
        "published_date": "2025-09-22T07:10:42+00:00",
        "updated_date": "2025-09-22T07:10:42+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Junhong Liang",
            "Bojun Zhang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces ReViCo benchmark for evaluating vision language models on visual spelling correction. Current models fall short of human performance, and the paper proposes two solution paradigms for improvement.",
        "tldr_zh": "本文介绍了用于评估视觉语言模型在视觉拼写纠正上的ReViCo基准。当前模型无法达到人类的表现水平，该论文提出了两种解决方案范式以改善。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Interpreting vision transformers via residual replacement model",
        "summary": "How do vision transformers (ViTs) represent and process the world? This paper\naddresses this long-standing question through the first systematic analysis of\n6.6K features across all layers, extracted via sparse autoencoders, and by\nintroducing the residual replacement model, which replaces ViT computations\nwith interpretable features in the residual stream. Our analysis reveals not\nonly a feature evolution from low-level patterns to high-level semantics, but\nalso how ViTs encode curves and spatial positions through specialized feature\ntypes. The residual replacement model scalably produces a faithful yet\nparsimonious circuit for human-scale interpretability by significantly\nsimplifying the original computations. As a result, this framework enables\nintuitive understanding of ViT mechanisms. Finally, we demonstrate the utility\nof our framework in debiasing spurious correlations.",
        "url": "http://arxiv.org/abs/2509.17401v1",
        "published_date": "2025-09-22T07:00:57+00:00",
        "updated_date": "2025-09-22T07:00:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jinyeong Kim",
            "Junhyeok Kim",
            "Yumin Shim",
            "Joohyeok Kim",
            "Sunyoung Jung",
            "Seong Jae Hwang"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "This paper introduces a residual replacement model to interpret vision transformers, revealing feature evolution and encoding mechanisms, with potential utility in debiasing spurious correlations.",
        "tldr_zh": "该论文引入了残差替换模型来解释视觉变换器，揭示了特征演变和编码机制，可能在去偏执偶然相关性方面有用。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Revisiting Vision Language Foundations for No-Reference Image Quality Assessment",
        "summary": "Large-scale vision language pre-training has recently shown promise for\nno-reference image-quality assessment (NR-IQA), yet the relative merits of\nmodern Vision Transformer foundations remain poorly understood. In this work,\nwe present the first systematic evaluation of six prominent pretrained\nbackbones, CLIP, SigLIP2, DINOv2, DINOv3, Perception, and ResNet, for the task\nof No-Reference Image Quality Assessment (NR-IQA), each finetuned using an\nidentical lightweight MLP head. Our study uncovers two previously overlooked\nfactors: (1) SigLIP2 consistently achieves strong performance; and (2) the\nchoice of activation function plays a surprisingly crucial role, particularly\nfor enhancing the generalization ability of image quality assessment models.\nNotably, we find that simple sigmoid activations outperform commonly used ReLU\nand GELU on several benchmarks. Motivated by this finding, we introduce a\nlearnable activation selection mechanism that adaptively determines the\nnonlinearity for each channel, eliminating the need for manual activation\ndesign, and achieving new state-of-the-art SRCC on CLIVE, KADID10K, and\nAGIQA3K. Extensive ablations confirm the benefits across architectures and\nregimes, establishing strong, resource-efficient NR-IQA baselines.",
        "url": "http://arxiv.org/abs/2509.17374v1",
        "published_date": "2025-09-22T06:24:42+00:00",
        "updated_date": "2025-09-22T06:24:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ankit Yadav",
            "Ta Duc Huy",
            "Lingqiao Liu"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "AIGC"
        ],
        "tldr": "The paper evaluates six pretrained backbones for no-reference image quality assessment, highlighting the importance of activation functions and introducing a learnable activation selection mechanism.",
        "tldr_zh": "本文评估了六种预训练的骨干网络用于无参考图像质量评估，强调激活函数的重要性，并引入了可学习的激活选择机制。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Pre-Trained CNN Architecture for Transformer-Based Image Caption Generation Model",
        "summary": "Automatic image captioning, a multifaceted task bridging computer vision and\nnatural lan- guage processing, aims to generate descriptive textual content\nfrom visual input. While Convolutional Neural Networks (CNNs) and Long\nShort-Term Memory (LSTM) networks have achieved significant advancements, they\npresent limitations. The inherent sequential nature of RNNs leads to sluggish\ntraining and inference times. LSTMs further struggle with retaining information\nfrom earlier sequence elements when dealing with very long se- quences. This\nproject presents a comprehensive guide to constructing and comprehending\ntransformer models for image captioning. Transformers employ self-attention\nmechanisms, capturing both short- and long-range dependencies within the data.\nThis facilitates efficient parallelization during both training and inference\nphases. We leverage the well-established Transformer architecture, recognized\nfor its effectiveness in managing sequential data, and present a meticulous\nmethodology. Utilizing the Flickr30k dataset, we conduct data pre- processing,\nconstruct a model architecture that integrates an EfficientNetB0 CNN for fea-\nture extraction, and train the model with attention mechanisms incorporated.\nOur approach exemplifies the utilization of parallelization for efficient\ntraining and inference. You can find the project on GitHub.",
        "url": "http://arxiv.org/abs/2509.17365v1",
        "published_date": "2025-09-22T05:32:52+00:00",
        "updated_date": "2025-09-22T05:32:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Amanuel Tafese Dufera"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a pre-trained CNN architecture for a transformer-based image caption generation model, aiming to improve efficiency and performance in generating descriptive captions from visual input.",
        "tldr_zh": "本文介绍了一种预先训练的CNN架构，用于基于Transformer的图像标题生成模型，旨在提高从视觉输入生成描述性标题的效率和性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Mano Report",
        "summary": "Graphical user interfaces (GUIs) are the primary medium for human-computer\ninteraction, yet automating GUI interactions remains challenging due to the\ncomplexity of visual elements, dynamic environments, and the need for\nmulti-step reasoning. Existing methods based on vision-language models (VLMs)\noften suffer from limited resolution, domain mismatch, and insufficient\nsequential decisionmaking capability. To address these issues, we propose Mano,\na robust GUI agent built upon a multi-modal foundation model pre-trained on\nextensive web and computer system data. Our approach integrates a novel\nsimulated environment for high-fidelity data generation, a three-stage training\npipeline (supervised fine-tuning, offline reinforcement learning, and online\nreinforcement learning), and a verification module for error recovery. Mano\ndemonstrates state-of-the-art performance on multiple GUI benchmarks, including\nMind2Web and OSWorld, achieving significant improvements in success rate and\noperational accuracy. Our work provides new insights into the effective\nintegration of reinforcement learning with VLMs for practical GUI agent\ndeployment, highlighting the importance of domain-specific data, iterative\ntraining, and holistic reward design.",
        "url": "http://arxiv.org/abs/2509.17336v1",
        "published_date": "2025-09-22T03:13:58+00:00",
        "updated_date": "2025-09-22T03:13:58+00:00",
        "categories": [
            "cs.MM",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Tianyu Fu",
            "Anyang Su",
            "Chenxu Zhao",
            "Hanning Wang",
            "Minghui Wu",
            "Zhe Yu",
            "Fei Hu",
            "Mingjia Shi",
            "Wei Dong",
            "Jiayao Wang",
            "Yuyang Chen",
            "Ruiyang Yu",
            "Siran Peng",
            "Menglin Li",
            "Nan Huang",
            "Haitian Wei",
            "Jiawei Yu",
            "Yi Xin",
            "Xilin Zhao",
            "Kai Gu",
            "Ping Jiang",
            "Sifan Zhou",
            "Shuo Wang"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality"
        ],
        "tldr": "The paper proposes a robust GUI agent, Mano, built on a multi-modal foundation model for automating GUI interactions by integrating reinforcement learning with vision-language models.",
        "tldr_zh": "本文提出了一种强大的GUI代理Mano，基于多模态基础模型，通过将强化学习与视觉语言模型相结合，实现自动化GUI交互。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SmokeSeer: 3D Gaussian Splatting for Smoke Removal and Scene Reconstruction",
        "summary": "Smoke in real-world scenes can severely degrade the quality of images and\nhamper visibility. Recent methods for image restoration either rely on\ndata-driven priors that are susceptible to hallucinations, or are limited to\nstatic low-density smoke. We introduce SmokeSeer, a method for simultaneous 3D\nscene reconstruction and smoke removal from a video capturing multiple views of\na scene. Our method uses thermal and RGB images, leveraging the fact that the\nreduced scattering in thermal images enables us to see through the smoke. We\nbuild upon 3D Gaussian splatting to fuse information from the two image\nmodalities, and decompose the scene explicitly into smoke and non-smoke\ncomponents. Unlike prior approaches, SmokeSeer handles a broad range of smoke\ndensities and can adapt to temporally varying smoke. We validate our approach\non synthetic data and introduce a real-world multi-view smoke dataset with RGB\nand thermal images. We provide open-source code and data at the project\nwebsite.",
        "url": "http://arxiv.org/abs/2509.17329v1",
        "published_date": "2025-09-22T03:05:22+00:00",
        "updated_date": "2025-09-22T03:05:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Neham Jain",
            "Andrew Jong",
            "Sebastian Scherer",
            "Ioannis Gkioulekas"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "SmokeSeer is a method for simultaneous 3D scene reconstruction and smoke removal from videos using thermal and RGB images.",
        "tldr_zh": "SmokeSeer是一种利用热像和RGB图像对视频中的3D场景重建和烟雾去除的方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DepTR-MOT: Unveiling the Potential of Depth-Informed Trajectory Refinement for Multi-Object Tracking",
        "summary": "Visual Multi-Object Tracking (MOT) is a crucial component of robotic\nperception, yet existing Tracking-By-Detection (TBD) methods often rely on 2D\ncues, such as bounding boxes and motion modeling, which struggle under\nocclusions and close-proximity interactions. Trackers relying on these 2D cues\nare particularly unreliable in robotic environments, where dense targets and\nfrequent occlusions are common. While depth information has the potential to\nalleviate these issues, most existing MOT datasets lack depth annotations,\nleading to its underexploited role in the domain. To unveil the potential of\ndepth-informed trajectory refinement, we introduce DepTR-MOT, a DETR-based\ndetector enhanced with instance-level depth information. Specifically, we\npropose two key innovations: (i) foundation model-based instance-level soft\ndepth label supervision, which refines depth prediction, and (ii) the\ndistillation of dense depth maps to maintain global depth consistency. These\nstrategies enable DepTR-MOT to output instance-level depth during inference,\nwithout requiring foundation models and without additional computational cost.\nBy incorporating depth cues, our method enhances the robustness of the TBD\nparadigm, effectively resolving occlusion and close-proximity challenges.\nExperiments on both the QuadTrack and DanceTrack datasets demonstrate the\neffectiveness of our approach, achieving HOTA scores of 27.59 and 44.47,\nrespectively. In particular, results on QuadTrack, a robotic platform MOT\ndataset, highlight the advantages of our method in handling occlusion and\nclose-proximity challenges in robotic tracking. The source code will be made\npublicly available at https://github.com/warriordby/DepTR-MOT.",
        "url": "http://arxiv.org/abs/2509.17323v1",
        "published_date": "2025-09-22T02:58:04+00:00",
        "updated_date": "2025-09-22T02:58:04+00:00",
        "categories": [
            "cs.CV",
            "cs.RO",
            "eess.IV"
        ],
        "authors": [
            "Buyin Deng",
            "Lingxin Huang",
            "Kai Luo",
            "Fei Teng",
            "Kailun Yang"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "DepTR-MOT introduces depth-informed trajectory refinement for multi-object tracking by enhancing a DETR-based detector with instance-level depth information, improving performance in occlusion and close-proximity challenges.",
        "tldr_zh": "DepTR-MOT通过增强带有实例级深度信息的DETR-based检测器来引入基于深度的轨迹细化，从而改善遮挡和近距离挑战的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Automated Coral Spawn Monitoring for Reef Restoration: The Coral Spawn and Larvae Imaging Camera System (CSLICS)",
        "summary": "Coral aquaculture for reef restoration requires accurate and continuous spawn\ncounting for resource distribution and larval health monitoring, but current\nmethods are labor-intensive and represent a critical bottleneck in the coral\nproduction pipeline. We propose the Coral Spawn and Larvae Imaging Camera\nSystem (CSLICS), which uses low cost modular cameras and object detectors\ntrained using human-in-the-loop labeling approaches for automated spawn\ncounting in larval rearing tanks. This paper details the system engineering,\ndataset collection, and computer vision techniques to detect, classify and\ncount coral spawn. Experimental results from mass spawning events demonstrate\nan F1 score of 82.4\\% for surface spawn detection at different embryogenesis\nstages, 65.3\\% F1 score for sub-surface spawn detection, and a saving of 5,720\nhours of labor per spawning event compared to manual sampling methods at the\nsame frequency. Comparison of manual counts with CSLICS monitoring during a\nmass coral spawning event on the Great Barrier Reef demonstrates CSLICS'\naccurate measurement of fertilization success and sub-surface spawn counts.\nThese findings enhance the coral aquaculture process and enable upscaling of\ncoral reef restoration efforts to address climate change threats facing\necosystems like the Great Barrier Reef.",
        "url": "http://arxiv.org/abs/2509.17299v1",
        "published_date": "2025-09-22T00:47:32+00:00",
        "updated_date": "2025-09-22T00:47:32+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Dorian Tsai",
            "Christopher A. Brunner",
            "Riki Lamont",
            "F. Mikaela Nordborg",
            "Andrea Severati",
            "Java Terry",
            "Karen Jackel",
            "Matthew Dunbabin",
            "Tobias Fischer",
            "Scarlett Raine"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper presents a Coral Spawn and Larvae Imaging Camera System (CSLICS) for automated spawn counting in coral aquaculture, demonstrating high accuracy and labor-saving benefits.",
        "tldr_zh": "该论文提出了用于自动计数珊瑚孵化的CSLICS系统，展示了高准确性和节约劳动力的好处。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Task-Oriented Communications for 3D Scene Representation: Balancing Timeliness and Fidelity",
        "summary": "Real-time Three-dimensional (3D) scene representation is a foundational\nelement that supports a broad spectrum of cutting-edge applications, including\ndigital manufacturing, Virtual, Augmented, and Mixed Reality (VR/AR/MR), and\nthe emerging metaverse. Despite advancements in real-time communication and\ncomputing, achieving a balance between timeliness and fidelity in 3D scene\nrepresentation remains a challenge. This work investigates a wireless network\nwhere multiple homogeneous mobile robots, equipped with cameras, capture an\nenvironment and transmit images to an edge server over channels for 3D\nrepresentation. We propose a contextual-bandit Proximal Policy Optimization\n(PPO) framework incorporating both Age of Information (AoI) and semantic\ninformation to optimize image selection for representation, balancing data\nfreshness and representation quality. Two policies -- the $\\omega$-threshold\nand $\\omega$-wait policies -- together with two benchmark methods are\nevaluated, timeliness embedding and weighted sum, on standard datasets and\nbaseline 3D scene representation models. Experimental results demonstrate\nimproved representation fidelity while maintaining low latency, offering\ninsight into the model's decision-making process. This work advances real-time\n3D scene representation by optimizing the trade-off between timeliness and\nfidelity in dynamic environments.",
        "url": "http://arxiv.org/abs/2509.17282v1",
        "published_date": "2025-09-21T23:40:11+00:00",
        "updated_date": "2025-09-21T23:40:11+00:00",
        "categories": [
            "cs.CV",
            "cs.NI"
        ],
        "authors": [
            "Xiangmin Xu",
            "Zhen Meng",
            "Kan Chen",
            "Jiaming Yang",
            "Emma Li",
            "Philip G. Zhao",
            "David Flynn"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper explores optimizing the trade-off between timeliness and fidelity in real-time 3D scene representation using wireless network and robots equipped with cameras.",
        "tldr_zh": "本文探讨利用无线网络和配备摄像头的机器人优化实时3D场景表示中及时性和忠实度之间的权衡。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Optimized Learned Image Compression for Facial Expression Recognition",
        "summary": "Efficient data compression is crucial for the storage and transmission of\nvisual data. However, in facial expression recognition (FER) tasks, lossy\ncompression often leads to feature degradation and reduced accuracy. To address\nthese challenges, this study proposes an end-to-end model designed to preserve\ncritical features and enhance both compression and recognition performance. A\ncustom loss function is introduced to optimize the model, tailored to balance\ncompression and recognition performance effectively. This study also examines\nthe influence of varying loss term weights on this balance. Experimental\nresults indicate that fine-tuning the compression model alone improves\nclassification accuracy by 0.71% and compression efficiency by 49.32%, while\njoint optimization achieves significant gains of 4.04% in accuracy and 89.12%\nin efficiency. Moreover, the findings demonstrate that the jointly optimized\nclassification model maintains high accuracy on both compressed and\nuncompressed data, while the compression model reliably preserves image\ndetails, even at high compression rates.",
        "url": "http://arxiv.org/abs/2509.17262v1",
        "published_date": "2025-09-21T22:35:19+00:00",
        "updated_date": "2025-09-21T22:35:19+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Xiumei Li",
            "Marc Windsheimer",
            "Misha Sadeghi",
            "Björn Eskofier",
            "André Kaup"
        ],
        "ai_categories": [
            "AIGC"
        ],
        "tldr": "This study introduces an optimized compression model for facial expression recognition that enhances both compression and recognition performance, achieving significant gains in accuracy and efficiency.",
        "tldr_zh": "本研究引入了一种针对面部表情识别的优化压缩模型，提高了压缩和识别性能，实现了显著的准确度和效率提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SPFSplatV2: Efficient Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views",
        "summary": "We introduce SPFSplatV2, an efficient feed-forward framework for 3D Gaussian\nsplatting from sparse multi-view images, requiring no ground-truth poses during\ntraining and inference. It employs a shared feature extraction backbone,\nenabling simultaneous prediction of 3D Gaussian primitives and camera poses in\na canonical space from unposed inputs. A masked attention mechanism is\nintroduced to efficiently estimate target poses during training, while a\nreprojection loss enforces pixel-aligned Gaussian primitives, providing\nstronger geometric constraints. We further demonstrate the compatibility of our\ntraining framework with different reconstruction architectures, resulting in\ntwo model variants. Remarkably, despite the absence of pose supervision, our\nmethod achieves state-of-the-art performance in both in-domain and\nout-of-domain novel view synthesis, even under extreme viewpoint changes and\nlimited image overlap, and surpasses recent methods that rely on geometric\nsupervision for relative pose estimation. By eliminating dependence on\nground-truth poses, our method offers the scalability to leverage larger and\nmore diverse datasets. Code and pretrained models will be available on our\nproject page: https://ranrhuang.github.io/spfsplatv2/.",
        "url": "http://arxiv.org/abs/2509.17246v1",
        "published_date": "2025-09-21T21:37:56+00:00",
        "updated_date": "2025-09-21T21:37:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ranran Huang",
            "Krystian Mikolajczyk"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "SPFSplatV2 introduces an efficient framework for 3D Gaussian splatting from sparse multi-view images without ground-truth poses, achieving state-of-the-art performance in novel view synthesis.",
        "tldr_zh": "SPFSplatV2 提出了一种有效的方法，从稀疏多视图图像中进行3D高斯喷洒，无需地面真实姿态，在新视图合成中取得了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DT-NeRF: A Diffusion and Transformer-Based Optimization Approach for Neural Radiance Fields in 3D Reconstruction",
        "summary": "This paper proposes a Diffusion Model-Optimized Neural Radiance Field\n(DT-NeRF) method, aimed at enhancing detail recovery and multi-view consistency\nin 3D scene reconstruction. By combining diffusion models with Transformers,\nDT-NeRF effectively restores details under sparse viewpoints and maintains high\naccuracy in complex geometric scenes. Experimental results demonstrate that\nDT-NeRF significantly outperforms traditional NeRF and other state-of-the-art\nmethods on the Matterport3D and ShapeNet datasets, particularly in metrics such\nas PSNR, SSIM, Chamfer Distance, and Fidelity. Ablation experiments further\nconfirm the critical role of the diffusion and Transformer modules in the\nmodel's performance, with the removal of either module leading to a decline in\nperformance. The design of DT-NeRF showcases the synergistic effect between\nmodules, providing an efficient and accurate solution for 3D scene\nreconstruction. Future research may focus on further optimizing the model,\nexploring more advanced generative models and network architectures to enhance\nits performance in large-scale dynamic scenes.",
        "url": "http://arxiv.org/abs/2509.17232v1",
        "published_date": "2025-09-21T20:45:22+00:00",
        "updated_date": "2025-09-21T20:45:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bo Liu",
            "Runlong Li",
            "Li Zhou",
            "Yan Zhou"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a DT-NeRF method for improving 3D scene reconstruction using diffusion models and Transformers, outperforming traditional methods in various metrics.",
        "tldr_zh": "本文提出了一种DT-NeRF方法，通过扩散模型和Transformer改进了3D场景重建，超越了传统方法在各种指标上的表现。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Point-RTD: Replaced Token Denoising for Pretraining Transformer Models on Point Clouds",
        "summary": "Pre-training strategies play a critical role in advancing the performance of\ntransformer-based models for 3D point cloud tasks. In this paper, we introduce\nPoint-RTD (Replaced Token Denoising), a novel pretraining strategy designed to\nimprove token robustness through a corruption-reconstruction framework. Unlike\ntraditional mask-based reconstruction tasks that hide data segments for later\nprediction, Point-RTD corrupts point cloud tokens and leverages a\ndiscriminator-generator architecture for denoising. This shift enables more\neffective learning of structural priors and significantly enhances model\nperformance and efficiency. On the ShapeNet dataset, Point-RTD reduces\nreconstruction error by over 93% compared to PointMAE, and achieves more than\n14x lower Chamfer Distance on the test set. Our method also converges faster\nand yields higher classification accuracy on ShapeNet, ModelNet10, and\nModelNet40 benchmarks, clearly outperforming the baseline Point-MAE framework\nin every case.",
        "url": "http://arxiv.org/abs/2509.17207v1",
        "published_date": "2025-09-21T19:25:25+00:00",
        "updated_date": "2025-09-21T19:25:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Gunner Stone",
            "Youngsook Choi",
            "Alireza Tavakkoli",
            "Ankita Shukla"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "Point-RTD introduces a novel pretraining strategy for transformer-based models on 3D point clouds, outperforming existing frameworks in terms of reconstruction error, distance metrics, and classification accuracy.",
        "tldr_zh": "Point-RTD在3D点云的变换器模型预训练中引入了一种新颖的策略，优于现有框架，包括重构误差，距离指标和分类准确性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Guided and Unguided Conditional Diffusion Mechanisms for Structured and Semantically-Aware 3D Point Cloud Generation",
        "summary": "Generating realistic 3D point clouds is a fundamental problem in computer\nvision with applications in remote sensing, robotics, and digital object\nmodeling. Existing generative approaches primarily capture geometry, and when\nsemantics are considered, they are typically imposed post hoc through external\nsegmentation or clustering rather than integrated into the generative process\nitself. We propose a diffusion-based framework that embeds per-point semantic\nconditioning directly within generation. Each point is associated with a\nconditional variable corresponding to its semantic label, which guides the\ndiffusion dynamics and enables the joint synthesis of geometry and semantics.\nThis design produces point clouds that are both structurally coherent and\nsegmentation-aware, with object parts explicitly represented during synthesis.\nThrough a comparative analysis of guided and unguided diffusion processes, we\ndemonstrate the significant impact of conditional variables on diffusion\ndynamics and generation quality. Extensive experiments validate the efficacy of\nour approach, producing detailed and accurate 3D point clouds tailored to\nspecific parts and features.",
        "url": "http://arxiv.org/abs/2509.17206v1",
        "published_date": "2025-09-21T19:19:36+00:00",
        "updated_date": "2025-09-21T19:19:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Gunner Stone",
            "Sushmita Sarker",
            "Alireza Tavakkoli"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a diffusion-based framework for generating 3D point clouds with embedded semantic conditioning, producing coherent and segmentation-aware results.",
        "tldr_zh": "该论文介绍了一种基于扩散的框架，用于生成具有嵌入式语义条件的3D点云，产生连贯且分段感知的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery",
        "summary": "Analyzing cultural-heritage artifacts remains challenging for MLLMs: general\nmodels lack domain expertise, and SFT often overfits superficial patterns,\nyielding brittle reasoning for authentication and historical attribution. This\nraises the question of how to equip MLLMs with robust, expert-level reasoning\nfor ancient Greek pottery. We present VaseVL, an SFT-then-RL system that turns\nevaluation into supervision: we construct a taxonomy of question types, probe\nthe SFT model to localize type-specific performance gaps, and optimize with\ntype-conditioned, compositionality-oriented rewards targeting those gaps. We\nalso release VaseVQA, a comprehensive benchmark of 31,773 images designed to\nprobe deep understanding. Experiments show state-of-the-art results on style\nclassification and historical attribution with marked gains in compositional\nrobustness over SFT-only baselines, validating diagnosis-guided,\ntaxonomy-conditioned reward engineering and providing a reusable resource for\nfuture research. Code and dataset will be available at\nhttps://github.com/AIGeeksGroup/VaseVQA.",
        "url": "http://arxiv.org/abs/2509.17191v1",
        "published_date": "2025-09-21T18:36:54+00:00",
        "updated_date": "2025-09-21T18:36:54+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Jinchao Ge",
            "Tengfei Cheng",
            "Biao Wu",
            "Zeyu Zhang",
            "Shiya Huang",
            "Judith Bishop",
            "Gillian Shepherd",
            "Meng Fang",
            "Ling Chen",
            "Yang Zhao"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper presents VaseVQA, a benchmark for ancient Greek pottery analysis using a specialized model for robust reasoning and deep understanding.",
        "tldr_zh": "本文介绍了VaseVQA，一个专门为古希腊陶器分析设计的基准测试，利用专门模型进行鲁棒推理和深度理解。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Ambiguous Medical Image Segmentation Using Diffusion Schrödinger Bridge",
        "summary": "Accurate segmentation of medical images is challenging due to unclear lesion\nboundaries and mask variability. We introduce \\emph{Segmentation Sch\\\"{o}dinger\nBridge (SSB)}, the first application of Sch\\\"{o}dinger Bridge for ambiguous\nmedical image segmentation, modelling joint image-mask dynamics to enhance\nperformance. SSB preserves structural integrity, delineates unclear boundaries\nwithout additional guidance, and maintains diversity using a novel loss\nfunction. We further propose the \\emph{Diversity Divergence Index} ($D_{DDI}$)\nto quantify inter-rater variability, capturing both diversity and consensus.\nSSB achieves state-of-the-art performance on LIDC-IDRI, COCA, and RACER\n(in-house) datasets.",
        "url": "http://arxiv.org/abs/2509.17187v1",
        "published_date": "2025-09-21T18:16:06+00:00",
        "updated_date": "2025-09-21T18:16:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Lalith Bharadwaj Baru",
            "Kamalaker Dadi",
            "Tapabrata Chakraborti",
            "Raju S. Bapi"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper introduces Segmentation Schrödinger Bridge (SSB) for ambiguous medical image segmentation, achieving state-of-the-art performance on various datasets.",
        "tldr_zh": "该论文引入了用于模糊医学图像分割的分割薛定谔桥（SSB），在各种数据集上取得了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Preconditioned Deformation Grids",
        "summary": "Dynamic surface reconstruction of objects from point cloud sequences is a\nchallenging field in computer graphics. Existing approaches either require\nmultiple regularization terms or extensive training data which, however, lead\nto compromises in reconstruction accuracy as well as over-smoothing or poor\ngeneralization to unseen objects and motions. To address these lim- itations,\nwe introduce Preconditioned Deformation Grids, a novel technique for estimating\ncoherent deformation fields directly from unstructured point cloud sequences\nwithout requiring or forming explicit correspondences. Key to our approach is\nthe use of multi-resolution voxel grids that capture the overall motion at\nvarying spatial scales, enabling a more flexible deformation representation. In\nconjunction with incorporating grid-based Sobolev preconditioning into\ngradient-based optimization, we show that applying a Chamfer loss between the\ninput point clouds as well as to an evolving template mesh is sufficient to\nobtain accurate deformations. To ensure temporal consistency along the object\nsurface, we include a weak isometry loss on mesh edges which complements the\nmain objective without constraining deformation fidelity. Extensive evaluations\ndemonstrate that our method achieves superior results, particularly for long\nsequences, compared to state-of-the-art techniques.",
        "url": "http://arxiv.org/abs/2509.18097v1",
        "published_date": "2025-09-22T17:59:55+00:00",
        "updated_date": "2025-09-22T17:59:55+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Julian Kaltheuner",
            "Alexander Oebel",
            "Hannah Droege",
            "Patrick Stotko",
            "Reinhard Klein"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces Preconditioned Deformation Grids for dynamic surface reconstruction from point cloud sequences without explicit correspondences, achieving superior results compared to existing techniques.",
        "tldr_zh": "本文引入了预处理变形网格用于点云序列的动态表面重建，无需显式对应，相较于现有技术取得了卓越结果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "GraDeT-HTR: A Resource-Efficient Bengali Handwritten Text Recognition System utilizing Grapheme-based Tokenizer and Decoder-only Transformer",
        "summary": "Despite Bengali being the sixth most spoken language in the world,\nhandwritten text recognition (HTR) systems for Bengali remain severely\nunderdeveloped. The complexity of Bengali script--featuring conjuncts,\ndiacritics, and highly variable handwriting styles--combined with a scarcity of\nannotated datasets makes this task particularly challenging. We present\nGraDeT-HTR, a resource-efficient Bengali handwritten text recognition system\nbased on a Grapheme-aware Decoder-only Transformer architecture. To address the\nunique challenges of Bengali script, we augment the performance of a\ndecoder-only transformer by integrating a grapheme-based tokenizer and\ndemonstrate that it significantly improves recognition accuracy compared to\nconventional subword tokenizers. Our model is pretrained on large-scale\nsynthetic data and fine-tuned on real human-annotated samples, achieving\nstate-of-the-art performance on multiple benchmark datasets.",
        "url": "http://arxiv.org/abs/2509.18081v1",
        "published_date": "2025-09-22T17:56:17+00:00",
        "updated_date": "2025-09-22T17:56:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Md. Mahmudul Hasan",
            "Ahmed Nesar Tahsin Choudhury",
            "Mahmudul Hasan",
            "Md. Mosaddek Khan"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a resource-efficient Bengali Handwritten Text Recognition system called GraDeT-HTR, utilizing a Grapheme-based Tokenizer and a Decoder-only Transformer model, achieving state-of-the-art performance on benchmark datasets.",
        "tldr_zh": "该论文介绍了一种资源高效的孟加拉手写文本识别系统GraDeT-HTR，利用基于字素的标记器和仅解码器变压器模型，在基准数据集上取得了最先进的性能。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Does Audio Matter for Modern Video-LLMs and Their Benchmarks?",
        "summary": "Modern multimodal large language models often claim \"video understanding,\"\nyet most evaluations use muted videos or simply discard audio. We ask a direct\nquestion: how much does audio actually matter for contemporary Video-LLMs and\nthe benchmarks that certify them? We audit widely used suites and observe that\nmany items are even solvable from a single frame, rendering audio largely\nredundant. Building on LLaVA-OneVision architecture, we attach a speech/audio\nencoder (e.g., Whisper) and analyze when audio helps, while addressing audio\ntoken explosion with a lightweight Mamba-based state-space token compressor. We\nfind that audio yields minimal gains on recent video benchmarks but is decisive\non curated, audio-sensitive subsets. To enable faithful evaluation, we release\nAVQA-Hard and Music-AVQA-Hard, our model, and code. Our findings surface a\ngrowing gap between current academic practice and real-world expectations, and\nprovide practical tools for scalable audio-visual Video-LLMs. We will fully\nopen-source our work at https://github.com/naver-ai/LLaVA-AV-SSM.",
        "url": "http://arxiv.org/abs/2509.17901v1",
        "published_date": "2025-09-22T15:28:54+00:00",
        "updated_date": "2025-09-22T15:28:54+00:00",
        "categories": [
            "cs.CV",
            "cs.MM",
            "cs.SD"
        ],
        "authors": [
            "Geewook Kim",
            "Minjoon Seo"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper investigates the importance of audio in contemporary Video-LLMs and their benchmarks, finding that audio is not crucial for recent video benchmarks but is significant for audio-sensitive subsets.",
        "tldr_zh": "该论文研究了当代视频大型语言模型及其基准测试中音频的重要性，发现音频对于最近的视频基准测试并不是至关重要的，但对于音频敏感的子集却至关重要。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7.5
    },
    {
        "title": "Sight Over Site: Perception-Aware Reinforcement Learning for Efficient Robotic Inspection",
        "summary": "Autonomous inspection is a central problem in robotics, with applications\nranging from industrial monitoring to search-and-rescue. Traditionally,\ninspection has often been reduced to navigation tasks, where the objective is\nto reach a predefined location while avoiding obstacles. However, this\nformulation captures only part of the real inspection problem. In real-world\nenvironments, the inspection targets may become visible well before their exact\ncoordinates are reached, making further movement both redundant and\ninefficient. What matters more for inspection is not simply arriving at the\ntarget's position, but positioning the robot at a viewpoint from which the\ntarget becomes observable. In this work, we revisit inspection from a\nperception-aware perspective. We propose an end-to-end reinforcement learning\nframework that explicitly incorporates target visibility as the primary\nobjective, enabling the robot to find the shortest trajectory that guarantees\nvisual contact with the target without relying on a map. The learned policy\nleverages both perceptual and proprioceptive sensing and is trained entirely in\nsimulation, before being deployed to a real-world robot. We further develop an\nalgorithm to compute ground-truth shortest inspection paths, which provides a\nreference for evaluation. Through extensive experiments, we show that our\nmethod outperforms existing classical and learning-based navigation approaches,\nyielding more efficient inspection trajectories in both simulated and\nreal-world settings. The project is avialable at\nhttps://sight-over-site.github.io/",
        "url": "http://arxiv.org/abs/2509.17877v1",
        "published_date": "2025-09-22T15:14:02+00:00",
        "updated_date": "2025-09-22T15:14:02+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Richard Kuhlmann",
            "Jakob Wolfram",
            "Boyang Sun",
            "Jiaxu Xing",
            "Davide Scaramuzza",
            "Marc Pollefeys",
            "Cesar Cadena"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a perception-aware reinforcement learning framework for efficient robotic inspection, outperforming existing approaches in both simulated and real-world settings.",
        "tldr_zh": "本文引入了一种感知感知强化学习框架，用于高效的机器人检查，在模拟和真实世界环境中均优于现有方法。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "Selecting Optimal Camera Views for Gait Analysis: A Multi-Metric Assessment of 2D Projections",
        "summary": "Objective: To systematically quantify the effect of the camera view (frontal\nvs. lateral) on the accuracy of 2D markerless gait analysis relative to 3D\nmotion capture ground truth. Methods: Gait data from 18 subjects were recorded\nsimultaneously using frontal, lateral and 3D motion capture systems. Pose\nestimation used YOLOv8. Four metrics were assessed to evaluate agreement:\nDynamic Time Warping (DTW) for temporal alignment, Maximum Cross-Correlation\n(MCC) for signal similarity, Kullback-Leibler Divergence (KLD) for distribution\ndifferences, and Information Entropy (IE) for complexity. Wilcoxon signed-rank\ntests (significance: $p < 0.05$) and Cliff's delta ($\\delta$) were used to\nmeasure statistical differences and effect sizes. Results: Lateral views\nsignificantly outperformed frontal views for sagittal plane kinematics: step\nlength (DTW: $53.08 \\pm 24.50$ vs. $69.87 \\pm 25.36$, $p = 0.005$) and knee\nrotation (DTW: $106.46 \\pm 38.57$ vs. $155.41 \\pm 41.77$, $p = 0.004$). Frontal\nviews were superior for symmetry parameters: trunk rotation (KLD: $0.09 \\pm\n0.06$ vs. $0.30 \\pm 0.19$, $p < 0.001$) and wrist-to-hipmid distance (MCC:\n$105.77 \\pm 29.72$ vs. $75.20 \\pm 20.38$, $p = 0.003$). Effect sizes were\nmedium-to-large ($\\delta: 0.34$--$0.76$). Conclusion: Camera view critically\nimpacts gait parameter accuracy. Lateral views are optimal for sagittal\nkinematics; frontal views excel for trunk symmetry. Significance: This first\nsystematic evidence enables data-driven camera deployment in 2D gait analysis,\nenhancing clinical utility. Future implementations should leverage both views\nvia disease-oriented setups.",
        "url": "http://arxiv.org/abs/2509.17805v1",
        "published_date": "2025-09-22T14:00:20+00:00",
        "updated_date": "2025-09-22T14:00:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dong Chen",
            "Huili Peng",
            "Yong Hu",
            "Kenneth MC. Cheung"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper systematically quantifies the impact of different camera views on the accuracy of 2D markerless gait analysis compared to 3D motion capture, finding that lateral views are better for sagittal kinematics while frontal views excel for trunk symmetry.",
        "tldr_zh": "本文系统量化不同摄像头视角对2D无标记步态分析准确性的影响，发现侧视图更适合用于矢状面运动学，而正视图在躯干对称性方面表现出色。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "Learning Neural Antiderivatives",
        "summary": "Neural fields offer continuous, learnable representations that extend beyond\ntraditional discrete formats in visual computing. We study the problem of\nlearning neural representations of repeated antiderivatives directly from a\nfunction, a continuous analogue of summed-area tables. Although widely used in\ndiscrete domains, such cumulative schemes rely on grids, which prevents their\napplicability in continuous neural contexts. We introduce and analyze a range\nof neural methods for repeated integration, including both adaptations of prior\nwork and novel designs. Our evaluation spans multiple input dimensionalities\nand integration orders, assessing both reconstruction quality and performance\nin downstream tasks such as filtering and rendering. These results enable\nintegrating classical cumulative operators into modern neural systems and offer\ninsights into learning tasks involving differential and integral operators.",
        "url": "http://arxiv.org/abs/2509.17755v1",
        "published_date": "2025-09-22T13:19:07+00:00",
        "updated_date": "2025-09-22T13:19:07+00:00",
        "categories": [
            "cs.LG",
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Fizza Rubab",
            "Ntumba Elie Nsampi",
            "Martin Balint",
            "Felix Mujkanovic",
            "Hans-Peter Seidel",
            "Tobias Ritschel",
            "Thomas Leimkühler"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper explores learning neural representations of repeated antiderivatives for continuous functions, enabling classical cumulative operators in modern neural systems.",
        "tldr_zh": "本文探讨了学习神经反导函数重复积分的表示，使得传统的累积运算符能够应用于现代神经系统。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Depth Edge Alignment Loss: DEALing with Depth in Weakly Supervised Semantic Segmentation",
        "summary": "Autonomous robotic systems applied to new domains require an abundance of\nexpensive, pixel-level dense labels to train robust semantic segmentation\nmodels under full supervision. This study proposes a model-agnostic Depth Edge\nAlignment Loss to improve Weakly Supervised Semantic Segmentation models across\ndifferent datasets. The methodology generates pixel-level semantic labels from\nimage-level supervision, avoiding expensive annotation processes. While weak\nsupervision is widely explored in traditional computer vision, our approach\nadds supervision with pixel-level depth information, a modality commonly\navailable in robotic systems. We demonstrate how our approach improves\nsegmentation performance across datasets and models, but can also be combined\nwith other losses for even better performance, with improvements up to +5.439,\n+1.274 and +16.416 points in mean Intersection over Union on the PASCAL VOC /\nMS COCO validation, and the HOPE static onboarding split, respectively. Our\ncode will be made publicly available.",
        "url": "http://arxiv.org/abs/2509.17702v1",
        "published_date": "2025-09-22T12:42:10+00:00",
        "updated_date": "2025-09-22T12:42:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Patrick Schmidt",
            "Vasileios Belagiannis",
            "Lazaros Nalpantidis"
        ],
        "ai_categories": [
            "Multimodality",
            "LoRA",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces Depth Edge Alignment Loss to improve Weakly Supervised Semantic Segmentation models by incorporating pixel-level depth information, showcasing performance improvements across different datasets and models.",
        "tldr_zh": "本文提出了深度边缘对齐损失，通过融入像素级深度信息来改进弱监督的语义分割模型，在不同数据集和模型上展示了性能提升。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Overview of PlantCLEF 2023: Image-based Plant Identification at Global Scale",
        "summary": "The world is estimated to be home to over 300,000 species of vascular plants.\nIn the face of the ongoing biodiversity crisis, expanding our understanding of\nthese species is crucial for the advancement of human civilization,\nencompassing areas such as agriculture, construction, and pharmacopoeia.\nHowever, the labor-intensive process of plant identification undertaken by\nhuman experts poses a significant obstacle to the accumulation of new data and\nknowledge. Fortunately, recent advancements in automatic identification,\nparticularly through the application of deep learning techniques, have shown\npromising progress. Despite challenges posed by data-related issues such as a\nvast number of classes, imbalanced class distribution, erroneous\nidentifications, duplications, variable visual quality, and diverse visual\ncontents (such as photos or herbarium sheets), deep learning approaches have\nreached a level of maturity which gives us hope that in the near future we will\nhave an identification system capable of accurately identifying all plant\nspecies worldwide. The PlantCLEF2023 challenge aims to contribute to this\npursuit by addressing a multi-image (and metadata) classification problem\ninvolving an extensive set of classes (80,000 plant species). This paper\nprovides an overview of the challenge's resources and evaluations, summarizes\nthe methods and systems employed by participating research groups, and presents\nan analysis of key findings.",
        "url": "http://arxiv.org/abs/2509.17622v1",
        "published_date": "2025-09-22T11:34:10+00:00",
        "updated_date": "2025-09-22T11:34:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Herve Goeau",
            "Pierre Bonnet",
            "Alexis Joly"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper presents an overview of the PlantCLEF 2023 challenge aiming to develop an image-based plant identification system for a large number of plant species.",
        "tldr_zh": "本文概述了旨在为大量植物物种开发基于图像的植物识别系统的PlantCLEF 2023挑战。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "PRNU-Bench: A Novel Benchmark and Model for PRNU-Based Camera Identification",
        "summary": "We propose a novel benchmark for camera identification via Photo Response\nNon-Uniformity (PRNU) estimation. The benchmark comprises 13K photos taken with\n120+ cameras, where the training and test photos are taken in different\nscenarios, enabling ``in-the-wild'' evaluation. In addition, we propose a novel\nPRNU-based camera identification model that employs a hybrid architecture,\ncomprising a denoising autoencoder to estimate the PRNU signal and a\nconvolutional network that can perform 1:N verification of camera devices.\nInstead of using a conventional approach based on contrastive learning, our\nmethod takes the Hadamard product between reference and query PRNU signals as\ninput. This novel design leads to significantly better results compared with\nstate-of-the-art models based on denoising autoencoders and contrastive\nlearning. We release our dataset and code at:\nhttps://github.com/CroitoruAlin/PRNU-Bench.",
        "url": "http://arxiv.org/abs/2509.17581v1",
        "published_date": "2025-09-22T11:07:15+00:00",
        "updated_date": "2025-09-22T11:07:15+00:00",
        "categories": [
            "cs.CV",
            "cs.CR",
            "cs.LG"
        ],
        "authors": [
            "Florinel Alin Croitoru",
            "Vlad Hondru",
            "Radu Tudor Ionescu"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a novel benchmark and model for camera identification using PRNU estimation with significantly improved results compared to existing methods.",
        "tldr_zh": "该论文介绍了一种利用PRNU估计的相机识别的新型基准和模型，与现有方法相比结果显著改善。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "MRN: Harnessing 2D Vision Foundation Models for Diagnosing Parkinson's Disease with Limited 3D MR Data",
        "summary": "The automatic diagnosis of Parkinson's disease is in high clinical demand due\nto its prevalence and the importance of targeted treatment. Current clinical\npractice often relies on diagnostic biomarkers in QSM and NM-MRI images.\nHowever, the lack of large, high-quality datasets makes training diagnostic\nmodels from scratch prone to overfitting. Adapting pre-trained 3D medical\nmodels is also challenging, as the diversity of medical imaging leads to\nmismatches in voxel spacing and modality between pre-training and fine-tuning\ndata. In this paper, we address these challenges by leveraging 2D vision\nfoundation models (VFMs). Specifically, we crop multiple key ROIs from NM and\nQSM images, process each ROI through separate branches to compress the ROI into\na token, and then combine these tokens into a unified patient representation\nfor classification. Within each branch, we use 2D VFMs to encode axial slices\nof the 3D ROI volume and fuse them into the ROI token, guided by an auxiliary\nsegmentation head that steers the feature extraction toward specific brain\nnuclei. Additionally, we introduce multi-ROI supervised contrastive learning,\nwhich improves diagnostic performance by pulling together representations of\npatients from the same class while pushing away those from different classes.\nOur approach achieved first place in the MICCAI 2025 PDCADxFoundation\nchallenge, with an accuracy of 86.0% trained on a dataset of only 300 labeled\nQSM and NM-MRI scans, outperforming the second-place method by 5.5%.These\nresults highlight the potential of 2D VFMs for clinical analysis of 3D MR\nimages.",
        "url": "http://arxiv.org/abs/2509.17566v1",
        "published_date": "2025-09-22T10:59:27+00:00",
        "updated_date": "2025-09-22T10:59:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ding Shaodong",
            "Liu Ziyang",
            "Zhou Yijun",
            "Liu Tao"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel method for diagnosing Parkinson's disease using 2D vision foundation models to process 3D MR data, achieving high accuracy with limited labeled scans.",
        "tldr_zh": "本文介绍了一种利用2D视觉基础模型处理3D磁共振数据进行帕金森病诊断的新方法，利用有限标记扫描数据实现较高准确性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Emergent 3D Correspondence from Neural Shape Representation",
        "summary": "This paper presents a new approach to estimate accurate and robust 3D\nsemantic correspondence with the hierarchical neural semantic representation.\nOur work has three key contributions. First, we design the hierarchical neural\nsemantic representation (HNSR), which consists of a global semantic feature to\ncapture high-level structure and multi-resolution local geometric features to\npreserve fine details, by carefully harnessing 3D priors from pre-trained 3D\ngenerative models. Second, we design a progressive global-to-local matching\nstrategy, which establishes coarse semantic correspondence using the global\nsemantic feature, then iteratively refines it with local geometric features,\nyielding accurate and semantically-consistent mappings. Third, our framework is\ntraining-free and broadly compatible with various pre-trained 3D generative\nbackbones, demonstrating strong generalization across diverse shape categories.\nOur method also supports various applications, such as shape co-segmentation,\nkeypoint matching, and texture transfer, and generalizes well to structurally\ndiverse shapes, with promising results even in cross-category scenarios. Both\nqualitative and quantitative evaluations show that our method outperforms\nprevious state-of-the-art techniques.",
        "url": "http://arxiv.org/abs/2509.17431v1",
        "published_date": "2025-09-22T07:23:07+00:00",
        "updated_date": "2025-09-22T07:23:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Keyu Du",
            "Jingyu Hu",
            "Haipeng Li",
            "Hao Xu",
            "Haibing Huang",
            "Chi-Wing Fu",
            "Shuaicheng Liu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a new method for 3D semantic correspondence using a hierarchical neural semantic representation, achieving accurate and robust mappings. It outperforms previous techniques in various applications and diverse shape categories.",
        "tldr_zh": "本文引入了一种利用分层神经语义表示的3D语义对应关系的新方法，实现了准确和稳健的映射。在各种应用和不同形状类别中，它优于先前的技术。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Diff-GNSS: Diffusion-based Pseudorange Error Estimation",
        "summary": "Global Navigation Satellite Systems (GNSS) are vital for reliable urban\npositioning. However, multipath and non-line-of-sight reception often introduce\nlarge measurement errors that degrade accuracy. Learning-based methods for\npredicting and compensating pseudorange errors have gained traction, but their\nperformance is limited by complex error distributions. To address this\nchallenge, we propose Diff-GNSS, a coarse-to-fine GNSS measurement\n(pseudorange) error estimation framework that leverages a conditional diffusion\nmodel to capture such complex distributions. Firstly, a Mamba-based module\nperforms coarse estimation to provide an initial prediction with appropriate\nscale and trend. Then, a conditional denoising diffusion layer refines the\nestimate, enabling fine-grained modeling of pseudorange errors. To suppress\nuncontrolled generative diversity and achieve controllable synthesis, three key\nfeatures related to GNSS measurement quality are used as conditions to\nprecisely guide the reverse denoising process. We further incorporate\nper-satellite uncertainty modeling within the diffusion stage to assess the\nreliability of the predicted errors. We have collected and publicly released a\nreal-world dataset covering various scenes. Experiments on public and\nself-collected datasets show that DiffGNSS consistently outperforms\nstate-of-the-art baselines across multiple metrics. To the best of our\nknowledge, this is the first application of diffusion models to pseudorange\nerror estimation. The proposed diffusion-based refinement module is\nplug-and-play and can be readily integrated into existing networks to markedly\nimprove estimation accuracy.",
        "url": "http://arxiv.org/abs/2509.17397v1",
        "published_date": "2025-09-22T06:57:06+00:00",
        "updated_date": "2025-09-22T06:57:06+00:00",
        "categories": [
            "cs.CV",
            "cs.ET"
        ],
        "authors": [
            "Jiaqi Zhu",
            "Shouyi Lu",
            "Ziyao Li",
            "Guirong Zhuo",
            "Lu Xiong"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "Diff-GNSS proposes a novel diffusion-based framework for GNSS pseudorange error estimation, outperforming state-of-the-art methods in accuracy.",
        "tldr_zh": "Diff-GNSS 提出了一种新颖的基于扩散的全球卫星导航系统伪距误差估计框架，精度优于现有最先进方法。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Automated Facility Enumeration for Building Compliance Checking using Door Detection and Large Language Models",
        "summary": "Building compliance checking (BCC) is a critical process for ensuring that\nconstructed facilities meet regulatory standards. A core component of BCC is\nthe accurate enumeration of facility types and their spatial distribution.\nDespite its importance, this problem has been largely overlooked in the\nliterature, posing a significant challenge for BCC and leaving a critical gap\nin existing workflows. Performing this task manually is time-consuming and\nlabor-intensive. Recent advances in large language models (LLMs) offer new\nopportunities to enhance automation by combining visual recognition with\nreasoning capabilities. In this paper, we introduce a new task for BCC:\nautomated facility enumeration, which involves validating the quantity of each\nfacility type against statutory requirements. To address it, we propose a novel\nmethod that integrates door detection with LLM-based reasoning. We are the\nfirst to apply LLMs to this task and further enhance their performance through\na Chain-of-Thought (CoT) pipeline. Our approach generalizes well across diverse\ndatasets and facility types. Experiments on both real-world and synthetic floor\nplan data demonstrate the effectiveness and robustness of our method.",
        "url": "http://arxiv.org/abs/2509.17283v1",
        "published_date": "2025-09-21T23:41:44+00:00",
        "updated_date": "2025-09-21T23:41:44+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.ET"
        ],
        "authors": [
            "Licheng Zhan",
            "Bach Le",
            "Naveed Akhtar",
            "Tuan Ngo"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "This paper introduces a new method for automating facility enumeration in building compliance checking using door detection and large language models, showing promising results on real-world and synthetic data.",
        "tldr_zh": "本文介绍了一种利用门检测和大型语言模型来自动化建筑合规检查中设施枚举的新方法，在真实世界和合成数据上展示了令人期待的结果。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "High Resolution UDF Meshing via Iterative Networks",
        "summary": "Unsigned Distance Fields (UDFs) are a natural implicit representation for\nopen surfaces but, unlike Signed Distance Fields (SDFs), are challenging to\ntriangulate into explicit meshes. This is especially true at high resolutions\nwhere neural UDFs exhibit higher noise levels, which makes it hard to capture\nfine details. Most current techniques perform within single voxels without\nreference to their neighborhood, resulting in missing surface and holes where\nthe UDF is ambiguous or noisy. We show that this can be remedied by performing\nseveral passes and by reasoning on previously extracted surface elements to\nincorporate neighborhood information. Our key contribution is an iterative\nneural network that does this and progressively improves surface recovery\nwithin each voxel by spatially propagating information from increasingly\ndistant neighbors. Unlike single-pass methods, our approach integrates newly\ndetected surfaces, distance values, and gradients across multiple iterations,\neffectively correcting errors and stabilizing extraction in challenging\nregions. Experiments on diverse 3D models demonstrate that our method produces\nsignificantly more accurate and complete meshes than existing approaches,\nparticularly for complex geometries, enabling UDF surface extraction at higher\nresolutions where traditional methods fail.",
        "url": "http://arxiv.org/abs/2509.17212v1",
        "published_date": "2025-09-21T19:39:54+00:00",
        "updated_date": "2025-09-21T19:39:54+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Federico Stella",
            "Nicolas Talabot",
            "Hieu Le",
            "Pascal Fua"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper introduces an iterative neural network for high resolution meshing of Unsigned Distance Fields, improving surface recovery and mesh accuracy in challenging regions.",
        "tldr_zh": "本文引入了一种迭代神经网络，用于高分辨率的Unsigned Distance Fields网格化，改善了表面恢复和网格准确性。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction",
        "summary": "Reconstructing accurate surfaces with radiance fields has achieved remarkable\nprogress in recent years. However, prevailing approaches, primarily based on\nGaussian Splatting, are increasingly constrained by representational\nbottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based\nframework that explores and extends the under-investigated potential of sparse\nvoxels for achieving accurate, detailed, and complete surface reconstruction.\nAs strengths, sparse voxels support preserving the coverage completeness and\ngeometric clarity, while corresponding challenges also arise from absent scene\nconstraints and locality in surface refinement. To ensure correct scene\nconvergence, we first propose a Voxel-Uncertainty Depth Constraint that\nmaximizes the effect of monocular depth cues while presenting a voxel-oriented\nuncertainty to avoid quality degradation, enabling effective and robust scene\nconstraints yet preserving highly accurate geometries. Subsequently, Sparse\nVoxel Surface Regularization is designed to enhance geometric consistency for\ntiny voxels and facilitate the voxel-based formation of sharp and accurate\nsurfaces. Extensive experiments demonstrate our superior performance compared\nto existing methods across diverse challenging scenarios, excelling in\ngeometric accuracy, detail preservation, and reconstruction completeness while\nmaintaining high efficiency. Code is available at\nhttps://github.com/Fictionarry/GeoSVR.",
        "url": "http://arxiv.org/abs/2509.18090v1",
        "published_date": "2025-09-22T17:58:48+00:00",
        "updated_date": "2025-09-22T17:58:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahe Li",
            "Jiawei Zhang",
            "Youmin Zhang",
            "Xiao Bai",
            "Jin Zheng",
            "Xiaohan Yu",
            "Lin Gu"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "GeoSVR introduces a voxel-based framework for accurate surface reconstruction, addressing challenges like scene constraints and surface refinement.",
        "tldr_zh": "GeoSVR引入了一种基于体素的框架，用于准确的表面重建，解决了场景约束和表面细化等挑战。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Detection of Misreporting Attacks on Software-Defined Immersive Environments",
        "summary": "The ability to centrally control network infrastructure using a programmable\nmiddleware has made Software-Defined Networking (SDN) ideal for emerging\napplications, such as immersive environments. However, such flexibility\nintroduces new vulnerabilities, such as switch misreporting led load imbalance,\nwhich in turn make such immersive environment vulnerable to severe quality\ndegradation. In this paper, we present a hybrid machine learning (ML)-based\nnetwork anomaly detection framework that identifies such stealthy misreporting\nby capturing temporal inconsistencies in switch-reported loads, and thereby\ncounter potentially catastrophic quality degradation of hosted immersive\napplication. The detection system combines unsupervised anomaly scoring with\nsupervised classification to robustly distinguish malicious behavior. Data\ncollected from a realistic testbed deployment under both benign and adversarial\nconditions is used to train and evaluate the model. Experimental results show\nthat the framework achieves high recall in detecting misreporting behavior,\nmaking it effective for early and reliable detection in SDN environments.",
        "url": "http://arxiv.org/abs/2509.18040v1",
        "published_date": "2025-09-22T17:14:40+00:00",
        "updated_date": "2025-09-22T17:14:40+00:00",
        "categories": [
            "cs.NI",
            "cs.CV"
        ],
        "authors": [
            "Sourya Saha",
            "Md Nurul Absur",
            "Shima Yousefi",
            "Saptarshi Debroy"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces a machine learning-based framework to detect misreporting attacks in Software-Defined Networking, focusing on preventing quality degradation in immersive environments.",
        "tldr_zh": "该论文介绍了一种基于机器学习的框架，用于检测软件定义网络中的虚报攻击，重点是防止沉浸式环境质量下降。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Joint Optimization of Memory Frequency, Computing Frequency, Transmission Power and Task Offloading for Energy-efficient DNN Inference",
        "summary": "Deep neural networks (DNNs) have been widely applied in diverse applications,\nbut the problems of high latency and energy overhead are inevitable on\nresource-constrained devices. To address this challenge, most researchers focus\non the dynamic voltage and frequency scaling (DVFS) technique to balance the\nlatency and energy consumption by changing the computing frequency of\nprocessors. However, the adjustment of memory frequency is usually ignored and\nnot fully utilized to achieve efficient DNN inference, which also plays a\nsignificant role in the inference time and energy consumption. In this paper,\nwe first investigate the impact of joint memory frequency and computing\nfrequency scaling on the inference time and energy consumption with a\nmodel-based and data-driven method. Then by combining with the fitting\nparameters of different DNN models, we give a preliminary analysis for the\nproposed model to see the effects of adjusting memory frequency and computing\nfrequency simultaneously. Finally, simulation results in local inference and\ncooperative inference cases further validate the effectiveness of jointly\nscaling the memory frequency and computing frequency to reduce the energy\nconsumption of devices.",
        "url": "http://arxiv.org/abs/2509.17970v1",
        "published_date": "2025-09-22T16:20:29+00:00",
        "updated_date": "2025-09-22T16:20:29+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Yunchu Han",
            "Zhaojun Nan",
            "Sheng Zhou",
            "Zhisheng Niu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper explores the joint optimization of memory frequency and computing frequency for energy-efficient DNN inference, showing promising results in reducing energy consumption.",
        "tldr_zh": "本文研究了内存频率和计算频率的联合优化，以实现节能的深度神经网络推断，在减少能耗方面取得了显著成果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 7.25
    },
    {
        "title": "DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous Driving",
        "summary": "End-to-end autonomous driving has substantially progressed by directly\npredicting future trajectories from raw perception inputs, which bypasses\ntraditional modular pipelines. However, mainstream methods trained via\nimitation learning suffer from critical safety limitations, as they fail to\ndistinguish between trajectories that appear human-like but are potentially\nunsafe. Some recent approaches attempt to address this by regressing multiple\nrule-driven scores but decoupling supervision from policy optimization,\nresulting in suboptimal performance. To tackle these challenges, we propose\nDriveDPO, a Safety Direct Preference Optimization Policy Learning framework.\nFirst, we distill a unified policy distribution from human imitation similarity\nand rule-based safety scores for direct policy optimization. Further, we\nintroduce an iterative Direct Preference Optimization stage formulated as\ntrajectory-level preference alignment. Extensive experiments on the NAVSIM\nbenchmark demonstrate that DriveDPO achieves a new state-of-the-art PDMS of\n90.0. Furthermore, qualitative results across diverse challenging scenarios\nhighlight DriveDPO's ability to produce safer and more reliable driving\nbehaviors.",
        "url": "http://arxiv.org/abs/2509.17940v1",
        "published_date": "2025-09-22T16:01:11+00:00",
        "updated_date": "2025-09-22T16:01:11+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Shuyao Shang",
            "Yuntao Chen",
            "Yuqi Wang",
            "Yingyan Li",
            "Zhaoxiang Zhang"
        ],
        "ai_categories": [
            "Multimodality",
            "GAN",
            "Dataset"
        ],
        "tldr": "DriveDPO proposes a policy learning framework for end-to-end autonomous driving, focusing on safety and performance. It achieves state-of-the-art results and produces safer driving behaviors.",
        "tldr_zh": "DriveDPO 提出了一个针对端到端自动驾驶的策略学习框架，着重于安全性和性能。它取得了最新的研究成果，并产生更安全的驾驶行为。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Multi-needle Localization for Pelvic Seed Implant Brachytherapy based on Tip-handle Detection and Matching",
        "summary": "Accurate multi-needle localization in intraoperative CT images is crucial for\noptimizing seed placement in pelvic seed implant brachytherapy. However, this\ntask is challenging due to poor image contrast and needle adhesion. This paper\npresents a novel approach that reframes needle localization as a tip-handle\ndetection and matching problem to overcome these difficulties. An anchor-free\nnetwork, based on HRNet, is proposed to extract multi-scale features and\naccurately detect needle tips and handles by predicting their centers and\norientations using decoupled branches for heatmap regression and polar angle\nprediction. To associate detected tips and handles into individual needles, a\ngreedy matching and merging (GMM) method designed to solve the unbalanced\nassignment problem with constraints (UAP-C) is presented. The GMM method\niteratively selects the most probable tip-handle pairs and merges them based on\na distance metric to reconstruct 3D needle paths. Evaluated on a dataset of 100\npatients, the proposed method demonstrates superior performance, achieving\nhigher precision and F1 score compared to a segmentation-based method utilizing\nthe nnUNet model,thereby offering a more robust and accurate solution for\nneedle localization in complex clinical scenarios.",
        "url": "http://arxiv.org/abs/2509.17931v1",
        "published_date": "2025-09-22T15:53:37+00:00",
        "updated_date": "2025-09-22T15:53:37+00:00",
        "categories": [
            "cs.CV",
            "physics.med-ph"
        ],
        "authors": [
            "Zhuo Xiao",
            "Fugen Zhou",
            "Jingjing Wang",
            "Chongyu He",
            "Bo Liu",
            "Haitao Sun",
            "Zhe Ji",
            "Yuliang Jiang",
            "Junjie Wang",
            "Qiuwen Wu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel approach for accurate multi-needle localization in pelvic seed implant brachytherapy using tip-handle detection and matching, achieving superior performance compared to existing methods.",
        "tldr_zh": "该论文提出了一种新颖的方法，通过端-柄检测和匹配，在盆腔种子植入治疗中实现了准确的多针定位，表现优于现有方法。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "SmaRT: Style-Modulated Robust Test-Time Adaptation for Cross-Domain Brain Tumor Segmentation in MRI",
        "summary": "Reliable brain tumor segmentation in MRI is indispensable for treatment\nplanning and outcome monitoring, yet models trained on curated benchmarks often\nfail under domain shifts arising from scanner and protocol variability as well\nas population heterogeneity. Such gaps are especially severe in low-resource\nand pediatric cohorts, where conventional test-time or source-free adaptation\nstrategies often suffer from instability and structural inconsistency. We\npropose SmaRT, a style-modulated robust test-time adaptation framework that\nenables source-free cross-domain generalization. SmaRT integrates style-aware\naugmentation to mitigate appearance discrepancies, a dual-branch momentum\nstrategy for stable pseudo-label refinement, and structural priors enforcing\nconsistency, integrity, and connectivity. This synergy ensures both adaptation\nstability and anatomical fidelity under extreme domain shifts. Extensive\nevaluations on sub-Saharan Africa and pediatric glioma datasets show that SmaRT\nconsistently outperforms state-of-the-art methods, with notable gains in Dice\naccuracy and boundary precision. Overall, SmaRT bridges the gap between\nalgorithmic advances and equitable clinical applicability, supporting robust\ndeployment of MRI-based neuro-oncology tools in diverse clinical environments.\nOur source code is available at https://github.com/baiyou1234/SmaRT.",
        "url": "http://arxiv.org/abs/2509.17925v1",
        "published_date": "2025-09-22T15:50:59+00:00",
        "updated_date": "2025-09-22T15:50:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuanhan Wang",
            "Yifei Chen",
            "Shuo Jiang",
            "Wenjing Yu",
            "Mingxuan Liu",
            "Beining Wu",
            "Jinying Zong",
            "Feiwei Qin",
            "Changmiao Wang",
            "Qiyuan Tian"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces SmaRT, a framework for robust brain tumor segmentation in MRI with cross-domain generalization, outperforming state-of-the-art methods.",
        "tldr_zh": "本文介绍了一种用于在MRI中进行强健的脑肿瘤分割的SmaRT框架，具有跨领域泛化能力，超越了现有技术。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "From Restoration to Reconstruction: Rethinking 3D Gaussian Splatting for Underwater Scenes",
        "summary": "Underwater image degradation poses significant challenges for 3D\nreconstruction, where simplified physical models often fail in complex scenes.\nWe propose \\textbf{R-Splatting}, a unified framework that bridges underwater\nimage restoration (UIR) with 3D Gaussian Splatting (3DGS) to improve both\nrendering quality and geometric fidelity. Our method integrates multiple\nenhanced views produced by diverse UIR models into a single reconstruction\npipeline. During inference, a lightweight illumination generator samples latent\ncodes to support diverse yet coherent renderings, while a contrastive loss\nensures disentangled and stable illumination representations. Furthermore, we\npropose \\textit{Uncertainty-Aware Opacity Optimization (UAOO)}, which models\nopacity as a stochastic function to regularize training. This suppresses abrupt\ngradient responses triggered by illumination variation and mitigates\noverfitting to noisy or view-specific artifacts. Experiments on Seathru-NeRF\nand our new BlueCoral3D dataset demonstrate that R-Splatting outperforms strong\nbaselines in both rendering quality and geometric accuracy.",
        "url": "http://arxiv.org/abs/2509.17789v1",
        "published_date": "2025-09-22T13:50:20+00:00",
        "updated_date": "2025-09-22T13:50:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guoxi Huang",
            "Haoran Wang",
            "Zipeng Qi",
            "Wenjun Lu",
            "David Bull",
            "Nantheera Anantrasirichai"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a unified framework called R-Splatting that combines underwater image restoration with 3D Gaussian Splatting to improve rendering quality and geometric fidelity in underwater scenes.",
        "tldr_zh": "本文提出了一种统一框架称为R-Splatting，将水下图像恢复与3D高斯喷洒结合起来，以提高水下场景中的渲染质量和几何保真度。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Automatic Intermodal Loading Unit Identification using Computer Vision: A Scoping Review",
        "summary": "The standardisation of Intermodal Loading Units (ILUs), such as containers,\nsemi-trailers and swap bodies, has revolutionised global trade yet their\nefficient and robust identification remains a critical bottleneck in\nhigh-throughput ports and terminals. This paper reviews 63 empirical studies\nthat propose computer vision (CV) based solutions. It covers the last 35 years\n(1990-2025), tracing the field's evolution from early digital image processing\n(DIP) and traditional machine learning (ML) to the current dominance of deep\nlearning (DL) techniques. While CV offers cost-effective alternatives for other\ntypes of identification techniques, its development is hindered by the lack of\npublicly available benchmarking datasets. This results in high variance for the\nreported results such as end-to-end accuracy ranging from 5 % to 96 %. Beyond\ndataset limitations, this review highlights the emerging challenges especially\nintroduced by the shift from character-based text recognition to scene-text\nspotting and the integration of mobile cameras (e.g. drones, sensor equipped\nground vehicles) for dynamic terminal monitoring. To advance the field, the\npaper calls for standardised terminology, open-access datasets, shared source\ncode, while outlining future research directions such as contextless text\nrecognition optimised for ISO6346 codes.",
        "url": "http://arxiv.org/abs/2509.17707v1",
        "published_date": "2025-09-22T12:45:35+00:00",
        "updated_date": "2025-09-22T12:45:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Emre Gülsoylu",
            "Alhassan Abdelhalim",
            "Derya Kara Boztas",
            "Ole Grasse",
            "Carlos Jahn",
            "Simone Frintrop",
            "Janick Edinger"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper reviews computer vision solutions for identifying Intermodal Loading Units (ILUs) like containers and semi-trailers in high-throughput ports and terminals, highlighting challenges and calling for standardized terminology and open-access datasets.",
        "tldr_zh": "本文回顾了用于在高通量港口和终端识别集装箱和半挂车等国际装卸单位 (ILUs) 的计算机视觉解决方案，突出挑战并呼吁标准化术语和开放访问数据集。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7.25
    },
    {
        "title": "SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models",
        "summary": "While vision language models (VLMs) excel in 2D semantic visual\nunderstanding, their ability to quantitatively reason about 3D spatial\nrelationships remains under-explored, due to the deficiency of 2D images'\nspatial representation ability. In this paper, we analyze the problem hindering\nVLMs' spatial understanding abilities and propose SD-VLM, a novel framework\nthat significantly enhances fundamental spatial perception abilities of VLMs\nthrough two key contributions: (1) propose Massive Spatial Measuring and\nUnderstanding (MSMU) dataset with precise spatial annotations, and (2)\nintroduce a simple depth positional encoding method strengthening VLMs' spatial\nawareness. MSMU dataset covers massive quantitative spatial tasks with 700K QA\npairs, 2.5M physical numerical annotations, and 10K chain-of-thought augmented\nsamples. We have trained SD-VLM, a strong generalist VLM which shows superior\nquantitative spatial measuring and understanding capability. SD-VLM not only\nachieves state-of-the-art performance on our proposed MSMU-Bench, but also\nshows spatial generalization abilities on other spatial understanding\nbenchmarks including Q-Spatial and SpatialRGPT-Bench. Extensive experiments\ndemonstrate that SD-VLM outperforms GPT-4o and Intern-VL3-78B by 26.91% and\n25.56% respectively on MSMU-Bench. Code and models are released at\nhttps://github.com/cpystan/SD-VLM.",
        "url": "http://arxiv.org/abs/2509.17664v1",
        "published_date": "2025-09-22T12:08:12+00:00",
        "updated_date": "2025-09-22T12:08:12+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Pingyi Chen",
            "Yujing Lou",
            "Shen Cao",
            "Jinhui Guo",
            "Lubin Fan",
            "Yue Wu",
            "Lin Yang",
            "Lizhuang Ma",
            "Jieping Ye"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces SD-VLM, a framework enhancing spatial perception abilities of Vision-Language Models through a new dataset and depth positional encoding method.",
        "tldr_zh": "本文引入了SD-VLM，通过一个新数据集和深度位置编码方法，增强了视觉语言模型的空间感知能力。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Tensor-Based Self-Calibration of Cameras via the TrifocalCalib Method",
        "summary": "Estimating camera intrinsic parameters without prior scene knowledge is a\nfundamental challenge in computer vision. This capability is particularly\nimportant for applications such as autonomous driving and vehicle platooning,\nwhere precalibrated setups are impractical and real-time adaptability is\nnecessary. To advance the state-of-the-art, we present a set of equations based\non the calibrated trifocal tensor, enabling projective camera self-calibration\nfrom minimal image data. Our method, termed TrifocalCalib, significantly\nimproves accuracy and robustness compared to both recent learning-based and\nclassical approaches. Unlike many existing techniques, our approach requires no\ncalibration target, imposes no constraints on camera motion, and simultaneously\nestimates both focal length and principal point. Evaluations in both\nprocedurally generated synthetic environments and structured dataset-based\nscenarios demonstrate the effectiveness of our approach. To support\nreproducibility, we make the code publicly available.",
        "url": "http://arxiv.org/abs/2509.17620v1",
        "published_date": "2025-09-22T11:31:57+00:00",
        "updated_date": "2025-09-22T11:31:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Gregory Schroeder",
            "Mohamed Sabry",
            "Cristina Olaverri-Monreal"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper introduces a new method, TrifocalCalib, for self-calibration of projective cameras without prior scene knowledge.",
        "tldr_zh": "该论文介绍了一种新的方法，TrifocalCalib，用于自校准投影相机，无需先验场景知识。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Domain Adaptive Object Detection for Space Applications with Real-Time Constraints",
        "summary": "Object detection is essential in space applications targeting Space Domain\nAwareness and also applications involving relative navigation scenarios.\nCurrent deep learning models for Object Detection in space applications are\noften trained on synthetic data from simulators, however, the model performance\ndrops significantly on real-world data due to the domain gap. However, domain\nadaptive object detection is an overlooked problem in the community. In this\nwork, we first show the importance of domain adaptation and then explore\nSupervised Domain Adaptation (SDA) to reduce this gap using minimal labeled\nreal data. We build on a recent semi-supervised adaptation method and tailor it\nfor object detection. Our approach combines domain-invariant feature learning\nwith a CNN-based domain discriminator and invariant risk minimization using a\ndomain-independent regression head. To meet real-time deployment needs, we test\nour method on a lightweight Single Shot Multibox Detector (SSD) with MobileNet\nbackbone and on the more advanced Fully Convolutional One-Stage object detector\n(FCOS) with ResNet-50 backbone. We evaluated on two space datasets, SPEED+ and\nSPARK. The results show up to 20-point improvements in average precision (AP)\nwith just 250 labeled real images.",
        "url": "http://arxiv.org/abs/2509.17593v1",
        "published_date": "2025-09-22T11:17:14+00:00",
        "updated_date": "2025-09-22T11:17:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Samet Hicsonmez",
            "Abd El Rahman Shabayek",
            "Arunkumar Rathinam",
            "Djamila Aouada"
        ],
        "ai_categories": [
            "Dataset",
            "GAN",
            "Transformer"
        ],
        "tldr": "The paper introduces a novel approach for domain adaptive object detection in space applications, achieving significant improvements with minimal labeled data.",
        "tldr_zh": "本文介绍了一种新颖的领域自适应目标检测方法，利用少量标记数据实现了显著的改进。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Chat-CBM: Towards Interactive Concept Bottleneck Models with Frozen Large Language Models",
        "summary": "Concept Bottleneck Models (CBMs) provide inherent interpretability by first\npredicting a set of human-understandable concepts and then mapping them to\nlabels through a simple classifier. While users can intervene in the concept\nspace to improve predictions, traditional CBMs typically employ a fixed linear\nclassifier over concept scores, which restricts interventions to manual value\nadjustments and prevents the incorporation of new concepts or domain knowledge\nat test time. These limitations are particularly severe in unsupervised CBMs,\nwhere concept activations are often noisy and densely activated, making user\ninterventions ineffective. We introduce Chat-CBM, which replaces score-based\nclassifiers with a language-based classifier that reasons directly over concept\nsemantics. By grounding prediction in the semantic space of concepts, Chat-CBM\npreserves the interpretability of CBMs while enabling richer and more intuitive\ninterventions, such as concept correction, addition or removal of concepts,\nincorporation of external knowledge, and high-level reasoning guidance.\nLeveraging the language understanding and few-shot capabilities of frozen large\nlanguage models, Chat-CBM extends the intervention interface of CBMs beyond\nnumerical editing and remains effective even in unsupervised settings.\nExperiments on nine datasets demonstrate that Chat-CBM achieves higher\npredictive performance and substantially improves user interactivity while\nmaintaining the concept-based interpretability of CBMs.",
        "url": "http://arxiv.org/abs/2509.17522v1",
        "published_date": "2025-09-22T08:48:04+00:00",
        "updated_date": "2025-09-22T08:48:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hangzhou He",
            "Lei Zhu",
            "Kaiwen Li",
            "Xinliang Zhang",
            "Jiakui Hu",
            "Ourui Fu",
            "Zhengjian Yao",
            "Yanye Lu"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces Chat-CBM, a model that enhances Concept Bottleneck Models by using a language-based classifier for more intuitive user interventions and improved predictive performance.",
        "tldr_zh": "这篇论文介绍了Chat-CBM，一个通过使用基于语言的分类器来增强概念瓶颈模型的模型，以实现更直观的用户干预和改进的预测性能。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "MAESTRO: Task-Relevant Optimization via Adaptive Feature Enhancement and Suppression for Multi-task 3D Perception",
        "summary": "The goal of multi-task learning is to learn to conduct multiple tasks\nsimultaneously based on a shared data representation. While this approach can\nimprove learning efficiency, it may also cause performance degradation due to\ntask conflicts that arise when optimizing the model for different objectives.\nTo address this challenge, we introduce MAESTRO, a structured framework\ndesigned to generate task-specific features and mitigate feature interference\nin multi-task 3D perception, including 3D object detection, bird's-eye view\n(BEV) map segmentation, and 3D occupancy prediction. MAESTRO comprises three\ncomponents: the Class-wise Prototype Generator (CPG), the Task-Specific Feature\nGenerator (TSFG), and the Scene Prototype Aggregator (SPA). CPG groups class\ncategories into foreground and background groups and generates group-wise\nprototypes. The foreground and background prototypes are assigned to the 3D\nobject detection task and the map segmentation task, respectively, while both\nare assigned to the 3D occupancy prediction task. TSFG leverages these\nprototype groups to retain task-relevant features while suppressing irrelevant\nfeatures, thereby enhancing the performance for each task. SPA enhances the\nprototype groups assigned for 3D occupancy prediction by utilizing the\ninformation produced by the 3D object detection head and the map segmentation\nhead. Extensive experiments on the nuScenes and Occ3D benchmarks demonstrate\nthat MAESTRO consistently outperforms existing methods across 3D object\ndetection, BEV map segmentation, and 3D occupancy prediction tasks.",
        "url": "http://arxiv.org/abs/2509.17462v1",
        "published_date": "2025-09-22T07:55:43+00:00",
        "updated_date": "2025-09-22T07:55:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Changwon Kang",
            "Jisong Kim",
            "Hongjae Shin",
            "Junseo Park",
            "Jun Won Choi"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "MAESTRO is a framework designed for multi-task 3D perception, improving tasks like object detection and map segmentation by generating task-specific features and mitigating feature interference.",
        "tldr_zh": "MAESTRO 是一个针对多任务3D感知的框架，通过生成任务特定特征和减少特征干扰，提高了对象检测和地图分割等任务的性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Training-Free Label Space Alignment for Universal Domain Adaptation",
        "summary": "Universal domain adaptation (UniDA) transfers knowledge from a labeled source\ndomain to an unlabeled target domain, where label spaces may differ and the\ntarget domain may contain private classes. Previous UniDA methods primarily\nfocused on visual space alignment but often struggled with visual ambiguities\ndue to content differences, which limited their robustness and\ngeneralizability. To overcome this, we introduce a novel approach that\nleverages the strong \\textit{zero-shot capabilities} of recent vision-language\nfoundation models (VLMs) like CLIP, concentrating solely on label space\nalignment to enhance adaptation stability. CLIP can generate task-specific\nclassifiers based only on label names. However, adapting CLIP to UniDA is\nchallenging because the label space is not fully known in advance. In this\nstudy, we first utilize generative vision-language models to identify unknown\ncategories in the target domain. Noise and semantic ambiguities in the\ndiscovered labels -- such as those similar to source labels (e.g., synonyms,\nhypernyms, hyponyms) -- complicate label alignment. To address this, we propose\na training-free label-space alignment method for UniDA (\\ours). Our method\naligns label spaces instead of visual spaces by filtering and refining noisy\nlabels between the domains. We then construct a \\textit{universal classifier}\nthat integrates both shared knowledge and target-private class information,\nthereby improving generalizability under domain shifts. The results reveal that\nthe proposed method considerably outperforms existing UniDA techniques across\nkey DomainBed benchmarks, delivering an average improvement of\n\\textcolor{blue}{+7.9\\%}in H-score and \\textcolor{blue}{+6.1\\%} in H$^3$-score.\nFurthermore, incorporating self-training further enhances performance and\nachieves an additional (\\textcolor{blue}{+1.6\\%}) increment in both H- and\nH$^3$-scores.",
        "url": "http://arxiv.org/abs/2509.17452v1",
        "published_date": "2025-09-22T07:46:10+00:00",
        "updated_date": "2025-09-22T07:46:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Dujin Lee",
            "Sojung An",
            "Jungmyung Wi",
            "Kuniaki Saito",
            "Donghyun Kim"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a new method for universal domain adaptation using label space alignment leveraging vision-language models like CLIP, outperforming existing techniques across benchmarks.",
        "tldr_zh": "本文介绍了一种利用视觉-语言模型如CLIP进行标签空间对齐的通用领域自适应新方法，在基准测试中优于现有技术。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Beyond Diagnosis: Evaluating Multimodal LLMs for Pathology Localization in Chest Radiographs",
        "summary": "Recent work has shown promising performance of frontier large language models\n(LLMs) and their multimodal counterparts in medical quizzes and diagnostic\ntasks, highlighting their potential for broad clinical utility given their\naccessible, general-purpose nature. However, beyond diagnosis, a fundamental\naspect of medical image interpretation is the ability to localize pathological\nfindings. Evaluating localization not only has clinical and educational\nrelevance but also provides insight into a model's spatial understanding of\nanatomy and disease. Here, we systematically assess two general-purpose MLLMs\n(GPT-4 and GPT-5) and a domain-specific model (MedGemma) in their ability to\nlocalize pathologies on chest radiographs, using a prompting pipeline that\noverlays a spatial grid and elicits coordinate-based predictions. Averaged\nacross nine pathologies in the CheXlocalize dataset, GPT-5 exhibited a\nlocalization accuracy of 49.7%, followed by GPT-4 (39.1%) and MedGemma (17.7%),\nall lower than a task-specific CNN baseline (59.9%) and a radiologist benchmark\n(80.1%). Despite modest performance, error analysis revealed that GPT-5's\npredictions were largely in anatomically plausible regions, just not always\nprecisely localized. GPT-4 performed well on pathologies with fixed anatomical\nlocations, but struggled with spatially variable findings and exhibited\nanatomically implausible predictions more frequently. MedGemma demonstrated the\nlowest performance on all pathologies, showing limited capacity to generalize\nto this novel task. Our findings highlight both the promise and limitations of\ncurrent MLLMs in medical imaging and underscore the importance of integrating\nthem with task-specific tools for reliable use.",
        "url": "http://arxiv.org/abs/2509.18015v1",
        "published_date": "2025-09-22T16:54:23+00:00",
        "updated_date": "2025-09-22T16:54:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Advait Gosai",
            "Arun Kavishwar",
            "Stephanie L. McNamara",
            "Soujanya Samineni",
            "Renato Umeton",
            "Alexander Chowdhury",
            "William Lotter"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper evaluates the performance of large language models (LLMs) in localizing pathologies in chest radiographs, highlighting promise but also limitations in their spatial understanding.",
        "tldr_zh": "本文评估了大语言模型（LLMs）在胸部X射线检查中定位病变的表现，突出了它们在空间理解方面的潜力和局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Intra-Cluster Mixup: An Effective Data Augmentation Technique for Complementary-Label Learning",
        "summary": "In this paper, we investigate the challenges of complementary-label learning\n(CLL), a specialized form of weakly-supervised learning (WSL) where models are\ntrained with labels indicating classes to which instances do not belong, rather\nthan standard ordinary labels. This alternative supervision is appealing\nbecause collecting complementary labels is generally cheaper and less\nlabor-intensive. Although most existing research in CLL emphasizes the\ndevelopment of novel loss functions, the potential of data augmentation in this\ndomain remains largely underexplored. In this work, we uncover that the\nwidely-used Mixup data augmentation technique is ineffective when directly\napplied to CLL. Through in-depth analysis, we identify that the\ncomplementary-label noise generated by Mixup negatively impacts the performance\nof CLL models. We then propose an improved technique called Intra-Cluster Mixup\n(ICM), which only synthesizes augmented data from nearby examples, to mitigate\nthe noise effect. ICM carries the benefits of encouraging complementary label\nsharing of nearby examples, and leads to substantial performance improvements\nacross synthetic and real-world labeled datasets. In particular, our wide\nspectrum of experimental results on both balanced and imbalanced CLL settings\njustifies the potential of ICM in allying with state-of-the-art CLL algorithms,\nachieving significant accuracy increases of 30% and 10% on MNIST and CIFAR\ndatasets, respectively.",
        "url": "http://arxiv.org/abs/2509.17971v1",
        "published_date": "2025-09-22T16:20:41+00:00",
        "updated_date": "2025-09-22T16:20:41+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Tan-Ha Mai",
            "Hsuan-Tien Lin"
        ],
        "ai_categories": [
            "Dataset",
            "GAN"
        ],
        "tldr": "The paper introduces a new data augmentation technique, Intra-Cluster Mixup (ICM), to improve complementary-label learning (CLL) models by synthesizing augmented data from nearby examples.",
        "tldr_zh": "该论文提出了一种新的数据增强技术，Intra-Cluster Mixup (ICM)，通过从相邻样本合成增强数据来改善互补标签学习（CLL）模型。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Visual Detector Compression via Location-Aware Discriminant Analysis",
        "summary": "Deep neural networks are powerful, yet their high complexity greatly limits\ntheir potential to be deployed on billions of resource-constrained edge\ndevices. Pruning is a crucial network compression technique, yet most existing\nmethods focus on classification models, with limited attention to detection.\nEven among those addressing detection, there is a lack of utilization of\nessential localization information. Also, many pruning methods passively rely\non pre-trained models, in which useful and useless components are intertwined,\nmaking it difficult to remove the latter without harming the former at the\nneuron/filter level. To address the above issues, in this paper, we propose a\nproactive detection-discriminants-based network compression approach for deep\nvisual detectors, which alternates between two steps: (1) maximizing and\ncompressing detection-related discriminants and aligning them with a subset of\nneurons/filters immediately before the detection head, and (2) tracing the\ndetection-related discriminating power across the layers and discarding\nfeatures of lower importance. Object location information is exploited in both\nsteps. Extensive experiments, employing four advanced detection models and four\nstate-of-the-art competing methods on the KITTI and COCO datasets, highlight\nthe superiority of our approach. Remarkably, our compressed models can even\nbeat the original base models with a substantial reduction in complexity.",
        "url": "http://arxiv.org/abs/2509.17968v1",
        "published_date": "2025-09-22T16:19:00+00:00",
        "updated_date": "2025-09-22T16:19:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qizhen Lan",
            "Jung Im Choi",
            "Qing Tian"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a proactive detection-discriminants-based network compression approach for deep visual detectors by maximizing and compressing detection-related discriminants, leading to superior performance compared to existing methods.",
        "tldr_zh": "本文提出了一种积极的基于检测区分器的深度视觉检测器网络压缩方法，通过最大化和压缩与检测相关的区分器，与现有方法相比，获得了更优越的性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Breaking the Discretization Barrier of Continuous Physics Simulation Learning",
        "summary": "The modeling of complicated time-evolving physical dynamics from partial\nobservations is a long-standing challenge. Particularly, observations can be\nsparsely distributed in a seemingly random or unstructured manner, making it\ndifficult to capture highly nonlinear features in a variety of scientific and\nengineering problems. However, existing data-driven approaches are often\nconstrained by fixed spatial and temporal discretization. While some\nresearchers attempt to achieve spatio-temporal continuity by designing novel\nstrategies, they either overly rely on traditional numerical methods or fail to\ntruly overcome the limitations imposed by discretization. To address these, we\npropose CoPS, a purely data-driven methods, to effectively model continuous\nphysics simulation from partial observations. Specifically, we employ\nmultiplicative filter network to fuse and encode spatial information with the\ncorresponding observations. Then we customize geometric grids and use\nmessage-passing mechanism to map features from original spatial domain to the\ncustomized grids. Subsequently, CoPS models continuous-time dynamics by\ndesigning multi-scale graph ODEs, while introducing a Markov-based neural\nauto-correction module to assist and constrain the continuous extrapolations.\nComprehensive experiments demonstrate that CoPS advances the state-of-the-art\nmethods in space-time continuous modeling across various scenarios.",
        "url": "http://arxiv.org/abs/2509.17955v1",
        "published_date": "2025-09-22T16:10:58+00:00",
        "updated_date": "2025-09-22T16:10:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fan Xu",
            "Hao Wu",
            "Nan Wang",
            "Lilan Peng",
            "Kun Wang",
            "Wei Gong",
            "Xibin Zhao"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper presents CoPS, a data-driven method to model continuous physics simulation from partial observations, advancing the state-of-the-art in space-time continuous modeling.",
        "tldr_zh": "本文提出了CoPS，一种数据驱动的方法，用于从部分观测中对连续物理模拟进行建模，提升了时空连续建模的最新技术。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "DragOSM: Extract Building Roofs and Footprints from Aerial Images by Aligning Historical Labels",
        "summary": "Extracting polygonal roofs and footprints from remote sensing images is\ncritical for large-scale urban analysis. Most existing methods rely on\nsegmentation-based models that assume clear semantic boundaries of roofs, but\nthese approaches struggle in off- nadir images, where the roof and footprint\nare significantly displaced, and facade pixels are fused with the roof\nboundary. With the increasing availability of open vector map annotations,\ne.g., OpenStreetMap, utilizing historical labels for off-nadir image annotation\nhas become viable because remote sensing images are georeferenced once\ncaptured. However, these historical labels commonly suffer from significant\npositional discrepancies with new images and only have one annotation (roof or\nfootprint), which fails to describe the correct structures of a building. To\naddress these discrepancies, we first introduce a concept of an alignment\ntoken, which encodes the correction vector to guide the label correction. Based\non this concept, we then propose Drag OpenStreetMap Labels (DragOSM), a novel\nmodel designed to align dislocated historical labels with roofs and footprints.\nSpecifically, DragOSM formulates the label alignment as an interactive\ndenoising process, modeling the positional discrepancy as a Gaussian\ndistribution. During training, it learns to correct these errors by simulating\nmisalignment with random Gaussian perturbations; during inference, it\niteratively refines the positions of input labels. To validate our method, we\nfurther present a new dataset, Repairing Buildings in OSM (ReBO), comprising\n179,265 buildings with both OpenStreetMap and manually corrected annotations\nacross 5,473 images from 41 cities. Experimental results on ReBO demonstrate\nthe effectiveness of DragOSM. Code, dataset, and trained models are publicly\navailable at https://github.com/likaiucas/DragOSM.git.",
        "url": "http://arxiv.org/abs/2509.17951v1",
        "published_date": "2025-09-22T16:10:13+00:00",
        "updated_date": "2025-09-22T16:10:13+00:00",
        "categories": [
            "cs.CV",
            "I.5.4"
        ],
        "authors": [
            "Kai Li",
            "Xingxing Weng",
            "Yupeng Deng",
            "Yu Meng",
            "Chao Pang",
            "Gui-Song Xia",
            "Xiangyu Zhao"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces DragOSM, a model that aligns historical map labels with roofs and footprints in aerial images to improve accuracy in off-nadir images, demonstrating effectiveness on a new dataset called ReBO.",
        "tldr_zh": "该论文介绍了DragOSM模型，该模型将历史地图标签与航拍图像中的屋顶和建筑轮廓对齐，从而提高了在off-nadir图像中的准确性，并在名为ReBO的新数据集上进行了验证。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion",
        "summary": "This paper considers the problem of enabling robots to navigate dynamic\nenvironments while following instructions. The challenge lies in the\ncombinatorial nature of instruction specifications: each instruction can\ninclude multiple specifications, and the number of possible specification\ncombinations grows exponentially as the robot's skill set expands. For example,\n\"overtake the pedestrian while staying on the right side of the road\" consists\nof two specifications: \"overtake the pedestrian\" and \"walk on the right side of\nthe road.\" To tackle this challenge, we propose ComposableNav, based on the\nintuition that following an instruction involves independently satisfying its\nconstituent specifications, each corresponding to a distinct motion primitive.\nUsing diffusion models, ComposableNav learns each primitive separately, then\ncomposes them in parallel at deployment time to satisfy novel combinations of\nspecifications unseen in training. Additionally, to avoid the onerous need for\ndemonstrations of individual motion primitives, we propose a two-stage training\nprocedure: (1) supervised pre-training to learn a base diffusion model for\ndynamic navigation, and (2) reinforcement learning fine-tuning that molds the\nbase model into different motion primitives. Through simulation and real-world\nexperiments, we show that ComposableNav enables robots to follow instructions\nby generating trajectories that satisfy diverse and unseen combinations of\nspecifications, significantly outperforming both non-compositional VLM-based\npolicies and costmap composing baselines. Videos and additional materials can\nbe found on the project page: https://amrl.cs.utexas.edu/ComposableNav/",
        "url": "http://arxiv.org/abs/2509.17941v1",
        "published_date": "2025-09-22T16:04:50+00:00",
        "updated_date": "2025-09-22T16:04:50+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Zichao Hu",
            "Chen Tang",
            "Michael J. Munje",
            "Yifeng Zhu",
            "Alex Liu",
            "Shuijing Liu",
            "Garrett Warnell",
            "Peter Stone",
            "Joydeep Biswas"
        ],
        "ai_categories": [
            "Diffusion"
        ],
        "tldr": "ComposableNav is a method for enabling robots to navigate dynamic environments while following complex instructions by independently learning and composing motion primitives.",
        "tldr_zh": "ComposableNav是一种方法，使机器人能够在遵循复杂指令的同时独立学习和组合运动基元来导航动态环境。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Trainee Action Recognition through Interaction Analysis in CCATT Mixed-Reality Training",
        "summary": "This study examines how Critical Care Air Transport Team (CCATT) members are\ntrained using mixed-reality simulations that replicate the high-pressure\nconditions of aeromedical evacuation. Each team - a physician, nurse, and\nrespiratory therapist - must stabilize severely injured soldiers by managing\nventilators, IV pumps, and suction devices during flight. Proficient\nperformance requires clinical expertise and cognitive skills, such as\nsituational awareness, rapid decision-making, effective communication, and\ncoordinated task management, all of which must be maintained under stress.\nRecent advances in simulation and multimodal data analytics enable more\nobjective and comprehensive performance evaluation. In contrast, traditional\ninstructor-led assessments are subjective and may overlook critical events,\nthereby limiting generalizability and consistency. However, AI-based automated\nand more objective evaluation metrics still demand human input to train the AI\nalgorithms to assess complex team dynamics in the presence of environmental\nnoise and the need for accurate re-identification in multi-person tracking. To\naddress these challenges, we introduce a systematic, data-driven assessment\nframework that combines Cognitive Task Analysis (CTA) with Multimodal Learning\nAnalytics (MMLA). We have developed a domain-specific CTA model for CCATT\ntraining and a vision-based action recognition pipeline using a fine-tuned\nHuman-Object Interaction model, the Cascade Disentangling Network (CDN), to\ndetect and track trainee-equipment interactions over time. These interactions\nautomatically yield performance indicators (e.g., reaction time, task\nduration), which are mapped onto a hierarchical CTA model tailored to CCATT\noperations, enabling interpretable, domain-relevant performance evaluations.",
        "url": "http://arxiv.org/abs/2509.17888v1",
        "published_date": "2025-09-22T15:19:45+00:00",
        "updated_date": "2025-09-22T15:19:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Divya Mereddy",
            "Marcos Quinones-Grueiro",
            "Ashwin T S",
            "Eduardo Davalos",
            "Gautam Biswas",
            "Kent Etherton",
            "Tyler Davis",
            "Katelyn Kay",
            "Jill Lear",
            "Benjamin Goldberg"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper explores how AI can be used to train Critical Care Air Transport Team (CCATT) members through mixed-reality simulations, improving performance evaluation and training effectiveness.",
        "tldr_zh": "本文探讨了如何利用人工智能培训关键护理航空运输小组（CCATT）成员，通过混合现实模拟提高表现评估和培训效果。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "TASO: Task-Aligned Sparse Optimization for Parameter-Efficient Model Adaptation",
        "summary": "LoRA has become one of the most widely used parameter-efficient fine-tuning\nmethods due to its simplicity and effectiveness. However, numerous studies have\nshown that LoRA often introduces substantial parameter redundancy, which not\nonly increases the number of trainable parameters but also hinders the\neffectiveness of fine-tuning. Since identifying redundant parameters in LoRA is\ninherently difficult, how to eliminate them efficiently and accurately remains\na challenging problem. In this paper, we propose TASO, a redundancy reduction\nmethod that leverages importance information from the pretrained model's\nweights to mitigate LoRA redundancy. Specifically, we estimate parameter\nimportance on downstream tasks and identify task-specific core regions based on\nthe distribution of importance scores. The location information of these core\nregions is then used to determine the sparse structure of LoRA modules,\nenabling redundancy removal before fine-tuning. Our approach significantly\nreduces the number of trainable parameters required for task adaptation, while\nproviding a novel task-aligned perspective for LoRA redundancy reduction.\nExperimental results demonstrate that, with a parameter budget comparable to\nLoRA with rank $r = 1$, TASO consistently outperforms standard LoRA across\nmultiple tasks, achieving strong fine-tuning performance while effectively\neliminating redundant parameters.",
        "url": "http://arxiv.org/abs/2509.17688v1",
        "published_date": "2025-09-22T12:29:43+00:00",
        "updated_date": "2025-09-22T12:29:43+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Daiye Miao",
            "Yufang Liu",
            "Jie Wang",
            "Changzhi Sun",
            "Yunke Zhang",
            "Demei Yan",
            "Shaokang Dong",
            "Qi Zhang",
            "Yuanbin Wu"
        ],
        "ai_categories": [
            "LoRA"
        ],
        "tldr": "The paper proposes TASO, a method for reducing redundancy in fine-tuning models by leveraging importance information from pretrained model weights, outperforming standard methods in multiple tasks.",
        "tldr_zh": "本文提出了 TASO 方法，通过利用预训练模型权重的重要性信息来减少微调模型中的冗余， 在多个任务中胜过标准方法。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.0
    },
    {
        "title": "Development and validation of an AI foundation model for endoscopic diagnosis of esophagogastric junction adenocarcinoma: a cohort and deep learning study",
        "summary": "The early detection of esophagogastric junction adenocarcinoma (EGJA) is\ncrucial for improving patient prognosis, yet its current diagnosis is highly\noperator-dependent. This paper aims to make the first attempt to develop an\nartificial intelligence (AI) foundation model-based method for both screening\nand staging diagnosis of EGJA using endoscopic images. In this cohort and\nlearning study, we conducted a multicentre study across seven Chinese hospitals\nbetween December 28, 2016 and December 30, 2024. It comprises 12,302 images\nfrom 1,546 patients; 8,249 of them were employed for model training, while the\nremaining were divided into the held-out (112 patients, 914 images), external\n(230 patients, 1,539 images), and prospective (198 patients, 1,600 images) test\nsets for evaluation. The proposed model employs DINOv2 (a vision foundation\nmodel) and ResNet50 (a convolutional neural network) to extract features of\nglobal appearance and local details of endoscopic images for EGJA staging\ndiagnosis. Our model demonstrates satisfactory performance for EGJA staging\ndiagnosis across three test sets, achieving an accuracy of 0.9256, 0.8895, and\n0.8956, respectively. In contrast, among representative AI models, the best one\n(ResNet50) achieves an accuracy of 0.9125, 0.8382, and 0.8519 on the three test\nsets, respectively; the expert endoscopists achieve an accuracy of 0.8147 on\nthe held-out test set. Moreover, with the assistance of our model, the overall\naccuracy for the trainee, competent, and expert endoscopists improves from\n0.7035, 0.7350, and 0.8147 to 0.8497, 0.8521, and 0.8696, respectively. To our\nknowledge, our model is the first application of foundation models for EGJA\nstaging diagnosis and demonstrates great potential in both diagnostic accuracy\nand efficiency.",
        "url": "http://arxiv.org/abs/2509.17660v1",
        "published_date": "2025-09-22T12:03:40+00:00",
        "updated_date": "2025-09-22T12:03:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yikun Ma",
            "Bo Li",
            "Ying Chen",
            "Zijie Yue",
            "Shuchang Xu",
            "Jingyao Li",
            "Lei Ma",
            "Liang Zhong",
            "Duowu Zou",
            "Leiming Xu",
            "Yunshi Zhong",
            "Xiaobo Li",
            "Weiqun Ding",
            "Minmin Zhang",
            "Dongli He",
            "Zhenghong Li",
            "Ye Chen",
            "Ye Zhao",
            "Jialong Zhuo",
            "Xiaofen Wu",
            "Lisha Yi",
            "Miaojing Shi",
            "Huihui Sun"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper presents an artificial intelligence foundation model for diagnosing esophagogastric junction adenocarcinoma using endoscopic images, showing promising results compared to existing AI models and expert endoscopists.",
        "tldr_zh": "该论文提出了一种人工智能基础模型，用于使用内窥镜图像诊断食管胃交界腺癌，与现有人工智能模型和专家内镜医生相比，表现出较好的结果。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Overview of PlantCLEF 2025: Multi-Species Plant Identification in Vegetation Quadrat Images",
        "summary": "Quadrat images are essential for ecological studies, as they enable\nstandardized sampling, the assessment of plant biodiversity, long-term\nmonitoring, and large-scale field campaigns. These images typically cover an\narea of fifty centimetres or one square meter, and botanists carefully identify\nall the species present. Integrating AI could help specialists accelerate their\ninventories and expand the spatial coverage of ecological studies. To assess\nprogress in this area, the PlantCLEF 2025 challenge relies on a new test set of\n2,105 high-resolution multi-label images annotated by experts and covering\naround 400 species. It also provides a large training set of 1.4 million\nindividual plant images, along with vision transformer models pre-trained on\nthis data. The task is formulated as a (weakly labelled) multi-label\nclassification problem, where the goal is to predict all species present in a\nquadrat image using single-label training data. This paper provides a detailed\ndescription of the data, the evaluation methodology, the methods and models\nused by participants, and the results achieved.",
        "url": "http://arxiv.org/abs/2509.17602v1",
        "published_date": "2025-09-22T11:21:53+00:00",
        "updated_date": "2025-09-22T11:21:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Giulio Martellucci",
            "Herve Goeau",
            "Pierre Bonnet",
            "Fabrice Vinatier",
            "Alexis Joly"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper discusses the PlantCLEF 2025 challenge on multi-species plant identification in vegetation quadrat images using AI, providing detailed data, methodology, participant methods, and results.",
        "tldr_zh": "该论文讨论了使用人工智能在植被四分环图像中进行多物种植物识别的PlantCLEF 2025挑战，提供了详细的数据、方法论、参与者方法和结果。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "4D-MoDe: Towards Editable and Scalable Volumetric Streaming via Motion-Decoupled 4D Gaussian Compression",
        "summary": "Volumetric video has emerged as a key medium for immersive telepresence and\naugmented/virtual reality, enabling six-degrees-of-freedom (6DoF) navigation\nand realistic spatial interactions. However, delivering high-quality dynamic\nvolumetric content at scale remains challenging due to massive data volume,\ncomplex motion, and limited editability of existing representations. In this\npaper, we present 4D-MoDe, a motion-decoupled 4D Gaussian compression framework\ndesigned for scalable and editable volumetric video streaming. Our method\nintroduces a layered representation that explicitly separates static\nbackgrounds from dynamic foregrounds using a lookahead-based motion\ndecomposition strategy, significantly reducing temporal redundancy and enabling\nselective background/foreground streaming. To capture continuous motion\ntrajectories, we employ a multi-resolution motion estimation grid and a\nlightweight shared MLP, complemented by a dynamic Gaussian compensation\nmechanism to model emergent content. An adaptive grouping scheme dynamically\ninserts background keyframes to balance temporal consistency and compression\nefficiency. Furthermore, an entropy-aware training pipeline jointly optimizes\nthe motion fields and Gaussian parameters under a rate-distortion (RD)\nobjective, while employing range-based and KD-tree compression to minimize\nstorage overhead. Extensive experiments on multiple datasets demonstrate that\n4D-MoDe consistently achieves competitive reconstruction quality with an order\nof magnitude lower storage cost (e.g., as low as \\textbf{11.4} KB/frame)\ncompared to state-of-the-art methods, while supporting practical applications\nsuch as background replacement and foreground-only streaming.",
        "url": "http://arxiv.org/abs/2509.17506v1",
        "published_date": "2025-09-22T08:35:46+00:00",
        "updated_date": "2025-09-22T08:35:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Houqiang Zhong",
            "Zihan Zheng",
            "Qiang Hu",
            "Yuan Tian",
            "Ning Cao",
            "Lan Xu",
            "Xiaoyun Zhang",
            "Zhengxue Cheng",
            "Li Song",
            "Wenjun Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces 4D-MoDe, a framework for scalable and editable volumetric video streaming using motion-decoupled 4D Gaussian compression, achieving high-quality content at a low storage cost.",
        "tldr_zh": "本文引入了4D-MoDe，使用运动解耦的4D高斯压缩框架，实现可扩展和可编辑的体积视频流，以极低的存储成本实现高质量内容。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Multi-scale Temporal Prediction via Incremental Generation and Multi-agent Collaboration",
        "summary": "Accurate temporal prediction is the bridge between comprehensive scene\nunderstanding and embodied artificial intelligence. However, predicting\nmultiple fine-grained states of a scene at multiple temporal scales is\ndifficult for vision-language models. We formalize the Multi-Scale Temporal\nPrediction (MSTP) task in general and surgical scenes by decomposing\nmulti-scale into two orthogonal dimensions: the temporal scale, forecasting\nstates of humans and surgery at varying look-ahead intervals, and the state\nscale, modeling a hierarchy of states in general and surgical scenes. For\nexample, in general scenes, states of contact relationships are finer-grained\nthan states of spatial relationships. In surgical scenes, medium-level steps\nare finer-grained than high-level phases yet remain constrained by their\nencompassing phase. To support this unified task, we introduce the first MSTP\nBenchmark, featuring synchronized annotations across multiple state scales and\ntemporal scales. We further propose a method, Incremental Generation and\nMulti-agent Collaboration (IG-MC), which integrates two key innovations. First,\nwe present a plug-and-play incremental generation module that continuously\nsynthesizes up-to-date visual previews at expanding temporal scales to inform\nmultiple decision-making agents, keeping decisions and generated visuals\nsynchronized and preventing performance degradation as look-ahead intervals\nlengthen. Second, we present a decision-driven multi-agent collaboration\nframework for multi-state prediction, comprising generation, initiation, and\nmulti-state assessment agents that dynamically trigger and evaluate prediction\ncycles to balance global coherence and local fidelity.",
        "url": "http://arxiv.org/abs/2509.17429v1",
        "published_date": "2025-09-22T07:22:27+00:00",
        "updated_date": "2025-09-22T07:22:27+00:00",
        "categories": [
            "cs.CV",
            "68T45",
            "I.2.10"
        ],
        "authors": [
            "Zhitao Zeng",
            "Guojian Yuan",
            "Junyuan Mao",
            "Yuxuan Wang",
            "Xiaoshuang Jia",
            "Yueming Jin"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a Multi-Scale Temporal Prediction task and proposes a method called Incremental Generation and Multi-agent Collaboration for this task.",
        "tldr_zh": "本文介绍了一个多尺度时间预测任务，并提出了一种称为增量生成和多智能体协作的方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Real-Time Fish Detection in Indonesian Marine Ecosystems Using Lightweight YOLOv10-nano Architecture",
        "summary": "Indonesia's marine ecosystems, part of the globally recognized Coral\nTriangle, are among the richest in biodiversity, requiring efficient monitoring\ntools to support conservation. Traditional fish detection methods are\ntime-consuming and demand expert knowledge, prompting the need for automated\nsolutions. This study explores the implementation of YOLOv10-nano, a\nstate-of-the-art deep learning model, for real-time marine fish detection in\nIndonesian waters, using test data from Bunaken National Marine Park. YOLOv10's\narchitecture, featuring improvements like the CSPNet backbone, PAN for feature\nfusion, and Pyramid Spatial Attention Block, enables efficient and accurate\nobject detection even in complex environments. The model was evaluated on the\nDeepFish and OpenImages V7-Fish datasets. Results show that YOLOv10-nano\nachieves a high detection accuracy with mAP50 of 0.966 and mAP50:95 of 0.606\nwhile maintaining low computational demand (2.7M parameters, 8.4 GFLOPs). It\nalso delivered an average inference speed of 29.29 FPS on the CPU, making it\nsuitable for real-time deployment. Although OpenImages V7-Fish alone provided\nlower accuracy, it complemented DeepFish in enhancing model robustness.\nOverall, this study demonstrates YOLOv10-nano's potential for efficient,\nscalable marine fish monitoring and conservation applications in data-limited\nenvironments.",
        "url": "http://arxiv.org/abs/2509.17406v1",
        "published_date": "2025-09-22T07:02:48+00:00",
        "updated_date": "2025-09-22T07:02:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jonathan Wuntu",
            "Muhamad Dwisnanto Putro",
            "Rendy Syahputra"
        ],
        "ai_categories": [
            "Dataset",
            "GAN"
        ],
        "tldr": "The paper explores using YOLOv10-nano for real-time fish detection in Indonesian waters, showing high accuracy and low computational demand.",
        "tldr_zh": "该论文研究了在印尼水域中使用YOLOv10-nano进行实时鱼类检测，表现出高准确度和低计算需求。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "UIPro: Unleashing Superior Interaction Capability For GUI Agents",
        "summary": "Building autonomous agents that perceive and operate graphical user\ninterfaces (GUIs) like humans has long been a vision in the field of artificial\nintelligence. Central to these agents is the capability for GUI interaction,\nwhich involves GUI understanding and planning capabilities. Existing methods\nhave tried developing GUI agents based on the multi-modal comprehension ability\nof vision-language models (VLMs). However, the limited scenario, insufficient\nsize, and heterogeneous action spaces hinder the progress of building\ngeneralist GUI agents. To resolve these issues, this paper proposes\n\\textbf{UIPro}, a novel generalist GUI agent trained with extensive\nmulti-platform and multi-task GUI interaction data, coupled with a unified\naction space. We first curate a comprehensive dataset encompassing 20.6 million\nGUI understanding tasks to pre-train UIPro, granting it a strong GUI grounding\ncapability, which is key to downstream GUI agent tasks. Subsequently, we\nestablish a unified action space to harmonize heterogeneous GUI agent task\ndatasets and produce a merged dataset to foster the action prediction ability\nof UIPro via continued fine-tuning. Experimental results demonstrate UIPro's\nsuperior performance across multiple GUI task benchmarks on various platforms,\nhighlighting the effectiveness of our approach.",
        "url": "http://arxiv.org/abs/2509.17328v1",
        "published_date": "2025-09-22T03:04:53+00:00",
        "updated_date": "2025-09-22T03:04:53+00:00",
        "categories": [
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Hongxin Li",
            "Jingran Su",
            "Jingfan Chen",
            "Zheng Ju",
            "Yuntao Chen",
            "Qing Li",
            "Zhaoxiang Zhang"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces UIPro, a generalist GUI agent trained with extensive data, showcasing superior performance across multiple GUI task benchmarks on various platforms.",
        "tldr_zh": "本文引入了UIPro，一个使用大量数据训练的通用GUI代理，显示在不同平台上的多个GUI任务基准中表现出色。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Computational Scaffolding of Composition, Value, and Color for Disciplined Drawing",
        "summary": "One way illustrators engage in disciplined drawing - the process of drawing\nto improve technical skills - is through studying and replicating reference\nimages. However, for many novice and intermediate digital artists, knowing how\nto approach studying a reference image can be challenging. It can also be\ndifficult to receive immediate feedback on their works-in-progress. To help\nthese users develop their professional vision, we propose ArtKrit, a tool that\nscaffolds the process of replicating a reference image into three main steps:\ncomposition, value, and color. At each step, our tool offers computational\nguidance, such as adaptive composition line generation, and automatic feedback,\nsuch as value and color accuracy. Evaluating this tool with intermediate\ndigital artists revealed that ArtKrit could flexibly accommodate their unique\nworkflows. Our code and supplemental materials are available at\nhttps://majiaju.io/artkrit .",
        "url": "http://arxiv.org/abs/2509.17268v1",
        "published_date": "2025-09-21T22:59:56+00:00",
        "updated_date": "2025-09-21T22:59:56+00:00",
        "categories": [
            "cs.HC",
            "cs.CV"
        ],
        "authors": [
            "Jiaju Ma",
            "Chau Vu",
            "Asya Lyubavina",
            "Catherine Liu",
            "Jingyi Li"
        ],
        "ai_categories": [
            "Multimodality"
        ],
        "tldr": "The paper proposes a tool called ArtKrit that helps digital artists improve their skills by guiding them through the process of replicating reference images in three main steps: composition, value, and color.",
        "tldr_zh": "本文提出了一种名为ArtKrit的工具，通过引导数码艺术家在复制参考图像的过程中完成主要步骤：构图、价值和色彩，帮助他们提高技能。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "FROQ: Observing Face Recognition Models for Efficient Quality Assessment",
        "summary": "Face Recognition (FR) plays a crucial role in many critical (high-stakes)\napplications, where errors in the recognition process can lead to serious\nconsequences. Face Image Quality Assessment (FIQA) techniques enhance FR\nsystems by providing quality estimates of face samples, enabling the systems to\ndiscard samples that are unsuitable for reliable recognition or lead to\nlow-confidence recognition decisions. Most state-of-the-art FIQA techniques\nrely on extensive supervised training to achieve accurate quality estimation.\nIn contrast, unsupervised techniques eliminate the need for additional training\nbut tend to be slower and typically exhibit lower performance. In this paper,\nwe introduce FROQ (Face Recognition Observer of Quality), a semi-supervised,\ntraining-free approach that leverages specific intermediate representations\nwithin a given FR model to estimate face-image quality, and combines the\nefficiency of supervised FIQA models with the training-free approach of\nunsupervised methods. A simple calibration step based on pseudo-quality labels\nallows FROQ to uncover specific representations, useful for quality assessment,\nin any modern FR model. To generate these pseudo-labels, we propose a novel\nunsupervised FIQA technique based on sample perturbations. Comprehensive\nexperiments with four state-of-the-art FR models and eight benchmark datasets\nshow that FROQ leads to highly competitive results compared to the\nstate-of-the-art, achieving both strong performance and efficient runtime,\nwithout requiring explicit training.",
        "url": "http://arxiv.org/abs/2509.17689v1",
        "published_date": "2025-09-22T12:29:44+00:00",
        "updated_date": "2025-09-22T12:29:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Žiga Babnik",
            "Deepak Kumar Jain",
            "Peter Peer",
            "Vitomir Štruc"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces a semi-supervised approach, FROQ, for estimating face image quality in face recognition models without explicit training, achieving competitive results with efficient runtime.",
        "tldr_zh": "本文介绍了一种半监督方法FROQ，用于在面部识别模型中估计面部图像质量，无需显式训练，同时实现了竞争性结果和高效的运行时间。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "A$^2$M$^2$-Net: Adaptively Aligned Multi-Scale Moment for Few-Shot Action Recognition",
        "summary": "Thanks to capability to alleviate the cost of large-scale annotation,\nfew-shot action recognition (FSAR) has attracted increased attention of\nresearchers in recent years. Existing FSAR approaches typically neglect the\nrole of individual motion pattern in comparison, and under-explore the feature\nstatistics for video dynamics. Thereby, they struggle to handle the challenging\ntemporal misalignment in video dynamics, particularly by using 2D backbones. To\novercome these limitations, this work proposes an adaptively aligned\nmulti-scale second-order moment network, namely A$^2$M$^2$-Net, to describe the\nlatent video dynamics with a collection of powerful representation candidates\nand adaptively align them in an instance-guided manner. To this end, our\nA$^2$M$^2$-Net involves two core components, namely, adaptive alignment (A$^2$\nmodule) for matching, and multi-scale second-order moment (M$^2$ block) for\nstrong representation. Specifically, M$^2$ block develops a collection of\nsemantic second-order descriptors at multiple spatio-temporal scales.\nFurthermore, A$^2$ module aims to adaptively select informative candidate\ndescriptors while considering the individual motion pattern. By such means, our\nA$^2$M$^2$-Net is able to handle the challenging temporal misalignment problem\nby establishing an adaptive alignment protocol for strong representation.\nNotably, our proposed method generalizes well to various few-shot settings and\ndiverse metrics. The experiments are conducted on five widely used FSAR\nbenchmarks, and the results show our A$^2$M$^2$-Net achieves very competitive\nperformance compared to state-of-the-arts, demonstrating its effectiveness and\ngeneralization.",
        "url": "http://arxiv.org/abs/2509.17638v1",
        "published_date": "2025-09-22T11:44:14+00:00",
        "updated_date": "2025-09-22T11:44:14+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zilin Gao",
            "Qilong Wang",
            "Bingbing Zhang",
            "Qinghua Hu",
            "Peihua Li"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "Proposes A$^2$M$^2$-Net for few-shot action recognition, handling temporal misalignment with adaptive alignment and multi-scale second-order moment components.",
        "tldr_zh": "提出了 A$^2$M$^2$-Net 用于少样本动作识别，通过自适应对齐和多尺度二阶时刻组件处理时间错位。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Vision-Based Driver Drowsiness Monitoring: Comparative Analysis of YOLOv5-v11 Models",
        "summary": "Driver drowsiness remains a critical factor in road accidents, accounting for\nthousands of fatalities and injuries each year. This paper presents a\ncomprehensive evaluation of real-time, non-intrusive drowsiness detection\nmethods, focusing on computer vision based YOLO (You Look Only Once)\nalgorithms. A publicly available dataset namely, UTA-RLDD was used, containing\nboth awake and drowsy conditions, ensuring variability in gender, eyewear,\nillumination, and skin tone. Seven YOLO variants (v5s, v9c, v9t, v10n, v10l,\nv11n, v11l) are fine-tuned, with performance measured in terms of Precision,\nRecall, mAP0.5, and mAP 0.5-0.95. Among these, YOLOv9c achieved the highest\naccuracy (0.986 mAP 0.5, 0.978 Recall) while YOLOv11n strikes the optimal\nbalance between precision (0.954) and inference efficiency, making it highly\nsuitable for embedded deployment. Additionally, we implement an Eye Aspect\nRatio (EAR) approach using Dlib's facial landmarks, which despite its low\ncomputational footprint exhibits reduced robustness under pose variation and\nocclusions. Our findings illustrate clear trade offs between accuracy, latency,\nand resource requirements, and offer practical guidelines for selecting or\ncombining detection methods in autonomous driving and industrial safety\napplications.",
        "url": "http://arxiv.org/abs/2509.17498v1",
        "published_date": "2025-09-22T08:30:02+00:00",
        "updated_date": "2025-09-22T08:30:02+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Dilshara Herath",
            "Chinthaka Abeyrathne",
            "Prabhani Jayaweera"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper evaluates YOLO-based driver drowsiness detection methods on a dataset, highlighting trade-offs between accuracy and efficiency for real-world deployment.",
        "tldr_zh": "本文评估了基于YOLO的司机瞌睡检测方法在数据集上的表现，突出了在实际部署中准确性和效率之间的权衡。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "ChartHal: A Fine-grained Framework Evaluating Hallucination of Large Vision Language Models in Chart Understanding",
        "summary": "Large Vision-Language Models (LVLMs) have recently demonstrated remarkable\nprogress, yet hallucination remains a critical barrier, particularly in chart\nunderstanding, which requires sophisticated perceptual and cognitive abilities\nas well as rigorous factual accuracy. While prior work has investigated\nhallucinations and chart comprehension independently, their intersection\nremains largely unexplored. To address this gap, we present ChartHal, a\nbenchmark that features a fine-grained taxonomy of hallucination scenarios in\nchart understanding, along with a human-validated dataset of 1,062 samples. Our\nevaluation shows that state-of-the-art LVLMs suffer from severe hallucinations\non ChartHal, including proprietary models such as GPT-5 and o4-mini, which\nachieve only 34.46% and 22.79% accuracy, respectively. Further analysis reveals\nthat questions involving information absent from or contradictory to charts are\nespecially likely to trigger hallucinations, underscoring the urgent need for\nmore robust mitigation strategies. Code and data are available at\nhttps://github.com/ymcui/ChartHal .",
        "url": "http://arxiv.org/abs/2509.17481v1",
        "published_date": "2025-09-22T08:15:55+00:00",
        "updated_date": "2025-09-22T08:15:55+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Xingqi Wang",
            "Yiming Cui",
            "Xin Yao",
            "Shijin Wang",
            "Guoping Hu",
            "Xiaoyu Qin"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces ChartHal, a benchmark to evaluate hallucination in large vision-language models for chart understanding.",
        "tldr_zh": "该论文介绍了ChartHal，一个用于评估大型视觉语言模型在图表理解中的幻觉的基准。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "From Benchmarks to Reality: Advancing Visual Anomaly Detection by the VAND 3.0 Challenge",
        "summary": "Visual anomaly detection is a strongly application-driven field of research.\nConsequently, the connection between academia and industry is of paramount\nimportance. In this regard, we present the VAND 3.0 Challenge to showcase\ncurrent progress in anomaly detection across different practical settings\nwhilst addressing critical issues in the field. The challenge hosted two\ntracks, fostering the development of anomaly detection methods robust against\nreal-world distribution shifts (Category 1) and exploring the capabilities of\nVision Language Models within the few-shot regime (Category 2), respectively.\nThe participants' solutions reached significant improvements over previous\nbaselines by combining or adapting existing approaches and fusing them with\nnovel pipelines. While for both tracks the progress in large pre-trained vision\n(language) backbones played a pivotal role for the performance increase,\nscaling up anomaly detection methods more efficiently needs to be addressed by\nfuture research to meet real-time and computational constraints on-site.",
        "url": "http://arxiv.org/abs/2509.17615v1",
        "published_date": "2025-09-22T11:27:49+00:00",
        "updated_date": "2025-09-22T11:27:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lars Heckler-Kram",
            "Ashwin Vaidya",
            "Jan-Hendrik Neudeck",
            "Ulla Scheler",
            "Dick Ameln",
            "Samet Akcay",
            "Paula Ramos"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces the VAND 3.0 Challenge to advance visual anomaly detection in practical settings, addressing issues and showcasing progress in anomaly detection methods.",
        "tldr_zh": "本文介绍了VAND 3.0挑战赛，旨在推进视觉异常检测在实际场景中的发展，解决问题并展示方法的进展。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.25
    },
    {
        "title": "Tailored Transformation Invariance for Industrial Anomaly Detection",
        "summary": "Industrial Anomaly Detection (IAD) is a subproblem within Computer Vision\nAnomaly Detection that has been receiving increasing amounts of attention due\nto its applicability to real-life scenarios. Recent research has focused on how\nto extract the most informative features, contrasting older kNN-based methods\nthat use only pretrained features. These recent methods are much more expensive\nto train however and could complicate real-life application. Careful study of\nrelated work with regards to transformation invariance leads to the idea that\npopular benchmarks require robustness to only minor translations. With this\nidea we then formulate LWinNN, a local window based approach that creates a\nmiddle ground between kNN based methods that have either complete or no\ntranslation invariance. Our experiments demonstrate that this small change\nincreases accuracy considerably, while simultaneously decreasing both train and\ntest time. This teaches us two things: first, the gap between kNN-based\napproaches and more complex state-of-the-art methodology can still be narrowed\nby effective usage of the limited data available. Second, our assumption of\nrequiring only limited translation invariance highlights potential areas of\ninterest for future work and the need for more spatially diverse benchmarks,\nfor which our method can hopefully serve as a new baseline. Our code can be\nfound at https://github.com/marietteschonfeld/LWinNN .",
        "url": "http://arxiv.org/abs/2509.17670v1",
        "published_date": "2025-09-22T12:13:58+00:00",
        "updated_date": "2025-09-22T12:13:58+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Mariette Schönfeld",
            "Wannes Meert",
            "Hendrik Blockeel"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a new approach, LWinNN, for industrial anomaly detection by focusing on tailored transformation invariance, improving accuracy while reducing training and testing time.",
        "tldr_zh": "本文引入了一种新的方法，LWinNN，用于工业异常检测，通过专注于定制的转换不变性，提高准确性同时减少训练和测试时间。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]