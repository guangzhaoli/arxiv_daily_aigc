[
    {
        "title": "MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware Alignment and Disentanglement",
        "summary": "Multi-subject personalized generation presents unique challenges in\nmaintaining identity fidelity and semantic coherence when synthesizing images\nconditioned on multiple reference subjects. Existing methods often suffer from\nidentity blending and attribute leakage due to inadequate modeling of how\ndifferent subjects should interact within shared representation spaces. We\npresent MOSAIC, a representation-centric framework that rethinks multi-subject\ngeneration through explicit semantic correspondence and orthogonal feature\ndisentanglement. Our key insight is that multi-subject generation requires\nprecise semantic alignment at the representation level - knowing exactly which\nregions in the generated image should attend to which parts of each reference.\nTo enable this, we introduce SemAlign-MS, a meticulously annotated dataset\nproviding fine-grained semantic correspondences between multiple reference\nsubjects and target images, previously unavailable in this domain. Building on\nthis foundation, we propose the semantic correspondence attention loss to\nenforce precise point-to-point semantic alignment, ensuring high consistency\nfrom each reference to its designated regions. Furthermore, we develop the\nmulti-reference disentanglement loss to push different subjects into orthogonal\nattention subspaces, preventing feature interference while preserving\nindividual identity characteristics. Extensive experiments demonstrate that\nMOSAIC achieves state-of-the-art performance on multiple benchmarks. Notably,\nwhile existing methods typically degrade beyond 3 subjects, MOSAIC maintains\nhigh fidelity with 4+ reference subjects, opening new possibilities for complex\nmulti-subject synthesis applications.",
        "url": "http://arxiv.org/abs/2509.01977v1",
        "published_date": "2025-09-02T05:40:07+00:00",
        "updated_date": "2025-09-02T05:40:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dong She",
            "Siming Fu",
            "Mushui Liu",
            "Qiaoqiao Jin",
            "Hualiang Wang",
            "Mu Liu",
            "Jidong Jiang"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper presents MOSAIC, a framework for multi-subject personalized image generation with precise semantic alignment and feature disentanglement, achieving state-of-the-art performance on multiple benchmarks.",
        "tldr_zh": "本文提出了MOSAIC框架，用于多主体个性化图像生成，通过精确的语义对齐和特征解耦，在多个基准测试上取得了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "Motion-Refined DINOSAUR for Unsupervised Multi-Object Discovery",
        "summary": "Unsupervised multi-object discovery (MOD) aims to detect and localize\ndistinct object instances in visual scenes without any form of human\nsupervision. Recent approaches leverage object-centric learning (OCL) and\nmotion cues from video to identify individual objects. However, these\napproaches use supervision to generate pseudo labels to train the OCL model. We\naddress this limitation with MR-DINOSAUR -- Motion-Refined DINOSAUR -- a\nminimalistic unsupervised approach that extends the self-supervised pre-trained\nOCL model, DINOSAUR, to the task of unsupervised multi-object discovery. We\ngenerate high-quality unsupervised pseudo labels by retrieving video frames\nwithout camera motion for which we perform motion segmentation of unsupervised\noptical flow. We refine DINOSAUR's slot representations using these pseudo\nlabels and train a slot deactivation module to assign slots to foreground and\nbackground. Despite its conceptual simplicity, MR-DINOSAUR achieves strong\nmulti-object discovery results on the TRI-PD and KITTI datasets, outperforming\nthe previous state of the art despite being fully unsupervised.",
        "url": "http://arxiv.org/abs/2509.02545v1",
        "published_date": "2025-09-02T17:44:49+00:00",
        "updated_date": "2025-09-02T17:44:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinrui Gong",
            "Oliver Hahn",
            "Christoph Reich",
            "Krishnakant Singh",
            "Simone Schaub-Meyer",
            "Daniel Cremers",
            "Stefan Roth"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces MR-DINOSAUR, an unsupervised approach to multi-object discovery in visual scenes, achieving strong results without the need for human supervision.",
        "tldr_zh": "本文介绍了MR-DINOSAUR，这是一种无监督的方法，用于在视觉场景中进行多对象发现，无需人工监督即可取得较好的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Mix-modal Federated Learning for MRI Image Segmentation",
        "summary": "Magnetic resonance imaging (MRI) image segmentation is crucial in diagnosing\nand treating many diseases, such as brain tumors. Existing MRI image\nsegmentation methods mainly fall into a centralized multimodal paradigm, which\nis inapplicable in engineering non-centralized mix-modal medical scenarios. In\nthis situation, each distributed client (hospital) processes multiple mixed MRI\nmodalities, and the modality set and image data for each client are diverse,\nsuffering from extensive client-wise modality heterogeneity and data\nheterogeneity. In this paper, we first formulate non-centralized mix-modal MRI\nimage segmentation as a new paradigm for federated learning (FL) that involves\nmultiple modalities, called mix-modal federated learning (MixMFL). It\ndistinguishes from existing multimodal federating learning (MulMFL) and\ncross-modal federating learning (CroMFL) paradigms. Then, we proposed a novel\nmodality decoupling and memorizing mix-modal federated learning framework\n(MDM-MixMFL) for MRI image segmentation, which is characterized by a modality\ndecoupling strategy and a modality memorizing mechanism. Specifically, the\nmodality decoupling strategy disentangles each modality into modality-tailored\nand modality-shared information. During mix-modal federated updating,\ncorresponding modality encoders undergo tailored and shared updating,\nrespectively. It facilitates stable and adaptive federating aggregation of\nheterogeneous data and modalities from distributed clients. Besides, the\nmodality memorizing mechanism stores client-shared modality prototypes\ndynamically refreshed from every modality-tailored encoder to compensate for\nincomplete modalities in each local client. It further benefits modality\naggregation and fusion processes during mixmodal federated learning. Extensive\nexperiments on two public datasets for MRI image segmentation demonstrate the\neffectiveness and superiority of our methods.",
        "url": "http://arxiv.org/abs/2509.02541v1",
        "published_date": "2025-09-02T17:43:51+00:00",
        "updated_date": "2025-09-02T17:43:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guyue Hu",
            "Siyuan Song",
            "Jingpeng Sun",
            "Zhe Jin",
            "Chenglong Li",
            "Jin Tang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a new paradigm called mix-modal federated learning for MRI image segmentation, proposing a novel framework that addresses data and modality heterogeneity in distributed medical scenarios.",
        "tldr_zh": "本文介绍了一种新的混合模态联邦学习范式，用于MRI图像分割，并提出了一种新颖的框架，解决了分布式医疗场景中的数据和模态异质性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "TeRA: Rethinking Text-driven Realistic 3D Avatar Generation",
        "summary": "In this paper, we rethink text-to-avatar generative models by proposing TeRA,\na more efficient and effective framework than the previous SDS-based models and\ngeneral large 3D generative models. Our approach employs a two-stage training\nstrategy for learning a native 3D avatar generative model. Initially, we\ndistill a decoder to derive a structured latent space from a large human\nreconstruction model. Subsequently, a text-controlled latent diffusion model is\ntrained to generate photorealistic 3D human avatars within this latent space.\nTeRA enhances the model performance by eliminating slow iterative optimization\nand enables text-based partial customization through a structured 3D human\nrepresentation. Experiments have proven our approach's superiority over\nprevious text-to-avatar generative models in subjective and objective\nevaluation.",
        "url": "http://arxiv.org/abs/2509.02466v1",
        "published_date": "2025-09-02T16:20:20+00:00",
        "updated_date": "2025-09-02T16:20:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanwen Wang",
            "Yiyu Zhuang",
            "Jiawei Zhang",
            "Li Wang",
            "Yifei Zeng",
            "Xun Cao",
            "Xinxin Zuo",
            "Hao Zhu"
        ],
        "ai_categories": [
            "GAN",
            "AIGC",
            "Transformer"
        ],
        "tldr": "The paper introduces TeRA, a more efficient text-driven 3D avatar generation model that outperforms previous models in terms of speed and customization.",
        "tldr_zh": "本文介绍了TeRA，一种更高效的文本驱动3D头像生成模型，以速度和定制性能而胜过先前的模型。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Towards High-Fidelity, Identity-Preserving Real-Time Makeup Transfer: Decoupling Style Generation",
        "summary": "We present a novel framework for real-time virtual makeup try-on that\nachieves high-fidelity, identity-preserving cosmetic transfer with robust\ntemporal consistency. In live makeup transfer applications, it is critical to\nsynthesize temporally coherent results that accurately replicate fine-grained\nmakeup and preserve user's identity. However, existing methods often struggle\nto disentangle semitransparent cosmetics from skin tones and other identify\nfeatures, causing identity shifts and raising fairness concerns. Furthermore,\ncurrent methods lack real-time capabilities and fail to maintain temporal\nconsistency, limiting practical adoption. To address these challenges, we\ndecouple makeup transfer into two steps: transparent makeup mask extraction and\ngraphics-based mask rendering. After the makeup extraction step, the makeup\nrendering can be performed in real time, enabling live makeup try-on. Our\nmakeup extraction model trained on pseudo-ground-truth data generated via two\ncomplementary methods: a graphics-based rendering pipeline and an unsupervised\nk-means clustering approach. To further enhance transparency estimation and\ncolor fidelity, we propose specialized training objectives, including\nalpha-weighted reconstruction and lip color losses. Our method achieves robust\nmakeup transfer across diverse poses, expressions, and skin tones while\npreserving temporal smoothness. Extensive experiments demonstrate that our\napproach outperforms existing baselines in capturing fine details, maintaining\ntemporal stability, and preserving identity integrity.",
        "url": "http://arxiv.org/abs/2509.02445v1",
        "published_date": "2025-09-02T15:52:56+00:00",
        "updated_date": "2025-09-02T15:52:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lydia Kin Ching Chau",
            "Zhi Yu",
            "Ruo Wei Jiang"
        ],
        "ai_categories": [
            "GAN",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a framework for real-time virtual makeup try-on that achieves high-fidelity, identity-preserving cosmetic transfer with robust temporal consistency.",
        "tldr_zh": "该论文介绍了一个实时虚拟化妆试穿的框架，实现了高保真度、保留用户身份的化妆转移，具有强大的时间一致性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "A Multimodal Cross-View Model for Predicting Postoperative Neck Pain in Cervical Spondylosis Patients",
        "summary": "Neck pain is the primary symptom of cervical spondylosis, yet its underlying\nmechanisms remain unclear, leading to uncertain treatment outcomes. To address\nthe challenges of multimodal feature fusion caused by imaging differences and\nspatial mismatches, this paper proposes an Adaptive Bidirectional Pyramid\nDifference Convolution (ABPDC) module that facilitates multimodal integration\nby exploiting the advantages of difference convolution in texture extraction\nand grayscale invariance, and a Feature Pyramid Registration Auxiliary Network\n(FPRAN) to mitigate structural misalignment. Experiments on the MMCSD dataset\ndemonstrate that the proposed model achieves superior prediction accuracy of\npostoperative neck pain recovery compared with existing methods, and ablation\nstudies further confirm its effectiveness.",
        "url": "http://arxiv.org/abs/2509.02256v1",
        "published_date": "2025-09-02T12:33:43+00:00",
        "updated_date": "2025-09-02T12:33:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingyang Shan",
            "Qishuai Yu",
            "Jiacen Liu",
            "Shaolin Zhang",
            "Wen Shen",
            "Yanxiao Zhao",
            "Tianyi Wang",
            "Xiaolin Qin",
            "Yiheng Yin"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes a model to predict postoperative neck pain in cervical spondylosis patients by integrating multimodal features and addressing imaging differences and spatial mismatches.",
        "tldr_zh": "该论文提出了一个模型，通过整合多模态特征并解决图像差异和空间不匹配问题，以预测颈椎病患者的术后颈痛。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "BM-CL: Bias Mitigation through the lens of Continual Learning",
        "summary": "Biases in machine learning pose significant challenges, particularly when\nmodels amplify disparities that affect disadvantaged groups. Traditional bias\nmitigation techniques often lead to a {\\itshape leveling-down effect}, whereby\nimproving outcomes of disadvantaged groups comes at the expense of reduced\nperformance for advantaged groups. This study introduces Bias Mitigation\nthrough Continual Learning (BM-CL), a novel framework that leverages the\nprinciples of continual learning to address this trade-off. We postulate that\nmitigating bias is conceptually similar to domain-incremental continual\nlearning, where the model must adjust to changing fairness conditions,\nimproving outcomes for disadvantaged groups without forgetting the knowledge\nthat benefits advantaged groups. Drawing inspiration from techniques such as\nLearning without Forgetting and Elastic Weight Consolidation, we reinterpret\nbias mitigation as a continual learning problem. This perspective allows models\nto incrementally balance fairness objectives, enhancing outcomes for\ndisadvantaged groups while preserving performance for advantaged groups.\nExperiments on synthetic and real-world image datasets, characterized by\ndiverse sources of bias, demonstrate that the proposed framework mitigates\nbiases while minimizing the loss of original knowledge. Our approach bridges\nthe fields of fairness and continual learning, offering a promising pathway for\ndeveloping machine learning systems that are both equitable and effective.",
        "url": "http://arxiv.org/abs/2509.01730v1",
        "published_date": "2025-09-01T19:23:24+00:00",
        "updated_date": "2025-09-01T19:23:24+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Lucas Mansilla",
            "Rodrigo Echeveste",
            "Camila Gonzalez",
            "Diego H. Milone",
            "Enzo Ferrante"
        ],
        "ai_categories": [
            "Other"
        ],
        "tldr": "The paper introduces a novel Bias Mitigation through Continual Learning framework to address biases in machine learning by balancing fairness objectives without reducing performance for advantaged groups.",
        "tldr_zh": "该论文介绍了一种新颖的通过持续学习来减轻偏见的框架，以在不减少优势群体的表现的情况下平衡公平目标。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "FastVGGT: Training-Free Acceleration of Visual Geometry Transformer",
        "summary": "Foundation models for 3D vision have recently demonstrated remarkable\ncapabilities in 3D perception. However, scaling these models to long-sequence\nimage inputs remains a significant challenge due to inference-time\ninefficiency. In this work, we present a detailed analysis of VGGT, a\nstate-of-the-art feed-forward visual geometry model and identify its primary\nbottleneck. Visualization further reveals a token collapse phenomenon in the\nattention maps. Motivated by these findings, we explore the potential of token\nmerging in the feed-forward visual geometry model. Owing to the unique\narchitectural and task-specific properties of 3D models, directly applying\nexisting merging techniques proves challenging. To this end, we propose\nFastVGGT, which, for the first time, leverages token merging in the 3D domain\nthrough a training-free mechanism for accelerating VGGT. we devise a unique\ntoken partitioning strategy tailored to 3D architectures and tasks, effectively\neliminating redundant computation while preserving VGGT's powerful\nreconstruction capacity. Extensive experiments on multiple 3D geometry\nbenchmarks validate the effectiveness of our approach. Notably, with 1000 input\nimages, FastVGGT achieves a 4x speedup over VGGT while mitigating error\naccumulation in long-sequence scenarios. These findings underscore the\npotential of token merging as a principled solution for scalable 3D vision\nsystems. Code is available at: https://mystorm16.github.io/fastvggt/.",
        "url": "http://arxiv.org/abs/2509.02560v1",
        "published_date": "2025-09-02T17:54:21+00:00",
        "updated_date": "2025-09-02T17:54:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "You Shen",
            "Zhipeng Zhang",
            "Yansong Qu",
            "Liujuan Cao"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces FastVGGT, a training-free acceleration method for a 3D visual geometry model, achieving a 4x speedup while maintaining accuracy on multiple benchmarks.",
        "tldr_zh": "本文介绍了FastVGGT，一种用于3D视觉几何模型的无需训练的加速方法，在多个基准测试中实现了4倍加速，同时保持准确性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "2nd Place Solution for CVPR2024 E2E Challenge: End-to-End Autonomous Driving Using Vision Language Model",
        "summary": "End-to-end autonomous driving has drawn tremendous attention recently. Many\nworks focus on using modular deep neural networks to construct the end-to-end\narchi-tecture. However, whether using powerful large language models (LLM),\nespecially multi-modality Vision Language Models (VLM) could benefit the\nend-to-end driving tasks remain a question. In our work, we demonstrate that\ncombining end-to-end architectural design and knowledgeable VLMs yield\nimpressive performance on the driving tasks. It is worth noting that our method\nonly uses a single camera and is the best camera-only solution across the\nleaderboard, demonstrating the effectiveness of vision-based driving approach\nand the potential for end-to-end driving tasks.",
        "url": "http://arxiv.org/abs/2509.02659v1",
        "published_date": "2025-09-02T17:52:29+00:00",
        "updated_date": "2025-09-02T17:52:29+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Zilong Guo",
            "Yi Luo",
            "Long Sha",
            "Dongxu Wang",
            "Panqu Wang",
            "Chenyang Xu",
            "Yi Yang"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces an end-to-end autonomous driving system using Vision Language Models, showing impressive performance with a single camera setup.",
        "tldr_zh": "本文介绍了一种利用视觉语言模型的端到端自动驾驶系统，采用单摄像头设置表现出色。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unifi3D: A Study on 3D Representations for Generation and Reconstruction in a Common Framework",
        "summary": "Following rapid advancements in text and image generation, research has\nincreasingly shifted towards 3D generation. Unlike the well-established\npixel-based representation in images, 3D representations remain diverse and\nfragmented, encompassing a wide variety of approaches such as voxel grids,\nneural radiance fields, signed distance functions, point clouds, or octrees,\neach offering distinct advantages and limitations. In this work, we present a\nunified evaluation framework designed to assess the performance of 3D\nrepresentations in reconstruction and generation. We compare these\nrepresentations based on multiple criteria: quality, computational efficiency,\nand generalization performance. Beyond standard model benchmarking, our\nexperiments aim to derive best practices over all steps involved in the 3D\ngeneration pipeline, including preprocessing, mesh reconstruction, compression\nwith autoencoders, and generation. Our findings highlight that reconstruction\nerrors significantly impact overall performance, underscoring the need to\nevaluate generation and reconstruction jointly. We provide insights that can\ninform the selection of suitable 3D models for various applications,\nfacilitating the development of more robust and application-specific solutions\nin 3D generation. The code for our framework is available at\nhttps://github.com/isl-org/unifi3d.",
        "url": "http://arxiv.org/abs/2509.02474v1",
        "published_date": "2025-09-02T16:25:12+00:00",
        "updated_date": "2025-09-02T16:25:12+00:00",
        "categories": [
            "cs.GR",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Nina Wiedemann",
            "Sainan Liu",
            "Quentin Leboutet",
            "Katelyn Gao",
            "Benjamin Ummenhofer",
            "Michael Paulitsch",
            "Kai Yuan"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper presents a unified evaluation framework for assessing 3D representations in reconstruction and generation, highlighting the importance of evaluating both together.",
        "tldr_zh": "本文提出了一个统一的评估框架，用于评估三维表示在重建和生成中的表现，强调了评估两者结合的重要性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GenCompositor: Generative Video Compositing with Diffusion Transformer",
        "summary": "Video compositing combines live-action footage to create video production,\nserving as a crucial technique in video creation and film production.\nTraditional pipelines require intensive labor efforts and expert collaboration,\nresulting in lengthy production cycles and high manpower costs. To address this\nissue, we automate this process with generative models, called generative video\ncompositing. This new task strives to adaptively inject identity and motion\ninformation of foreground video to the target video in an interactive manner,\nallowing users to customize the size, motion trajectory, and other attributes\nof the dynamic elements added in final video. Specifically, we designed a novel\nDiffusion Transformer (DiT) pipeline based on its intrinsic properties. To\nmaintain consistency of the target video before and after editing, we revised a\nlight-weight DiT-based background preservation branch with masked token\ninjection. As to inherit dynamic elements from other sources, a DiT fusion\nblock is proposed using full self-attention, along with a simple yet effective\nforeground augmentation for training. Besides, for fusing background and\nforeground videos with different layouts based on user control, we developed a\nnovel position embedding, named Extended Rotary Position Embedding (ERoPE).\nFinally, we curated a dataset comprising 61K sets of videos for our new task,\ncalled VideoComp. This data includes complete dynamic elements and high-quality\ntarget videos. Experiments demonstrate that our method effectively realizes\ngenerative video compositing, outperforming existing possible solutions in\nfidelity and consistency.",
        "url": "http://arxiv.org/abs/2509.02460v1",
        "published_date": "2025-09-02T16:10:13+00:00",
        "updated_date": "2025-09-02T16:10:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuzhou Yang",
            "Xiaoyu Li",
            "Xiaodong Cun",
            "Guangzhi Wang",
            "Lingen Li",
            "Ying Shan",
            "Jian Zhang"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces GenCompositor, a generative video compositing method using Diffusion Transformer. It aims to automate the video compositing process by injecting identity and motion information from foreground videos to target videos.",
        "tldr_zh": "本文介绍了GenCompositor，一种使用Diffusion Transformer的生成视频合成方法。其旨在通过将前景视频的身份和运动信息注入目标视频，自动化视频合成过程。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "RiverScope: High-Resolution River Masking Dataset",
        "summary": "Surface water dynamics play a critical role in Earth's climate system,\ninfluencing ecosystems, agriculture, disaster resilience, and sustainable\ndevelopment. Yet monitoring rivers and surface water at fine spatial and\ntemporal scales remains challenging -- especially for narrow or sediment-rich\nrivers that are poorly captured by low-resolution satellite data. To address\nthis, we introduce RiverScope, a high-resolution dataset developed through\ncollaboration between computer science and hydrology experts. RiverScope\ncomprises 1,145 high-resolution images (covering 2,577 square kilometers) with\nexpert-labeled river and surface water masks, requiring over 100 hours of\nmanual annotation. Each image is co-registered with Sentinel-2, SWOT, and the\nSWOT River Database (SWORD), enabling the evaluation of cost-accuracy\ntrade-offs across sensors -- a key consideration for operational water\nmonitoring. We also establish the first global, high-resolution benchmark for\nriver width estimation, achieving a median error of 7.2 meters -- significantly\noutperforming existing satellite-derived methods. We extensively evaluate deep\nnetworks across multiple architectures (e.g., CNNs and transformers),\npretraining strategies (e.g., supervised and self-supervised), and training\ndatasets (e.g., ImageNet and satellite imagery). Our best-performing models\ncombine the benefits of transfer learning with the use of all the multispectral\nPlanetScope channels via learned adaptors. RiverScope provides a valuable\nresource for fine-scale and multi-sensor hydrological modeling, supporting\nclimate adaptation and sustainable water management.",
        "url": "http://arxiv.org/abs/2509.02451v1",
        "published_date": "2025-09-02T16:00:27+00:00",
        "updated_date": "2025-09-02T16:00:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rangel Daroya",
            "Taylor Rowley",
            "Jonathan Flores",
            "Elisa Friedmann",
            "Fiona Bennitt",
            "Heejin An",
            "Travis Simmons",
            "Marissa Jean Hughes",
            "Camryn L Kluetmeier",
            "Solomon Kica",
            "J. Daniel Vélez",
            "Sarah E. Esenther",
            "Thomas E. Howard",
            "Yanqi Ye",
            "Audrey Turcotte",
            "Colin Gleason",
            "Subhransu Maji"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer",
            "GAN"
        ],
        "tldr": "RiverScope is a high-resolution dataset for river masking with expert-labeled images, providing a valuable resource for hydrological modeling.",
        "tldr_zh": "RiverScope是一个高分辨率河流遮罩数据集，提供了一个对水文模拟十分有价值的资源。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AppCopilot: Toward General, Accurate, Long-Horizon, and Efficient Mobile Agent",
        "summary": "With the raid evolution of large language models and multimodal foundation\nmodels, the mobile-agent landscape has proliferated without converging on the\nfundamental challenges. This paper identifies four core problems that must be\nsolved for mobile agents to deliver practical, scalable impact: (1)\ngeneralization across tasks, modalities, apps, and devices; (2) accuracy,\nspecifically precise on-screen interaction and click targeting; (3)\nlong-horizon capability for sustained, multi-step goals; and (4) efficiency,\nspecifically high-performance runtime on resource-constrained devices. We\npresent AppCopilot, a multimodal, multi-agent, general-purpose on-device\nassistant that operates across applications and constitutes a full-stack,\nclosed-loop system from data to deployment. AppCopilot operationalizes this\nposition through an end-to-end autonomous pipeline spanning data collection,\ntraining, deployment, high-quality and efficient inference, and mobile\napplication development. At the model layer, it integrates multimodal\nfoundation models with robust Chinese-English support. At the reasoning and\ncontrol layer, it combines chain-of-thought reasoning, hierarchical task\nplanning and decomposition, and multi-agent collaboration. At the execution\nlayer, it enables user personalization and experiential adaptation, voice\ninteraction, function calling, cross-app and cross-device orchestration, and\ncomprehensive mobile app support. The system design incorporates\nprofiling-driven optimization for latency, memory, and energy across\nheterogeneous hardware. Empirically, AppCopilot achieves significant\nimprovements along all four dimensions: stronger generalization,\nhigher-precision on-screen actions, more reliable long-horizon task completion,\nand faster, more resource-efficient runtime.",
        "url": "http://arxiv.org/abs/2509.02444v1",
        "published_date": "2025-09-02T15:48:21+00:00",
        "updated_date": "2025-09-02T15:48:21+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Jingru Fan",
            "Yufan Dang",
            "Jingyao Wu",
            "Huatao Li",
            "Runde Yang",
            "Xiyuan Yang",
            "Yuheng Wang",
            "Zhong Zhang",
            "Yaxi Lu",
            "Yankai Lin",
            "Zhiyuan Liu",
            "Dahai Li",
            "Chen Qian"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Transformer"
        ],
        "tldr": "The paper introduces AppCopilot, a mobile-agent assistant that addresses key challenges like generalization, accuracy, long-horizon tasks, and efficiency on resource-constrained devices.",
        "tldr_zh": "本文介绍了AppCopilot，一种移动代理助手，解决了关于泛化、精度、长期目标和在资源受限设备上的效率等关键挑战。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Faster and Better: Reinforced Collaborative Distillation and Self-Learning for Infrared-Visible Image Fusion",
        "summary": "Infrared and visible image fusion plays a critical role in enhancing scene\nperception by combining complementary information from different modalities.\nDespite recent advances, achieving high-quality image fusion with lightweight\nmodels remains a significant challenge. To bridge this gap, we propose a novel\ncollaborative distillation and self-learning framework for image fusion driven\nby reinforcement learning. Unlike conventional distillation, this approach not\nonly enables the student model to absorb image fusion knowledge from the\nteacher model, but more importantly, allows the student to perform\nself-learning on more challenging samples to enhance its capabilities.\nParticularly, in our framework, a reinforcement learning agent explores and\nidentifies a more suitable training strategy for the student.The agent takes\nboth the student's performance and the teacher-student gap as inputs, which\nleads to the generation of challenging samples to facilitate the student's\nself-learning. Simultaneously, it dynamically adjusts the teacher's guidance\nstrength based on the student's state to optimize the knowledge transfer.\nExperimental results demonstrate that our method can significantly improve\nstudent performance and achieve better fusion results compared to existing\ntechniques.",
        "url": "http://arxiv.org/abs/2509.02424v2",
        "published_date": "2025-09-02T15:25:53+00:00",
        "updated_date": "2025-09-03T07:27:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuhao Wang",
            "Lingjuan Miao",
            "Zhiqiang Zhou",
            "Yajun Qiao",
            "Lei Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper proposes a new framework for infrared-visible image fusion using collaborative distillation and self-learning driven by reinforcement learning, showing improvements over existing techniques.",
        "tldr_zh": "本文提出了一种利用强化学习驱动的协同蒸馏和自学习的红外-可见图像融合新框架，相较于现有技术有显著改进。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Why Do MLLMs Struggle with Spatial Understanding? A Systematic Analysis from Data to Architecture",
        "summary": "Spatial understanding is essential for Multimodal Large Language Models\n(MLLMs) to support perception, reasoning, and planning in embodied\nenvironments. Despite recent progress, existing studies reveal that MLLMs still\nstruggle with spatial understanding. However, existing research lacks a\ncomprehensive and systematic evaluation of these limitations, often restricted\nto isolated scenarios, such as single-view or video. In this work, we present a\nsystematic analysis of spatial understanding from both data and architectural\nperspectives across three representative scenarios: single-view, multi-view,\nand video. We propose a benchmark named MulSeT (Multi-view Spatial\nUnderstanding Tasks), and design a series of experiments to analyze the spatial\nreasoning capabilities of MLLMs. From the data perspective, the performance of\nspatial understanding converges quickly as the training data increases, and the\nupper bound is relatively low, especially for tasks that require spatial\nimagination. This indicates that merely expanding training data is insufficient\nto achieve satisfactory performance. From the architectural perspective, we\nfind that spatial understanding relies more heavily on the positional encoding\nwithin the visual encoder than within the language model, in both cascaded and\nnative MLLMs. Moreover, we explore reasoning injection and envision future\nimprovements through architectural design to optimize spatial understanding.\nThese insights shed light on the limitations of current MLLMs and suggest new\ndirections for improving spatial reasoning capabilities through data scaling\nand architectural tuning.",
        "url": "http://arxiv.org/abs/2509.02359v1",
        "published_date": "2025-09-02T14:22:43+00:00",
        "updated_date": "2025-09-02T14:22:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wanyue Zhang",
            "Yibin Huang",
            "Yangbin Xu",
            "JingJing Huang",
            "Helu Zhi",
            "Shuo Ren",
            "Wang Xu",
            "Jiajun Zhang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper systematically analyzes why Multimodal Large Language Models struggle with spatial understanding, proposing a benchmark and experiments to improve spatial reasoning capabilities through data scaling and architectural tuning.",
        "tldr_zh": "本文系统分析了为何多模态大型语言模型在空间理解方面存在困难，提出了一个基准和实验，通过数据扩展和架构调整来改善空间推理能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Category-Aware 3D Object Composition with Disentangled Texture and Shape Multi-view Diffusion",
        "summary": "In this paper, we tackle a new task of 3D object synthesis, where a 3D model\nis composited with another object category to create a novel 3D model. However,\nmost existing text/image/3D-to-3D methods struggle to effectively integrate\nmultiple content sources, often resulting in inconsistent textures and\ninaccurate shapes. To overcome these challenges, we propose a straightforward\nyet powerful approach, category+3D-to-3D (C33D), for generating novel and\nstructurally coherent 3D models. Our method begins by rendering multi-view\nimages and normal maps from the input 3D model, then generating a novel 2D\nobject using adaptive text-image harmony (ATIH) with the front-view image and a\ntext description from another object category as inputs. To ensure texture\nconsistency, we introduce texture multi-view diffusion, which refines the\ntextures of the remaining multi-view RGB images based on the novel 2D object.\nFor enhanced shape accuracy, we propose shape multi-view diffusion to improve\nthe 2D shapes of both the multi-view RGB images and the normal maps, also\nconditioned on the novel 2D object. Finally, these outputs are used to\nreconstruct a complete and novel 3D model. Extensive experiments demonstrate\nthe effectiveness of our method, yielding impressive 3D creations, such as\nshark(3D)-crocodile(text) in the first row of Fig. 1. A project page is\navailable at: https://xzr52.github.io/C33D/",
        "url": "http://arxiv.org/abs/2509.02357v1",
        "published_date": "2025-09-02T14:19:21+00:00",
        "updated_date": "2025-09-02T14:19:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zeren Xiong",
            "Zikun Chen",
            "Zedong Zhang",
            "Xiang Li",
            "Ying Tai",
            "Jian Yang",
            "Jun Li"
        ],
        "ai_categories": [
            "Diffusion"
        ],
        "tldr": "The paper introduces a method for synthesizing novel 3D objects by compositing multiple content sources, achieving texture consistency and shape accuracy.",
        "tldr_zh": "本文介绍了一种通过合成多个内容来源来合成新颖的3D对象的方法，实现了纹理一致性和形状准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Data-Driven Loss Functions for Inference-Time Optimization in Text-to-Image Generation",
        "summary": "Text-to-image diffusion models can generate stunning visuals, yet they often\nfail at tasks children find trivial--like placing a dog to the right of a teddy\nbear rather than to the left. When combinations get more unusual--a giraffe\nabove an airplane--these failures become even more pronounced. Existing methods\nattempt to fix these spatial reasoning failures through model fine-tuning or\ntest-time optimization with handcrafted losses that are suboptimal. Rather than\nimposing our assumptions about spatial encoding, we propose learning these\nobjectives directly from the model's internal representations. We introduce\nLearn-to-Steer, a novel framework that learns data-driven objectives for\ntest-time optimization rather than handcrafting them. Our key insight is to\ntrain a lightweight classifier that decodes spatial relationships from the\ndiffusion model's cross-attention maps, then deploy this classifier as a\nlearned loss function during inference. Training such classifiers poses a\nsurprising challenge: they can take shortcuts by detecting linguistic traces\nrather than learning true spatial patterns. We solve this with a dual-inversion\nstrategy that enforces geometric understanding. Our method dramatically\nimproves spatial accuracy: from 0.20 to 0.61 on FLUX.1-dev and from 0.07 to\n0.54 on SD2.1 across standard benchmarks. Moreover, our approach generalizes to\nmultiple relations and significantly improves accuracy.",
        "url": "http://arxiv.org/abs/2509.02295v1",
        "published_date": "2025-09-02T13:17:11+00:00",
        "updated_date": "2025-09-02T13:17:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sapir Esther Yiflach",
            "Yuval Atzmon",
            "Gal Chechik"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a data-driven approach for optimizing text-to-image generation models by learning spatial objectives directly from the model's internal representations.",
        "tldr_zh": "本文提出了一种数据驱动的方法，通过直接从模型的内部表示中学习空间目标来优化文本到图像生成模型。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SynthGenNet: a self-supervised approach for test-time generalization using synthetic multi-source domain mixing of street view images",
        "summary": "Unstructured urban environments present unique challenges for scene\nunderstanding and generalization due to their complex and diverse layouts. We\nintroduce SynthGenNet, a self-supervised student-teacher architecture designed\nto enable robust test-time domain generalization using synthetic multi-source\nimagery. Our contributions include the novel ClassMix++ algorithm, which blends\nlabeled data from various synthetic sources while maintaining semantic\nintegrity, enhancing model adaptability. We further employ Grounded Mask\nConsistency Loss (GMC), which leverages source ground truth to improve\ncross-domain prediction consistency and feature alignment. The Pseudo-Label\nGuided Contrastive Learning (PLGCL) mechanism is integrated into the student\nnetwork to facilitate domain-invariant feature learning through iterative\nknowledge distillation from the teacher network. This self-supervised strategy\nimproves prediction accuracy, addresses real-world variability, bridges the\nsim-to-real domain gap, and reliance on labeled target data, even in complex\nurban areas. Outcomes show our model outperforms the state-of-the-art (relying\non single source) by achieving 50% Mean Intersection-Over-Union (mIoU) value on\nreal-world datasets like Indian Driving Dataset (IDD).",
        "url": "http://arxiv.org/abs/2509.02287v1",
        "published_date": "2025-09-02T13:08:03+00:00",
        "updated_date": "2025-09-02T13:08:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pushpendra Dhakara",
            "Prachi Chachodhia",
            "Vaibhav Kumar"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "SynthGenNet is a self-supervised approach using synthetic imagery for test-time domain generalization in complex urban environments, outperforming state-of-the-art methods.",
        "tldr_zh": "SynthGenNet是一种自监督方法，利用合成图像来进行复杂城市环境中的测试时间域泛化，在实现状态下优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RS-OOD: A Vision-Language Augmented Framework for Out-of-Distribution Detection in Remote Sensing",
        "summary": "Out-of-distribution (OOD) detection represents a critical challenge in remote\nsensing applications, where reliable identification of novel or anomalous\npatterns is essential for autonomous monitoring, disaster response, and\nenvironmental assessment. Despite remarkable progress in OOD detection for\nnatural images, existing methods and benchmarks remain poorly suited to remote\nsensing imagery due to data scarcity, complex multi-scale scene structures, and\npronounced distribution shifts. To this end, we propose RS-OOD, a novel\nframework that leverages remote sensing-specific vision-language modeling to\nenable robust few-shot OOD detection. Our approach introduces three key\ninnovations: spatial feature enhancement that improved scene discrimination, a\ndual-prompt alignment mechanism that cross-verifies scene context against\nfine-grained semantics for spatial-semantic consistency, and a\nconfidence-guided self-training loop that dynamically mines pseudo-labels to\nexpand training data without manual annotation. RS-OOD consistently outperforms\nexisting methods across multiple remote sensing benchmarks and enables\nefficient adaptation with minimal labeled data, demonstrating the critical\nvalue of spatial-semantic integration.",
        "url": "http://arxiv.org/abs/2509.02273v1",
        "published_date": "2025-09-02T12:48:39+00:00",
        "updated_date": "2025-09-02T12:48:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yingrui Ji",
            "Jiansheng Chen",
            "Jingbo Chen",
            "Anzhi Yue",
            "Chenhao Wang",
            "Kai Li",
            "Yao Zhu"
        ],
        "ai_categories": [
            "Multimodality",
            "AIGC",
            "Dataset"
        ],
        "tldr": "RS-OOD proposes a framework for out-of-distribution detection in remote sensing using vision-language models, outperforming existing methods and enabling efficient adaptation with minimal labeled data.",
        "tldr_zh": "RS-OOD提出了一种远程感知中的异常检测框架，利用视觉-语言模型，优于现有方法，并能够在最少标记数据的情况下实现有效的适应。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Omnidirectional Spatial Modeling from Correlated Panoramas",
        "summary": "Omnidirectional scene understanding is vital for various downstream\napplications, such as embodied AI, autonomous driving, and immersive\nenvironments, yet remains challenging due to geometric distortion and complex\nspatial relations in 360{\\deg} imagery. Existing omnidirectional methods\nachieve scene understanding within a single frame while neglecting cross-frame\ncorrelated panoramas. To bridge this gap, we introduce \\textbf{CFpano}, the\n\\textbf{first} benchmark dataset dedicated to cross-frame correlated panoramas\nvisual question answering in the holistic 360{\\deg} scenes. CFpano consists of\nover 2700 images together with over 8000 question-answer pairs, and the\nquestion types include both multiple choice and open-ended VQA. Building upon\nour CFpano, we further present \\methodname, a multi-modal large language model\n(MLLM) fine-tuned with Group Relative Policy Optimization (GRPO) and a set of\ntailored reward functions for robust and consistent reasoning with cross-frame\ncorrelated panoramas. Benchmark experiments with existing MLLMs are conducted\nwith our CFpano. The experimental results demonstrate that \\methodname achieves\nstate-of-the-art performance across both multiple-choice and open-ended VQA\ntasks, outperforming strong baselines on all major reasoning categories\n(\\textbf{+5.37\\%} in overall performance). Our analyses validate the\neffectiveness of GRPO and establish a new benchmark for panoramic scene\nunderstanding.",
        "url": "http://arxiv.org/abs/2509.02164v1",
        "published_date": "2025-09-02T10:14:55+00:00",
        "updated_date": "2025-09-02T10:14:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinshen Zhang",
            "Tongxi Fu",
            "Xu Zheng"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a new dataset and model for cross-frame correlated panoramas visual question answering in 360-degree scenes, achieving state-of-the-art performance.",
        "tldr_zh": "该论文介绍了一个新的数据集和模型，用于360度场景中的跨帧相关全景视觉问答，取得了最新技术水平。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing Zero-Shot Pedestrian Attribute Recognition with Synthetic Data Generation: A Comparative Study with Image-To-Image Diffusion Models",
        "summary": "Pedestrian Attribute Recognition (PAR) involves identifying various human\nattributes from images with applications in intelligent monitoring systems. The\nscarcity of large-scale annotated datasets hinders the generalization of PAR\nmodels, specially in complex scenarios involving occlusions, varying poses, and\ndiverse environments. Recent advances in diffusion models have shown promise\nfor generating diverse and realistic synthetic images, allowing to expand the\nsize and variability of training data. However, the potential of\ndiffusion-based data expansion for generating PAR-like images remains\nunderexplored. Such expansion may enhance the robustness and adaptability of\nPAR models in real-world scenarios. This paper investigates the effectiveness\nof diffusion models in generating synthetic pedestrian images tailored to PAR\ntasks. We identify key parameters of img2img diffusion-based data expansion;\nincluding text prompts, image properties, and the latest enhancements in\ndiffusion-based data augmentation, and examine their impact on the quality of\ngenerated images for PAR. Furthermore, we employ the best-performing expansion\napproach to generate synthetic images for training PAR models, by enriching the\nzero-shot datasets. Experimental results show that prompt alignment and image\nproperties are critical factors in image generation, with optimal selection\nleading to a 4.5% improvement in PAR recognition performance.",
        "url": "http://arxiv.org/abs/2509.02161v1",
        "published_date": "2025-09-02T10:12:53+00:00",
        "updated_date": "2025-09-02T10:12:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pablo Ayuso-Albizu",
            "Juan C. SanMiguel",
            "Pablo Carballeira"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper explores using diffusion models to generate synthetic pedestrian images for improving Pedestrian Attribute Recognition (PAR) models, showing a 4.5% improvement in recognition performance.",
        "tldr_zh": "本文探讨使用扩散模型生成合成行人图像，以提高行人属性识别（PAR）模型的性能，显示识别性能提高了4.5%。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Conditional-$t^3$VAE: Equitable Latent Space Allocation for Fair Generation",
        "summary": "Variational Autoencoders (VAEs) with global priors mirror the training set's\nclass frequency in latent space, underrepresenting tail classes and reducing\ngenerative fairness on imbalanced datasets. While $t^3$VAE improves robustness\nvia heavy-tailed Student's t-distribution priors, it still allocates latent\nvolume proportionally to the class frequency.In this work, we address this\nissue by explicitly enforcing equitable latent space allocation across classes.\nTo this end, we propose Conditional-$t^3$VAE, which defines a per-class\n\\mbox{Student's t} joint prior over latent and output variables, preventing\ndominance by majority classes. Our model is optimized using a closed-form\nobjective derived from the $\\gamma$-power divergence. Moreover, for\nclass-balanced generation, we derive an equal-weight latent mixture of\nStudent's t-distributions. On SVHN-LT, CIFAR100-LT, and CelebA,\nConditional-$t^3$VAE consistently achieves lower FID scores than both $t^3$VAE\nand Gaussian-based VAE baselines, particularly under severe class imbalance. In\nper-class F1 evaluations, Conditional-$t^3$VAE also outperforms the conditional\nGaussian VAE across all highly imbalanced settings. While Gaussian-based models\nremain competitive under mild imbalance ratio ($\\rho \\lesssim 3$), our approach\nsubstantially improves generative fairness and diversity in more extreme\nregimes.",
        "url": "http://arxiv.org/abs/2509.02154v1",
        "published_date": "2025-09-02T10:03:10+00:00",
        "updated_date": "2025-09-02T10:03:10+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "stat.ML"
        ],
        "authors": [
            "Aymene Mohammed Bouayed",
            "Samuel Deslauriers-Gauthier",
            "Adrian Iaccovelli",
            "David Naccache"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "Conditional-$t^3$VAE aims to address imbalanced datasets in VAEs by enforcing equitable latent space allocation, leading to improved generative fairness and diversity.",
        "tldr_zh": "Conditional-$t^3$VAE旨在通过强制平衡的潜在空间分配来解决VAEs中的不平衡数据集问题，从而提高生成公平性和多样性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GRMM: Real-Time High-Fidelity Gaussian Morphable Head Model with Learned Residuals",
        "summary": "3D Morphable Models (3DMMs) enable controllable facial geometry and\nexpression editing for reconstruction, animation, and AR/VR, but traditional\nPCA-based mesh models are limited in resolution, detail, and photorealism.\nNeural volumetric methods improve realism but remain too slow for interactive\nuse. Recent Gaussian Splatting (3DGS) based facial models achieve fast,\nhigh-quality rendering but still depend solely on a mesh-based 3DMM prior for\nexpression control, limiting their ability to capture fine-grained geometry,\nexpressions, and full-head coverage. We introduce GRMM, the first full-head\nGaussian 3D morphable model that augments a base 3DMM with residual geometry\nand appearance components, additive refinements that recover high-frequency\ndetails such as wrinkles, fine skin texture, and hairline variations. GRMM\nprovides disentangled control through low-dimensional, interpretable parameters\n(e.g., identity shape, facial expressions) while separately modelling residuals\nthat capture subject- and expression-specific detail beyond the base model's\ncapacity. Coarse decoders produce vertex-level mesh deformations, fine decoders\nrepresent per-Gaussian appearance, and a lightweight CNN refines rasterised\nimages for enhanced realism, all while maintaining 75 FPS real-time rendering.\nTo learn consistent, high-fidelity residuals, we present EXPRESS-50, the first\ndataset with 60 aligned expressions across 50 identities, enabling robust\ndisentanglement of identity and expression in Gaussian-based 3DMMs. Across\nmonocular 3D face reconstruction, novel-view synthesis, and expression\ntransfer, GRMM surpasses state-of-the-art methods in fidelity and expression\naccuracy while delivering interactive real-time performance.",
        "url": "http://arxiv.org/abs/2509.02141v1",
        "published_date": "2025-09-02T09:43:47+00:00",
        "updated_date": "2025-09-02T09:43:47+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Mohit Mendiratta",
            "Mayur Deshmukh",
            "Kartik Teotia",
            "Vladislav Golyanik",
            "Adam Kortylewski",
            "Christian Theobalt"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces GRMM, a real-time high-fidelity Gaussian morphable head model that improves facial geometry and expression editing by adding residual components for fine details, surpassing state-of-the-art methods in fidelity and expression accuracy.",
        "tldr_zh": "本文介绍了GRMM，这是一个实时高保真度的高斯可变头部模型，通过添加残余组件以获得精细细节来改善面部几何和表情编辑，超越了现有方法在保真度和表情准确性方面的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Scale, Don't Fine-tune: Guiding Multimodal LLMs for Efficient Visual Place Recognition at Test-Time",
        "summary": "Visual Place Recognition (VPR) has evolved from handcrafted descriptors to\ndeep learning approaches, yet significant challenges remain. Current\napproaches, including Vision Foundation Models (VFMs) and Multimodal Large\nLanguage Models (MLLMs), enhance semantic understanding but suffer from high\ncomputational overhead and limited cross-domain transferability when\nfine-tuned. To address these limitations, we propose a novel zero-shot\nframework employing Test-Time Scaling (TTS) that leverages MLLMs'\nvision-language alignment capabilities through Guidance-based methods for\ndirect similarity scoring. Our approach eliminates two-stage processing by\nemploying structured prompts that generate length-controllable JSON outputs.\nThe TTS framework with Uncertainty-Aware Self-Consistency (UASC) enables\nreal-time adaptation without additional training costs, achieving superior\ngeneralization across diverse environments. Experimental results demonstrate\nsignificant improvements in cross-domain VPR performance with up to 210$\\times$\ncomputational efficiency gains.",
        "url": "http://arxiv.org/abs/2509.02129v1",
        "published_date": "2025-09-02T09:25:13+00:00",
        "updated_date": "2025-09-02T09:25:13+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Jintao Cheng",
            "Weibin Li",
            "Jiehao Luo",
            "Xiaoyu Tang",
            "Zhijian He",
            "Jin Wu",
            "Yao Zou",
            "Wei Zhang"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Transformer"
        ],
        "tldr": "The paper proposes a zero-shot framework for efficient visual place recognition by leveraging Multimodal Large Language Models with Test-Time Scaling, achieving significant improvements in cross-domain VPR performance with computational efficiency gains.",
        "tldr_zh": "本文提出了一种零-shot框架，通过利用多模态大语言模型与测试时间缩放，在视觉地点识别方面实现了显著的改进，获得了计算效率的提升。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "NOOUGAT: Towards Unified Online and Offline Multi-Object Tracking",
        "summary": "The long-standing division between \\textit{online} and \\textit{offline}\nMulti-Object Tracking (MOT) has led to fragmented solutions that fail to\naddress the flexible temporal requirements of real-world deployment scenarios.\nCurrent \\textit{online} trackers rely on frame-by-frame hand-crafted\nassociation strategies and struggle with long-term occlusions, whereas\n\\textit{offline} approaches can cover larger time gaps, but still rely on\nheuristic stitching for arbitrarily long sequences. In this paper, we introduce\nNOOUGAT, the first tracker designed to operate with arbitrary temporal\nhorizons. NOOUGAT leverages a unified Graph Neural Network (GNN) framework that\nprocesses non-overlapping subclips, and fuses them through a novel\nAutoregressive Long-term Tracking (ALT) layer. The subclip size controls the\ntrade-off between latency and temporal context, enabling a wide range of\ndeployment scenarios, from frame-by-frame to batch processing. NOOUGAT achieves\nstate-of-the-art performance across both tracking regimes, improving\n\\textit{online} AssA by +2.3 on DanceTrack, +9.2 on SportsMOT, and +5.0 on\nMOT20, with even greater gains in \\textit{offline} mode.",
        "url": "http://arxiv.org/abs/2509.02111v1",
        "published_date": "2025-09-02T09:08:24+00:00",
        "updated_date": "2025-09-02T09:08:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Benjamin Missaoui",
            "Orcun Cetintas",
            "Guillem Brasó",
            "Tim Meinhardt",
            "Laura Leal-Taixé"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "NOOUGAT introduces a unified tracker for online and offline multi-object tracking with arbitrary temporal horizons using a Graph Neural Network framework and achieves state-of-the-art performance in both tracking regimes.",
        "tldr_zh": "NOOUGAT引入了一种统一的跟踪器，可在任意时间范围内进行在线和离线多目标跟踪，使用图神经网络框架，在两种跟踪模式下均取得最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SALAD -- Semantics-Aware Logical Anomaly Detection",
        "summary": "Recent surface anomaly detection methods excel at identifying structural\nanomalies, such as dents and scratches, but struggle with logical anomalies,\nsuch as irregular or missing object components. The best-performing logical\nanomaly detection approaches rely on aggregated pretrained features or\nhandcrafted descriptors (most often derived from composition maps), which\ndiscard spatial and semantic information, leading to suboptimal performance. We\npropose SALAD, a semantics-aware discriminative logical anomaly detection\nmethod that incorporates a newly proposed composition branch to explicitly\nmodel the distribution of object composition maps, consequently learning\nimportant semantic relationships. Additionally, we introduce a novel procedure\nfor extracting composition maps that requires no hand-made labels or\ncategory-specific information, in contrast to previous methods. By effectively\nmodelling the composition map distribution, SALAD significantly improves upon\nstate-of-the-art methods on the standard benchmark for logical anomaly\ndetection, MVTec LOCO, achieving an impressive image-level AUROC of 96.1%.\nCode: https://github.com/MaticFuc/SALAD",
        "url": "http://arxiv.org/abs/2509.02101v1",
        "published_date": "2025-09-02T08:58:39+00:00",
        "updated_date": "2025-09-02T08:58:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Matic Fučka",
            "Vitjan Zavrtanik",
            "Danijel Skočaj"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "SALAD is a new method for detecting logical anomalies in objects by incorporating semantic information and a novel composition branch, achieving impressive results on benchmark datasets.",
        "tldr_zh": "SALAD是一种新的方法，通过整合语义信息和新的构图分支来检测物体中的逻辑异常，在基准数据集上取得了令人印象深刻的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Data-Centric Approach to Pedestrian Attribute Recognition: Synthetic Augmentation via Prompt-driven Diffusion Models",
        "summary": "Pedestrian Attribute Recognition (PAR) is a challenging task as models are\nrequired to generalize across numerous attributes in real-world data.\nTraditional approaches focus on complex methods, yet recognition performance is\noften constrained by training dataset limitations, particularly the\nunder-representation of certain attributes. In this paper, we propose a\ndata-centric approach to improve PAR by synthetic data augmentation guided by\ntextual descriptions. First, we define a protocol to identify weakly recognized\nattributes across multiple datasets. Second, we propose a prompt-driven\npipeline that leverages diffusion models to generate synthetic pedestrian\nimages while preserving the consistency of PAR datasets. Finally, we derive a\nstrategy to seamlessly incorporate synthetic samples into training data, which\nconsiders prompt-based annotation rules and modifies the loss function. Results\non popular PAR datasets demonstrate that our approach not only boosts\nrecognition of underrepresented attributes but also improves overall model\nperformance beyond the targeted attributes. Notably, this approach strengthens\nzero-shot generalization without requiring architectural changes of the model,\npresenting an efficient and scalable solution to improve the recognition of\nattributes of pedestrians in the real world.",
        "url": "http://arxiv.org/abs/2509.02099v1",
        "published_date": "2025-09-02T08:56:39+00:00",
        "updated_date": "2025-09-02T08:56:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Alejandro Alonso",
            "Sawaiz A. Chaudhry",
            "Juan C. SanMiguel",
            "Álvaro García-Martín",
            "Pablo Ayuso-Albizu",
            "Pablo Carballeira"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes a data-centric approach for improving Pedestrian Attribute Recognition using synthetic data generated by prompt-driven diffusion models.",
        "tldr_zh": "本文提出了一种数据中心的方法，通过由提示驱动的扩散模型生成的合成数据来改善行人属性识别。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ContextFusion and Bootstrap: An Effective Approach to Improve Slot Attention-Based Object-Centric Learning",
        "summary": "A key human ability is to decompose a scene into distinct objects and use\ntheir relationships to understand the environment. Object-centric learning aims\nto mimic this process in an unsupervised manner. Recently, the slot\nattention-based framework has emerged as a leading approach in this area and\nhas been widely used in various downstream tasks. However, existing slot\nattention methods face two key limitations: (1) a lack of high-level semantic\ninformation. In current methods, image areas are assigned to slots based on\nlow-level features such as color and texture. This makes the model overly\nsensitive to low-level features and limits its understanding of object\ncontours, shapes, or other semantic characteristics. (2) The inability to\nfine-tune the encoder. Current methods require a stable feature space\nthroughout training to enable reconstruction from slots, which restricts the\nflexibility needed for effective object-centric learning. To address these\nlimitations, we propose a novel ContextFusion stage and a Bootstrap Branch,\nboth of which can be seamlessly integrated into existing slot attention models.\nIn the ContextFusion stage, we exploit semantic information from the foreground\nand background, incorporating an auxiliary indicator that provides additional\ncontextual cues about them to enrich the semantic content beyond low-level\nfeatures. In the Bootstrap Branch, we decouple feature adaptation from the\noriginal reconstruction phase and introduce a bootstrap strategy to train a\nfeature-adaptive mechanism, allowing for more flexible adaptation. Experimental\nresults show that our method significantly improves the performance of\ndifferent SOTA slot attention models on both simulated and real-world datasets.",
        "url": "http://arxiv.org/abs/2509.02032v1",
        "published_date": "2025-09-02T07:19:25+00:00",
        "updated_date": "2025-09-02T07:19:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pinzhuo Tian",
            "Shengjie Yang",
            "Hang Yu",
            "Alex C. Kot"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces ContextFusion and Bootstrap to improve slot attention-based object-centric learning by addressing limitations related to semantic information and encoder fine-tuning.",
        "tldr_zh": "本文介绍了ContextFusion和Bootstrap，通过解决与语义信息和编码器微调相关的限制，改进了基于槽关注的对象中心学习。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Vision-Based Embedded System for Noncontact Monitoring of Preterm Infant Behavior in Low-Resource Care Settings",
        "summary": "Preterm birth remains a leading cause of neonatal mortality,\ndisproportionately affecting low-resource settings with limited access to\nadvanced neonatal intensive care units (NICUs).Continuous monitoring of infant\nbehavior, such as sleep/awake states and crying episodes, is critical but\nrelies on manual observation or invasive sensors, which are prone to error,\nimpractical, and can cause skin damage. This paper presents a novel,\nnoninvasive, and automated vision-based framework to address this gap. We\nintroduce an embedded monitoring system that utilizes a quantized MobileNet\nmodel deployed on a Raspberry Pi for real-time behavioral state detection. When\ntrained and evaluated on public neonatal image datasets, our system achieves\nstate-of-the-art accuracy (91.8% for sleep detection and 97.7% for\ncrying/normal classification) while maintaining computational efficiency\nsuitable for edge deployment. Through comparative benchmarking, we provide a\ncritical analysis of the trade-offs between model size, inference latency, and\ndiagnostic accuracy. Our findings demonstrate that while larger architectures\n(e.g., ResNet152, VGG19) offer marginal gains in accuracy, their computational\ncost is prohibitive for real-time edge use. The proposed framework integrates\nthree key innovations: model quantization for memory-efficient inference (68%\nreduction in size), Raspberry Pi-optimized vision pipelines, and secure IoT\ncommunication for clinical alerts. This work conclusively shows that\nlightweight, optimized models such as the MobileNet offer the most viable\nfoundation for scalable, low-cost, and clinically actionable NICU monitoring\nsystems, paving the way for improved preterm care in resource-constrained\nenvironments.",
        "url": "http://arxiv.org/abs/2509.02018v1",
        "published_date": "2025-09-02T07:05:47+00:00",
        "updated_date": "2025-09-02T07:05:47+00:00",
        "categories": [
            "cs.CV",
            "cs.CY",
            "cs.LG",
            "I.4.9"
        ],
        "authors": [
            "Stanley Mugisha",
            "Rashid Kisitu",
            "Francis Komakech",
            "Excellence Favor"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper presents a vision-based embedded system for noncontact monitoring of preterm infant behavior in low-resource care settings, achieving high accuracy and efficiency.",
        "tldr_zh": "本文提出了一种基于视觉的嵌入式系统，用于在资源匮乏的护理环境中非接触监测早产婴儿行为，实现高准确性和效率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Palette Aligned Image Diffusion",
        "summary": "We introduce the Palette-Adapter, a novel method for conditioning\ntext-to-image diffusion models on a user-specified color palette. While\npalettes are a compact and intuitive tool widely used in creative workflows,\nthey introduce significant ambiguity and instability when used for conditioning\nimage generation. Our approach addresses this challenge by interpreting\npalettes as sparse histograms and introducing two scalar control parameters:\nhistogram entropy and palette-to-histogram distance, which allow flexible\ncontrol over the degree of palette adherence and color variation. We further\nintroduce a negative histogram mechanism that allows users to suppress specific\nundesired hues, improving adherence to the intended palette under the standard\nclassifier-free guidance mechanism. To ensure broad generalization across the\ncolor space, we train on a carefully curated dataset with balanced coverage of\nrare and common colors. Our method enables stable, semantically coherent\ngeneration across a wide range of palettes and prompts. We evaluate our method\nqualitatively, quantitatively, and through a user study, and show that it\nconsistently outperforms existing approaches in achieving both strong palette\nadherence and high image quality.",
        "url": "http://arxiv.org/abs/2509.02000v1",
        "published_date": "2025-09-02T06:26:52+00:00",
        "updated_date": "2025-09-02T06:26:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Elad Aharoni",
            "Noy Porat",
            "Dani Lischinski",
            "Ariel Shamir"
        ],
        "ai_categories": [
            "Multimodality",
            "LoRA",
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper introduces a method, Palette-Adapter, for conditioning text-to-image models on user-specified color palettes, improving adherence and color variation. It outperforms existing approaches in achieving strong palette adherence and high image quality.",
        "tldr_zh": "本文介绍了一种方法，Palette-Adapter，用于将文本到图像模型条件化为用户指定的色彩调色板，提高了色彩调色板的一致性和颜色变化。它在实现强大的色彩调色板一致性和高图像质量方面优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Draw-In-Mind: Learning Precise Image Editing via Chain-of-Thought Imagination",
        "summary": "In recent years, integrating multimodal understanding and generation into a\nsingle unified model has emerged as a promising paradigm. While this approach\nachieves strong results in text-to-image (T2I) generation, it still struggles\nwith precise image editing. We attribute this limitation to an imbalanced\ndivision of responsibilities. The understanding module primarily functions as a\ntranslator that encodes user instructions into semantic conditions, while the\ngeneration module must simultaneously act as designer and painter, inferring\nthe original layout, identifying the target editing region, and rendering the\nnew content. This imbalance is counterintuitive because the understanding\nmodule is typically trained with several times more data on complex reasoning\ntasks than the generation module. To address this issue, we introduce\nDraw-In-Mind (DIM), a dataset comprising two complementary subsets: (i)\nDIM-T2I, containing 14M long-context image-text pairs to enhance complex\ninstruction comprehension; and (ii) DIM-Edit, consisting of 233K\nchain-of-thought imaginations generated by GPT-4o, serving as explicit design\nblueprints for image edits. We connect a frozen Qwen2.5-VL-3B with a trainable\nSANA1.5-1.6B via a lightweight two-layer MLP, and train it on the proposed DIM\ndataset, resulting in DIM-4.6B-T2I/Edit. Despite its modest parameter scale,\nDIM-4.6B-Edit achieves SOTA or competitive performance on the ImgEdit and\nGEdit-Bench benchmarks, outperforming much larger models such as UniWorld-V1\nand Step1X-Edit. These findings demonstrate that explicitly assigning the\ndesign responsibility to the understanding module provides significant benefits\nfor image editing. Our dataset and models will be available at\nhttps://github.com/showlab/DIM.",
        "url": "http://arxiv.org/abs/2509.01986v1",
        "published_date": "2025-09-02T06:06:52+00:00",
        "updated_date": "2025-09-02T06:06:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ziyun Zeng",
            "Junhao Zhang",
            "Wei Li",
            "Mike Zheng Shou"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces Draw-In-Mind (DIM), a dataset and model that improves image editing by assigning design responsibility to the understanding module.",
        "tldr_zh": "该论文介绍了Draw-In-Mind（DIM），一种通过将设计责任分配给理解模块来改善图像编辑的数据集和模型。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Discrete Noise Inversion for Next-scale Autoregressive Text-based Image Editing",
        "summary": "Visual autoregressive models (VAR) have recently emerged as a promising class\nof generative models, achieving performance comparable to diffusion models in\ntext-to-image generation tasks. While conditional generation has been widely\nexplored, the ability to perform prompt-guided image editing without additional\ntraining is equally critical, as it supports numerous practical real-world\napplications. This paper investigates the text-to-image editing capabilities of\nVAR by introducing Visual AutoRegressive Inverse Noise (VARIN), the first noise\ninversion-based editing technique designed explicitly for VAR models. VARIN\nleverages a novel pseudo-inverse function for argmax sampling, named\nLocation-aware Argmax Inversion (LAI), to generate inverse Gumbel noises. These\ninverse noises enable precise reconstruction of the source image and facilitate\ntargeted, controllable edits aligned with textual prompts. Extensive\nexperiments demonstrate that VARIN effectively modifies source images according\nto specified prompts while significantly preserving the original background and\nstructural details, thus validating its efficacy as a practical editing\napproach.",
        "url": "http://arxiv.org/abs/2509.01984v2",
        "published_date": "2025-09-02T06:01:52+00:00",
        "updated_date": "2025-09-03T05:25:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Quan Dao",
            "Xiaoxiao He",
            "Ligong Han",
            "Ngan Hoai Nguyen",
            "Amin Heyrani Nobar",
            "Faez Ahmed",
            "Han Zhang",
            "Viet Anh Nguyen",
            "Dimitris Metaxas"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Multimodality"
        ],
        "tldr": "The paper introduces a noise inversion-based editing technique for autoregressive models to perform prompt-guided image editing.",
        "tldr_zh": "本文介绍了一种基于噪声反演的编辑技术，用于自回归模型执行指导图像编辑。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "2D Gaussian Splatting with Semantic Alignment for Image Inpainting",
        "summary": "Gaussian Splatting (GS), a recent technique for converting discrete points\ninto continuous spatial representations, has shown promising results in 3D\nscene modeling and 2D image super-resolution. In this paper, we explore its\nuntapped potential for image inpainting, which demands both locally coherent\npixel synthesis and globally consistent semantic restoration. We propose the\nfirst image inpainting framework based on 2D Gaussian Splatting, which encodes\nincomplete images into a continuous field of 2D Gaussian splat coefficients and\nreconstructs the final image via a differentiable rasterization process. The\ncontinuous rendering paradigm of GS inherently promotes pixel-level coherence\nin the inpainted results. To improve efficiency and scalability, we introduce a\npatch-wise rasterization strategy that reduces memory overhead and accelerates\ninference. For global semantic consistency, we incorporate features from a\npretrained DINO model. We observe that DINO's global features are naturally\nrobust to small missing regions and can be effectively adapted to guide\nsemantic alignment in large-mask scenarios, ensuring that the inpainted content\nremains contextually consistent with the surrounding scene. Extensive\nexperiments on standard benchmarks demonstrate that our method achieves\ncompetitive performance in both quantitative metrics and perceptual quality,\nestablishing a new direction for applying Gaussian Splatting to 2D image\nprocessing.",
        "url": "http://arxiv.org/abs/2509.01964v1",
        "published_date": "2025-09-02T05:12:52+00:00",
        "updated_date": "2025-09-02T05:12:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hongyu Li",
            "Chaofeng Chen",
            "Xiaoming Li",
            "Guangming Lu"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Dataset"
        ],
        "tldr": "This paper introduces a new method using 2D Gaussian Splatting for image inpainting, combining local pixel synthesis and global semantic restoration.",
        "tldr_zh": "本文介绍了一种新的方法，使用2D高斯喷射技术进行图像修复，结合了局部像素合成和全局语义修复。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Structure-aware Contrastive Learning for Diagram Understanding of Multimodal Models",
        "summary": "Multimodal models, such as the Contrastive Language-Image Pre-training (CLIP)\nmodel, have demonstrated remarkable success in aligning visual and linguistic\nrepresentations. However, these models exhibit limitations when applied to\nspecialised visual domains, such as diagrams, which encode structured, symbolic\ninformation distinct from that of natural imagery.\n  In this paper, we introduce a novel training paradigm explicitly designed to\nenhance the comprehension of diagrammatic images within vision-language models.\nOur approach uses ``hard'' samples for our proposed contrastive learning that\nincorporates two specialised loss functions that leverage the inherent\nstructural properties of diagrams. By integrating these objectives into model\ntraining, our method enables models to develop a more structured and\nsemantically coherent understanding of diagrammatic content.\n  We empirically validate our approach on a benchmark dataset of flowcharts, as\na representative class of diagrammatic imagery, demonstrating substantial\nimprovements over standard CLIP and conventional hard negative CLIP learning\nparadigms for both image-text matching and visual question answering tasks. Our\nfindings underscore the significance of tailored training strategies for\nspecialised tasks and contribute to advancing diagrammatic understanding within\nthe broader landscape of vision-language integration.",
        "url": "http://arxiv.org/abs/2509.01959v1",
        "published_date": "2025-09-02T05:02:23+00:00",
        "updated_date": "2025-09-02T05:02:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Hiroshi Sasaki"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a structure-aware contrastive learning method to improve diagram understanding in multimodal models, showing significant improvements over existing methods on flowchart datasets.",
        "tldr_zh": "本文提出了一种结构感知对比学习方法，以改善多模态模型对图表的理解，在流程图数据集上显示出与现有方法相比的显着改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Diffusion-Based Framework for Configurable and Realistic Multi-Storage Trace Generation",
        "summary": "We propose DiTTO, a novel diffusion-based framework for generating realistic,\nprecisely configurable, and diverse multi-device storage traces. Leveraging\nadvanced diffusion tech- niques, DiTTO enables the synthesis of high-fidelity\ncontinuous traces that capture temporal dynamics and inter-device dependencies\nwith user-defined configurations. Our experimental results demonstrate that\nDiTTO can generate traces with high fidelity and diversity while aligning\nclosely with guided configurations with only 8% errors.",
        "url": "http://arxiv.org/abs/2509.01919v1",
        "published_date": "2025-09-02T03:29:48+00:00",
        "updated_date": "2025-09-02T03:29:48+00:00",
        "categories": [
            "cs.CV",
            "cs.PF"
        ],
        "authors": [
            "Seohyun Kim",
            "Junyoung Lee",
            "Jongho Park",
            "Jinhyung Koo",
            "Sungjin Lee",
            "Yeseong Kim"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "DiTTO is a diffusion-based framework for generating realistic and configurable multi-device storage traces with high fidelity and diversity.",
        "tldr_zh": "DiTTO是一个基于扩散的框架，用于生成具有高保真度和多样性的多设备存储痕迹。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events",
        "summary": "Remote sensing is critical for disaster monitoring, yet existing datasets\nlack temporal image pairs and detailed textual annotations. While\nsingle-snapshot imagery dominates current resources, it fails to capture\ndynamic disaster impacts over time. To address this gap, we introduce the\nRemote Sensing Change Caption (RSCC) dataset, a large-scale benchmark\ncomprising 62,315 pre-/post-disaster image pairs (spanning earthquakes, floods,\nwildfires, and more) paired with rich, human-like change captions. By bridging\nthe temporal and semantic divide in remote sensing data, RSCC enables robust\ntraining and evaluation of vision-language models for disaster-aware\nbi-temporal understanding. Our results highlight RSCC's ability to facilitate\ndetailed disaster-related analysis, paving the way for more accurate,\ninterpretable, and scalable vision-language applications in remote sensing.\nCode and dataset are available at https://github.com/Bili-Sakura/RSCC.",
        "url": "http://arxiv.org/abs/2509.01907v1",
        "published_date": "2025-09-02T03:01:23+00:00",
        "updated_date": "2025-09-02T03:01:23+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Zhenyuan Chen",
            "Chenxi Wang",
            "Ningyu Zhang",
            "Feng Zhang"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper introduces a new dataset for disaster event monitoring using remote sensing data, enabling training of vision-language models for understanding disaster impacts over time.",
        "tldr_zh": "本文介绍了一个新的数据集，用于利用遥感数据监测灾害事件，从而实现训练视觉-语言模型以理解灾害影响。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DroneSR: Rethinking Few-shot Thermal Image Super-Resolution from Drone-based Perspective",
        "summary": "Although large scale models achieve significant improvements in performance,\nthe overfitting challenge still frequently undermines their generalization\nability. In super resolution tasks on images, diffusion models as\nrepresentatives of generative models typically adopt large scale architectures.\nHowever, few-shot drone-captured infrared training data frequently induces\nsevere overfitting in large-scale architectures. To address this key challenge,\nour method proposes a new Gaussian quantization representation learning method\noriented to diffusion models that alleviates overfitting and enhances\nrobustness. At the same time, an effective monitoring mechanism tracks large\nscale architectures during training to detect signs of overfitting. By\nintroducing Gaussian quantization representation learning, our method\neffectively reduces overfitting while maintaining architecture complexity. On\nthis basis, we construct a multi source drone-based infrared image benchmark\ndataset for detection and use it to emphasize overfitting issues of large scale\narchitectures in few sample, drone-based diverse drone-based image\nreconstruction scenarios. To verify the efficacy of the method in mitigating\noverfitting, experiments are conducted on the constructed benchmark.\nExperimental results demonstrate that our method outperforms existing super\nresolution approaches and significantly mitigates overfitting of large scale\narchitectures under complex conditions. The code and DroneSR dataset will be\navailable at: https://github.com/wengzp1/GARLSR.",
        "url": "http://arxiv.org/abs/2509.01898v1",
        "published_date": "2025-09-02T02:37:42+00:00",
        "updated_date": "2025-09-02T02:37:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhipeng Weng",
            "Xiaopeng Liu",
            "Ce Liu",
            "Xingyuan Guo",
            "Yukai Shi",
            "Liang Lin"
        ],
        "ai_categories": [
            "Dataset",
            "Diffusion"
        ],
        "tldr": "The paper introduces a method called DroneSR for few-shot thermal image super-resolution from a drone-based perspective, focusing on mitigating overfitting in large-scale architectures.",
        "tldr_zh": "本文介绍了一种名为DroneSR的方法，针对从基于无人机的视角进行少样本热红外图像超分辨率，重点解决大规模架构中的过拟合问题。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HydroVision: Predicting Optically Active Parameters in Surface Water Using Computer Vision",
        "summary": "Ongoing advancements in computer vision, particularly in pattern recognition\nand scene classification, have enabled new applications in environmental\nmonitoring. Deep learning now offers non-contact methods for assessing water\nquality and detecting contamination, both critical for disaster response and\npublic health protection. This work introduces HydroVision, a deep\nlearning-based scene classification framework that estimates optically active\nwater quality parameters including Chlorophyll-Alpha, Chlorophylls, Colored\nDissolved Organic Matter (CDOM), Phycocyanins, Suspended Sediments, and\nTurbidity from standard Red-Green-Blue (RGB) images of surface water.\nHydroVision supports early detection of contamination trends and strengthens\nmonitoring by regulatory agencies during external environmental stressors,\nindustrial activities, and force majeure events. The model is trained on more\nthan 500,000 seasonally varied images collected from the United States\nGeological Survey Hydrologic Imagery Visualization and Information System\nbetween 2022 and 2024. This approach leverages widely available RGB imagery as\na scalable, cost-effective alternative to traditional multispectral and\nhyperspectral remote sensing. Four state-of-the-art convolutional neural\nnetworks (VGG-16, ResNet50, MobileNetV2, DenseNet121) and a Vision Transformer\nare evaluated through transfer learning to identify the best-performing\narchitecture. DenseNet121 achieves the highest validation performance, with an\nR2 score of 0.89 in predicting CDOM, demonstrating the framework's promise for\nreal-world water quality monitoring across diverse conditions. While the\ncurrent model is optimized for well-lit imagery, future work will focus on\nimproving robustness under low-light and obstructed scenarios to expand its\noperational utility.",
        "url": "http://arxiv.org/abs/2509.01882v2",
        "published_date": "2025-09-02T02:12:52+00:00",
        "updated_date": "2025-09-03T13:00:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shubham Laxmikant Deshmukh",
            "Matthew Wilchek",
            "Feras A. Batarseh"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "HydroVision is a deep learning-based framework that predicts optically active water quality parameters using RGB images, showing promise for real-world water quality monitoring.",
        "tldr_zh": "HydroVision 是一种基于深度学习的框架，使用 RGB 图像预测光学活性水质参数，对实际水质监测具有潜在前景。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AI-Driven Marine Robotics: Emerging Trends in Underwater Perception and Ecosystem Monitoring",
        "summary": "Marine ecosystems face increasing pressure due to climate change, driving the\nneed for scalable, AI-powered monitoring solutions. This paper examines the\nrapid emergence of underwater AI as a major research frontier and analyzes the\nfactors that have transformed marine perception from a niche application into a\ncatalyst for AI innovation. We identify three convergent drivers: environmental\nnecessity for ecosystem-scale monitoring, democratization of underwater\ndatasets through citizen science platforms, and researcher migration from\nsaturated terrestrial computer vision domains. Our analysis reveals how unique\nunderwater challenges - turbidity, cryptic species detection, expert annotation\nbottlenecks, and cross-ecosystem generalization - are driving fundamental\nadvances in weakly supervised learning, open-set recognition, and robust\nperception under degraded conditions. We survey emerging trends in datasets,\nscene understanding and 3D reconstruction, highlighting the paradigm shift from\npassive observation toward AI-driven, targeted intervention capabilities. The\npaper demonstrates how underwater constraints are pushing the boundaries of\nfoundation models, self-supervised learning, and perception, with\nmethodological innovations that extend far beyond marine applications to\nbenefit general computer vision, robotics, and environmental monitoring.",
        "url": "http://arxiv.org/abs/2509.01878v1",
        "published_date": "2025-09-02T01:51:31+00:00",
        "updated_date": "2025-09-02T01:51:31+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Scarlett Raine",
            "Tobias Fischer"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper explores the use of AI in marine robotics for underwater perception and ecosystem monitoring, highlighting challenges and innovations.",
        "tldr_zh": "本文探讨了AI在海洋机器人领域的应用，重点关注水下感知和生态系统监测，突出挑战和创新。",
        "relevance_score": 1,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Doctoral Thesis: Geometric Deep Learning For Camera Pose Prediction, Registration, Depth Estimation, and 3D Reconstruction",
        "summary": "Modern deep learning developments create new opportunities for 3D mapping\ntechnology, scene reconstruction pipelines, and virtual reality development.\nDespite advances in 3D deep learning technology, direct training of deep\nlearning models on 3D data faces challenges due to the high dimensionality\ninherent in 3D data and the scarcity of labeled datasets. Structure-from-motion\n(SfM) and Simultaneous Localization and Mapping (SLAM) exhibit robust\nperformance when applied to structured indoor environments but often struggle\nwith ambiguous features in unstructured environments. These techniques often\nstruggle to generate detailed geometric representations effective for\ndownstream tasks such as rendering and semantic analysis. Current limitations\nrequire the development of 3D representation methods that combine traditional\ngeometric techniques with deep learning capabilities to generate robust\ngeometry-aware deep learning models.\n  The dissertation provides solutions to the fundamental challenges in 3D\nvision by developing geometric deep learning methods tailored for essential\ntasks such as camera pose estimation, point cloud registration, depth\nprediction, and 3D reconstruction. The integration of geometric priors or\nconstraints, such as including depth information, surface normals, and\nequivariance into deep learning models, enhances both the accuracy and\nrobustness of geometric representations. This study systematically investigates\nkey components of 3D vision, including camera pose estimation, point cloud\nregistration, depth estimation, and high-fidelity 3D reconstruction,\ndemonstrating their effectiveness across real-world applications such as\ndigital cultural heritage preservation and immersive VR/AR environments.",
        "url": "http://arxiv.org/abs/2509.01873v1",
        "published_date": "2025-09-02T01:35:44+00:00",
        "updated_date": "2025-09-02T01:35:44+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xueyang Kang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces geometric deep learning methods for camera pose prediction, registration, depth estimation, and 3D reconstruction, addressing challenges in 3D data processing and representation.",
        "tldr_zh": "本文介绍了用于相机姿态预测、注册、深度估计和三维重建的几何深度学习方法，解决了三维数据处理和表示中的挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "PractiLight: Practical Light Control Using Foundational Diffusion Models",
        "summary": "Light control in generated images is a difficult task, posing specific\nchallenges, spanning over the entire image and frequency spectrum. Most\napproaches tackle this problem by training on extensive yet domain-specific\ndatasets, limiting the inherent generalization and applicability of the\nfoundational backbones used. Instead, PractiLight is a practical approach,\neffectively leveraging foundational understanding of recent generative models\nfor the task. Our key insight is that lighting relationships in an image are\nsimilar in nature to token interaction in self-attention layers, and hence are\nbest represented there. Based on this and other analyses regarding the\nimportance of early diffusion iterations, PractiLight trains a lightweight LoRA\nregressor to produce the direct irradiance map for a given image, using a small\nset of training images. We then employ this regressor to incorporate the\ndesired lighting into the generation process of another image using Classifier\nGuidance. This careful design generalizes well to diverse conditions and image\ndomains. We demonstrate state-of-the-art performance in terms of quality and\ncontrol with proven parameter and data efficiency compared to leading works\nover a wide variety of scenes types. We hope this work affirms that image\nlighting can feasibly be controlled by tapping into foundational knowledge,\nenabling practical and general relighting.",
        "url": "http://arxiv.org/abs/2509.01837v1",
        "published_date": "2025-09-01T23:38:40+00:00",
        "updated_date": "2025-09-01T23:38:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yotam Erel",
            "Rishabh Dabral",
            "Vladislav Golyanik",
            "Amit H. Bermano",
            "Christian Theobalt"
        ],
        "ai_categories": [
            "Diffusion",
            "LoRA",
            "Dataset",
            "GAN"
        ],
        "tldr": "PractiLight proposes a practical approach to light control in generated images by leveraging foundational understanding of generative models, achieving state-of-the-art performance with parameter and data efficiency.",
        "tldr_zh": "PractiLight提出了一种实用的方法来控制生成图像中的光线，通过利用生成模型的基础理解，在参数和数据效率上实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TransMatch: A Transfer-Learning Framework for Defect Detection in Laser Powder Bed Fusion Additive Manufacturing",
        "summary": "Surface defects in Laser Powder Bed Fusion (LPBF) pose significant risks to\nthe structural integrity of additively manufactured components. This paper\nintroduces TransMatch, a novel framework that merges transfer learning and\nsemi-supervised few-shot learning to address the scarcity of labeled AM defect\ndata. By effectively leveraging both labeled and unlabeled novel-class images,\nTransMatch circumvents the limitations of previous meta-learning approaches.\nExperimental evaluations on a Surface Defects dataset of 8,284 images\ndemonstrate the efficacy of TransMatch, achieving 98.91% accuracy with minimal\nloss, alongside high precision, recall, and F1-scores for multiple defect\nclasses. These findings underscore its robustness in accurately identifying\ndiverse defects, such as cracks, pinholes, holes, and spatter. TransMatch thus\nrepresents a significant leap forward in additive manufacturing defect\ndetection, offering a practical and scalable solution for quality assurance and\nreliability across a wide range of industrial applications.",
        "url": "http://arxiv.org/abs/2509.01754v1",
        "published_date": "2025-09-01T20:15:26+00:00",
        "updated_date": "2025-09-01T20:15:26+00:00",
        "categories": [
            "cs.CV",
            "physics.comp-ph"
        ],
        "authors": [
            "Mohsen Asghari Ilani",
            "Yaser Mike Banad"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "TransMatch introduces a novel transfer-learning framework for defect detection in additive manufacturing, achieving high accuracy and robustness in identifying diverse defects.",
        "tldr_zh": "TransMatch引入了一种新颖的迁移学习框架，用于在增材制造中检测缺陷，实现了高准确性和强大的识别各种缺陷的能力。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "GaussianGAN: Real-Time Photorealistic controllable Human Avatars",
        "summary": "Photorealistic and controllable human avatars have gained popularity in the\nresearch community thanks to rapid advances in neural rendering, providing fast\nand realistic synthesis tools. However, a limitation of current solutions is\nthe presence of noticeable blurring. To solve this problem, we propose\nGaussianGAN, an animatable avatar approach developed for photorealistic\nrendering of people in real-time. We introduce a novel Gaussian splatting\ndensification strategy to build Gaussian points from the surface of cylindrical\nstructures around estimated skeletal limbs. Given the camera calibration, we\nrender an accurate semantic segmentation with our novel view segmentation\nmodule. Finally, a UNet generator uses the rendered Gaussian splatting features\nand the segmentation maps to create photorealistic digital avatars. Our method\nruns in real-time with a rendering speed of 79 FPS. It outperforms previous\nmethods regarding visual perception and quality, achieving a state-of-the-art\nresults in terms of a pixel fidelity of 32.94db on the ZJU Mocap dataset and\n33.39db on the Thuman4 dataset.",
        "url": "http://arxiv.org/abs/2509.01681v1",
        "published_date": "2025-09-01T18:01:34+00:00",
        "updated_date": "2025-09-01T18:01:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mohamed Ilyes Lakhal",
            "Richard Bowden"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "GaussianGAN is a novel approach for generating photorealistic and controllable human avatars in real-time, outperforming previous methods in visual perception and quality.",
        "tldr_zh": "GaussianGAN是一种新方法，用于实时生成逼真可控的人类化身，优于以往的方法在视觉感知和质量方面。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Fake & Square: Training Self-Supervised Vision Transformers with Synthetic Data and Synthetic Hard Negatives",
        "summary": "This paper does not introduce a new method per se. Instead, we build on\nexisting self-supervised learning approaches for vision, drawing inspiration\nfrom the adage \"fake it till you make it\". While contrastive self-supervised\nlearning has achieved remarkable success, it typically relies on vast amounts\nof real-world data and carefully curated hard negatives. To explore\nalternatives to these requirements, we investigate two forms of \"faking it\" in\nvision transformers. First, we study the potential of generative models for\nunsupervised representation learning, leveraging synthetic data to augment\nsample diversity. Second, we examine the feasibility of generating synthetic\nhard negatives in the representation space, creating diverse and challenging\ncontrasts. Our framework - dubbed Syn2Co - combines both approaches and\nevaluates whether synthetically enhanced training can lead to more robust and\ntransferable visual representations on DeiT-S and Swin-T architectures. Our\nfindings highlight the promise and limitations of synthetic data in\nself-supervised learning, offering insights for future work in this direction.",
        "url": "http://arxiv.org/abs/2509.02029v1",
        "published_date": "2025-09-02T07:17:46+00:00",
        "updated_date": "2025-09-02T07:17:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Nikolaos Giakoumoglou",
            "Andreas Floros",
            "Kleanthis Marios Papadopoulos",
            "Tania Stathaki"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper explores using synthetic data and synthetic hard negatives to enhance self-supervised learning in vision transformers, highlighting the potential and limitations of this approach for future work.",
        "tldr_zh": "本文探讨了使用合成数据和合成困难样本来增强自监督学习中的视觉转换器，强调了这种方法对未来工作的潜力和局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.75
    },
    {
        "title": "Examination of PCA Utilisation for Multilabel Classifier of Multispectral Images",
        "summary": "This paper investigates the utility of Principal Component Analysis (PCA) for\nmulti-label classification of multispectral images using ResNet50 and DINOv2,\nacknowledging the high dimensionality of such data and the associated\nprocessing challenges. Multi-label classification, where each image may belong\nto multiple classes, adds further complexity to feature extraction. Our\npipeline includes an optional PCA step that reduces the data to three\ndimensions before feeding it into a three-layer classifier. The findings\ndemonstrate that the effectiveness of PCA for multi-label multispectral image\nclassification depends strongly on the chosen deep learning architecture and\ntraining strategy, opening avenues for future research into self-supervised\npre-training and alternative dimensionality reduction approaches.",
        "url": "http://arxiv.org/abs/2509.01691v1",
        "published_date": "2025-09-01T18:06:34+00:00",
        "updated_date": "2025-09-01T18:06:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Filip Karpowicz",
            "Wiktor Kępiński",
            "Bartosz Staszyński",
            "Grzegorz Sarwas"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper explores the use of Principal Component Analysis (PCA) for multi-label classification of multispectral images, showing varying effectiveness depending on the deep learning architecture and training strategy.",
        "tldr_zh": "本文探讨了在多光谱图像的多标签分类中使用主成分分析（PCA），结果显示效果取决于深度学习架构和训练策略。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.75
    },
    {
        "title": "Enhancing Fitness Movement Recognition with Attention Mechanism and Pre-Trained Feature Extractors",
        "summary": "Fitness movement recognition, a focused subdomain of human activity\nrecognition (HAR), plays a vital role in health monitoring, rehabilitation, and\npersonalized fitness training by enabling automated exercise classification\nfrom video data. However, many existing deep learning approaches rely on\ncomputationally intensive 3D models, limiting their feasibility in real-time or\nresource-constrained settings. In this paper, we present a lightweight and\neffective framework that integrates pre-trained 2D Convolutional Neural\nNetworks (CNNs) such as ResNet50, EfficientNet, and Vision Transformers (ViT)\nwith a Long Short-Term Memory (LSTM) network enhanced by spatial attention.\nThese models efficiently extract spatial features while the LSTM captures\ntemporal dependencies, and the attention mechanism emphasizes informative\nsegments. We evaluate the framework on a curated subset of the UCF101 dataset,\nachieving a peak accuracy of 93.34\\% with the ResNet50-based configuration.\nComparative results demonstrate the superiority of our approach over several\nstate-of-the-art HAR systems. The proposed method offers a scalable and\nreal-time-capable solution for fitness activity recognition with broader\napplications in vision-based health and activity monitoring.",
        "url": "http://arxiv.org/abs/2509.02511v1",
        "published_date": "2025-09-02T17:04:42+00:00",
        "updated_date": "2025-09-02T17:04:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shanjid Hasan Nishat",
            "Srabonti Deb",
            "Mohiuddin Ahmed"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a lightweight framework that combines pre-trained 2D CNNs with LSTM and attention mechanism for fitness movement recognition, showing superior results compared to existing systems.",
        "tldr_zh": "该论文提出了一个轻量级框架，将预训练的2D CNNs与LSTM和注意力机制结合起来，用于健身动作识别，在结果上表现优于现有系统。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Anisotropic Fourier Features for Positional Encoding in Medical Imaging",
        "summary": "The adoption of Transformer-based architectures in the medical domain is\ngrowing rapidly. In medical imaging, the analysis of complex shapes - such as\norgans, tissues, or other anatomical structures - combined with the often\nanisotropic nature of high-dimensional images complicates these adaptations. In\nthis study, we critically examine the role of Positional Encodings (PEs),\narguing that commonly used approaches may be suboptimal for the specific\nchallenges of medical imaging. Sinusoidal Positional Encodings (SPEs) have\nproven effective in vision tasks, but they struggle to preserve Euclidean\ndistances in higher-dimensional spaces. Isotropic Fourier Feature Positional\nEncodings (IFPEs) have been proposed to better preserve Euclidean distances,\nbut they lack the ability to account for anisotropy in images. To address these\nlimitations, we propose Anisotropic Fourier Feature Positional Encoding (AFPE),\na generalization of IFPE that incorporates anisotropic, class-specific, and\ndomain-specific spatial dependencies. We systematically benchmark AFPE against\ncommonly used PEs on multi-label classification in chest X-rays, organ\nclassification in CT images, and ejection fraction regression in\nechocardiography. Our results demonstrate that choosing the correct PE can\nsignificantly improve model performance. We show that the optimal PE depends on\nthe shape of the structure of interest and the anisotropy of the data. Finally,\nour proposed AFPE significantly outperforms state-of-the-art PEs in all tested\nanisotropic settings. We conclude that, in anisotropic medical images and\nvideos, it is of paramount importance to choose an anisotropic PE that fits the\ndata and the shape of interest.",
        "url": "http://arxiv.org/abs/2509.02488v1",
        "published_date": "2025-09-02T16:38:53+00:00",
        "updated_date": "2025-09-02T16:38:53+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Nabil Jabareen",
            "Dongsheng Yuan",
            "Dingming Liu",
            "Foo-Wei Ten",
            "Sören Lukassen"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces Anisotropic Fourier Feature Positional Encoding (AFPE) for medical imaging to address challenges in preserving Euclidean distances and accounting for anisotropy, outperforming existing methods in various medical tasks.",
        "tldr_zh": "本文提出了用于医学影像的各向异性傅里叶特征位置编码（AFPE），以解决保持欧氏距离和考虑各向异性的挑战，在各种医学任务中优于现有方法。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "MedDINOv3: How to adapt vision foundation models for medical image segmentation?",
        "summary": "Accurate segmentation of organs and tumors in CT and MRI scans is essential\nfor diagnosis, treatment planning, and disease monitoring. While deep learning\nhas advanced automated segmentation, most models remain task-specific, lacking\ngeneralizability across modalities and institutions. Vision foundation models\n(FMs) pretrained on billion-scale natural images offer powerful and\ntransferable representations. However, adapting them to medical imaging faces\ntwo key challenges: (1) the ViT backbone of most foundation models still\nunderperform specialized CNNs on medical image segmentation, and (2) the large\ndomain gap between natural and medical images limits transferability. We\nintroduce MedDINOv3, a simple and effective framework for adapting DINOv3 to\nmedical segmentation. We first revisit plain ViTs and design a simple and\neffective architecture with multi-scale token aggregation. Then, we perform\ndomain-adaptive pretraining on CT-3M, a curated collection of 3.87M axial CT\nslices, using a multi-stage DINOv3 recipe to learn robust dense features.\nMedDINOv3 matches or exceeds state-of-the-art performance across four\nsegmentation benchmarks, demonstrating the potential of vision foundation\nmodels as unified backbones for medical image segmentation. The code is\navailable at https://github.com/ricklisz/MedDINOv3.",
        "url": "http://arxiv.org/abs/2509.02379v2",
        "published_date": "2025-09-02T14:44:43+00:00",
        "updated_date": "2025-09-03T03:08:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuheng Li",
            "Yizhou Wu",
            "Yuxiang Lai",
            "Mingzhe Hu",
            "Xiaofeng Yang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "MedDINOv3 introduces a framework to adapt vision foundation models for medical image segmentation, achieving state-of-the-art performance across four benchmarks.",
        "tldr_zh": "MedDINOv3提出了一个框架，用于调整视觉基础模型以进行医学图像分割，在四个基准测试中取得了最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "SegFormer Fine-Tuning with Dropout: Advancing Hair Artifact Removal in Skin Lesion Analysis",
        "summary": "Hair artifacts in dermoscopic images present significant challenges for\naccurate skin lesion analysis, potentially obscuring critical diagnostic\nfeatures in dermatological assessments. This work introduces a fine-tuned\nSegFormer model augmented with dropout regularization to achieve precise hair\nmask segmentation. The proposed SegformerWithDropout architecture leverages the\nMiT-B2 encoder, pretrained on ImageNet, with an in-channel count of 3 and 2\noutput classes, incorporating a dropout probability of 0.3 in the segmentation\nhead to prevent overfitting. Training is conducted on a specialized dataset of\n500 dermoscopic skin lesion images with fine-grained hair mask annotations,\nemploying 10-fold cross-validation, AdamW optimization with a learning rate of\n0.001, and cross-entropy loss. Early stopping is applied based on validation\nloss, with a patience of 3 epochs and a maximum of 20 epochs per fold.\nPerformance is evaluated using a comprehensive suite of metrics, including\nIntersection over Union (IoU), Dice coefficient, Peak Signal-to-Noise Ratio\n(PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch\nSimilarity (LPIPS). Experimental results from the cross-validation demonstrate\nrobust performance, with average Dice coefficients reaching approximately 0.96\nand IoU values of 0.93, alongside favorable PSNR (around 34 dB), SSIM (0.97),\nand low LPIPS (0.06), highlighting the model's effectiveness in accurate hair\nartifact segmentation and its potential to enhance preprocessing for downstream\nskin cancer detection tasks.",
        "url": "http://arxiv.org/abs/2509.02156v1",
        "published_date": "2025-09-02T10:06:26+00:00",
        "updated_date": "2025-09-02T10:06:26+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Asif Mohammed Saad",
            "Umme Niraj Mahi"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a SegFormer model with dropout for precise hair mask segmentation in skin lesion analysis, achieving high performance metrics.",
        "tldr_zh": "该论文介绍了一种具有辍学特性的SegFormer模型，用于在皮肤病变分析中精确提取头发掩模，取得了高性能指标。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "See No Evil: Adversarial Attacks Against Linguistic-Visual Association in Referring Multi-Object Tracking Systems",
        "summary": "Language-vision understanding has driven the development of advanced\nperception systems, most notably the emerging paradigm of Referring\nMulti-Object Tracking (RMOT). By leveraging natural-language queries, RMOT\nsystems can selectively track objects that satisfy a given semantic\ndescription, guided through Transformer-based spatial-temporal reasoning\nmodules. End-to-End (E2E) RMOT models further unify feature extraction,\ntemporal memory, and spatial reasoning within a Transformer backbone, enabling\nlong-range spatial-temporal modeling over fused textual-visual representations.\nDespite these advances, the reliability and robustness of RMOT remain\nunderexplored. In this paper, we examine the security implications of RMOT\nsystems from a design-logic perspective, identifying adversarial\nvulnerabilities that compromise both the linguistic-visual referring and\ntrack-object matching components. Additionally, we uncover a novel\nvulnerability in advanced RMOT models employing FIFO-based memory, whereby\ntargeted and consistent attacks on their spatial-temporal reasoning introduce\nerrors that persist within the history buffer over multiple subsequent frames.\nWe present VEIL, a novel adversarial framework designed to disrupt the unified\nreferring-matching mechanisms of RMOT models. We show that carefully crafted\ndigital and physical perturbations can corrupt the tracking logic reliability,\ninducing track ID switches and terminations. We conduct comprehensive\nevaluations using the Refer-KITTI dataset to validate the effectiveness of VEIL\nand demonstrate the urgent need for security-aware RMOT designs for critical\nlarge-scale applications.",
        "url": "http://arxiv.org/abs/2509.02028v2",
        "published_date": "2025-09-02T07:17:32+00:00",
        "updated_date": "2025-09-03T02:28:19+00:00",
        "categories": [
            "cs.CV",
            "cs.CR"
        ],
        "authors": [
            "Halima Bouzidi",
            "Haoyu Liu",
            "Mohammad Abdullah Al Faruque"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper explores security vulnerabilities in Referring Multi-Object Tracking systems and introduces an adversarial framework called VEIL to disrupt tracking mechanisms.",
        "tldr_zh": "本文探讨了引用多目标跟踪系统中的安全漏洞，并引入了一种名为VEIL的敌对框架来破坏跟踪机制。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "EgoTouch: On-Body Touch Input Using AR/VR Headset Cameras",
        "summary": "In augmented and virtual reality (AR/VR) experiences, a user's arms and hands\ncan provide a convenient and tactile surface for touch input. Prior work has\nshown on-body input to have significant speed, accuracy, and ergonomic benefits\nover in-air interfaces, which are common today. In this work, we demonstrate\nhigh accuracy, bare hands (i.e., no special instrumentation of the user) skin\ninput using just an RGB camera, like those already integrated into all modern\nXR headsets. Our results show this approach can be accurate, and robust across\ndiverse lighting conditions, skin tones, and body motion (e.g., input while\nwalking). Finally, our pipeline also provides rich input metadata including\ntouch force, finger identification, angle of attack, and rotation. We believe\nthese are the requisite technical ingredients to more fully unlock on-skin\ninterfaces that have been well motivated in the HCI literature but have lacked\nrobust and practical methods.",
        "url": "http://arxiv.org/abs/2509.01786v1",
        "published_date": "2025-09-01T21:32:30+00:00",
        "updated_date": "2025-09-01T21:32:30+00:00",
        "categories": [
            "cs.HC",
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Vimal Mollyn",
            "Chris Harrison"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper presents EgoTouch, a method for on-body touch input using AR/VR headset cameras, showing high accuracy and robustness across different conditions.",
        "tldr_zh": "本文提出了EgoTouch，一种使用AR / VR头戴设备摄像头进行身体触摸输入的方法，展示了在不同条件下的高准确性和稳健性。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7.5
    },
    {
        "title": "Clinical Metadata Guided Limited-Angle CT Image Reconstruction",
        "summary": "Limited-angle computed tomography (LACT) offers improved temporal resolution\nand reduced radiation dose for cardiac imaging, but suffers from severe\nartifacts due to truncated projections. To address the ill-posedness of LACT\nreconstruction, we propose a two-stage diffusion framework guided by structured\nclinical metadata. In the first stage, a transformer-based diffusion model\nconditioned exclusively on metadata, including acquisition parameters, patient\ndemographics, and diagnostic impressions, generates coarse anatomical priors\nfrom noise. The second stage further refines the images by integrating both the\ncoarse prior and metadata to produce high-fidelity results. Physics-based data\nconsistency is enforced at each sampling step in both stages using an\nAlternating Direction Method of Multipliers module, ensuring alignment with the\nmeasured projections. Extensive experiments on both synthetic and real cardiac\nCT datasets demonstrate that incorporating metadata significantly improves\nreconstruction fidelity, particularly under severe angular truncation. Compared\nto existing metadata-free baselines, our method achieves superior performance\nin SSIM, PSNR, nMI, and PCC. Ablation studies confirm that different types of\nmetadata contribute complementary benefits, particularly diagnostic and\ndemographic priors under limited-angle conditions. These findings highlight the\ndual role of clinical metadata in improving both reconstruction quality and\nefficiency, supporting their integration into future metadata-guided medical\nimaging frameworks.",
        "url": "http://arxiv.org/abs/2509.01752v1",
        "published_date": "2025-09-01T20:14:15+00:00",
        "updated_date": "2025-09-01T20:14:15+00:00",
        "categories": [
            "cs.CV",
            "physics.med-ph"
        ],
        "authors": [
            "Yu Shi",
            "Shuyi Fan",
            "Changsheng Fang",
            "Shuo Han",
            "Haodong Li",
            "Li Zhou",
            "Bahareh Morovati",
            "Dayang Wang",
            "Hengyong Yu"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper proposes a two-stage diffusion framework guided by clinical metadata to improve limited-angle CT image reconstruction, showing superior performance compared to metadata-free methods.",
        "tldr_zh": "本文提出了一种受临床元数据指导的两阶段扩散框架，用于改善有限角度CT图像重建，显示出比无元数据方法更优越的性能。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "Manipulation as in Simulation: Enabling Accurate Geometry Perception in Robots",
        "summary": "Modern robotic manipulation primarily relies on visual observations in a 2D\ncolor space for skill learning but suffers from poor generalization. In\ncontrast, humans, living in a 3D world, depend more on physical properties-such\nas distance, size, and shape-than on texture when interacting with objects.\nSince such 3D geometric information can be acquired from widely available depth\ncameras, it appears feasible to endow robots with similar perceptual\ncapabilities. Our pilot study found that using depth cameras for manipulation\nis challenging, primarily due to their limited accuracy and susceptibility to\nvarious types of noise. In this work, we propose Camera Depth Models (CDMs) as\na simple plugin on daily-use depth cameras, which take RGB images and raw depth\nsignals as input and output denoised, accurate metric depth. To achieve this,\nwe develop a neural data engine that generates high-quality paired data from\nsimulation by modeling a depth camera's noise pattern. Our results show that\nCDMs achieve nearly simulation-level accuracy in depth prediction, effectively\nbridging the sim-to-real gap for manipulation tasks. Notably, our experiments\ndemonstrate, for the first time, that a policy trained on raw simulated depth,\nwithout the need for adding noise or real-world fine-tuning, generalizes\nseamlessly to real-world robots on two challenging long-horizon tasks involving\narticulated, reflective, and slender objects, with little to no performance\ndegradation. We hope our findings will inspire future research in utilizing\nsimulation data and 3D information in general robot policies.",
        "url": "http://arxiv.org/abs/2509.02530v1",
        "published_date": "2025-09-02T17:29:38+00:00",
        "updated_date": "2025-09-02T17:29:38+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Minghuan Liu",
            "Zhengbang Zhu",
            "Xiaoshen Han",
            "Peng Hu",
            "Haotong Lin",
            "Xinyao Li",
            "Jingxiao Chen",
            "Jiafeng Xu",
            "Yichu Yang",
            "Yunfeng Lin",
            "Xinghang Li",
            "Yong Yu",
            "Weinan Zhang",
            "Tao Kong",
            "Bingyi Kang"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces Camera Depth Models (CDMs) to improve depth perception in robots using depth cameras, bridging the sim-to-real gap for manipulation tasks.",
        "tldr_zh": "本文引入相机深度模型（CDMs），利用深度相机改善机器人的深度感知，弥合了模拟和真实世界的差距。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Efficient Pyramidal Analysis of Gigapixel Images on a Decentralized Modest Computer Cluster",
        "summary": "Analyzing gigapixel images is recognized as computationally demanding. In\nthis paper, we introduce PyramidAI, a technique for analyzing gigapixel images\nwith reduced computational cost. The proposed approach adopts a gradual\nanalysis of the image, beginning with lower resolutions and progressively\nconcentrating on regions of interest for detailed examination at higher\nresolutions. We investigated two strategies for tuning the accuracy-computation\nperformance trade-off when implementing the adaptive resolution selection,\nvalidated against the Camelyon16 dataset of biomedical images. Our results\ndemonstrate that PyramidAI substantially decreases the amount of processed data\nrequired for analysis by up to 2.65x, while preserving the accuracy in\nidentifying relevant sections on a single computer. To ensure democratization\nof gigapixel image analysis, we evaluated the potential to use mainstream\ncomputers to perform the computation by exploiting the parallelism potential of\nthe approach. Using a simulator, we estimated the best data distribution and\nload balancing algorithm according to the number of workers. The selected\nalgorithms were implemented and highlighted the same conclusions in a\nreal-world setting. Analysis time is reduced from more than an hour to a few\nminutes using 12 modest workers, offering a practical solution for efficient\nlarge-scale image analysis.",
        "url": "http://arxiv.org/abs/2509.02440v1",
        "published_date": "2025-09-02T15:44:25+00:00",
        "updated_date": "2025-09-02T15:44:25+00:00",
        "categories": [
            "cs.DC",
            "cs.CV"
        ],
        "authors": [
            "Marie Reinbigler",
            "Rishi Sharma",
            "Rafael Pires",
            "Elisabeth Brunet",
            "Anne-Marie Kermarrec",
            "Catalin Fetita"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces PyramidAI, a technique for analyzing gigapixel images with reduced computational cost, achieving up to 2.65x decrease in processed data while maintaining accuracy.",
        "tldr_zh": "本文介绍了PyramidAI，一种分析千兆像素图像的技术，以降低计算成本，实现了高达2.65倍的数据处理减少，同时保持准确性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Ordinal Adaptive Correction: A Data-Centric Approach to Ordinal Image Classification with Noisy Labels",
        "summary": "Labeled data is a fundamental component in training supervised deep learning\nmodels for computer vision tasks. However, the labeling process, especially for\nordinal image classification where class boundaries are often ambiguous, is\nprone to error and noise. Such label noise can significantly degrade the\nperformance and reliability of machine learning models. This paper addresses\nthe problem of detecting and correcting label noise in ordinal image\nclassification tasks. To this end, a novel data-centric method called ORDinal\nAdaptive Correction (ORDAC) is proposed for adaptive correction of noisy\nlabels. The proposed approach leverages the capabilities of Label Distribution\nLearning (LDL) to model the inherent ambiguity and uncertainty present in\nordinal labels. During training, ORDAC dynamically adjusts the mean and\nstandard deviation of the label distribution for each sample. Rather than\ndiscarding potentially noisy samples, this approach aims to correct them and\nmake optimal use of the entire training dataset. The effectiveness of the\nproposed method is evaluated on benchmark datasets for age estimation (Adience)\nand disease severity detection (Diabetic Retinopathy) under various asymmetric\nGaussian noise scenarios. Results show that ORDAC and its extended versions\n(ORDAC_C and ORDAC_R) lead to significant improvements in model performance.\nFor instance, on the Adience dataset with 40% noise, ORDAC_R reduced the mean\nabsolute error from 0.86 to 0.62 and increased the recall metric from 0.37 to\n0.49. The method also demonstrated its effectiveness in correcting intrinsic\nnoise present in the original datasets. This research indicates that adaptive\nlabel correction using label distributions is an effective strategy to enhance\nthe robustness and accuracy of ordinal classification models in the presence of\nnoisy data.",
        "url": "http://arxiv.org/abs/2509.02351v1",
        "published_date": "2025-09-02T14:17:16+00:00",
        "updated_date": "2025-09-02T14:17:16+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Alireza Sedighi Moghaddam",
            "Mohammad Reza Mohammadi"
        ],
        "ai_categories": [
            "AIGC"
        ],
        "tldr": "The paper proposes a method called ORDinal Adaptive Correction (ORDAC) for correcting label noise in ordinal image classification tasks using Label Distribution Learning (LDL), leading to significant improvements in model performance.",
        "tldr_zh": "本文提出了一种称为ORDinal Adaptive Correction (ORDAC)的方法，利用标签分布学习（LDL）来纠正序数图像分类任务中的标签噪声，从而显著提高模型性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "ADVMEM: Adversarial Memory Initialization for Realistic Test-Time Adaptation via Tracklet-Based Benchmarking",
        "summary": "We introduce a novel tracklet-based dataset for benchmarking test-time\nadaptation (TTA) methods. The aim of this dataset is to mimic the intricate\nchallenges encountered in real-world environments such as images captured by\nhand-held cameras, self-driving cars, etc. The current benchmarks for TTA focus\non how models face distribution shifts, when deployed, and on violations to the\ncustomary independent-and-identically-distributed (i.i.d.) assumption in\nmachine learning. Yet, these benchmarks fail to faithfully represent realistic\nscenarios that naturally display temporal dependencies, such as how consecutive\nframes from a video stream likely show the same object across time. We address\nthis shortcoming of current datasets by proposing a novel TTA benchmark we call\nthe \"Inherent Temporal Dependencies\" (ITD) dataset. We ensure the instances in\nITD naturally embody temporal dependencies by collecting them from\ntracklets-sequences of object-centric images we compile from the bounding boxes\nof an object-tracking dataset. We use ITD to conduct a thorough experimental\nanalysis of current TTA methods, and shed light on the limitations of these\nmethods when faced with the challenges of temporal dependencies. Moreover, we\nbuild upon these insights and propose a novel adversarial memory initialization\nstrategy to improve memory-based TTA methods. We find this strategy\nsubstantially boosts the performance of various methods on our challenging\nbenchmark.",
        "url": "http://arxiv.org/abs/2509.02182v1",
        "published_date": "2025-09-02T10:45:33+00:00",
        "updated_date": "2025-09-02T10:45:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shyma Alhuwaider",
            "Motasem Alfarra",
            "Juan C. Perez",
            "Merey Ramazanova",
            "Bernard Ghanem"
        ],
        "ai_categories": [
            "Dataset",
            "GAN",
            "Other"
        ],
        "tldr": "The paper introduces a new dataset to benchmark test-time adaptation methods, focusing on realistic scenarios with temporal dependencies. It proposes an adversarial memory initialization strategy to improve memory-based TTA methods.",
        "tldr_zh": "该论文引入了一个新的数据集来评估测试时间适应方法，重点关注具有时间依赖性的真实场景。它提出了一种对抗性内存初始化策略，以改进基于内存的TTA方法。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Ensemble-Based Event Camera Place Recognition Under Varying Illumination",
        "summary": "Compared to conventional cameras, event cameras provide a high dynamic range\nand low latency, offering greater robustness to rapid motion and challenging\nlighting conditions. Although the potential of event cameras for visual place\nrecognition (VPR) has been established, developing robust VPR frameworks under\nsevere illumination changes remains an open research problem. In this paper, we\nintroduce an ensemble-based approach to event camera place recognition that\ncombines sequence-matched results from multiple event-to-frame reconstructions,\nVPR feature extractors, and temporal resolutions. Unlike previous event-based\nensemble methods, which only utilise temporal resolution, our broader fusion\nstrategy delivers significantly improved robustness under varied lighting\nconditions (e.g., afternoon, sunset, night), achieving a 57% relative\nimprovement in Recall@1 across day-night transitions. We evaluate our approach\non two long-term driving datasets (with 8 km per traverse) without metric\nsubsampling, thereby preserving natural variations in speed and stop duration\nthat influence event density. We also conduct a comprehensive analysis of key\ndesign choices, including binning strategies, polarity handling, reconstruction\nmethods, and feature extractors, to identify the most critical components for\nrobust performance. Additionally, we propose a modification to the standard\nsequence matching framework that enhances performance at longer sequence\nlengths. To facilitate future research, we will release our codebase and\nbenchmarking framework.",
        "url": "http://arxiv.org/abs/2509.01968v1",
        "published_date": "2025-09-02T05:17:07+00:00",
        "updated_date": "2025-09-02T05:17:07+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Therese Joseph",
            "Tobias Fischer",
            "Michael Milford"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces an ensemble-based approach for event camera place recognition under varying illumination conditions, achieving significant improvements in performance across day-night transitions.",
        "tldr_zh": "本文介绍了一种基于集成的方法，用于在不同光照条件下识别事件相机位置，实现了在白天和黑夜转换中性能的显著改进。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Automated Wildfire Damage Assessment from Multi view Ground level Imagery Via Vision Language Models",
        "summary": "The escalating intensity and frequency of wildfires demand innovative\ncomputational methods for rapid and accurate property damage assessment.\nTraditional methods are often time consuming, while modern computer vision\napproaches typically require extensive labeled datasets, hindering immediate\npost-disaster deployment. This research introduces a novel, zero-shot framework\nleveraging pre-trained vision language models (VLMs) to classify damage from\nground-level imagery. We propose and evaluate two pipelines applied to the 2025\nEaton and Palisades fires in California, a VLM (Pipeline A) and a VLM + large\nlanguage model (LLM) approach (Pipeline B), that integrate structured prompts\nbased on specific wildfire damage indicators. A primary scientific contribution\nof this study is demonstrating the VLMs efficacy in synthesizing information\nfrom multiple perspectives to identify nuanced damage, a critical limitation in\nexisting literature. Our findings reveal that while single view assessments\nstruggled to classify affected structures (F1 scores ranging from 0.225 to\n0.511), the multi-view analysis yielded dramatic improvements (F1 scores\nranging from 0.857 to 0.947). Moreover, the McNemar test confirmed that\npipelines with a multi-view image assessment yields statistically significant\nclassification improvements; however, the improvements this research observed\nbetween Pipeline A and B were not statistically significant. Thus, future\nresearch can explore the potential of LLM prompting in damage assessment. The\npractical contribution is an immediately deployable, flexible, and\ninterpretable workflow that bypasses the need for supervised training,\nsignificantly accelerating triage and prioritization for disaster response\npractitioners.",
        "url": "http://arxiv.org/abs/2509.01895v1",
        "published_date": "2025-09-02T02:34:22+00:00",
        "updated_date": "2025-09-02T02:34:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Miguel Esparza",
            "Archit Gupta",
            "Ali Mostafavi",
            "Kai Yin",
            "Yiming Xiao"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper introduces a zero-shot framework using vision language models to assess wildfire damage from multi-view ground-level imagery, showing significant improvements in classification accuracy.",
        "tldr_zh": "本文介绍了一种零拍摄框架，使用视觉语言模型对多视角地面图像进行野火损害评估，显示出在分类准确性方面的显着改进。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "A Single Detect Focused YOLO Framework for Robust Mitotic Figure Detection",
        "summary": "Mitotic figure detection is a crucial task in computational pathology, as\nmitotic activity serves as a strong prognostic marker for tumor aggressiveness.\nHowever, domain variability that arises from differences in scanners, tissue\ntypes, and staining protocols poses a major challenge to the robustness of\nautomated detection methods. In this study, we introduce SDF-YOLO (Single\nDetect Focused YOLO), a lightweight yet domain-robust detection framework\ndesigned specifically for small, rare targets such as mitotic figures. The\nmodel builds on YOLOv11 with task-specific modifications, including a single\ndetection head aligned with mitotic figure scale, coordinate attention to\nenhance positional sensitivity, and improved cross-channel feature mixing.\nExperiments were conducted on three datasets that span human and canine tumors:\nMIDOG ++, canine cutaneous mast cell tumor (CCMCT), and canine mammary\ncarcinoma (CMC). When submitted to the preliminary test set for the MIDOG2025\nchallenge, SDF-YOLO achieved an average precision (AP) of 0.799, with a\nprecision of 0.758, a recall of 0.775, an F1 score of 0.766, and an FROC-AUC of\n5.793, demonstrating both competitive accuracy and computational efficiency.\nThese results indicate that SDF-YOLO provides a reliable and efficient\nframework for robust mitotic figure detection across diverse domains.",
        "url": "http://arxiv.org/abs/2509.02637v1",
        "published_date": "2025-09-01T20:41:48+00:00",
        "updated_date": "2025-09-01T20:41:48+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Yasemin Topuz",
            "M. Taha Gökcan",
            "Serdar Yıldız",
            "Songül Varlı"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a new detection framework, SDF-YOLO, designed for robust mitotic figure detection in computational pathology.",
        "tldr_zh": "本文介绍了一种新的检测框架SDF-YOLO，专门用于计算病理学中的可靠有丝分裂图像检测。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning",
        "summary": "The development of autonomous agents for graphical user interfaces (GUIs)\npresents major challenges in artificial intelligence. While recent advances in\nnative agent models have shown promise by unifying perception, reasoning,\naction, and memory through end-to-end learning, open problems remain in data\nscalability, multi-turn reinforcement learning (RL), the limitations of\nGUI-only operation, and environment stability. In this technical report, we\npresent UI-TARS-2, a native GUI-centered agent model that addresses these\nchallenges through a systematic training methodology: a data flywheel for\nscalable data generation, a stabilized multi-turn RL framework, a hybrid GUI\nenvironment that integrates file systems and terminals, and a unified sandbox\nplatform for large-scale rollouts. Empirical evaluation demonstrates that\nUI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5.\nOn GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on\nWindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines\nsuch as Claude and OpenAI agents. In game environments, it attains a mean\nnormalized score of 59.8 across a 15-game suite-roughly 60% of human-level\nperformance-and remains competitive with frontier proprietary models (e.g.,\nOpenAI o3) on LMGame-Bench. Additionally, the model can generalize to\nlong-horizon information-seeking tasks and software engineering benchmarks,\nhighlighting its robustness across diverse agent tasks. Detailed analyses of\ntraining dynamics further provide insights into achieving stability and\nefficiency in large-scale agent RL. These results underscore UI-TARS-2's\npotential to advance the state of GUI agents and exhibit strong generalization\nto real-world interactive scenarios.",
        "url": "http://arxiv.org/abs/2509.02544v1",
        "published_date": "2025-09-02T17:44:45+00:00",
        "updated_date": "2025-09-02T17:44:45+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Haoming Wang",
            "Haoyang Zou",
            "Huatong Song",
            "Jiazhan Feng",
            "Junjie Fang",
            "Junting Lu",
            "Longxiang Liu",
            "Qinyu Luo",
            "Shihao Liang",
            "Shijue Huang",
            "Wanjun Zhong",
            "Yining Ye",
            "Yujia Qin",
            "Yuwen Xiong",
            "Yuxin Song",
            "Zhiyong Wu",
            "Bo Li",
            "Chen Dun",
            "Chong Liu",
            "Fuxing Leng",
            "Hanbin Wang",
            "Hao Yu",
            "Haobin Chen",
            "Hongyi Guo",
            "Jing Su",
            "Jingjia Huang",
            "Kai Shen",
            "Kaiyu Shi",
            "Lin Yan",
            "Peiyao Zhao",
            "Pengfei Liu",
            "Qinghao Ye",
            "Renjie Zheng",
            "Wayne Xin Zhao",
            "Wen Heng",
            "Wenhao Huang",
            "Wenqian Wang",
            "Xiaobo Qin",
            "Yi Lin",
            "Youbin Wu",
            "Zehui Chen",
            "Zihao Wang",
            "Baoquan Zhong",
            "Xinchun Zhang",
            "Xujing Li",
            "Yuanfan Li",
            "Zhongkai Zhao",
            "Chengquan Jiang",
            "Faming Wu",
            "Haotian Zhou",
            "Jinlin Pang",
            "Li Han",
            "Qianli Ma",
            "Siyao Liu",
            "Songhua Cai",
            "Wenqi Fu",
            "Xin Liu",
            "Zhi Zhang",
            "Bo Zhou",
            "Guoliang Li",
            "Jiajun Shi",
            "Jiale Yang",
            "Jie Tang",
            "Li Li",
            "Taoran Lu",
            "Woyu Lin",
            "Xiaokang Tong",
            "Xinyao Li",
            "Yichi Zhang",
            "Yu Miao",
            "Zhengxuan Jiang",
            "Zili Li",
            "Ziyuan Zhao",
            "Chenxin Li",
            "Dehua Ma",
            "Feng Lin",
            "Ge Zhang",
            "Haihua Yang",
            "Hangyu Guo",
            "Hongda Zhu",
            "Jiaheng Liu",
            "Junda Du",
            "Kai Cai",
            "Kuanye Li",
            "Lichen Yuan",
            "Meilan Han",
            "Minchao Wang",
            "Shuyue Guo",
            "Tianhao Cheng",
            "Xiaobo Ma",
            "Xiaojun Xiao",
            "Xiaolong Huang",
            "Xinjie Chen",
            "Yidi Du",
            "Yilin Chen",
            "Yiwen Wang",
            "Zhaojian Li",
            "Zhenzhu Yang",
            "Zhiyuan Zeng",
            "Chaolin Jin",
            "Chen Li",
            "Hao Chen",
            "Haoli Chen",
            "Jian Chen",
            "Qinghao Zhao",
            "Guang Shi"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces UI-TARS-2, an autonomous agent model for GUIs that improves upon its predecessor in data scalability, multi-turn RL, GUI operation, and environment stability, showing promising results in various benchmarks and real-world scenarios.",
        "tldr_zh": "本文介绍了UI-TARS-2，这是一个用于GUI的自主代理模型，改进了其前身在数据可伸缩性、多轮RL、GUI操作和环境稳定性方面，展示了在各种基准测试和实际场景中的良好表现。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "From Noisy Labels to Intrinsic Structure: A Geometric-Structural Dual-Guided Framework for Noise-Robust Medical Image Segmentation",
        "summary": "The effectiveness of convolutional neural networks in medical image\nsegmentation relies on large-scale, high-quality annotations, which are costly\nand time-consuming to obtain. Even expert-labeled datasets inevitably contain\nnoise arising from subjectivity and coarse delineations, which disrupt feature\nlearning and adversely impact model performance. To address these challenges,\nthis study propose a Geometric-Structural Dual-Guided Network (GSD-Net), which\nintegrates geometric and structural cues to improve robustness against noisy\nannotations. It incorporates a Geometric Distance-Aware module that dynamically\nadjusts pixel-level weights using geometric features, thereby strengthening\nsupervision in reliable regions while suppressing noise. A Structure-Guided\nLabel Refinement module further refines labels with structural priors, and a\nKnowledge Transfer module enriches supervision and improves sensitivity to\nlocal details. To comprehensively assess its effectiveness, we evaluated\nGSD-Net on six publicly available datasets: four containing three types of\nsimulated label noise, and two with multi-expert annotations that reflect\nreal-world subjectivity and labeling inconsistencies. Experimental results\ndemonstrate that GSD-Net achieves state-of-the-art performance under noisy\nannotations, achieving improvements of 2.52% on Kvasir, 22.76% on Shenzhen,\n8.87% on BU-SUC, and 4.59% on BraTS2020 under SR simulated noise. The codes of\nthis study are available at https://github.com/ortonwang/GSD-Net.",
        "url": "http://arxiv.org/abs/2509.02419v1",
        "published_date": "2025-09-02T15:23:59+00:00",
        "updated_date": "2025-09-02T15:23:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Tao Wang",
            "Zhenxuan Zhang",
            "Yuanbo Zhou",
            "Xinlin Zhang",
            "Yuanbin Chen",
            "Tao Tan",
            "Guang Yang",
            "Tong Tong"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "Proposes a dual-guided framework for noise-robust medical image segmentation by integrating geometric and structural cues, achieving state-of-the-art performance under noisy annotations.",
        "tldr_zh": "提出了一种通过整合几何和结构线索来实现噪声鲁棒医学图像分割的双引导框架，在嘈杂注释下实现了最先进的性能。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Decoupling Bidirectional Geometric Representations of 4D cost volume with 2D convolution",
        "summary": "High-performance real-time stereo matching methods invariably rely on 3D\nregularization of the cost volume, which is unfriendly to mobile devices. And\n2D regularization based methods struggle in ill-posed regions. In this paper,\nwe present a deployment-friendly 4D cost aggregation network DBStereo, which is\nbased on pure 2D convolutions. Specifically, we first provide a thorough\nanalysis of the decoupling characteristics of 4D cost volume. And design a\nlightweight bidirectional geometry aggregation block to capture spatial and\ndisparity representation respectively. Through decoupled learning, our approach\nachieves real-time performance and impressive accuracy simultaneously.\nExtensive experiments demonstrate that our proposed DBStereo outperforms all\nexisting aggregation-based methods in both inference time and accuracy, even\nsurpassing the iterative-based method IGEV-Stereo. Our study break the\nempirical design of using 3D convolutions for 4D cost volume and provides a\nsimple yet strong baseline of the proposed decouple aggregation paradigm for\nfurther study. Code will be available at\n(\\href{https://github.com/happydummy/DBStereo}{https://github.com/happydummy/DBStereo})\nsoon.",
        "url": "http://arxiv.org/abs/2509.02415v1",
        "published_date": "2025-09-02T15:21:49+00:00",
        "updated_date": "2025-09-02T15:21:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaobao Wei",
            "Changyong Shu",
            "Zhaokun Yue",
            "Chang Huang",
            "Weiwei Liu",
            "Shuai Yang",
            "Lirong Yang",
            "Peng Gao",
            "Wenbin Zhang",
            "Gaochao Zhu",
            "Chengxiang Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper presents a deployment-friendly 4D cost aggregation network based on 2D convolutions for real-time stereo matching, outperforming existing methods in both speed and accuracy.",
        "tldr_zh": "本文提出了一种基于2D卷积的适合部署的4D成本聚合网络，用于实时立体匹配，在速度和准确性上均优于现有方法。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "OmniActor: A Generalist GUI and Embodied Agent for 2D&3D Worlds",
        "summary": "Multimodal large language models are evolving toward multimodal agents\ncapable of proactively executing tasks. Most agent research focuses on GUI or\nembodied scenarios, which correspond to agents interacting with 2D virtual\nworlds or 3D real worlds, respectively. However, many complex tasks typically\nrequire agents to interleavely interact with these two types of environment. We\ninitially mix GUI and embodied data to train, but find the performance\ndegeneration brought by the data conflict. Further analysis reveals that GUI\nand embodied data exhibit synergy and conflict at the shallow and deep layers,\nrespectively, which resembles the cerebrum-cerebellum mechanism in the human\nbrain. To this end, we propose a high-performance generalist agent OmniActor,\ndesigned from both structural and data perspectives. First, we propose\nLayer-heterogeneity MoE to eliminate the conflict between GUI and embodied data\nby separating deep-layer parameters, while leverage their synergy by sharing\nshallow-layer parameters. By successfully leveraging the synergy and\neliminating the conflict, OmniActor outperforms agents only trained by GUI or\nembodied data in GUI or embodied tasks. Furthermore, we unify the action spaces\nof GUI and embodied tasks, and collect large-scale GUI and embodied data from\nvarious sources for training. This significantly improves OmniActor under\ndifferent scenarios, especially in GUI tasks. The code will be publicly\navailable.",
        "url": "http://arxiv.org/abs/2509.02322v1",
        "published_date": "2025-09-02T13:47:54+00:00",
        "updated_date": "2025-09-02T13:47:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Longrong Yang",
            "Zhixiong Zeng",
            "Yufeng Zhong",
            "Jing Huang",
            "Liming Zheng",
            "Lei Chen",
            "Haibo Qiu",
            "Zequn Qin",
            "Lin Ma",
            "Xi Li"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality"
        ],
        "tldr": "The paper introduces OmniActor, a high-performance generalist agent designed to excel in GUI and embodied tasks by leveraging synergy and eliminating conflicts between data types.",
        "tldr_zh": "本论文介绍了OmniActor，一个高性能的通用代理，通过利用数据类型之间的协同作用和消除冲突，在GUI和实体任务中表现出色。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "DSGC-Net: A Dual-Stream Graph Convolutional Network for Crowd Counting via Feature Correlation Mining",
        "summary": "Deep learning-based crowd counting methods have achieved remarkable progress\nin recent years. However, in complex crowd scenarios, existing models still\nface challenges when adapting to significant density distribution differences\nbetween regions. Additionally, the inconsistency of individual representations\ncaused by viewpoint changes and body posture differences further limits the\ncounting accuracy of the models. To address these challenges, we propose\nDSGC-Net, a Dual-Stream Graph Convolutional Network based on feature\ncorrelation mining. DSGC-Net introduces a Density Approximation (DA) branch and\na Representation Approximation (RA) branch. By modeling two semantic graphs, it\ncaptures the potential feature correlations in density variations and\nrepresentation distributions. The DA branch incorporates a density prediction\nmodule that generates the density distribution map, and constructs a\ndensity-driven semantic graph based on density similarity. The RA branch\nestablishes a representation-driven semantic graph by computing global\nrepresentation similarity. Then, graph convolutional networks are applied to\nthe two semantic graphs separately to model the latent semantic relationships,\nwhich enhance the model's ability to adapt to density variations and improve\ncounting accuracy in multi-view and multi-pose scenarios. Extensive experiments\non three widely used datasets demonstrate that DSGC-Net outperforms current\nstate-of-the-art methods. In particular, we achieve MAE of 48.9 and 5.9 in\nShanghaiTech Part A and Part B datasets, respectively. The released code is\navailable at: https://github.com/Wu-eon/CrowdCounting-DSGCNet.",
        "url": "http://arxiv.org/abs/2509.02261v1",
        "published_date": "2025-09-02T12:35:33+00:00",
        "updated_date": "2025-09-02T12:35:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yihong Wu",
            "Jinqiao Wei",
            "Xionghui Zhao",
            "Yidi Li",
            "Shaoyi Du",
            "Bin Ren",
            "Nicu Sebe"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "DSGC-Net is a novel Dual-Stream Graph Convolutional Network for crowd counting that addresses challenges in crowd scenarios by capturing feature correlations in density and representation distributions.",
        "tldr_zh": "DSGC-Net是一种新颖的双流图卷积网络，用于人群计数，通过捕获密度和表示分布中的特征相关性来解决人群场景中的挑战。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.0
    },
    {
        "title": "Unsupervised Training of Vision Transformers with Synthetic Negatives",
        "summary": "This paper does not introduce a novel method per se. Instead, we address the\nneglected potential of hard negative samples in self-supervised learning.\nPrevious works explored synthetic hard negatives but rarely in the context of\nvision transformers. We build on this observation and integrate synthetic hard\nnegatives to improve vision transformer representation learning. This simple\nyet effective technique notably improves the discriminative power of learned\nrepresentations. Our experiments show performance improvements for both DeiT-S\nand Swin-T architectures.",
        "url": "http://arxiv.org/abs/2509.02024v1",
        "published_date": "2025-09-02T07:14:21+00:00",
        "updated_date": "2025-09-02T07:14:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Nikolaos Giakoumoglou",
            "Andreas Floros",
            "Kleanthis Marios Papadopoulos",
            "Tania Stathaki"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper explores the use of synthetic hard negatives in improving vision transformer representation learning, resulting in enhanced discriminative power.",
        "tldr_zh": "本文探讨了在改善视觉变换器表示学习方面使用合成困难负例，从而提高了区分能力。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Explaining What Machines See: XAI Strategies in Deep Object Detection Models",
        "summary": "In recent years, deep learning has achieved unprecedented success in various\ncomputer vision tasks, particularly in object detection. However, the black-box\nnature and high complexity of deep neural networks pose significant challenges\nfor interpretability, especially in critical domains such as autonomous\ndriving, medical imaging, and security systems. Explainable Artificial\nIntelligence (XAI) aims to address this challenge by providing tools and\nmethods to make model decisions more transparent, interpretable, and\ntrust-worthy for humans. This review provides a comprehensive analysis of\nstate-of-the-art explain-ability methods specifically applied to object\ndetection models. The paper be-gins by categorizing existing XAI techniques\nbased on their underlying mechanisms-perturbation-based, gradient-based,\nbackpropagation-based, and graph-based methods. Notable methods such as D-RISE,\nBODEM, D-CLOSE, and FSOD are discussed in detail. Furthermore, the paper\ninvestigates their applicability to various object detection architectures,\nincluding YOLO, SSD, Faster R-CNN, and EfficientDet. Statistical analysis of\npublication trends from 2022 to mid-2025 shows an accelerating interest in\nexplainable object detection, indicating its increasing importance. The study\nalso explores common datasets and evaluation metrics, and highlights the major\nchallenges associated with model interpretability. By providing a structured\ntaxonomy and a critical assessment of existing methods, this review aims to\nguide researchers and practitioners in selecting suitable explainability\ntechniques for object detection applications and to foster the development of\nmore interpretable AI systems.",
        "url": "http://arxiv.org/abs/2509.01991v1",
        "published_date": "2025-09-02T06:16:30+00:00",
        "updated_date": "2025-09-02T06:16:30+00:00",
        "categories": [
            "cs.CV",
            "68T07",
            "I.4.8"
        ],
        "authors": [
            "FatemehSadat Seyedmomeni",
            "Mohammad Ali Keyvanrad"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper discusses Explainable Artificial Intelligence (XAI) strategies for deep object detection models, categorizing methods and analyzing their applicability to various architectures.",
        "tldr_zh": "本文讨论了深度目标检测模型的可解释人工智能（XAI）策略，对方法进行分类，并分析它们在各种架构中的适用性。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving",
        "summary": "Vision-Language-Action (VLA) models in autonomous driving systems have\nrecently demonstrated transformative potential by integrating multimodal\nperception with decision-making capabilities. However, the interpretability and\ncoherence of the decision process and the plausibility of action sequences\nremain largely underexplored. To address these issues, we propose\nAutoDrive-R$^2$, a novel VLA framework that enhances both reasoning and\nself-reflection capabilities of autonomous driving systems through\nchain-of-thought (CoT) processing and reinforcement learning (RL).\nSpecifically, we first propose an innovative CoT dataset named nuScenesR$^2$-6K\nfor supervised fine-tuning, which effectively builds cognitive bridges between\ninput information and output trajectories through a four-step logical chain\nwith self-reflection for validation. Moreover, to maximize both reasoning and\nself-reflection during the RL stage, we further employ the Group Relative\nPolicy Optimization (GRPO) algorithm within a physics-grounded reward framework\nthat incorporates spatial alignment, vehicle dynamic, and temporal smoothness\ncriteria to ensure reliable and realistic trajectory planning. Extensive\nevaluation results across both nuScenes and Waymo datasets demonstrates the\nstate-of-the-art performance and robust generalization capacity of our proposed\nmethod.",
        "url": "http://arxiv.org/abs/2509.01944v1",
        "published_date": "2025-09-02T04:32:24+00:00",
        "updated_date": "2025-09-02T04:32:24+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Zhenlong Yuan",
            "Jing Tang",
            "Jinguo Luo",
            "Rui Chen",
            "Chengxuan Qian",
            "Lei Sun",
            "Xiangxiang Chu",
            "Yujun Cai",
            "Dapeng Zhang",
            "Shuo Li"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Transformer"
        ],
        "tldr": "AutoDrive-R$^2$ is a novel framework for enhancing reasoning and self-reflection capabilities in autonomous driving systems, showing state-of-the-art performance and generalization capacity.",
        "tldr_zh": "AutoDrive-R$^2$ 是一个新颖的框架，用于增强自动驾驶系统的推理和自我反思能力，展示了最先进的性能和泛化能力。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Towards Interpretable Geo-localization: a Concept-Aware Global Image-GPS Alignment Framework",
        "summary": "Worldwide geo-localization involves determining the exact geographic location\nof images captured globally, typically guided by geographic cues such as\nclimate, landmarks, and architectural styles. Despite advancements in\ngeo-localization models like GeoCLIP, which leverages images and location\nalignment via contrastive learning for accurate predictions, the\ninterpretability of these models remains insufficiently explored. Current\nconcept-based interpretability methods fail to align effectively with\nGeo-alignment image-location embedding objectives, resulting in suboptimal\ninterpretability and performance. To address this gap, we propose a novel\nframework integrating global geo-localization with concept bottlenecks. Our\nmethod inserts a Concept-Aware Alignment Module that jointly projects image and\nlocation embeddings onto a shared bank of geographic concepts (e.g., tropical\nclimate, mountain, cathedral) and minimizes a concept-level loss, enhancing\nalignment in a concept-specific subspace and enabling robust interpretability.\nTo our knowledge, this is the first work to introduce interpretability into\ngeo-localization. Extensive experiments demonstrate that our approach surpasses\nGeoCLIP in geo-localization accuracy and boosts performance across diverse\ngeospatial prediction tasks, revealing richer semantic insights into geographic\ndecision-making processes.",
        "url": "http://arxiv.org/abs/2509.01910v1",
        "published_date": "2025-09-02T03:07:26+00:00",
        "updated_date": "2025-09-02T03:07:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Furong Jia",
            "Lanxin Liu",
            "Ce Hou",
            "Fan Zhang",
            "Xinyan Liu",
            "Yu Liu"
        ],
        "ai_categories": [
            "AIGC",
            "Other"
        ],
        "tldr": "The paper introduces a new framework for interpretable geo-localization by aligning images and locations with geographic concepts, surpassing existing models in accuracy and performance.",
        "tldr_zh": "本文提出了一种新的可解释的地理定位框架，通过将图像和位置与地理概念对齐，在准确性和性能方面超越了现有模型。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Enabling Federated Object Detection for Connected Autonomous Vehicles: A Deployment-Oriented Evaluation",
        "summary": "Object detection is crucial for Connected Autonomous Vehicles (CAVs) to\nperceive their surroundings and make safe driving decisions. Centralized\ntraining of object detection models often achieves promising accuracy, fast\nconvergence, and simplified training process, but it falls short in\nscalability, adaptability, and privacy-preservation. Federated learning (FL),\nby contrast, enables collaborative, privacy-preserving, and continuous training\nacross naturally distributed CAV fleets. However, deploying FL in real-world\nCAVs remains challenging due to the substantial computational demands of\ntraining and inference, coupled with highly diverse operating conditions.\nPractical deployment must address three critical factors: (i) heterogeneity\nfrom non-IID data distributions, (ii) constrained onboard computing hardware,\nand (iii) environmental variability such as lighting and weather, alongside\nsystematic evaluation to ensure reliable performance. This work introduces the\nfirst holistic deployment-oriented evaluation of FL-based object detection in\nCAVs, integrating model performance, system-level resource profiling, and\nenvironmental robustness. Using state-of-the-art detectors, YOLOv5, YOLOv8,\nYOLOv11, and Deformable DETR, evaluated on the KITTI, BDD100K, and nuScenes\ndatasets, we analyze trade-offs between detection accuracy, computational cost,\nand resource usage under diverse resolutions, batch sizes, weather and lighting\nconditions, and dynamic client participation, paving the way for robust FL\ndeployment in CAVs.",
        "url": "http://arxiv.org/abs/2509.01868v1",
        "published_date": "2025-09-02T01:23:50+00:00",
        "updated_date": "2025-09-02T01:23:50+00:00",
        "categories": [
            "cs.CV",
            "cs.DC"
        ],
        "authors": [
            "Komala Subramanyam Cherukuri",
            "Kewei Sha",
            "Zhenhua Huang"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a comprehensive evaluation of federated learning-based object detection in connected autonomous vehicles, considering factors like model performance, resource profiling, and environmental robustness.",
        "tldr_zh": "本文介绍了在联网自动驾驶车辆中使用联邦学习的目标检测的综合评估，考虑了模型性能、资源配置和环境鲁棒性等因素。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Latent Gene Diffusion for Spatial Transcriptomics Completion",
        "summary": "Computer Vision has proven to be a powerful tool for analyzing Spatial\nTranscriptomics (ST) data. However, current models that predict spatially\nresolved gene expression from histopathology images suffer from significant\nlimitations due to data dropout. Most existing approaches rely on single-cell\nRNA sequencing references, making them dependent on alignment quality and\nexternal datasets while also risking batch effects and inherited dropout. In\nthis paper, we address these limitations by introducing LGDiST, the first\nreference-free latent gene diffusion model for ST data dropout. We show that\nLGDiST outperforms the previous state-of-the-art in gene expression completion,\nwith an average Mean Squared Error that is 18% lower across 26 datasets.\nFurthermore, we demonstrate that completing ST data with LGDiST improves gene\nexpression prediction performance on six state-of-the-art methods up to 10% in\nMSE. A key innovation of LGDiST is using context genes previously considered\nuninformative to build a rich and biologically meaningful genetic latent space.\nOur experiments show that removing key components of LGDiST, such as the\ncontext genes, the ST latent space, and the neighbor conditioning, leads to\nconsiderable drops in performance. These findings underscore that the full\narchitecture of LGDiST achieves substantially better performance than any of\nits isolated components.",
        "url": "http://arxiv.org/abs/2509.01864v1",
        "published_date": "2025-09-02T01:14:11+00:00",
        "updated_date": "2025-09-02T01:14:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Paula Cárdenas",
            "Leonardo Manrique",
            "Daniela Vega",
            "Daniela Ruiz",
            "Pablo Arbeláez"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces LGDiST, a reference-free latent gene diffusion model for completing Spatial Transcriptomics data, outperforming previous models by 18%. The model improves gene expression prediction performance on multiple methods by up to 10% in Mean Squared Error.",
        "tldr_zh": "本文引入了LGDiST，一种无需参考的潜在基因扩散模型，用于完成Spatial Transcriptomics数据，优于先前模型18%。该模型将基因表达预测性能提升了多种方法，最高可达10%的均方误差。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "HodgeFormer: Transformers for Learnable Operators on Triangular Meshes through Data-Driven Hodge Matrices",
        "summary": "Currently, prominent Transformer architectures applied on graphs and meshes\nfor shape analysis tasks employ traditional attention layers that heavily\nutilize spectral features requiring costly eigenvalue decomposition-based\nmethods. To encode the mesh structure, these methods derive positional\nembeddings, that heavily rely on eigenvalue decomposition based operations,\ne.g. on the Laplacian matrix, or on heat-kernel signatures, which are then\nconcatenated to the input features. This paper proposes a novel approach\ninspired by the explicit construction of the Hodge Laplacian operator in\nDiscrete Exterior Calculus as a product of discrete Hodge operators and\nexterior derivatives, i.e. $(L := \\star_0^{-1} d_0^T \\star_1 d_0)$. We adjust\nthe Transformer architecture in a novel deep learning layer that utilizes the\nmulti-head attention mechanism to approximate Hodge matrices $\\star_0$,\n$\\star_1$ and $\\star_2$ and learn families of discrete operators $L$ that act\non mesh vertices, edges and faces. Our approach results in a\ncomputationally-efficient architecture that achieves comparable performance in\nmesh segmentation and classification tasks, through a direct learning\nframework, while eliminating the need for costly eigenvalue decomposition\noperations or complex preprocessing operations.",
        "url": "http://arxiv.org/abs/2509.01839v2",
        "published_date": "2025-09-01T23:43:43+00:00",
        "updated_date": "2025-09-03T10:55:00+00:00",
        "categories": [
            "cs.GR",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Akis Nousias",
            "Stavros Nousias"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel approach using learnable operators on triangular meshes in Transformer architectures for shape analysis tasks, achieving comparable performance without the need for costly eigenvalue decomposition operations.",
        "tldr_zh": "该论文引入了一种新颖的方法，利用可学习的运算符在三角网格上的Transformer架构中进行形状分析任务，实现了可比较的性能，而无需进行昂贵的特征值分解操作。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Adaptive Learning Strategies for Mitotic Figure Classification in MIDOG2025 Challenge",
        "summary": "Atypical mitotic figures (AMFs) are clinically relevant indicators of\nabnormal cell division, yet their reliable detection remains challenging due to\nmorphological ambiguity and scanner variability. In this work, we investigated\nthree variants of adapting the pathology foundation model UNI2-h for the\nMIDOG2025 Track 2 challenge. Starting from a LoRA-based baseline, we found that\nvisual prompt tuning (VPT) substantially improved generalization, and that\nfurther integrating test-time augmentation (TTA) with Vahadane and Macenko\nstain normalization provided the best robustness. Our final submission achieved\na balanced accuracy of 0.8837 and an ROC-AUC of 0.9513 on the preliminary\nleaderboard, ranking within the top 10 teams. These results demonstrate that\nprompt-based adaptation combined with stain-normalization TTA offers an\neffective strategy for atypical mitosis classification under diverse imaging\nconditions.",
        "url": "http://arxiv.org/abs/2509.02640v1",
        "published_date": "2025-09-01T22:42:53+00:00",
        "updated_date": "2025-09-01T22:42:53+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Biwen Meng",
            "Xi Long",
            "Jingxin Liu"
        ],
        "ai_categories": [
            "Multimodality",
            "LoRA",
            "Dataset"
        ],
        "tldr": "The paper explores adaptive learning strategies for classifying atypical mitotic figures in medical images, achieving high accuracy and ranking in the top 10 teams in a challenge.",
        "tldr_zh": "本文探讨了针对医学图像中非典型有丝分裂图的自适应学习策略，取得了较高的准确度，并在挑战中排名前十。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Mixture of Balanced Information Bottlenecks for Long-Tailed Visual Recognition",
        "summary": "Deep neural networks (DNNs) have achieved significant success in various\napplications with large-scale and balanced data. However, data in real-world\nvisual recognition are usually long-tailed, bringing challenges to efficient\ntraining and deployment of DNNs. Information bottleneck (IB) is an elegant\napproach for representation learning. In this paper, we propose a balanced\ninformation bottleneck (BIB) approach, in which loss function re-balancing and\nself-distillation techniques are integrated into the original IB network. BIB\nis thus capable of learning a sufficient representation with essential\nlabel-related information fully preserved for long-tailed visual recognition.\nTo further enhance the representation learning capability, we also propose a\nnovel structure of mixture of multiple balanced information bottlenecks (MBIB),\nwhere different BIBs are responsible for combining knowledge from different\nnetwork layers. MBIB facilitates an end-to-end learning strategy that trains\nrepresentation and classification simultaneously from an information theory\nperspective. We conduct experiments on commonly used long-tailed datasets,\nincluding CIFAR100-LT, ImageNet-LT, and iNaturalist 2018. Both BIB and MBIB\nreach state-of-the-art performance for long-tailed visual recognition.",
        "url": "http://arxiv.org/abs/2509.01804v1",
        "published_date": "2025-09-01T22:14:12+00:00",
        "updated_date": "2025-09-01T22:14:12+00:00",
        "categories": [
            "cs.CV",
            "cs.IT",
            "math.IT"
        ],
        "authors": [
            "Yifan Lan",
            "Xin Cai",
            "Jun Cheng",
            "Shan Tan"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a balanced information bottleneck approach for long-tailed visual recognition, achieving state-of-the-art performance on popular datasets.",
        "tldr_zh": "本文提出了一种平衡信息瓶颈方法，用于长尾视觉识别，在流行数据集上取得了最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7.0
    },
    {
        "title": "Palmistry-Informed Feature Extraction and Analysis using Machine Learning",
        "summary": "This paper explores the automated analysis of palmar features using machine\nlearning techniques. We present a computer vision pipeline that extracts key\ncharacteristics from palm images, such as principal line structures, texture,\nand shape metrics. These features are used to train predictive models on a\nnovel dataset curated from annotated palm images. Our approach moves beyond\ntraditional subjective interpretation by providing a data-driven, quantitative\nframework for studying the correlations between palmar morphology and\nexternally validated traits or conditions. The methodology demonstrates\nfeasibility for applications in digital anthropometry and personalized user\nanalytics, with potential for deployment on mobile platforms. Results indicate\nthat machine learning models can identify complex patterns in palm data,\nopening avenues for research that intersects cultural practices with\ncomputational analysis.",
        "url": "http://arxiv.org/abs/2509.02248v1",
        "published_date": "2025-09-02T12:17:03+00:00",
        "updated_date": "2025-09-02T12:17:03+00:00",
        "categories": [
            "cs.CV",
            "I.4.9; I.2.10; J.5"
        ],
        "authors": [
            "Shweta Patil"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper discusses using machine learning to analyze palm features for applications in digital anthropometry and personalized user analytics.",
        "tldr_zh": "本文讨论了使用机器学习来分析手掌特征，用于数字人类测量学和个性化用户分析的应用。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Understanding Space Is Rocket Science - Only Top Reasoning Models Can Solve Spatial Understanding Tasks",
        "summary": "We propose RocketScience, an open-source contrastive VLM benchmark that tests\nfor spatial relation understanding. It is comprised of entirely new real-world\nimage-text pairs covering mostly relative spatial understanding and the order\nof objects. The benchmark is designed\n  to be very easy for humans and hard for the current generation of VLMs, and\nthis is empirically verified. Our results show a striking lack of spatial\nrelation understanding in open source and frontier commercial VLMs and a\nsurprisingly high performance of reasoning models. Additionally, we perform a\ndisentanglement analysis to separate the contributions of object localization\nand spatial reasoning in chain-of-thought-based models and find that the\nperformance on the benchmark is bottlenecked by spatial reasoning and not\nobject localization capabilities.\n  We release the dataset with a CC-BY-4.0 license and make the evaluation code\navailable at: https://github.com/nilshoehing/rocketscience",
        "url": "http://arxiv.org/abs/2509.02175v1",
        "published_date": "2025-09-02T10:32:58+00:00",
        "updated_date": "2025-09-02T10:32:58+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Nils Hoehing",
            "Mayug Maniparambil",
            "Ellen Rushe",
            "Noel E. O'Connor",
            "Anthony Ventresque"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces RocketScience, a benchmark for spatial relation understanding that shows a lack of performance in current VLMs but high performance in reasoning models.",
        "tldr_zh": "本文介绍了RocketScience，一个空间关系理解的基准测试，显示了现有VLM的性能不足，但在推理模型中表现出色。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Deep Learning-Based Rock Particulate Classification Using Attention-Enhanced ConvNeXt",
        "summary": "Accurate classification of rock sizes is a vital component in geotechnical\nengineering, mining, and resource management, where precise estimation\ninfluences operational efficiency and safety. In this paper, we propose an\nenhanced deep learning model based on the ConvNeXt architecture, augmented with\nboth self-attention and channel attention mechanisms. Building upon the\nfoundation of ConvNext, our proposed model, termed CNSCA, introduces\nself-attention to capture long-range spatial dependencies and channel attention\nto emphasize informative feature channels. This hybrid design enables the model\nto effectively capture both fine-grained local patterns and broader contextual\nrelationships within rock imagery, leading to improved classification accuracy\nand robustness. We evaluate our model on a rock size classification dataset and\ncompare it against three strong baseline. The results demonstrate that the\nincorporation of attention mechanisms significantly enhances the models\ncapability for fine-grained classification tasks involving natural textures\nlike rocks.",
        "url": "http://arxiv.org/abs/2509.01704v1",
        "published_date": "2025-09-01T18:24:40+00:00",
        "updated_date": "2025-09-01T18:24:40+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Anthony Amankwah",
            "Chris Aldrich"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a deep learning model called CNSCA for rock size classification using ConvNeXt with self-attention and channel attention mechanisms, showing improved accuracy and robustness.",
        "tldr_zh": "本文提出了一种名为CNSCA的深度学习模型，使用ConvNeXt结构并结合了自注意力和通道注意力机制，以提升岩石尺寸分类的准确性和鲁棒性。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Articulated Object Estimation in the Wild",
        "summary": "Understanding the 3D motion of articulated objects is essential in robotic\nscene understanding, mobile manipulation, and motion planning. Prior methods\nfor articulation estimation have primarily focused on controlled settings,\nassuming either fixed camera viewpoints or direct observations of various\nobject states, which tend to fail in more realistic unconstrained environments.\nIn contrast, humans effortlessly infer articulation by watching others\nmanipulate objects. Inspired by this, we introduce ArtiPoint, a novel\nestimation framework that can infer articulated object models under dynamic\ncamera motion and partial observability. By combining deep point tracking with\na factor graph optimization framework, ArtiPoint robustly estimates articulated\npart trajectories and articulation axes directly from raw RGB-D videos. To\nfoster future research in this domain, we introduce Arti4D, the first\nego-centric in-the-wild dataset that captures articulated object interactions\nat a scene level, accompanied by articulation labels and ground-truth camera\nposes. We benchmark ArtiPoint against a range of classical and learning-based\nbaselines, demonstrating its superior performance on Arti4D. We make code and\nArti4D publicly available at https://artipoint.cs.uni-freiburg.de.",
        "url": "http://arxiv.org/abs/2509.01708v1",
        "published_date": "2025-09-01T18:34:17+00:00",
        "updated_date": "2025-09-01T18:34:17+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Abdelrhman Werby",
            "Martin Büchner",
            "Adrian Röfer",
            "Chenguang Huang",
            "Wolfram Burgard",
            "Abhinav Valada"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces ArtiPoint, a framework for estimating articulated object models from raw RGB-D videos in dynamic camera motion scenarios. It also presents Arti4D, a dataset capturing articulated object interactions in realistic environments, to benchmark ArtiPoint.",
        "tldr_zh": "本文介绍了ArtiPoint，一个可以推断动态相机运动场景下的RGB-D视频中的关节对象模型的框架。还提出了Arti4D，这是一个捕捉现实环境中关节对象交互的数据集，用于评估ArtiPoint的性能。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.25
    },
    {
        "title": "Hues and Cues: Human vs. CLIP",
        "summary": "Playing games is inherently human, and a lot of games are created to\nchallenge different human characteristics. However, these tasks are often left\nout when evaluating the human-like nature of artificial models. The objective\nof this work is proposing a new approach to evaluate artificial models via\nboard games. To this effect, we test the color perception and color naming\ncapabilities of CLIP by playing the board game Hues & Cues and assess its\nalignment with humans. Our experiments show that CLIP is generally well aligned\nwith human observers, but our approach brings to light certain cultural biases\nand inconsistencies when dealing with different abstraction levels that are\nhard to identify with other testing strategies. Our findings indicate that\nassessing models with different tasks like board games can make certain\ndeficiencies in the models stand out in ways that are difficult to test with\nthe commonly used benchmarks.",
        "url": "http://arxiv.org/abs/2509.02305v2",
        "published_date": "2025-09-02T13:30:16+00:00",
        "updated_date": "2025-09-03T09:16:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nuria Alabau-Bosque",
            "Jorge Vila-Tomás",
            "Paula Daudén-Oliver",
            "Pablo Hernández-Cámara",
            "Jose Manuel Jaén-Lorites",
            "Valero Laparra",
            "Jesús Malo"
        ],
        "ai_categories": [
            "AIGC",
            "Other"
        ],
        "tldr": "The paper proposes a new approach to evaluate artificial models by testing their color perception and naming capabilities through board games. The findings suggest that assessing models with different tasks like board games can reveal deficiencies that are difficult to identify with traditional benchmarks.",
        "tldr_zh": "该论文提出了通过玩棋盘游戏测试人工模型的颜色感知和命名能力来评估这些模型的新方法。研究结果表明，用棋盘游戏等不同任务评估模型可以揭示难以用传统基准测试方法确定的不足之处。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.0
    }
]