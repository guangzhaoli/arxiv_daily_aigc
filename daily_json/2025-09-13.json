[
    {
        "title": "InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis",
        "summary": "Arbitrary resolution image generation provides a consistent visual experience\nacross devices, having extensive applications for producers and consumers.\nCurrent diffusion models increase computational demand quadratically with\nresolution, causing 4K image generation delays over 100 seconds. To solve this,\nwe explore the second generation upon the latent diffusion models, where the\nfixed latent generated by diffusion models is regarded as the content\nrepresentation and we propose to decode arbitrary resolution images with a\ncompact generated latent using a one-step generator. Thus, we present the\n\\textbf{InfGen}, replacing the VAE decoder with the new generator, for\ngenerating images at any resolution from a fixed-size latent without retraining\nthe diffusion models, which simplifies the process, reducing computational\ncomplexity and can be applied to any model using the same latent space.\nExperiments show InfGen is capable of improving many models into the arbitrary\nhigh-resolution era while cutting 4K image generation time to under 10 seconds.",
        "url": "http://arxiv.org/abs/2509.10441v1",
        "published_date": "2025-09-12T17:48:57+00:00",
        "updated_date": "2025-09-12T17:48:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tao Han",
            "Wanghan Xu",
            "Junchao Gong",
            "Xiaoyu Yue",
            "Song Guo",
            "Luping Zhou",
            "Lei Bai"
        ],
        "ai_categories": [
            "GAN"
        ],
        "tldr": "The paper introduces InfGen, a resolution-agnostic paradigm for scalable image synthesis that can generate high-resolution images quickly using a fixed-size latent representation without retraining the models.",
        "tldr_zh": "该论文介绍了InfGen，这是一种用于可扩展图像合成的分辨率不可知范式，可以使用固定大小的潜在表示快速生成高分辨率图像，而无需重新训练模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching",
        "summary": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion.",
        "url": "http://arxiv.org/abs/2509.10312v1",
        "published_date": "2025-09-12T14:53:45+00:00",
        "updated_date": "2025-09-12T14:53:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhixin Zheng",
            "Xinyu Wang",
            "Chang Zou",
            "Shaobo Wang",
            "Linfeng Zhang"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer"
        ],
        "tldr": "The paper introduces Cluster-Driven Feature Caching (ClusCa) to accelerate diffusion transformers by reducing the number of tokens in each timestep, showing significant improvements in text-to-image and text-to-video generation tasks.",
        "tldr_zh": "本文通过减少每个时间步中的令牌数量，引入了基于聚类的特征缓存(ClusCa)，在文本到图像和文本到视频生成任务中显示出显著的改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Ordinality of Visible-Thermal Image Intensities for Intrinsic Image Decomposition",
        "summary": "Decomposing an image into its intrinsic photometric factors--shading and\nreflectance--is a long-standing challenge due to the lack of extensive\nground-truth data for real-world scenes. Recent methods rely on synthetic data\nor sparse annotations for limited indoor and even fewer outdoor scenes. We\nintroduce a novel training-free approach for intrinsic image decomposition\nusing only a pair of visible and thermal images. We leverage the principle that\nlight not reflected from an opaque surface is absorbed and detected as heat by\na thermal camera. This allows us to relate the ordinalities between visible and\nthermal image intensities to the ordinalities of shading and reflectance, which\ncan densely self-supervise an optimizing neural network to recover shading and\nreflectance. We perform quantitative evaluations with known reflectance and\nshading under natural and artificial lighting, and qualitative experiments\nacross diverse outdoor scenes. The results demonstrate superior performance\nover recent learning-based models and point toward a scalable path to curating\nreal-world ordinal supervision, previously infeasible via manual labeling.",
        "url": "http://arxiv.org/abs/2509.10388v1",
        "published_date": "2025-09-12T16:29:02+00:00",
        "updated_date": "2025-09-12T16:29:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zeqing Leo Yuan",
            "Mani Ramanagopal",
            "Aswin C. Sankaranarayanan",
            "Srinivasa G. Narasimhan"
        ],
        "ai_categories": [
            "Multimodality"
        ],
        "tldr": "The paper introduces a new method for intrinsic image decomposition using visible and thermal images, achieving superior performance over recent models.",
        "tldr_zh": "本文介绍了一种使用可见光和热像图像进行内在图像分解的新方法，比最近的模型表现更出色。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation",
        "summary": "Text-to-image (T2I) generation has achieved remarkable progress in\ninstruction following and aesthetics. However, a persistent challenge is the\nprevalence of physical artifacts, such as anatomical and structural flaws,\nwhich severely degrade perceptual quality and limit application. Given the\ndiversity and complexity of these artifacts, a systematic and fine-grained\nevaluation framework is required, which is lacking in current benchmarks. To\nfill this gap, we introduce MagicMirror, a comprehensive framework for\nartifacts assessment. We first establish a detailed taxonomy of generated image\nartifacts. Guided by this taxonomy, we manually annotate MagicData340K, the\nfirst human-annotated large-scale dataset of 340K generated images with\nfine-grained artifact labels. Building on this dataset, we train MagicAssessor,\na Vision-Language Model (VLM) that provides detailed assessments and\ncorresponding labels. To overcome challenges like class imbalance and reward\nhacking, we design a novel data sampling strategy and a multi-level reward\nsystem for Group Relative Policy Optimization (GRPO). Finally, we leverage\nMagicAssessor to construct MagicBench, an automated benchmark for evaluating\nthe image artifacts of current T2I models. Our evaluation with MagicBench\nreveals that despite their widespread adoption, even top-tier models like\nGPT-image-1 are consistently plagued by significant artifacts, highlighting\nartifact reduction as a critical frontier for future T2I development. Project\npage: https://wj-inf.github.io/MagicMirror-page/.",
        "url": "http://arxiv.org/abs/2509.10260v1",
        "published_date": "2025-09-12T14:03:00+00:00",
        "updated_date": "2025-09-12T14:03:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jia Wang",
            "Jie Hu",
            "Xiaoqi Ma",
            "Hanghang Ma",
            "Yanbing Zeng",
            "Xiaoming Wei"
        ],
        "ai_categories": [
            "Dataset",
            "GAN"
        ],
        "tldr": "The paper introduces MagicMirror, a dataset and benchmark for fine-grained artifacts assessment in text-to-image generation, highlighting the prevalence of significant artifacts in current models.",
        "tldr_zh": "本论文介绍了MagicMirror，一个用于文本到图像生成中细粒度工件评估的数据集和基准，并突出了当前模型中重要工件的普遍存在。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Multimodal SAM-adapter for Semantic Segmentation",
        "summary": "Semantic segmentation, a key task in computer vision with broad applications\nin autonomous driving, medical imaging, and robotics, has advanced\nsubstantially with deep learning. Nevertheless, current approaches remain\nvulnerable to challenging conditions such as poor lighting, occlusions, and\nadverse weather. To address these limitations, multimodal methods that\nintegrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged,\nproviding complementary information that enhances robustness. In this work, we\npresent MM SAM-adapter, a novel framework that extends the capabilities of the\nSegment Anything Model (SAM) for multimodal semantic segmentation. The proposed\nmethod employs an adapter network that injects fused multimodal features into\nSAM's rich RGB features. This design enables the model to retain the strong\ngeneralization ability of RGB features while selectively incorporating\nauxiliary modalities only when they contribute additional cues. As a result, MM\nSAM-adapter achieves a balanced and efficient use of multimodal information. We\nevaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES,\nwhere MM SAM-adapter delivers state-of-the-art performance. To further analyze\nmodality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hard\nsubsets. Results consistently demonstrate that our framework outperforms\ncompeting methods in both favorable and adverse conditions, highlighting the\neffectiveness of multimodal adaptation for robust scene understanding. The code\nis available at the following link:\nhttps://github.com/iacopo97/Multimodal-SAM-Adapter.",
        "url": "http://arxiv.org/abs/2509.10408v1",
        "published_date": "2025-09-12T16:58:51+00:00",
        "updated_date": "2025-09-12T16:58:51+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Iacopo Curti",
            "Pierluigi Zama Ramirez",
            "Alioscia Petrelli",
            "Luigi Di Stefano"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces MM SAM-adapter, a framework for multimodal semantic segmentation using auxiliary sensor data to enhance robustness. It achieves state-of-the-art performance on challenging benchmarks.",
        "tldr_zh": "本文介绍了MM SAM-adapter，这是一个利用辅助传感器数据提高稳健性的多模态语义分割框架。在挑战性基准测试中取得了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Compressed Video Quality Enhancement: Classifying and Benchmarking over Standards",
        "summary": "Compressed video quality enhancement (CVQE) is crucial for improving user\nexperience with lossy video codecs like H.264/AVC, H.265/HEVC, and H.266/VVC.\nWhile deep learning based CVQE has driven significant progress, existing\nsurveys still suffer from limitations: lack of systematic classification\nlinking methods to specific standards and artifacts, insufficient comparative\nanalysis of architectural paradigms across coding types, and underdeveloped\nbenchmarking practices. To address these gaps, this paper presents three key\ncontributions. First, it introduces a novel taxonomy classifying CVQE methods\nacross architectural paradigms, coding standards, and compressed-domain feature\nutilization. Second, it proposes a unified benchmarking framework integrating\nmodern compression protocols and standard test sequences for fair\nmulti-criteria evaluation. Third, it provides a systematic analysis of the\ncritical trade-offs between reconstruction performance and computational\ncomplexity observed in state-of-the-art methods and highlighting promising\ndirections for future research. This comprehensive review aims to establish a\nfoundation for consistent assessment and informed model selection in CVQE\nresearch and deployment.",
        "url": "http://arxiv.org/abs/2509.10407v1",
        "published_date": "2025-09-12T16:58:20+00:00",
        "updated_date": "2025-09-12T16:58:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiem HoangVan",
            "Dang BuiDinh",
            "Sang NguyenQuang",
            "Wen-Hsiao Peng"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "This paper introduces a comprehensive review of Compressed Video Quality Enhancement (CVQE) methods, including a new taxonomy, benchmarking framework, and analysis of trade-offs between reconstruction performance and computational complexity.",
        "tldr_zh": "本文介绍了压缩视频质量增强（CVQE）方法的全面审查，其中包括新的分类法，基准框架以及重建性能与计算复杂性之间的权衡分析。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Efficient Learned Image Compression Through Knowledge Distillation",
        "summary": "Learned image compression sits at the intersection of machine learning and\nimage processing. With advances in deep learning, neural network-based\ncompression methods have emerged. In this process, an encoder maps the image to\na low-dimensional latent space, which is then quantized, entropy-coded into a\nbinary bitstream, and transmitted to the receiver. At the receiver end, the\nbitstream is entropy-decoded, and a decoder reconstructs an approximation of\nthe original image. Recent research suggests that these models consistently\noutperform conventional codecs. However, they require significant processing\npower, making them unsuitable for real-time use on resource-constrained\nplatforms, which hinders their deployment in mainstream applications. This\nstudy aims to reduce the resource requirements of neural networks used for\nimage compression by leveraging knowledge distillation, a training paradigm\nwhere smaller neural networks, partially trained on the outputs of larger, more\ncomplex models, can achieve better performance than when trained independently.\nOur work demonstrates that knowledge distillation can be effectively applied to\nimage compression tasks: i) across various architecture sizes, ii) to achieve\ndifferent image quality/bit rate tradeoffs, and iii) to save processing and\nenergy resources. This approach introduces new settings and hyperparameters,\nand future research could explore the impact of different teacher models, as\nwell as alternative loss functions. Knowledge distillation could also be\nextended to transformer-based models. The code is publicly available at:\nhttps://github.com/FABallemand/PRIM .",
        "url": "http://arxiv.org/abs/2509.10366v1",
        "published_date": "2025-09-12T15:59:55+00:00",
        "updated_date": "2025-09-12T15:59:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fabien Allemand",
            "Attilio Fiandrotti",
            "Sumanta Chaudhuri",
            "Alaa Eddine Mazouz"
        ],
        "ai_categories": [
            "AIGC"
        ],
        "tldr": "The paper introduces a method using knowledge distillation to reduce resource requirements for neural network-based image compression.",
        "tldr_zh": "本文介绍了一种利用知识蒸馏的方法，以减少神经网络图像压缩的资源需求。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Immunizing Images from Text to Image Editing via Adversarial Cross-Attention",
        "summary": "Recent advances in text-based image editing have enabled fine-grained\nmanipulation of visual content guided by natural language. However, such\nmethods are susceptible to adversarial attacks. In this work, we propose a\nnovel attack that targets the visual component of editing methods. We introduce\nAttention Attack, which disrupts the cross-attention between a textual prompt\nand the visual representation of the image by using an automatically generated\ncaption of the source image as a proxy for the edit prompt. This breaks the\nalignment between the contents of the image and their textual description,\nwithout requiring knowledge of the editing method or the editing prompt.\nReflecting on the reliability of existing metrics for immunization success, we\npropose two novel evaluation strategies: Caption Similarity, which quantifies\nsemantic consistency between original and adversarial edits, and semantic\nIntersection over Union (IoU), which measures spatial layout disruption via\nsegmentation masks. Experiments conducted on the TEDBench++ benchmark\ndemonstrate that our attack significantly degrades editing performance while\nremaining imperceptible.",
        "url": "http://arxiv.org/abs/2509.10359v1",
        "published_date": "2025-09-12T15:47:50+00:00",
        "updated_date": "2025-09-12T15:47:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Matteo Trippodo",
            "Federico Becattini",
            "Lorenzo Seidenari"
        ],
        "ai_categories": [
            "GAN",
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces an attack that disrupts the cross-attention between a textual prompt and the visual representation of an image, significantly degrading editing performance while remaining imperceptible.",
        "tldr_zh": "该论文介绍了一种攻击方法，破坏了文本提示和图像视觉表示之间的交叉关注，显著降低了编辑性能，同时保持不可察觉。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Understanding Visual Grounding in Visual Language Models",
        "summary": "Visual grounding refers to the ability of a model to identify a region within\nsome visual input that matches a textual description. Consequently, a model\nequipped with visual grounding capabilities can target a wide range of\napplications in various domains, including referring expression comprehension,\nanswering questions pertinent to fine-grained details in images or videos,\ncaption visual context by explicitly referring to entities, as well as low and\nhigh-level control in simulated and real environments. In this survey paper, we\nreview representative works across the key areas of research on modern\ngeneral-purpose vision language models (VLMs). We first outline the importance\nof grounding in VLMs, then delineate the core components of the contemporary\nparadigm for developing grounded models, and examine their practical\napplications, including benchmarks and evaluation metrics for grounded\nmultimodal generation. We also discuss the multifaceted interrelations among\nvisual grounding, multimodal chain-of-thought, and reasoning in VLMs. Finally,\nwe analyse the challenges inherent to visual grounding and suggest promising\ndirections for future research.",
        "url": "http://arxiv.org/abs/2509.10345v1",
        "published_date": "2025-09-12T15:33:49+00:00",
        "updated_date": "2025-09-12T15:33:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Georgios Pantazopoulos",
            "Eda B. Özyiğit"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "This paper reviews the importance of visual grounding in vision language models and discusses its practical applications and challenges, suggesting future research directions.",
        "tldr_zh": "本文回顾了视觉语言模型中视觉衬底的重要性，讨论了其实际应用和挑战，并提出了未来研究方向。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography",
        "summary": "Mammography screening is an essential tool for early detection of breast\ncancer. The speed and accuracy of mammography interpretation have the potential\nto be improved with deep learning methods. However, the development of a\nfoundation visual language model (VLM) is hindered by limited data and domain\ndifferences between natural and medical images. Existing mammography VLMs,\nadapted from natural images, often ignore domain-specific characteristics, such\nas multi-view relationships in mammography. Unlike radiologists who analyze\nboth views together to process ipsilateral correspondence, current methods\ntreat them as independent images or do not properly model the multi-view\ncorrespondence learning, losing critical geometric context and resulting in\nsuboptimal prediction. We propose GLAM: Global and Local Alignment for\nMulti-view mammography for VLM pretraining using geometry guidance. By\nleveraging the prior knowledge about the multi-view imaging process of\nmammograms, our model learns local cross-view alignments and fine-grained local\nfeatures through joint global and local, visual-visual, and visual-language\ncontrastive learning. Pretrained on EMBED [14], one of the largest open\nmammography datasets, our model outperforms baselines across multiple datasets\nunder different settings.",
        "url": "http://arxiv.org/abs/2509.10344v1",
        "published_date": "2025-09-12T15:33:18+00:00",
        "updated_date": "2025-09-12T15:33:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Yuexi Du",
            "Lihui Chen",
            "Nicha C. Dvornek"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "GLAM proposes a novel approach for multi-view VLP in mammography by considering geometric context and local alignment, outperforming baselines across datasets.",
        "tldr_zh": "GLAM提出了一种新颖的方法，通过考虑几何背景和局部对齐，在乳腺X线摄影中进行多视图VLP，优于基准线跨数据集。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT",
        "summary": "Optical Coherence Tomography (OCT) is a vital imaging modality for diagnosing\nand monitoring retinal diseases. However, OCT images are inherently degraded by\nspeckle noise, which obscures fine details and hinders accurate interpretation.\nWhile numerous denoising methods exist, many struggle to balance noise\nreduction with the preservation of crucial anatomical structures. This paper\nintroduces GARD (Gamma-based Anatomical Restoration and Denoising), a novel\ndeep learning approach for OCT image despeckling that leverages the strengths\nof diffusion probabilistic models. Unlike conventional diffusion models that\nassume Gaussian noise, GARD employs a Denoising Diffusion Gamma Model to more\naccurately reflect the statistical properties of speckle. Furthermore, we\nintroduce a Noise-Reduced Fidelity Term that utilizes a pre-processed,\nless-noisy image to guide the denoising process. This crucial addition prevents\nthe reintroduction of high-frequency noise. We accelerate the inference process\nby adapting the Denoising Diffusion Implicit Model framework to our Gamma-based\nmodel. Experiments on a dataset with paired noisy and less-noisy OCT B-scans\ndemonstrate that GARD significantly outperforms traditional denoising methods\nand state-of-the-art deep learning models in terms of PSNR, SSIM, and MSE.\nQualitative results confirm that GARD produces sharper edges and better\npreserves fine anatomical details.",
        "url": "http://arxiv.org/abs/2509.10341v1",
        "published_date": "2025-09-12T15:24:41+00:00",
        "updated_date": "2025-09-12T15:24:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Botond Fazekas",
            "Thomas Pinetz",
            "Guilherme Aresta",
            "Taha Emre",
            "Hrvoje Bogunovic"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces GARD, a novel deep learning approach for despeckling OCT images that outperforms traditional methods in terms of image quality metrics and preservation of fine anatomical details.",
        "tldr_zh": "本文介绍了一种新颖的深度学习方法GARD，用于去除OCT图像中的斑点噪音，表现优于传统方法在图像质量度量和解剖细节保留方面。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Adversarial robustness through Lipschitz-Guided Stochastic Depth in Neural Networks",
        "summary": "Deep neural networks and Vision Transformers achieve state-of-the-art\nperformance in computer vision but are highly vulnerable to adversarial\nperturbations. Standard defenses often incur high computational cost or lack\nformal guarantees. We propose a Lipschitz-guided stochastic depth (DropPath)\nmethod, where drop probabilities increase with depth to control the effective\nLipschitz constant of the network. This approach regularizes deeper layers,\nimproving robustness while preserving clean accuracy and reducing computation.\nExperiments on CIFAR-10 with ViT-Tiny show that our custom depth-dependent\nschedule maintains near-baseline clean accuracy, enhances robustness under\nFGSM, PGD-20, and AutoAttack, and significantly reduces FLOPs compared to\nbaseline and linear DropPath schedules.",
        "url": "http://arxiv.org/abs/2509.10298v1",
        "published_date": "2025-09-12T14:38:18+00:00",
        "updated_date": "2025-09-12T14:38:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Laith Nayal",
            "Mahmoud Mousatat",
            "Bader Rasheed"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper introduces a Lipschitz-guided stochastic depth method to improve robustness in neural networks, reducing computational cost while maintaining accuracy.",
        "tldr_zh": "本文引入了一种利普希茨引导的随机深度方法，以提高神经网络的鲁棒性，降低计算成本同时保持精度。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MCL-AD: Multimodal Collaboration Learning for Zero-Shot 3D Anomaly Detection",
        "summary": "Zero-shot 3D (ZS-3D) anomaly detection aims to identify defects in 3D objects\nwithout relying on labeled training data, making it especially valuable in\nscenarios constrained by data scarcity, privacy, or high annotation cost.\nHowever, most existing methods focus exclusively on point clouds, neglecting\nthe rich semantic cues available from complementary modalities such as RGB\nimages and texts priors. This paper introduces MCL-AD, a novel framework that\nleverages multimodal collaboration learning across point clouds, RGB images,\nand texts semantics to achieve superior zero-shot 3D anomaly detection.\nSpecifically, we propose a Multimodal Prompt Learning Mechanism (MPLM) that\nenhances the intra-modal representation capability and inter-modal\ncollaborative learning by introducing an object-agnostic decoupled text prompt\nand a multimodal contrastive loss. In addition, a collaborative modulation\nmechanism (CMM) is proposed to fully leverage the complementary representations\nof point clouds and RGB images by jointly modulating the RGB image-guided and\npoint cloud-guided branches. Extensive experiments demonstrate that the\nproposed MCL-AD framework achieves state-of-the-art performance in ZS-3D\nanomaly detection.",
        "url": "http://arxiv.org/abs/2509.10282v1",
        "published_date": "2025-09-12T14:21:54+00:00",
        "updated_date": "2025-09-12T14:21:54+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Gang Li",
            "Tianjiao Chen",
            "Mingle Zhou",
            "Min Li",
            "Delong Han",
            "Jin Wan"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "MCL-AD is a novel framework for zero-shot 3D anomaly detection that leverages multimodal collaboration learning with point clouds, RGB images, and text semantics.",
        "tldr_zh": "MCL-AD 是一个新颖的框架，用于零样本 3D 异常检测，利用多模态协作学习，包括点云、RGB 图像和文本语义。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SignClip: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion",
        "summary": "Sign language translation (SLT) aims to translate natural language from sign\nlanguage videos, serving as a vital bridge for inclusive communication. While\nrecent advances leverage powerful visual backbones and large language models,\nmost approaches mainly focus on manual signals (hand gestures) and tend to\noverlook non-manual cues like mouthing. In fact, mouthing conveys essential\nlinguistic information in sign languages and plays a crucial role in\ndisambiguating visually similar signs. In this paper, we propose SignClip, a\nnovel framework to improve the accuracy of sign language translation. It fuses\nmanual and non-manual cues, specifically spatial gesture and lip movement\nfeatures. Besides, SignClip introduces a hierarchical contrastive learning\nframework with multi-level alignment objectives, ensuring semantic consistency\nacross sign-lip and visual-text modalities. Extensive experiments on two\nbenchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our\napproach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip\nsurpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from\n24.32 to 24.71, and ROUGE from 46.57 to 48.38.",
        "url": "http://arxiv.org/abs/2509.10266v1",
        "published_date": "2025-09-12T14:08:06+00:00",
        "updated_date": "2025-09-12T14:08:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Wenfang Wu",
            "Tingting Yuan",
            "Yupeng Li",
            "Daling Wang",
            "Xiaoming Fu"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes SignClip, a framework that leverages manual and non-manual cues for sign language translation, achieving superior performance on benchmark datasets.",
        "tldr_zh": "本文提出了SignClip框架，利用手势和唇动特征进行手语翻译，在基准数据集上表现出色。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Mask Consistency Regularization in Object Removal",
        "summary": "Object removal, a challenging task within image inpainting, involves\nseamlessly filling the removed region with content that matches the surrounding\ncontext. Despite advancements in diffusion models, current methods still face\ntwo critical challenges. The first is mask hallucination, where the model\ngenerates irrelevant or spurious content inside the masked region, and the\nsecond is mask-shape bias, where the model fills the masked area with an object\nthat mimics the mask's shape rather than surrounding content. To address these\nissues, we propose Mask Consistency Regularization (MCR), a novel training\nstrategy designed specifically for object removal tasks. During training, our\napproach introduces two mask perturbations: dilation and reshape, enforcing\nconsistency between the outputs of these perturbed branches and the original\nmask. The dilated masks help align the model's output with the surrounding\ncontent, while reshaped masks encourage the model to break the mask-shape bias.\nThis combination of strategies enables MCR to produce more robust and\ncontextually coherent inpainting results. Our experiments demonstrate that MCR\nsignificantly reduces hallucinations and mask-shape bias, leading to improved\nperformance in object removal.",
        "url": "http://arxiv.org/abs/2509.10259v1",
        "published_date": "2025-09-12T14:02:52+00:00",
        "updated_date": "2025-09-12T14:02:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hua Yuan",
            "Jin Yuan",
            "Yicheng Jiang",
            "Yao Zhang",
            "Xin Geng",
            "Yong Rui"
        ],
        "ai_categories": [
            "GAN",
            "Transformer"
        ],
        "tldr": "The paper introduces Mask Consistency Regularization (MCR) to improve object removal in image inpainting by addressing mask hallucination and mask-shape bias.",
        "tldr_zh": "本文介绍了面膜一致性规范 (MCR)，通过解决面膜幻觉和面膜形状偏见，改进图像修补中的对象移除。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection",
        "summary": "With generative models becoming increasingly sophisticated and diverse,\ndetecting AI-generated images has become increasingly challenging. While\nexisting AI-genereted Image detectors achieve promising performance on\nin-distribution generated images, their generalization to unseen generative\nmodels remains limited. This limitation is largely attributed to their reliance\non generation-specific artifacts, such as stylistic priors and compression\npatterns. To address these limitations, we propose GAMMA, a novel training\nframework designed to reduce domain bias and enhance semantic alignment. GAMMA\nintroduces diverse manipulation strategies, such as inpainting-based\nmanipulation and semantics-preserving perturbations, to ensure consistency\nbetween manipulated and authentic content. We employ multi-task supervision\nwith dual segmentation heads and a classification head, enabling pixel-level\nsource attribution across diverse generative domains. In addition, a reverse\ncross-attention mechanism is introduced to allow the segmentation heads to\nguide and correct biased representations in the classification branch. Our\nmethod achieves state-of-the-art generalization performance on the GenImage\nbenchmark, imporving accuracy by 5.8%, but also maintains strong robustness on\nnewly released generative model such as GPT-4o.",
        "url": "http://arxiv.org/abs/2509.10250v1",
        "published_date": "2025-09-12T13:46:54+00:00",
        "updated_date": "2025-09-12T13:46:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haozhen Yan",
            "Yan Hong",
            "Suning Lang",
            "Jiahui Zhan",
            "Yikun Ji",
            "Yujie Gao",
            "Jun Lan",
            "Huijia Zhu",
            "Weiqiang Wang",
            "Jianfu Zhang"
        ],
        "ai_categories": [
            "AIGC",
            "GAN"
        ],
        "tldr": "The paper proposes GAMMA, a novel training framework for detecting AI-generated images by reducing domain bias and enhancing semantic alignment.",
        "tldr_zh": "本文提出了GAMMA，一种新的训练框架，用于检测AI生成的图像，通过减少领域偏见和增强语义对齐。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LayerLock: Non-collapsing Representation Learning with Progressive Freezing",
        "summary": "We introduce LayerLock, a simple yet effective approach for self-supervised\nvisual representation learning, that gradually transitions from pixel to latent\nprediction through progressive layer freezing. First, we make the observation\nthat during training of video masked-autoencoding (MAE) models, ViT layers\nconverge in the order of their depth: shallower layers converge early, deeper\nlayers converge late. We then show that this observation can be exploited to\naccelerate standard MAE by progressively freezing the model according to an\nexplicit schedule, throughout training. Furthermore, this same schedule can be\nused in a simple and scalable approach to latent prediction that does not\nsuffer from \"representation collapse\". We apply our proposed approach,\nLayerLock, to large models of up to 4B parameters with results surpassing those\nof non-latent masked prediction on the 4DS perception suite.",
        "url": "http://arxiv.org/abs/2509.10156v1",
        "published_date": "2025-09-12T11:32:51+00:00",
        "updated_date": "2025-09-12T11:32:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Goker Erdogan",
            "Nikhil Parthasarathy",
            "Catalin Ionescu",
            "Drew Hudson",
            "Alexander Lerchner",
            "Andrew Zisserman",
            "Mehdi Sajjadi",
            "Joao Carreira"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "AIGC"
        ],
        "tldr": "LayerLock is a method for self-supervised visual representation learning that uses progressive layer freezing to transition from pixel to latent prediction, achieving good results on perception tasks.",
        "tldr_zh": "LayerLock是一种用于自监督视觉表示学习的方法，通过渐进层冻结实现从像素到潜在预测的过渡，在感知任务上取得良好结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Scalable Training for Vector-Quantized Networks with 100% Codebook Utilization",
        "summary": "Vector quantization (VQ) is a key component in discrete tokenizers for image\ngeneration, but its training is often unstable due to straight-through\nestimation bias, one-step-behind updates, and sparse codebook gradients, which\nlead to suboptimal reconstruction performance and low codebook usage. In this\nwork, we analyze these fundamental challenges and provide a simple yet\neffective solution. To maintain high codebook usage in VQ networks (VQN) during\nlearning annealing and codebook size expansion, we propose VQBridge, a robust,\nscalable, and efficient projector based on the map function method. VQBridge\noptimizes code vectors through a compress-process-recover pipeline, enabling\nstable and effective codebook training. By combining VQBridge with learning\nannealing, our VQN achieves full (100%) codebook usage across diverse codebook\nconfigurations, which we refer to as FVQ (FullVQ). Through extensive\nexperiments, we demonstrate that FVQ is effective, scalable, and generalizable:\nit attains 100% codebook usage even with a 262k-codebook, achieves\nstate-of-the-art reconstruction performance, consistently improves with larger\ncodebooks, higher vector channels, or longer training, and remains effective\nacross different VQ variants. Moreover, when integrated with LlamaGen, FVQ\nsignificantly enhances image generation performance, surpassing visual\nautoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID,\nhighlighting the importance of high-quality tokenizers for strong\nautoregressive image generation.",
        "url": "http://arxiv.org/abs/2509.10140v1",
        "published_date": "2025-09-12T11:08:21+00:00",
        "updated_date": "2025-09-12T11:08:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yifan Chang",
            "Jie Qin",
            "Limeng Qiao",
            "Xiaofeng Wang",
            "Zheng Zhu",
            "Lin Ma",
            "Xingang Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a method called VQBridge to improve training of Vector-Quantized Networks (VQN) for image generation, achieving full codebook utilization and state-of-the-art reconstruction performance. This method, named FVQ, enhances image generation performance surpassing other models.",
        "tldr_zh": "该论文引入了一种名为VQBridge的方法，以提高矢量量化网络（VQN）的训练效果，实现了全码书利用和领先的重建性能。该方法名为FVQ，提高了图像生成性能，超越其他模型。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "VARCO-VISION-2.0 Technical Report",
        "summary": "We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model\n(VLM) for Korean and English with improved capabilities compared to the\nprevious model VARCO-VISION-14B. The model supports multi-image understanding\nfor complex inputs such as documents, charts, and tables, and delivers\nlayoutaware OCR by predicting both textual content and its spatial location.\nTrained with a four-stage curriculum with memory-efficient techniques, the\nmodel achieves enhanced multimodal alignment, while preserving core language\nabilities and improving safety via preference optimization. Extensive benchmark\nevaluations demonstrate strong spatial grounding and competitive results for\nboth languages, with the 14B model achieving 8th place on the OpenCompass VLM\nleaderboard among models of comparable scale. Alongside the 14B-scale model, we\nrelease a 1.7B version optimized for on-device deployment. We believe these\nmodels advance the development of bilingual VLMs and their practical\napplications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a\nfull-scale 14B model and a lightweight 1.7B model.",
        "url": "http://arxiv.org/abs/2509.10105v1",
        "published_date": "2025-09-12T09:55:56+00:00",
        "updated_date": "2025-09-12T09:55:56+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Young-rok Cha",
            "Jeongho Ju",
            "SunYoung Park",
            "Jong-Hyeon Lee",
            "Younghyun Yu",
            "Youngjune Kim"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces VARCO-VISION-2.0, an improved bilingual vision-language model for Korean and English that supports multi-image understanding and layout-aware OCR. It achieves strong spatial grounding and competitive results for both languages.",
        "tldr_zh": "该论文介绍了VARCO-VISION-2.0，这是一个针对韩语和英语的改进型双语视觉-语言模型，支持多图像理解和布局感知OCR。它在两种语言中实现了较强的空间基础和竞争力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Leveraging Multi-View Weak Supervision for Occlusion-Aware Multi-Human Parsing",
        "summary": "Multi-human parsing is the task of segmenting human body parts while\nassociating each part to the person it belongs to, combining instance-level and\npart-level information for fine-grained human understanding. In this work, we\ndemonstrate that, while state-of-the-art approaches achieved notable results on\npublic datasets, they struggle considerably in segmenting people with\noverlapping bodies. From the intuition that overlapping people may appear\nseparated from a different point of view, we propose a novel training framework\nexploiting multi-view information to improve multi-human parsing models under\nocclusions. Our method integrates such knowledge during the training process,\nintroducing a novel approach based on weak supervision on human instances and a\nmulti-view consistency loss. Given the lack of suitable datasets in the\nliterature, we propose a semi-automatic annotation strategy to generate human\ninstance segmentation masks from multi-view RGB+D data and 3D human skeletons.\nThe experiments demonstrate that the approach can achieve up to a 4.20\\%\nrelative improvement on human parsing over the baseline model in occlusion\nscenarios.",
        "url": "http://arxiv.org/abs/2509.10093v1",
        "published_date": "2025-09-12T09:36:23+00:00",
        "updated_date": "2025-09-12T09:36:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Laura Bragagnolo",
            "Matteo Terreran",
            "Leonardo Barcellona",
            "Stefano Ghidoni"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper proposes a novel training framework leveraging multi-view weak supervision for occlusion-aware multi-human parsing, achieving improvements in segmenting people with overlapping bodies.",
        "tldr_zh": "本文提出了一种利用多视角弱监督的训练框架，用于对重叠身体进行遮挡感知多人解析，实现了在区分具有重叠身体的人时的改进。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation",
        "summary": "Accurate color alignment in text-to-image (T2I) generation is critical for\napplications such as fashion, product visualization, and interior design, yet\ncurrent diffusion models struggle with nuanced and compound color terms (e.g.,\nTiffany blue, lime green, hot pink), often producing images that are misaligned\nwith human intent. Existing approaches rely on cross-attention manipulation,\nreference images, or fine-tuning but fail to systematically resolve ambiguous\ncolor descriptions. To precisely render colors under prompt ambiguity, we\npropose a training-free framework that enhances color fidelity by leveraging a\nlarge language model (LLM) to disambiguate color-related prompts and guiding\ncolor blending operations directly in the text embedding space. Our method\nfirst employs a large language model (LLM) to resolve ambiguous color terms in\nthe text prompt, and then refines the text embeddings based on the spatial\nrelationships of the resulting color terms in the CIELAB color space. Unlike\nprior methods, our approach improves color accuracy without requiring\nadditional training or external reference images. Experimental results\ndemonstrate that our framework improves color alignment without compromising\nimage quality, bridging the gap between text semantics and visual generation.",
        "url": "http://arxiv.org/abs/2509.10058v1",
        "published_date": "2025-09-12T08:44:22+00:00",
        "updated_date": "2025-09-12T08:44:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sung-Lin Tsai",
            "Bo-Lun Huang",
            "Yu Ting Shen",
            "Cheng Yu Yeo",
            "Chiang Tseng",
            "Bo-Kai Ruan",
            "Wen-Sheng Lien",
            "Hong-Han Shuai"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper proposes a training-free framework to improve color alignment in text-to-image generation by leveraging a large language model and color space relationships.",
        "tldr_zh": "本文提出了一个无需训练的框架，通过利用大型语言模型和颜色空间关系来提高文本到图像生成中的颜色对齐。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA",
        "summary": "As large vision language models (VLMs) advance, their capabilities in\nmultilingual visual question answering (mVQA) have significantly improved.\nChain-of-thought (CoT) reasoning has been proven to enhance interpretability\nand complex reasoning. However, most existing approaches rely primarily on\ntextual CoT and provide limited support for multilingual multimodal reasoning,\nconstraining their deployment in real-world applications. To address this gap,\nwe introduce \\textbf{LaV-CoT}, the first Language-aware Visual CoT framework\nwith Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable\nmulti-stage reasoning pipeline consisting of Text Summary with Bounding Box\n(BBox), Language Identification, Spatial Object-level Captioning, and\nStep-by-step Logical Reasoning. Following this reasoning pipeline, we design an\nautomated data curation method that generates multilingual CoT annotations\nthrough iterative generation, correction, and refinement, enabling scalable and\nhigh-quality training data. To improve reasoning and generalization, LaV-CoT\nadopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT)\nwith Language-aware Group Relative Policy Optimization (GRPO), guided by\nverifiable multi-aspect rewards including language consistency, structural\naccuracy, and semantic alignment. Extensive evaluations on public datasets\nincluding MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up\nto \\(\\sim\\)9.5\\% accuracy improvements over open-source baselines of similar\nsize and even surpasses models with 2$\\times$ larger scales by \\(\\sim\\)2.6\\%.\nMoreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513\nand Gemini-2.5-flash. We further conducted an online A/B test to validate our\nmethod on real-world data, highlighting its effectiveness for industrial\ndeployment. Our code is available at this link:\n\\href{https://github.com/HJNVR/LaV-CoT}",
        "url": "http://arxiv.org/abs/2509.10026v1",
        "published_date": "2025-09-12T07:45:44+00:00",
        "updated_date": "2025-09-12T07:45:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jing Huang",
            "Zhiya Tan",
            "Shutao Gong",
            "Fanwei Zeng",
            "Jianshu Li"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "LaV-CoT is a framework for multilingual visual question answering that incorporates a multi-stage reasoning pipeline and leverages automated data curation and two-stage training for improved reasoning and generalization, achieving impressive accuracy improvements over existing models.",
        "tldr_zh": "LaV-CoT是一个用于多语言视觉问答的框架，它结合了多阶段推理流程，并利用自动数据整理和两阶段训练以提高推理和泛化能力，实现了对现有模型的显著准确率提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Hierarchical MLANet: Multi-level Attention for 3D Face Reconstruction From Single Images",
        "summary": "Recovering 3D face models from 2D in-the-wild images has gained considerable\nattention in the computer vision community due to its wide range of potential\napplications. However, the lack of ground-truth labeled datasets and the\ncomplexity of real-world environments remain significant challenges. In this\nchapter, we propose a convolutional neural network-based approach, the\nHierarchical Multi-Level Attention Network (MLANet), for reconstructing 3D face\nmodels from single in-the-wild images. Our model predicts detailed facial\ngeometry, texture, pose, and illumination parameters from a single image.\nSpecifically, we employ a pre-trained hierarchical backbone network and\nintroduce multi-level attention mechanisms at different stages of 2D face image\nfeature extraction. A semi-supervised training strategy is employed,\nincorporating 3D Morphable Model (3DMM) parameters from publicly available\ndatasets along with a differentiable renderer, enabling an end-to-end training\nprocess. Extensive experiments, including both comparative and ablation\nstudies, were conducted on two benchmark datasets, AFLW2000-3D and MICC\nFlorence, focusing on 3D face reconstruction and 3D face alignment tasks. The\neffectiveness of the proposed method was evaluated both quantitatively and\nqualitatively.",
        "url": "http://arxiv.org/abs/2509.10024v1",
        "published_date": "2025-09-12T07:42:27+00:00",
        "updated_date": "2025-09-12T07:42:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Danling Cao"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "Hierarchical MLANet proposes a CNN-based approach for 3D face reconstruction from single in-the-wild images, incorporating multi-level attention mechanisms and semi-supervised training.",
        "tldr_zh": "Hierarchical MLANet 提出了一种基于 CNN 的方法，用于从单个野外图像中重建 3D 面部模型，包括多级注意力机制和半监督训练。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TUNI: Real-time RGB-T Semantic Segmentation with Unified Multi-Modal Feature Extraction and Cross-Modal Feature Fusion",
        "summary": "RGB-thermal (RGB-T) semantic segmentation improves the environmental\nperception of autonomous platforms in challenging conditions. Prevailing models\nemploy encoders pre-trained on RGB images to extract features from both RGB and\ninfrared inputs, and design additional modules to achieve cross-modal feature\nfusion. This results in limited thermal feature extraction and suboptimal\ncross-modal fusion, while the redundant encoders further compromises the\nmodel's real-time efficiency. To address the above issues, we propose TUNI,\nwith an RGB-T encoder consisting of multiple stacked blocks that simultaneously\nperform multi-modal feature extraction and cross-modal fusion. By leveraging\nlarge-scale pre-training with RGB and pseudo-thermal data, the RGB-T encoder\nlearns to integrate feature extraction and fusion in a unified manner. By\nslimming down the thermal branch, the encoder achieves a more compact\narchitecture. Moreover, we introduce an RGB-T local module to strengthen the\nencoder's capacity for cross-modal local feature fusion. The RGB-T local module\nemploys adaptive cosine similarity to selectively emphasize salient consistent\nand distinct local features across RGB-T modalities. Experimental results show\nthat TUNI achieves competitive performance with state-of-the-art models on FMB,\nPST900 and CART, with fewer parameters and lower computational cost. Meanwhile,\nit achieves an inference speed of 27 FPS on a Jetson Orin NX, demonstrating its\nreal-time capability in deployment. Codes are available at\nhttps://github.com/xiaodonguo/TUNI.",
        "url": "http://arxiv.org/abs/2509.10005v1",
        "published_date": "2025-09-12T07:02:45+00:00",
        "updated_date": "2025-09-12T07:02:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaodong Guo",
            "Tong Liu",
            "Yike Li",
            "Zi'ang Lin",
            "Zhihong Deng"
        ],
        "ai_categories": [
            "Multimodality",
            "LoRA",
            "Dataset"
        ],
        "tldr": "TUNI proposes a real-time RGB-T semantic segmentation model that integrates feature extraction and fusion, achieving competitive performance with fewer parameters and lower computational cost.",
        "tldr_zh": "TUNI提出了一种实时的RGB-T语义分割模型，同时进行特征提取和融合，性能竞争力强，参数更少，计算成本更低。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ISTASTrack: Bridging ANN and SNN via ISTA Adapter for RGB-Event Tracking",
        "summary": "RGB-Event tracking has become a promising trend in visual object tracking to\nleverage the complementary strengths of both RGB images and dynamic spike\nevents for improved performance. However, existing artificial neural networks\n(ANNs) struggle to fully exploit the sparse and asynchronous nature of event\nstreams. Recent efforts toward hybrid architectures combining ANNs and spiking\nneural networks (SNNs) have emerged as a promising solution in RGB-Event\nperception, yet effectively fusing features across heterogeneous paradigms\nremains a challenge. In this work, we propose ISTASTrack, the first\ntransformer-based \\textbf{A}NN-\\textbf{S}NN hybrid \\textbf{Track}er equipped\nwith \\textbf{ISTA} adapters for RGB-Event tracking. The two-branch model\nemploys a vision transformer to extract spatial context from RGB inputs and a\nspiking transformer to capture spatio-temporal dynamics from event streams. To\nbridge the modality and paradigm gap between ANN and SNN features, we\nsystematically design a model-based ISTA adapter for bidirectional feature\ninteraction between the two branches, derived from sparse representation theory\nby unfolding the iterative shrinkage thresholding algorithm. Additionally, we\nincorporate a temporal downsampling attention module within the adapter to\nalign multi-step SNN features with single-step ANN features in the latent\nspace, improving temporal fusion. Experimental results on RGB-Event tracking\nbenchmarks, such as FE240hz, VisEvent, COESOT, and FELT, have demonstrated that\nISTASTrack achieves state-of-the-art performance while maintaining high energy\nefficiency, highlighting the effectiveness and practicality of hybrid ANN-SNN\ndesigns for robust visual tracking. The code is publicly available at\nhttps://github.com/lsying009/ISTASTrack.git.",
        "url": "http://arxiv.org/abs/2509.09977v1",
        "published_date": "2025-09-12T05:37:17+00:00",
        "updated_date": "2025-09-12T05:37:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Siying Liu",
            "Zikai Wang",
            "Hanle Zheng",
            "Yifan Hu",
            "Xilin Wang",
            "Qingkai Yang",
            "Jibin Wu",
            "Hao Guo",
            "Lei Deng"
        ],
        "ai_categories": [
            "Transformer",
            "AIGC"
        ],
        "tldr": "ISTASTrack proposes a transformer-based ANN-SNN hybrid tracker for RGB-Event tracking, achieving state-of-the-art performance with high energy efficiency.",
        "tldr_zh": "ISTASTrack提出了一种基于转换器的ANN-SNN混合跟踪器，用于RGB-Event跟踪，在保持高能效的同时实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey",
        "summary": "Event camera sensors are bio-inspired sensors which asynchronously capture\nper-pixel brightness changes and output a stream of events encoding the\npolarity, location and time of these changes. These systems are witnessing\nrapid advancements as an emerging field, driven by their low latency, reduced\npower consumption, and ultra-high capture rates. This survey explores the\nevolution of fusing event-stream captured with traditional frame-based capture,\nhighlighting how this synergy significantly benefits various video restoration\nand 3D reconstruction tasks. The paper systematically reviews major deep\nlearning contributions to image/video enhancement and restoration, focusing on\ntwo dimensions: temporal enhancement (such as frame interpolation and motion\ndeblurring) and spatial enhancement (including super-resolution, low-light and\nHDR enhancement, and artifact reduction). This paper also explores how the 3D\nreconstruction domain evolves with the advancement of event driven fusion.\nDiverse topics are covered, with in-depth discussions on recent works for\nimproving visual quality under challenging conditions. Additionally, the survey\ncompiles a comprehensive list of openly available datasets, enabling\nreproducible research and benchmarking. By consolidating recent progress and\ninsights, this survey aims to inspire further research into leveraging event\ncamera systems, especially in combination with deep learning, for advanced\nvisual media restoration and enhancement.",
        "url": "http://arxiv.org/abs/2509.09971v1",
        "published_date": "2025-09-12T05:16:54+00:00",
        "updated_date": "2025-09-12T05:16:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aupendu Kar",
            "Vishnu Raj",
            "Guan-Ming Su"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper explores the fusion of event-stream captured with traditional frame-based capture for video restoration and 3D reconstruction tasks, focusing on deep learning contributions for image/video enhancement and restoration.",
        "tldr_zh": "本文探讨了事件流捕获与传统基于帧的捕获的融合，重点关注图像/视频增强和恢复的深度学习贡献。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Augment to Segment: Tackling Pixel-Level Imbalance in Wheat Disease and Pest Segmentation",
        "summary": "Accurate segmentation of foliar diseases and insect damage in wheat is\ncrucial for effective crop management and disease control. However, the insect\ndamage typically occupies only a tiny fraction of annotated pixels. This\nextreme pixel-level imbalance poses a significant challenge to the segmentation\nperformance, which can result in overfitting to common classes and insufficient\nlearning of rare classes, thereby impairing overall performance. In this paper,\nwe propose a Random Projected Copy-and-Paste (RPCP) augmentation technique to\naddress the pixel imbalance problem. Specifically, we extract rare\ninsect-damage patches from annotated training images and apply random geometric\ntransformations to simulate variations. The transformed patches are then pasted\nin appropriate regions while avoiding overlaps with lesions or existing damaged\nregions. In addition, we apply a random projection filter to the pasted\nregions, refining local features and ensuring a natural blend with the new\nbackground. Experiments show that our method substantially improves\nsegmentation performance on the insect damage class, while maintaining or even\nslightly enhancing accuracy on other categories. Our results highlight the\neffectiveness of targeted augmentation in mitigating extreme pixel imbalance,\noffering a straightforward yet effective solution for agricultural segmentation\nproblems.",
        "url": "http://arxiv.org/abs/2509.09961v1",
        "published_date": "2025-09-12T04:38:32+00:00",
        "updated_date": "2025-09-12T04:38:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianqi Wei",
            "Xin Yu",
            "Zhi Chen",
            "Scott Chapman",
            "Zi Huang"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a Random Projected Copy-and-Paste augmentation technique to address pixel-level imbalance in wheat disease and pest segmentation, improving segmentation performance on rare classes while maintaining accuracy on common categories.",
        "tldr_zh": "本文介绍了一种随机投影复制粘贴增强技术，以解决小麦疾病和害虫分割中的像素级不平衡问题，提高了罕见类别的分割性能，同时保持了常见类别的准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Chord: Chain of Rendering Decomposition for PBR Material Estimation from Generated Texture Images",
        "summary": "Material creation and reconstruction are crucial for appearance modeling but\ntraditionally require significant time and expertise from artists. While recent\nmethods leverage visual foundation models to synthesize PBR materials from\nuser-provided inputs, they often fall short in quality, flexibility, and user\ncontrol. We propose a novel two-stage generate-and-estimate framework for PBR\nmaterial generation. In the generation stage, a fine-tuned diffusion model\nsynthesizes shaded, tileable texture images aligned with user input. In the\nestimation stage, we introduce a chained decomposition scheme that sequentially\npredicts SVBRDF channels by passing previously extracted representation as\ninput into a single-step image-conditional diffusion model. Our method is\nefficient, high quality, and enables flexible user control. We evaluate our\napproach against existing material generation and estimation methods,\ndemonstrating superior performance. Our material estimation method shows strong\nrobustness on both generated textures and in-the-wild photographs. Furthermore,\nwe highlight the flexibility of our framework across diverse applications,\nincluding text-to-material, image-to-material, structure-guided generation, and\nmaterial editing.",
        "url": "http://arxiv.org/abs/2509.09952v1",
        "published_date": "2025-09-12T04:03:07+00:00",
        "updated_date": "2025-09-12T04:03:07+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Zhi Ying",
            "Boxiang Rong",
            "Jingyu Wang",
            "Maoyuan Xu"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a two-stage framework for generating PBR materials efficiently and with high quality, showcasing strong performance compared to existing methods.",
        "tldr_zh": "本文提出了一个两阶段的框架，用于高效生成PBR材质，与现有方法相比表现出强大性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "An Autoencoder and Vision Transformer-based Interpretability Analysis of the Differences in Automated Staging of Second and Third Molars",
        "summary": "The practical adoption of deep learning in high-stakes forensic applications,\nsuch as dental age estimation, is often limited by the 'black box' nature of\nthe models. This study introduces a framework designed to enhance both\nperformance and transparency in this context. We use a notable performance\ndisparity in the automated staging of mandibular second (tooth 37) and third\n(tooth 38) molars as a case study. The proposed framework, which combines a\nconvolutional autoencoder (AE) with a Vision Transformer (ViT), improves\nclassification accuracy for both teeth over a baseline ViT, increasing from\n0.712 to 0.815 for tooth 37 and from 0.462 to 0.543 for tooth 38. Beyond\nimproving performance, the framework provides multi-faceted diagnostic\ninsights. Analysis of the AE's latent space metrics and image reconstructions\nindicates that the remaining performance gap is data-centric, suggesting high\nintra-class morphological variability in the tooth 38 dataset is a primary\nlimiting factor. This work highlights the insufficiency of relying on a single\nmode of interpretability, such as attention maps, which can appear anatomically\nplausible yet fail to identify underlying data issues. By offering a\nmethodology that both enhances accuracy and provides evidence for why a model\nmay be uncertain, this framework serves as a more robust tool to support expert\ndecision-making in forensic age estimation.",
        "url": "http://arxiv.org/abs/2509.09911v1",
        "published_date": "2025-09-12T00:54:07+00:00",
        "updated_date": "2025-09-12T00:54:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "68T07 (Primary)"
        ],
        "authors": [
            "Barkin Buyukcakir",
            "Jannick De Tobel",
            "Patrick Thevissen",
            "Dirk Vandermeulen",
            "Peter Claes"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a framework combining an autoencoder and Vision Transformer to enhance performance and transparency in dental age estimation using deep learning. It improves classification accuracy and provides insights into data-centric limitations.",
        "tldr_zh": "本文介绍了一个结合自动编码器和Vision Transformer的框架，旨在利用深度学习增强牙齿年龄估计的性能和透明度。它提高了分类准确性，并提供了关于数据中心限制的见解。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Automated Tuning for Diffusion Inverse Problem Solvers without Generative Prior Retraining",
        "summary": "Diffusion/score-based models have recently emerged as powerful generative\npriors for solving inverse problems, including accelerated MRI reconstruction.\nWhile their flexibility allows decoupling the measurement model from the\nlearned prior, their performance heavily depends on carefully tuned data\nfidelity weights, especially under fast sampling schedules with few denoising\nsteps. Existing approaches often rely on heuristics or fixed weights, which\nfail to generalize across varying measurement conditions and irregular timestep\nschedules. In this work, we propose Zero-shot Adaptive Diffusion Sampling\n(ZADS), a test-time optimization method that adaptively tunes fidelity weights\nacross arbitrary noise schedules without requiring retraining of the diffusion\nprior. ZADS treats the denoising process as a fixed unrolled sampler and\noptimizes fidelity weights in a self-supervised manner using only undersampled\nmeasurements. Experiments on the fastMRI knee dataset demonstrate that ZADS\nconsistently outperforms both traditional compressed sensing and recent\ndiffusion-based methods, showcasing its ability to deliver high-fidelity\nreconstructions across varying noise schedules and acquisition settings.",
        "url": "http://arxiv.org/abs/2509.09880v1",
        "published_date": "2025-09-11T22:22:32+00:00",
        "updated_date": "2025-09-11T22:22:32+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "physics.med-ph"
        ],
        "authors": [
            "Yaşar Utku Alçalar",
            "Junno Yun",
            "Mehmet Akçakaya"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper introduces Zero-shot Adaptive Diffusion Sampling (ZADS), a method that adaptively tunes fidelity weights for solving inverse problems without retraining of the diffusion prior, outperforming traditional methods in MRI reconstruction.",
        "tldr_zh": "本文介绍了零击中自适应扩散采样（ZADS）方法，该方法自适应调整忠实度权重以解决反问题，无需重新训练扩散先验，胜过传统方法在MRI重建中表现。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "WAVE-DETR Multi-Modal Visible and Acoustic Real-Life Drone Detector",
        "summary": "We introduce a multi-modal WAVE-DETR drone detector combining visible RGB and\nacoustic signals for robust real-life UAV object detection. Our approach fuses\nvisual and acoustic features in a unified object detector model relying on the\nDeformable DETR and Wav2Vec2 architectures, achieving strong performance under\nchallenging environmental conditions. Our work leverage the existing\nDrone-vs-Bird dataset and the newly generated ARDrone dataset containing more\nthan 7,500 synchronized images and audio segments. We show how the acoustic\ninformation is used to improve the performance of the Deformable DETR object\ndetector on the real ARDrone dataset. We developed, trained and tested four\ndifferent fusion configurations based on a gated mechanism, linear layer, MLP\nand cross attention. The Wav2Vec2 acoustic embeddings are fused with the multi\nresolution feature mappings of the Deformable DETR and enhance the object\ndetection performance over all drones dimensions. The best performer is the\ngated fusion approach, which improves the mAP of the Deformable DETR object\ndetector on our in-distribution and out-of-distribution ARDrone datasets by\n11.1% to 15.3% for small drones across all IoU thresholds between 0.5 and 0.9.\nThe mAP scores for medium and large drones are also enhanced, with overall\ngains across all drone sizes ranging from 3.27% to 5.84%.",
        "url": "http://arxiv.org/abs/2509.09859v1",
        "published_date": "2025-09-11T21:18:32+00:00",
        "updated_date": "2025-09-11T21:18:32+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "68W99"
        ],
        "authors": [
            "Razvan Stefanescu",
            "Ethan Oh",
            "Ruben Vazquez",
            "Chris Mesterharm",
            "Constantin Serban",
            "Ritu Chadha"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper presents a multi-modal drone detector that combines visible and acoustic signals to enhance object detection performance under challenging environmental conditions.",
        "tldr_zh": "本文介绍了一种多模态无人机检测器，结合可见光和声学信号以增强在复杂环境条件下的目标检测性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception",
        "summary": "Robust semantic perception for autonomous vehicles relies on effectively\ncombining multiple sensors with complementary strengths and weaknesses.\nState-of-the-art sensor fusion approaches to semantic perception often treat\nsensor data uniformly across the spatial extent of the input, which hinders\nperformance when faced with challenging conditions. By contrast, we propose a\nnovel depth-guided multimodal fusion method that upgrades condition-aware\nfusion by integrating depth information. Our network, DGFusion, poses\nmultimodal segmentation as a multi-task problem, utilizing the lidar\nmeasurements, which are typically available in outdoor sensor suites, both as\none of the model's inputs and as ground truth for learning depth. Our\ncorresponding auxiliary depth head helps to learn depth-aware features, which\nare encoded into spatially varying local depth tokens that condition our\nattentive cross-modal fusion. Together with a global condition token, these\nlocal depth tokens dynamically adapt sensor fusion to the spatially varying\nreliability of each sensor across the scene, which largely depends on depth. In\naddition, we propose a robust loss for our depth, which is essential for\nlearning from lidar inputs that are typically sparse and noisy in adverse\nconditions. Our method achieves state-of-the-art panoptic and semantic\nsegmentation performance on the challenging MUSES and DELIVER datasets. Code\nand models will be available at https://github.com/timbroed/DGFusion",
        "url": "http://arxiv.org/abs/2509.09828v1",
        "published_date": "2025-09-11T20:03:00+00:00",
        "updated_date": "2025-09-11T20:03:00+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Tim Broedermannn",
            "Christos Sakaridis",
            "Luigi Piccinelli",
            "Wim Abbeloos",
            "Luc Van Gool"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes a depth-guided sensor fusion method for robust semantic perception in autonomous vehicles, achieving state-of-the-art performance on challenging datasets.",
        "tldr_zh": "本文提出了一种深度引导的传感器融合方法，用于自动驾驶车辆的鲁棒语义感知，在具有挑战性的数据集上取得了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Realism Control One-step Diffusion for Real-World Image Super-Resolution",
        "summary": "Pre-trained diffusion models have shown great potential in real-world image\nsuper-resolution (Real-ISR) tasks by enabling high-resolution reconstructions.\nWhile one-step diffusion (OSD) methods significantly improve efficiency\ncompared to traditional multi-step approaches, they still have limitations in\nbalancing fidelity and realism across diverse scenarios. Since the OSDs for SR\nare usually trained or distilled by a single timestep, they lack flexible\ncontrol mechanisms to adaptively prioritize these competing objectives, which\nare inherently manageable in multi-step methods through adjusting sampling\nsteps. To address this challenge, we propose a Realism Controlled One-step\nDiffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping\nstrategy that enables explicit control over fidelity-realism trade-offs during\nthe noise prediction phase with minimal training paradigm modifications and\noriginal training data. A degradation-aware sampling strategy is also\nintroduced to align distillation regularization with the grouping strategy and\nenhance the controlling of trade-offs. Moreover, a visual prompt injection\nmodule is used to replace conventional text prompts with degradation-aware\nvisual tokens, enhancing both restoration accuracy and semantic consistency.\nOur method achieves superior fidelity and perceptual quality while maintaining\ncomputational efficiency. Extensive experiments demonstrate that RCOD\noutperforms state-of-the-art OSD methods in both quantitative metrics and\nvisual qualities, with flexible realism control capabilities in the inference\nstage. The code will be released.",
        "url": "http://arxiv.org/abs/2509.10122v1",
        "published_date": "2025-09-12T10:32:04+00:00",
        "updated_date": "2025-09-12T10:32:04+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zongliang Wu",
            "Siming Zheng",
            "Peng-Tao Jiang",
            "Xin Yuan"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN"
        ],
        "tldr": "The paper introduces a Realism Controlled One-step Diffusion (RCOD) framework for Real-ISR, which improves realism and fidelity in image super-resolution tasks while maintaining efficiency.",
        "tldr_zh": "该论文介绍了一种 Real-ISR 的 Realism Controlled One-step Diffusion（RCOD）框架，可以在保持效率的同时提高图像超分辨率任务中的逼真度和保真度。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "HHI-Assist: A Dataset and Benchmark of Human-Human Interaction in Physical Assistance Scenario",
        "summary": "The increasing labor shortage and aging population underline the need for\nassistive robots to support human care recipients. To enable safe and\nresponsive assistance, robots require accurate human motion prediction in\nphysical interaction scenarios. However, this remains a challenging task due to\nthe variability of assistive settings and the complexity of coupled dynamics in\nphysical interactions. In this work, we address these challenges through two\nkey contributions: (1) HHI-Assist, a dataset comprising motion capture clips of\nhuman-human interactions in assistive tasks; and (2) a conditional\nTransformer-based denoising diffusion model for predicting the poses of\ninteracting agents. Our model effectively captures the coupled dynamics between\ncaregivers and care receivers, demonstrating improvements over baselines and\nstrong generalization to unseen scenarios. By advancing interaction-aware\nmotion prediction and introducing a new dataset, our work has the potential to\nsignificantly enhance robotic assistance policies. The dataset and code are\navailable at: https://sites.google.com/view/hhi-assist/home",
        "url": "http://arxiv.org/abs/2509.10096v1",
        "published_date": "2025-09-12T09:38:17+00:00",
        "updated_date": "2025-09-12T09:38:17+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Saeed Saadatnejad",
            "Reyhaneh Hosseininejad",
            "Jose Barreiros",
            "Katherine M. Tsui",
            "Alexandre Alahi"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a new dataset and model for predicting human motion in physical assistance scenarios, aiming to enhance robotic assistance policies.",
        "tldr_zh": "该论文介绍了一个用于预测物理辅助场景中人体运动的新数据集和模型，旨在提高机器人辅助政策的效果。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "FLARE-SSM: Deep State Space Models with Influence-Balanced Loss for 72-Hour Solar Flare Prediction",
        "summary": "Accurate and reliable solar flare predictions are essential to mitigate\npotential impacts on critical infrastructure. However, the current performance\nof solar flare forecasting is insufficient. In this study, we address the task\nof predicting the class of the largest solar flare expected to occur within the\nnext 72 hours. Existing methods often fail to adequately address the severe\nclass imbalance across flare classes. To address this issue, we propose a solar\nflare prediction model based on multiple deep state space models. In addition,\nwe introduce the frequency & local-boundary-aware reliability loss (FLARE loss)\nto improve predictive performance and reliability under class imbalance.\nExperiments were conducted on a multi-wavelength solar image dataset covering a\nfull 11-year solar activity cycle. As a result, our method outperformed\nbaseline approaches in terms of both the Gandin-Murphy-Gerrity score and the\ntrue skill statistic, which are standard metrics in terms of the performance\nand reliability.",
        "url": "http://arxiv.org/abs/2509.09988v1",
        "published_date": "2025-09-12T06:09:09+00:00",
        "updated_date": "2025-09-12T06:09:09+00:00",
        "categories": [
            "cs.CV",
            "astro-ph.SR"
        ],
        "authors": [
            "Yusuke Takagi",
            "Shunya Nagashima",
            "Komei Sugiura"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces FLARE-SSM, a model for predicting solar flares with improved performance and reliability compared to existing methods.",
        "tldr_zh": "该论文介绍了一种用于预测太阳耀斑的模型FLARE-SSM，相较于现有方法有着更好的性能和可靠性。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Drone-Based Multispectral Imaging and Deep Learning for Timely Detection of Branched Broomrape in Tomato Farms",
        "summary": "This study addresses the escalating threat of branched broomrape (Phelipanche\nramosa) to California's tomato industry, which supplies over 90 percent of U.S.\nprocessing tomatoes. The parasite's largely underground life cycle makes early\ndetection difficult, while conventional chemical controls are costly,\nenvironmentally harmful, and often ineffective. To address this, we combined\ndrone-based multispectral imagery with Long Short-Term Memory (LSTM) deep\nlearning networks, using the Synthetic Minority Over-sampling Technique (SMOTE)\nto handle class imbalance. Research was conducted on a known broomrape-infested\ntomato farm in Woodland, Yolo County, CA, across five key growth stages\ndetermined by growing degree days (GDD). Multispectral images were processed to\nisolate tomato canopy reflectance. At 897 GDD, broomrape could be detected with\n79.09 percent overall accuracy and 70.36 percent recall without integrating\nlater stages. Incorporating sequential growth stages with LSTM improved\ndetection substantially. The best-performing scenario, which integrated all\ngrowth stages with SMOTE augmentation, achieved 88.37 percent overall accuracy\nand 95.37 percent recall. These results demonstrate the strong potential of\ntemporal multispectral analysis and LSTM networks for early broomrape\ndetection. While further real-world data collection is needed for practical\ndeployment, this study shows that UAV-based multispectral sensing coupled with\ndeep learning could provide a powerful precision agriculture tool to reduce\nlosses and improve sustainability in tomato production.",
        "url": "http://arxiv.org/abs/2509.09972v1",
        "published_date": "2025-09-12T05:16:56+00:00",
        "updated_date": "2025-09-12T05:16:56+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Mohammadreza Narimani",
            "Alireza Pourreza",
            "Ali Moghimi",
            "Mohsen Mesgaran",
            "Parastoo Farajpoor",
            "Hamid Jafarbiglu"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper presents a method using drone-based multispectral imaging and deep learning to detect branched broomrape in tomato farms, with promising results for early detection and control.",
        "tldr_zh": "本文提出了一种利用多光谱成像与深度学习技术来检测番茄农场中分枝菟丝子的方法，对早期检测和控制有着很好的效果。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation",
        "summary": "In this paper, we propose a training-free framework for vision-and-language\nnavigation (VLN). Existing zero-shot VLN methods are mainly designed for\ndiscrete environments or involve unsupervised training in continuous simulator\nenvironments, which makes it challenging to generalize and deploy them in\nreal-world scenarios. To achieve a training-free framework in continuous\nenvironments, our framework formulates navigation guidance as graph constraint\noptimization by decomposing instructions into explicit spatial constraints. The\nconstraint-driven paradigm decodes spatial semantics through constraint\nsolving, enabling zero-shot adaptation to unseen environments. Specifically, we\nconstruct a spatial constraint library covering all types of spatial\nrelationship mentioned in VLN instructions. The human instruction is decomposed\ninto a directed acyclic graph, with waypoint nodes, object nodes and edges,\nwhich are used as queries to retrieve the library to build the graph\nconstraints. The graph constraint optimization is solved by the constraint\nsolver to determine the positions of waypoints, obtaining the robot's\nnavigation path and final goal. To handle cases of no solution or multiple\nsolutions, we construct a navigation tree and the backtracking mechanism.\nExtensive experiments on standard benchmarks demonstrate significant\nimprovements in success rate and navigation efficiency compared to\nstate-of-the-art zero-shot VLN methods. We further conduct real-world\nexperiments to show that our framework can effectively generalize to new\nenvironments and instruction sets, paving the way for a more robust and\nautonomous navigation framework.",
        "url": "http://arxiv.org/abs/2509.10454v1",
        "published_date": "2025-09-12T17:59:58+00:00",
        "updated_date": "2025-09-12T17:59:58+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Hang Yin",
            "Haoyu Wei",
            "Xiuwei Xu",
            "Wenxuan Guo",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper proposes a training-free framework for vision-and-language navigation by formulating navigation guidance as graph constraints, leading to significant improvements in success rate and navigation efficiency compared to existing methods.",
        "tldr_zh": "本文提出了一种无需训练的视觉与语言导航框架，通过将导航指导表述为图约束，相比现有方法，成功率和导航效率显著提高。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery: Benchmarking, Analysis, and Exploration",
        "summary": "Mathematical reasoning is critical for tasks such as precise distance and\narea computations, trajectory estimations, and spatial analysis in unmanned\naerial vehicle (UAV) based remote sensing, yet current vision-language models\n(VLMs) have not been adequately tested in this domain. To address this gap, we\nintroduce AVI-Math, the first benchmark to rigorously evaluate multimodal\nmathematical reasoning in aerial vehicle imagery, moving beyond simple counting\ntasks to include domain-specific knowledge in areas such as geometry, logic,\nand algebra. The dataset comprises 3,773 high-quality vehicle-related questions\ncaptured from UAV views, covering 6 mathematical subjects and 20 topics. The\ndata, collected at varying altitudes and from multiple UAV angles, reflects\nreal-world UAV scenarios, ensuring the diversity and complexity of the\nconstructed mathematical problems. In this paper, we benchmark 14 prominent\nVLMs through a comprehensive evaluation and demonstrate that, despite their\nsuccess on previous multimodal benchmarks, these models struggle with the\nreasoning tasks in AVI-Math. Our detailed analysis highlights significant\nlimitations in the mathematical reasoning capabilities of current VLMs and\nsuggests avenues for future research. Furthermore, we explore the use of\nChain-of-Thought prompting and fine-tuning techniques, which show promise in\naddressing the reasoning challenges in AVI-Math. Our findings not only expose\nthe limitations of VLMs in mathematical reasoning but also offer valuable\ninsights for advancing UAV-based trustworthy VLMs in real-world applications.\nThe code, and datasets will be released at\nhttps://github.com/VisionXLab/avi-math",
        "url": "http://arxiv.org/abs/2509.10059v1",
        "published_date": "2025-09-12T08:46:49+00:00",
        "updated_date": "2025-09-12T08:46:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yue Zhou",
            "Litong Feng",
            "Mengcheng Lan",
            "Xue Yang",
            "Qingyun Li",
            "Yiping Ke",
            "Xue Jiang",
            "Wayne Zhang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces AVI-Math, a benchmark for evaluating multimodal mathematical reasoning in aerial vehicle imagery and suggests improvements for current vision-language models.",
        "tldr_zh": "本文介绍了AVI-Math，这是一个用于评估航空器图像中多模态数学推理的基准，并为当前的视觉语言模型提出了改进建议。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Few-Part-Shot Font Generation",
        "summary": "This paper proposes a novel model of few-part-shot font generation, which\ndesigns an entire font based on a set of partial design elements, i.e., partial\nshapes. Unlike conventional few-shot font generation, which requires entire\ncharacter shapes for a couple of character classes, our approach only needs\npartial shapes as input. The proposed model not only improves the efficiency of\nfont creation but also provides insights into how partial design details\ninfluence the entire structure of the individual characters.",
        "url": "http://arxiv.org/abs/2509.10006v1",
        "published_date": "2025-09-12T07:04:25+00:00",
        "updated_date": "2025-09-12T07:04:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Masaki Akiba",
            "Shumpei Takezaki",
            "Daichi Haraguchi",
            "Seiichi Uchida"
        ],
        "ai_categories": [
            "GAN",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a new model for generating fonts based on partial design elements, improving efficiency and providing insights into design details.",
        "tldr_zh": "本文提出了一种基于部分设计元素生成字体的新模型，提高了效率并为设计细节提供了见解。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Adaptive Token Merging for Efficient Transformer Semantic Communication at the Edge",
        "summary": "Large-scale transformers are central to modern semantic communication, yet\ntheir high computational and communication costs hinder deployment on\nresource-constrained edge devices. This paper introduces a training-free\nframework for adaptive token merging, a novel mechanism that compresses\ntransformer representations at runtime by selectively merging semantically\nredundant tokens under per-layer similarity thresholds. Unlike prior\nfixed-ratio reduction, our approach couples merging directly to input\nredundancy, enabling data-dependent adaptation that balances efficiency and\ntask relevance without retraining. We cast the discovery of merging strategies\nas a multi-objective optimization problem and leverage Bayesian optimization to\nobtain Pareto-optimal trade-offs between accuracy, inference cost, and\ncommunication cost. On ImageNet classification, we match the accuracy of the\nunmodified transformer with 30\\% fewer floating-point operations per second and\nunder 20\\% of the original communication cost, while for visual question\nanswering our method achieves performance competitive with the full LLaVA model\nat less than one-third of the compute and one-tenth of the bandwidth. Finally,\nwe show that our adaptive merging is robust across varying channel conditions\nand provides inherent privacy benefits, substantially degrading the efficacy of\nmodel inversion attacks. Our framework provides a practical and versatile\nsolution for deploying powerful transformer models in resource-limited edge\nintelligence scenarios.",
        "url": "http://arxiv.org/abs/2509.09955v1",
        "published_date": "2025-09-12T04:11:59+00:00",
        "updated_date": "2025-09-12T04:11:59+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Omar Erak",
            "Omar Alhussein",
            "Hatem Abou-Zeid",
            "Mehdi Bennis",
            "Sami Muhaidat"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion"
        ],
        "tldr": "The paper introduces a training-free framework for adaptive token merging in transformers to compress representations at runtime, achieving efficiency without retraining.",
        "tldr_zh": "本文介绍了一个用于自适应token合并的培训免费框架，可以在运行时压缩transformer的表示，实现效率而无需重新训练。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "SCoDA: Self-supervised Continual Domain Adaptation",
        "summary": "Source-Free Domain Adaptation (SFDA) addresses the challenge of adapting a\nmodel to a target domain without access to the data of the source domain.\nPrevailing methods typically start with a source model pre-trained with full\nsupervision and distill the knowledge by aligning instance-level features.\nHowever, these approaches, relying on cosine similarity over L2-normalized\nfeature vectors, inadvertently discard crucial geometric information about the\nlatent manifold of the source model. We introduce Self-supervised Continual\nDomain Adaptation (SCoDA) to address these limitations. We make two key\ndepartures from standard practice: first, we avoid the reliance on supervised\npre-training by initializing the proposed framework with a teacher model\npre-trained entirely via self-supervision (SSL). Second, we adapt the principle\nof geometric manifold alignment to the SFDA setting. The student is trained\nwith a composite objective combining instance-level feature matching with a\nSpace Similarity Loss. To combat catastrophic forgetting, the teacher's\nparameters are updated via an Exponential Moving Average (EMA) of the student's\nparameters. Extensive experiments on benchmark datasets demonstrate that SCoDA\nsignificantly outperforms state-of-the-art SFDA methods.",
        "url": "http://arxiv.org/abs/2509.09935v1",
        "published_date": "2025-09-12T02:53:03+00:00",
        "updated_date": "2025-09-12T02:53:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chirayu Agrawal",
            "Snehasis Mukherjee"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "GAN"
        ],
        "tldr": "SCoDA introduces a method for self-supervised continual domain adaptation that outperforms existing methods by aligning instance-level features with geometric manifold information.",
        "tldr_zh": "SCoDA 提出了一种自监督连续域适应方法，通过将实例级特征与几何流形信息进行对齐，优于现有方法。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Patch-based Automatic Rosacea Detection Using the ResNet Deep Learning Framework",
        "summary": "Rosacea, which is a chronic inflammatory skin condition that manifests with\nfacial redness, papules, and visible blood vessels, often requirs precise and\nearly detection for significantly improving treatment effectiveness. This paper\npresents new patch-based automatic rosacea detection strategies using the\nResNet-18 deep learning framework. The contributions of the proposed strategies\ncome from the following aspects. First, various image pateches are extracted\nfrom the facial images of people in different sizes, shapes, and locations.\nSecond, a number of investigation studies are carried out to evaluate how the\nlocalized visual information influences the deep learing model performance.\nThird, thorough experiments are implemented to reveal that several patch-based\nautomatic rosacea detection strategies achieve competitive or superior accuracy\nand sensitivity than the full-image based methods. And finally, the proposed\npatch-based strategies, which use only localized patches, inherently preserve\npatient privacy by excluding any identifiable facial features from the data.\nThe experimental results indicate that the proposed patch-based strategies\nguide the deep learning model to focus on clinically relevant regions, enhance\nrobustness and interpretability, and protect patient privacy. As a result, the\nproposed strategies offer practical insights for improving automated\ndermatological diagnostics.",
        "url": "http://arxiv.org/abs/2509.09841v1",
        "published_date": "2025-09-11T20:44:13+00:00",
        "updated_date": "2025-09-11T20:44:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chengyu Yang",
            "Rishik Reddy Yesgari",
            "Chengjun Liu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "This paper introduces patch-based automatic rosacea detection using a deep learning framework, showing competitive accuracy and sensitivity compared to full-image methods, while preserving patient privacy.",
        "tldr_zh": "本文介绍了使用深度学习框架进行基于图块的自动鲁疹检测，显示出与全图像方法相比具有竞争力的准确性和敏感性，同时保护患者隐私。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Early Detection of Visual Impairments at Home Using a Smartphone Red-Eye Reflex Test",
        "summary": "Numerous visual impairments can be detected in red-eye reflex images from\nyoung children. The so-called Bruckner test is traditionally performed by\nophthalmologists in clinical settings. Thanks to the recent technological\nadvances in smartphones and artificial intelligence, it is now possible to\nrecreate the Bruckner test using a mobile device. In this paper, we present a\nfirst study conducted during the development of KidsVisionCheck, a free\napplication that can perform vision screening with a mobile device using\nred-eye reflex images. The underlying model relies on deep neural networks\ntrained on children's pupil images collected and labeled by an ophthalmologist.\nWith an accuracy of 90% on unseen test data, our model provides highly reliable\nperformance without the necessity of specialist equipment. Furthermore, we can\nidentify the optimal conditions for data collection, which can in turn be used\nto provide immediate feedback to the users. In summary, this work marks a first\nstep toward accessible pediatric vision screenings and early intervention for\nvision abnormalities worldwide.",
        "url": "http://arxiv.org/abs/2509.09808v1",
        "published_date": "2025-09-11T19:27:57+00:00",
        "updated_date": "2025-09-11T19:27:57+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Judith Massmann",
            "Alexander Lichtenstein",
            "Francisco M. López"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "This paper presents KidsVisionCheck, a mobile application using AI to detect visual impairments in children through red-eye reflex images, achieving 90% accuracy on unseen data.",
        "tldr_zh": "本文介绍了KidsVisionCheck，一款利用人工智能在移动设备上检测儿童视觉障碍的应用程序，通过红眼反射图像，在未见数据上达到90%的准确率。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Fine-Grained Cross-View Localization via Local Feature Matching and Monocular Depth Priors",
        "summary": "We propose an accurate and highly interpretable fine-grained cross-view\nlocalization method that estimates the 3 Degrees of Freedom pose of a\nground-level image by matching its local features with a reference aerial\nimage. Previous methods typically transform the ground image into a bird's-eye\nview (BEV) representation and then align it with the aerial image for\nlocalization. However, this transformation often leads to information loss due\nto perspective distortion or compression of height information, thereby\ndegrading alignment quality with the aerial view. In contrast, our method\ndirectly establishes correspondences between ground and aerial images and lifts\nonly the matched keypoints to BEV space using monocular depth prior. Notably,\nmodern depth predictors can provide reliable metric depth when the test samples\nare similar to the training data. When the depth distribution differs, they\nstill produce consistent relative depth, i.e., depth accurate up to an unknown\nscale. Our method supports both metric and relative depth. It employs a\nscale-aware Procrustes alignment to estimate the camera pose from the\ncorrespondences and optionally recover the scale when using relative depth.\nExperimental results demonstrate that, with only weak supervision on camera\npose, our method learns accurate local feature correspondences and achieves\nsuperior localization performance under challenging conditions, such as\ncross-area generalization and unknown orientation. Moreover, our method is\ncompatible with various relative depth models without requiring per-model\nfinetuning. This flexibility, combined with strong localization performance,\nmakes it well-suited for real-world deployment.",
        "url": "http://arxiv.org/abs/2509.09792v1",
        "published_date": "2025-09-11T18:52:16+00:00",
        "updated_date": "2025-09-11T18:52:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zimin Xia",
            "Chenghao Xu",
            "Alexandre Alahi"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a method for accurate cross-view localization using local feature matching and monocular depth priors without transforming images, achieving superior performance under challenging conditions.",
        "tldr_zh": "本文提出了一种使用局部特征匹配和单眼深度先验进行准确的跨视图定位的方法，不需要转换图像，在挑战性条件下表现优秀。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and Adaptability Across Alzheimer's Prediction Tasks and Datasets",
        "summary": "Alzheimer's disease is a progressive, neurodegenerative disorder that causes\nmemory loss and cognitive decline. While there has been extensive research in\napplying deep learning models to Alzheimer's prediction tasks, these models\nremain limited by lack of available labeled data, poor generalization across\ndatasets, and inflexibility to varying numbers of input scans and time\nintervals between scans. In this study, we adapt three state-of-the-art\ntemporal self-supervised learning (SSL) approaches for 3D brain MRI analysis,\nand add novel extensions designed to handle variable-length inputs and learn\nrobust spatial features. We aggregate four publicly available datasets\ncomprising 3,161 patients for pre-training, and show the performance of our\nmodel across multiple Alzheimer's prediction tasks including diagnosis\nclassification, conversion detection, and future conversion prediction.\nImportantly, our SSL model implemented with temporal order prediction and\ncontrastive learning outperforms supervised learning on six out of seven\ndownstream tasks. It demonstrates adaptability and generalizability across\ntasks and number of input images with varying time intervals, highlighting its\ncapacity for robust performance across clinical applications. We release our\ncode and model publicly at https://github.com/emilykaczmarek/SSL-AD.",
        "url": "http://arxiv.org/abs/2509.10453v1",
        "published_date": "2025-09-12T17:59:32+00:00",
        "updated_date": "2025-09-12T17:59:32+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Emily Kaczmarek",
            "Justin Szeto",
            "Brennan Nichyporuk",
            "Tal Arbel"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes a Spatiotemporal Self-Supervised Learning model for Alzheimer's prediction tasks with good generalizability and adaptability.",
        "tldr_zh": "本文提出了一种面向阿尔茨海默病预测任务的时空自监督学习模型，具有良好的泛化性和适应性。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Multi-pathology Chest X-ray Classification with Rejection Mechanisms",
        "summary": "Overconfidence in deep learning models poses a significant risk in\nhigh-stakes medical imaging tasks, particularly in multi-label classification\nof chest X-rays, where multiple co-occurring pathologies must be detected\nsimultaneously. This study introduces an uncertainty-aware framework for chest\nX-ray diagnosis based on a DenseNet-121 backbone, enhanced with two selective\nprediction mechanisms: entropy-based rejection and confidence interval-based\nrejection. Both methods enable the model to abstain from uncertain predictions,\nimproving reliability by deferring ambiguous cases to clinical experts. A\nquantile-based calibration procedure is employed to tune rejection thresholds\nusing either global or class-specific strategies. Experiments conducted on\nthree large public datasets (PadChest, NIH ChestX-ray14, and MIMIC-CXR)\ndemonstrate that selective rejection improves the trade-off between diagnostic\naccuracy and coverage, with entropy-based rejection yielding the highest\naverage AUC across all pathologies. These results support the integration of\nselective prediction into AI-assisted diagnostic workflows, providing a\npractical step toward safer, uncertainty-aware deployment of deep learning in\nclinical settings.",
        "url": "http://arxiv.org/abs/2509.10348v1",
        "published_date": "2025-09-12T15:36:26+00:00",
        "updated_date": "2025-09-12T15:36:26+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yehudit Aperstein",
            "Amit Tzahar",
            "Alon Gottlib",
            "Tal Verber",
            "Ravit Shagan Damti",
            "Alexander Apartsin"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "This paper introduces an uncertainty-aware framework for chest X-ray diagnosis with rejection mechanisms to improve reliability of deep learning models in detecting multiple co-occurring pathologies.",
        "tldr_zh": "该论文引入了一种基于拒绝机制的不确定性感知框架，以提高深度学习模型在检测多个共同发生病症时的可靠性。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "On the Geometric Accuracy of Implicit and Primitive-based Representations Derived from View Rendering Constraints",
        "summary": "We present the first systematic comparison of implicit and explicit Novel\nView Synthesis methods for space-based 3D object reconstruction, evaluating the\nrole of appearance embeddings. While embeddings improve photometric fidelity by\nmodeling lighting variation, we show they do not translate into meaningful\ngains in geometric accuracy - a critical requirement for space robotics\napplications. Using the SPEED+ dataset, we compare K-Planes, Gaussian\nSplatting, and Convex Splatting, and demonstrate that embeddings primarily\nreduce the number of primitives needed for explicit methods rather than\nenhancing geometric fidelity. Moreover, convex splatting achieves more compact\nand clutter-free representations than Gaussian splatting, offering advantages\nfor safety-critical applications such as interaction and collision avoidance.\nOur findings clarify the limits of appearance embeddings for geometry-centric\ntasks and highlight trade-offs between reconstruction quality and\nrepresentation efficiency in space scenarios.",
        "url": "http://arxiv.org/abs/2509.10241v1",
        "published_date": "2025-09-12T13:37:18+00:00",
        "updated_date": "2025-09-12T13:37:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Elias De Smijter",
            "Renaud Detry",
            "Christophe De Vleeschouwer"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper compares implicit and explicit methods for 3D object reconstruction, showing that appearance embeddings do not significantly improve geometric accuracy but reduce the complexity of explicit methods. Convex splatting is more efficient than Gaussian splatting for space applications.",
        "tldr_zh": "本文比较了隐式和显式方法用于3D对象重建，结果表明外观嵌入并不显著提高几何精度，但降低了显式方法的复杂性。凸面片比高斯光照模型在空间应用中更有效率。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment",
        "summary": "Accurate segmentation of the optic disc and cup is critical for the early\ndiagnosis and management of ocular diseases such as glaucoma. However,\nsegmentation models trained on one dataset often suffer significant performance\ndegradation when applied to target data acquired under different imaging\nprotocols or conditions. To address this challenge, we propose\n\\textbf{Grad-CL}, a novel source-free domain adaptation framework that\nleverages a pre-trained source model and unlabeled target data to robustly\nadapt segmentation performance without requiring access to the original source\ndata. Grad-CL combines a gradient-guided pseudolabel refinement module with a\ncosine similarity-based contrastive learning strategy. In the first stage,\nsalient class-specific features are extracted via a gradient-based mechanism,\nenabling more accurate uncertainty quantification and robust prototype\nestimation for refining noisy pseudolabels. In the second stage, a contrastive\nloss based on cosine similarity is employed to explicitly enforce inter-class\nseparability between the gradient-informed features of the optic cup and disc.\nExtensive experiments on challenging cross-domain fundus imaging datasets\ndemonstrate that Grad-CL outperforms state-of-the-art unsupervised and\nsource-free domain adaptation methods, achieving superior segmentation accuracy\nand improved boundary delineation. Project and code are available at\nhttps://visdomlab.github.io/GCL/.",
        "url": "http://arxiv.org/abs/2509.10134v1",
        "published_date": "2025-09-12T10:51:46+00:00",
        "updated_date": "2025-09-12T10:51:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rini Smita Thakur",
            "Rajeev Ranjan Dwivedi",
            "Vinod K Kurmi"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces Grad-CL, a source-free domain adaptation framework for robust segmentation of the optic disc and cup in ocular images, outperforming existing methods.",
        "tldr_zh": "本文介绍了Grad-CL，这是一个用于在眼部图像中准确分割视盘和杯的无源领域自适应框架，胜过现有方法。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "A Lightweight Ensemble-Based Face Image Quality Assessment Method with Correlation-Aware Loss",
        "summary": "Face image quality assessment (FIQA) plays a critical role in face\nrecognition and verification systems, especially in uncontrolled, real-world\nenvironments. Although several methods have been proposed, general-purpose\nno-reference image quality assessment techniques often fail to capture\nface-specific degradations. Meanwhile, state-of-the-art FIQA models tend to be\ncomputationally intensive, limiting their practical applicability. We propose a\nlightweight and efficient method for FIQA, designed for the perceptual\nevaluation of face images in the wild. Our approach integrates an ensemble of\ntwo compact convolutional neural networks, MobileNetV3-Small and ShuffleNetV2,\nwith prediction-level fusion via simple averaging. To enhance alignment with\nhuman perceptual judgments, we employ a correlation-aware loss (MSECorrLoss),\ncombining mean squared error (MSE) with a Pearson correlation regularizer. Our\nmethod achieves a strong balance between accuracy and computational cost,\nmaking it suitable for real-world deployment. Experiments on the VQualA FIQA\nbenchmark demonstrate that our model achieves a Spearman rank correlation\ncoefficient (SRCC) of 0.9829 and a Pearson linear correlation coefficient\n(PLCC) of 0.9894, remaining within competition efficiency constraints.",
        "url": "http://arxiv.org/abs/2509.10114v1",
        "published_date": "2025-09-12T10:13:38+00:00",
        "updated_date": "2025-09-12T10:13:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "MohammadAli Hamidi",
            "Hadi Amirpour",
            "Luigi Atzori",
            "Christian Timmerer"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper proposes a lightweight and efficient method for face image quality assessment using an ensemble of two compact convolutional neural networks with correlation-aware loss, achieving high accuracy with low computational cost.",
        "tldr_zh": "本文提出了一种轻量且高效的人脸图像质量评估方法，利用两个紧凑的卷积神经网络集成和相关性感知损失，以较低的计算成本获得高准确度。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird's-Eye View with Deformable Attention and Sparse Goal Proposals",
        "summary": "In autonomous driving, trajectory prediction is essential for ensuring safe\nand efficient navigation. To improve prediction accuracy, recent approaches\noften rely on pre-built high-definition (HD) maps or real-time local map\nconstruction modules to incorporate static environmental information. However,\npre-built HD maps are limited to specific regions and cannot adapt to transient\nchanges. In addition, local map construction modules, which recognize only\npredefined elements, may fail to capture critical scene details or introduce\nerrors that degrade prediction performance. To overcome these limitations, we\npropose Bird's-Eye View Trajectory Prediction (BEVTraj), a novel trajectory\nprediction framework that operates directly in the bird's-eye view (BEV) space\nutilizing real-time sensor data without relying on any pre-built maps. The\nBEVTraj leverages deformable attention to efficiently extract relevant context\nfrom dense BEV features. Furthermore, we introduce a Sparse Goal Candidate\nProposal (SGCP) module, which enables full end-to-end prediction without\nrequiring any post-processing steps. Extensive experiments demonstrate that the\nBEVTraj achieves performance comparable to state-of-the-art HD map-based models\nwhile offering greater flexibility by eliminating the dependency on pre-built\nmaps. The source code is available at https://github.com/Kongminsang/bevtraj.",
        "url": "http://arxiv.org/abs/2509.10080v1",
        "published_date": "2025-09-12T09:17:54+00:00",
        "updated_date": "2025-09-12T09:17:54+00:00",
        "categories": [
            "cs.CV",
            "I.2.9; I.4.8"
        ],
        "authors": [
            "Minsang Kong",
            "Myeongjun Kim",
            "Sang Gu Kang",
            "Sang Hun Lee"
        ],
        "ai_categories": [
            "Transformer",
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel trajectory prediction framework for autonomous driving in bird's-eye view without relying on pre-built maps, achieving performance comparable to state-of-the-art models.",
        "tldr_zh": "该论文提出了一种新颖的在鸟瞰视图中进行自动驾驶轨迹预测的框架，无需依赖预先构建的地图，性能可与现有先进模型相媲美。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Zero-Shot Referring Expression Comprehension via Visual-Language True/False Verification",
        "summary": "Referring Expression Comprehension (REC) is usually addressed with\ntask-trained grounding models. We show that a zero-shot workflow, without any\nREC-specific training, can achieve competitive or superior performance. Our\napproach reformulates REC as box-wise visual-language verification: given\nproposals from a COCO-clean generic detector (YOLO-World), a general-purpose\nVLM independently answers True/False queries for each region. This simple\nprocedure reduces cross-box interference, supports abstention and multiple\nmatches, and requires no fine-tuning. On RefCOCO, RefCOCO+, and RefCOCOg, our\nmethod not only surpasses a zero-shot GroundingDINO baseline but also exceeds\nreported results for GroundingDINO trained on REC and GroundingDINO+CRG.\nControlled studies with identical proposals confirm that verification\nsignificantly outperforms selection-based prompting, and results hold with open\nVLMs. Overall, we show that workflow design, rather than task-specific\npretraining, drives strong zero-shot REC performance.",
        "url": "http://arxiv.org/abs/2509.09958v1",
        "published_date": "2025-09-12T04:32:52+00:00",
        "updated_date": "2025-09-12T04:32:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jeffrey Liu",
            "Rongbin Hu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a zero-shot workflow for referring expression comprehension without specific training, achieving competitive results by reformulating the task as visual-language verification.",
        "tldr_zh": "该论文介绍了一种零-shot工作流，用于指代表达理解，无需特定训练，通过将任务重新构造为视觉-语言验证来实现竞争性结果。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Segment Anything for Cell Tracking",
        "summary": "Tracking cells and detecting mitotic events in time-lapse microscopy image\nsequences is a crucial task in biomedical research. However, it remains highly\nchallenging due to dividing objects, low signal-tonoise ratios, indistinct\nboundaries, dense clusters, and the visually similar appearance of individual\ncells. Existing deep learning-based methods rely on manually labeled datasets\nfor training, which is both costly and time-consuming. Moreover, their\ngeneralizability to unseen datasets remains limited due to the vast diversity\nof microscopy data. To overcome these limitations, we propose a zero-shot cell\ntracking framework by integrating Segment Anything 2 (SAM2), a large foundation\nmodel designed for general image and video segmentation, into the tracking\npipeline. As a fully-unsupervised approach, our method does not depend on or\ninherit biases from any specific training dataset, allowing it to generalize\nacross diverse microscopy datasets without finetuning. Our approach achieves\ncompetitive accuracy in both 2D and large-scale 3D time-lapse microscopy videos\nwhile eliminating the need for dataset-specific adaptation.",
        "url": "http://arxiv.org/abs/2509.09943v1",
        "published_date": "2025-09-12T03:19:35+00:00",
        "updated_date": "2025-09-12T03:19:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhu Chen",
            "Mert Edgü",
            "Er Jin",
            "Johannes Stegmaier"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a zero-shot cell tracking framework using a large foundation model for general image segmentation, achieving competitive accuracy without the need for dataset-specific adaptation.",
        "tldr_zh": "该论文介绍了一种零样本细胞跟踪框架，利用大型基础模型进行图像分割，实现竞争性准确性，无需特定数据集适应。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios",
        "summary": "Long-tailed learning has garnered increasing attention due to its wide\napplicability in real-world scenarios. Among existing approaches, Long-Tailed\nSemi-Supervised Learning (LTSSL) has emerged as an effective solution by\nincorporating a large amount of unlabeled data into the imbalanced labeled\ndataset. However, most prior LTSSL methods are designed to train models from\nscratch, which often leads to issues such as overconfidence and low-quality\npseudo-labels. To address these challenges, we extend LTSSL into the foundation\nmodel fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed\nsemi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate\nthat fine-tuned foundation models can generate more reliable pseudolabels,\nthereby benefiting imbalanced learning. Furthermore, we explore a more\npractical setting by investigating semi-supervised learning under open-world\nconditions, where the unlabeled data may include out-of-distribution (OOD)\nsamples. To handle this problem, we propose LoFT-OW (LoFT under Open-World\nscenarios) to improve the discriminative ability. Experimental results on\nmultiple benchmarks demonstrate that our method achieves superior performance\ncompared to previous approaches, even when utilizing only 1\\% of the unlabeled\ndata compared with previous works.",
        "url": "http://arxiv.org/abs/2509.09926v1",
        "published_date": "2025-09-12T02:28:32+00:00",
        "updated_date": "2025-09-12T02:28:32+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Jiahao Chen",
            "Zhiyuan Huang",
            "Yurou Liu",
            "Bing Su"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces LoFT, a parameter-efficient fine-tuning framework for long-tailed semi-supervised learning in open-world scenarios, achieving superior performance with minimal unlabeled data.",
        "tldr_zh": "该论文介绍了LoFT，一种用于长尾半监督学习在开放世界场景中的参数有效微调框架，以极少量未标记数据实现了出色性能。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Surrogate Supervision for Robust and Generalizable Deformable Image Registration",
        "summary": "Objective: Deep learning-based deformable image registration has achieved\nstrong accuracy, but remains sensitive to variations in input image\ncharacteristics such as artifacts, field-of-view mismatch, or modality\ndifference. We aim to develop a general training paradigm that improves the\nrobustness and generalizability of registration networks. Methods: We introduce\nsurrogate supervision, which decouples the input domain from the supervision\ndomain by applying estimated spatial transformations to surrogate images. This\nallows training on heterogeneous inputs while ensuring supervision is computed\nin domains where similarity is well defined. We evaluate the framework through\nthree representative applications: artifact-robust brain MR registration,\nmask-agnostic lung CT registration, and multi-modal MR registration. Results:\nAcross tasks, surrogate supervision demonstrated strong resilience to input\nvariations including inhomogeneity field, inconsistent field-of-view, and\nmodality differences, while maintaining high performance on well-curated data.\nConclusions: Surrogate supervision provides a principled framework for training\nrobust and generalizable deep learning-based registration models without\nincreasing complexity. Significance: Surrogate supervision offers a practical\npathway to more robust and generalizable medical image registration, enabling\nbroader applicability in diverse biomedical imaging scenarios.",
        "url": "http://arxiv.org/abs/2509.09869v1",
        "published_date": "2025-09-11T21:43:45+00:00",
        "updated_date": "2025-09-11T21:43:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yihao Liu",
            "Junyu Chen",
            "Lianrui Zuo",
            "Shuwen Wei",
            "Brian D. Boyd",
            "Carmen Andreescu",
            "Olusola Ajilore",
            "Warren D. Taylor",
            "Aaron Carass",
            "Bennett A. Landman"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces surrogate supervision for training deep learning-based registration models, improving robustness and generalizability in medical image registration tasks.",
        "tldr_zh": "该论文引入了替代监督来训练基于深度学习的注册模型，在医学图像注册任务中改善了鲁棒性和泛化能力。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Privacy-Preserving Automated Rosacea Detection Based on Medically Inspired Region of Interest Selection",
        "summary": "Rosacea is a common but underdiagnosed inflammatory skin condition that\nprimarily affects the central face and presents with subtle redness, pustules,\nand visible blood vessels. Automated detection remains challenging due to the\ndiffuse nature of symptoms, the scarcity of labeled datasets, and privacy\nconcerns associated with using identifiable facial images. A novel\nprivacy-preserving automated rosacea detection method inspired by clinical\npriors and trained entirely on synthetic data is presented in this paper.\nSpecifically, the proposed method, which leverages the observation that rosacea\nmanifests predominantly through central facial erythema, first constructs a\nfixed redness-informed mask by selecting regions with consistently high red\nchannel intensity across facial images. The mask thus is able to focus on\ndiagnostically relevant areas such as the cheeks, nose, and forehead and\nexclude identity-revealing features. Second, the ResNet-18 deep learning\nmethod, which is trained on the masked synthetic images, achieves superior\nperformance over the full-face baselines with notable gains in terms of\naccuracy, recall and F1 score when evaluated using the real-world test data.\nThe experimental results demonstrate that the synthetic data and clinical\npriors can jointly enable accurate and ethical dermatological AI systems,\nespecially for privacy sensitive applications in telemedicine and large-scale\nscreening.",
        "url": "http://arxiv.org/abs/2509.09844v1",
        "published_date": "2025-09-11T20:54:26+00:00",
        "updated_date": "2025-09-11T20:54:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chengyu Yang",
            "Rishik Reddy Yesgari",
            "Chengjun Liu"
        ],
        "ai_categories": [
            "LoRA",
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper presents a privacy-preserving automated rosacea detection method using synthetic data and clinical priors to focus on diagnostically relevant areas, achieving superior performance compared to full-face baselines.",
        "tldr_zh": "该论文提出了一种隐私保护的自动鲜红痤疮检测方法，利用合成数据和临床先验知识聚焦于诊断相关区域，实现了较全脸基准更优越的性能。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Purge-Gate: Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token Purging",
        "summary": "Test-time adaptation (TTA) is crucial for mitigating performance degradation\ncaused by distribution shifts in 3D point cloud classification. In this work,\nwe introduce Token Purging (PG), a novel backpropagation-free approach that\nremoves tokens highly affected by domain shifts before they reach attention\nlayers. Unlike existing TTA methods, PG operates at the token level, ensuring\nrobust adaptation without iterative updates. We propose two variants: PG-SP,\nwhich leverages source statistics, and PG-SF, a fully source-free version\nrelying on CLS-token-driven adaptation. Extensive evaluations on ModelNet40-C,\nShapeNet-C, and ScanObjectNN-C demonstrate that PG-SP achieves an average of\n+10.3\\% higher accuracy than state-of-the-art backpropagation-free methods,\nwhile PG-SF sets new benchmarks for source-free adaptation. Moreover, PG is\n12.4 times faster and 5.5 times more memory efficient than our baseline, making\nit suitable for real-world deployment. Code is available at\n\\hyperlink{https://github.com/MosyMosy/Purge-Gate}{https://github.com/MosyMosy/Purge-Gate}",
        "url": "http://arxiv.org/abs/2509.09785v1",
        "published_date": "2025-09-11T18:33:40+00:00",
        "updated_date": "2025-09-11T18:33:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Moslem Yazdanpanah",
            "Ali Bahri",
            "Mehrdad Noori",
            "Sahar Dastani",
            "Gustavo Adolfo Vargas Hakim",
            "David Osowiechi",
            "Ismail Ben Ayed",
            "Christian Desrosiers"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "Introduces Token Purging (PG), a backpropagation-free approach for test-time adaptation in 3D point cloud classification, achieving high accuracy and efficiency.",
        "tldr_zh": "引入了Token Purging（PG），一种用于3D点云分类的无反向传播的测试时适应方法，实现了高精度和高效率。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "A Stochastic Birth-and-Death Approach for Street Furniture Geolocation in Urban Environments",
        "summary": "In this paper we address the problem of precise geolocation of street\nfurniture in complex urban environments, which is a critical task for effective\nmonitoring and maintenance of public infrastructure by local authorities and\nprivate stakeholders. To this end, we propose a probabilistic framework based\non energy maps that encode the spatial likelihood of object locations.\nRepresenting the energy in a map-based geopositioned format allows the\noptimisation process to seamlessly integrate external geospatial information,\nsuch as GIS layers, road maps, or placement constraints, which improves\ncontextual awareness and localisation accuracy. A stochastic birth-and-death\noptimisation algorithm is introduced to infer the most probable configuration\nof assets. We evaluate our approach using a realistic simulation informed by a\ngeolocated dataset of street lighting infrastructure in Dublin city centre,\ndemonstrating its potential for scalable and accurate urban asset mapping. The\nimplementation of the algorithm will be made available in the GitHub repository\nhttps://github.com/EMurphy0108/SBD_Street_Furniture.",
        "url": "http://arxiv.org/abs/2509.10310v1",
        "published_date": "2025-09-12T14:52:42+00:00",
        "updated_date": "2025-09-12T14:52:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Evan Murphy",
            "Marco Viola",
            "Vladimir A. Krylov"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper proposes a probabilistic framework for geolocating street furniture in urban environments using energy maps. A stochastic birth-and-death algorithm is introduced for asset configuration inference.",
        "tldr_zh": "本文提出了一种基于能量地图的概率框架，用于在城市环境中定位街道家具。引入了随机诞生和死亡算法用于资产配置推断。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Online 3D Multi-Camera Perception through Robust 2D Tracking and Depth-based Late Aggregation",
        "summary": "Multi-Target Multi-Camera Tracking (MTMC) is an essential computer vision\ntask for automating large-scale surveillance. With camera calibration and depth\ninformation, the targets in the scene can be projected into 3D space, offering\nunparalleled levels of automatic perception of a 3D environment. However,\ntracking in the 3D space requires replacing all 2D tracking components from the\nground up, which may be infeasible for existing MTMC systems. In this paper, we\npresent an approach for extending any online 2D multi-camera tracking system\ninto 3D space by utilizing depth information to reconstruct a target in\npoint-cloud space, and recovering its 3D box through clustering and yaw\nrefinement following tracking. We also introduced an enhanced online data\nassociation mechanism that leverages the target's local ID consistency to\nassign global IDs across frames. The proposed framework is evaluated on the\n2025 AI City Challenge's 3D MTMC dataset, achieving 3rd place on the\nleaderboard.",
        "url": "http://arxiv.org/abs/2509.09946v1",
        "published_date": "2025-09-12T03:28:35+00:00",
        "updated_date": "2025-09-12T03:28:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Vu-Minh Le",
            "Thao-Anh Tran",
            "Duc Huy Do",
            "Xuan Canh Do",
            "Huong Ninh",
            "Hai Tran"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a method for extending 2D multi-camera tracking systems to 3D space using depth information, achieving 3rd place in the 2025 AI City Challenge.",
        "tldr_zh": "该论文介绍了一种利用深度信息将2D多摄像机跟踪系统扩展到3D空间的方法，在2025年AI城市挑战中取得了第三名。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Robustness and Diagnostic Performance of Super-Resolution Fetal Brain MRI",
        "summary": "Fetal brain MRI relies on rapid multi-view 2D slice acquisitions to reduce\nmotion artifacts caused by fetal movement. However, these stacks are typically\nlow resolution, may suffer from motion corruption, and do not adequately\ncapture 3D anatomy. Super-resolution reconstruction (SRR) methods aim to\naddress these limitations by combining slice-to-volume registration and\nsuper-resolution techniques to generate high-resolution (HR) 3D volumes. While\nseveral SRR methods have been proposed, their comparative performance -\nparticularly in pathological cases - and their influence on downstream\nvolumetric analysis and diagnostic tasks remain underexplored. In this study,\nwe applied three state-of-the-art SRR method - NiftyMIC, SVRTK, and NeSVoR - to\n140 fetal brain MRI scans, including both healthy controls (HC) and\npathological cases (PC) with ventriculomegaly (VM). Each HR reconstruction was\nsegmented using the BoUNTi algorithm to extract volumes of nine principal brain\nstructures. We evaluated visual quality, SRR success rates, volumetric\nmeasurement agreement, and diagnostic classification performance. NeSVoR\ndemonstrated the highest and most consistent reconstruction success rate (>90%)\nacross both HC and PC groups. Although significant differences in volumetric\nestimates were observed between SRR methods, classification performance for VM\nwas not affected by the choice of SRR method. These findings highlight NeSVoR's\nrobustness and the resilience of diagnostic performance despite SRR-induced\nvolumetric variability.",
        "url": "http://arxiv.org/abs/2509.10257v1",
        "published_date": "2025-09-12T13:59:23+00:00",
        "updated_date": "2025-09-12T13:59:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ema Masterl",
            "Tina Vipotnik Vesnaver",
            "Žiga Špiclin"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper evaluates the robustness and diagnostic performance of super-resolution fetal brain MRI methods, particularly in pathological cases, using three different techniques.",
        "tldr_zh": "本文评估了超分辨率胎儿脑MRI方法在病理情况下的鲁棒性和诊断性能，并使用三种不同技术进行了研究。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.25
    },
    {
        "title": "I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic Segmentation",
        "summary": "Vision Transformers (ViTs) have recently achieved strong results in semantic\nsegmentation, yet their deployment on resource-constrained devices remains\nlimited due to their high memory footprint and computational cost. Quantization\noffers an effective strategy to improve efficiency, but ViT-based segmentation\nmodels are notoriously fragile under low precision, as quantization errors\naccumulate across deep encoder-decoder pipelines. We introduce I-Segmenter, the\nfirst fully integer-only ViT segmentation framework. Building on the Segmenter\narchitecture, I-Segmenter systematically replaces floating-point operations\nwith integer-only counterparts. To further stabilize both training and\ninference, we propose $\\lambda$-ShiftGELU, a novel activation function that\nmitigates the limitations of uniform quantization in handling long-tailed\nactivation distributions. In addition, we remove the L2 normalization layer and\nreplace bilinear interpolation in the decoder with nearest neighbor upsampling,\nensuring integer-only execution throughout the computational graph. Extensive\nexperiments show that I-Segmenter achieves accuracy within a reasonable margin\nof its FP32 baseline (5.1 % on average), while reducing model size by up to\n3.8x and enabling up to 1.2x faster inference with optimized runtimes. Notably,\neven in one-shot PTQ with a single calibration image, I-Segmenter delivers\ncompetitive accuracy, underscoring its practicality for real-world deployment.",
        "url": "http://arxiv.org/abs/2509.10334v1",
        "published_date": "2025-09-12T15:14:19+00:00",
        "updated_date": "2025-09-12T15:14:19+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Jordan Sassoon",
            "Michal Szczepanski",
            "Martyna Poreba"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "I-Segmenter is an integer-only Vision Transformer for efficient semantic segmentation with promising results in model size reduction and faster inference.",
        "tldr_zh": "I-Segmenter 是一种仅使用整数的视觉Transformer，用于高效语义分割，在减小模型大小和提高推断速度方面表现出色。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "Detecting Text Manipulation in Images using Vision Language Models",
        "summary": "Recent works have shown the effectiveness of Large Vision Language Models\n(VLMs or LVLMs) in image manipulation detection. However, text manipulation\ndetection is largely missing in these studies. We bridge this knowledge gap by\nanalyzing closed- and open-source VLMs on different text manipulation datasets.\nOur results suggest that open-source models are getting closer, but still\nbehind closed-source ones like GPT- 4o. Additionally, we benchmark image\nmanipulation detection-specific VLMs for text manipulation detection and show\nthat they suffer from the generalization problem. We benchmark VLMs for\nmanipulations done on in-the-wild scene texts and on fantasy ID cards, where\nthe latter mimic a challenging real-world misuse.",
        "url": "http://arxiv.org/abs/2509.10278v1",
        "published_date": "2025-09-12T14:20:29+00:00",
        "updated_date": "2025-09-12T14:20:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Vidit Vidit",
            "Pavel Korshunov",
            "Amir Mohammadi",
            "Christophe Ecabert",
            "Ketan Kotwal",
            "Sébastien Marcel"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper explores text manipulation detection using vision language models, comparing open-source and closed-source models and highlighting a generalization issue in specific VLMs.",
        "tldr_zh": "本文探讨了使用视觉语言模型进行文本操作检测，比较了开源和闭源模型，并突出了特定VLM中的概括问题。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "Polarization Denoising and Demosaicking: Dataset and Baseline Method",
        "summary": "A division-of-focal-plane (DoFP) polarimeter enables us to acquire images\nwith multiple polarization orientations in one shot and thus it is valuable for\nmany applications using polarimetric information. The image processing pipeline\nfor a DoFP polarimeter entails two crucial tasks: denoising and demosaicking.\nWhile polarization demosaicking for a noise-free case has increasingly been\nstudied, the research for the joint task of polarization denoising and\ndemosaicking is scarce due to the lack of a suitable evaluation dataset and a\nsolid baseline method. In this paper, we propose a novel dataset and method for\npolarization denoising and demosaicking. Our dataset contains 40 real-world\nscenes and three noise-level conditions, consisting of pairs of noisy mosaic\ninputs and noise-free full images. Our method takes a\ndenoising-then-demosaicking approach based on well-accepted signal processing\ncomponents to offer a reproducible method. Experimental results demonstrate\nthat our method exhibits higher image reconstruction performance than other\nalternative methods, offering a solid baseline.",
        "url": "http://arxiv.org/abs/2509.10098v1",
        "published_date": "2025-09-12T09:40:42+00:00",
        "updated_date": "2025-09-12T09:40:42+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Muhamad Daniel Ariff Bin Abdul Rahman",
            "Yusuke Monno",
            "Masayuki Tanaka",
            "Masatoshi Okutomi"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a dataset and method for polarization denoising and demosaicking using a division-of-focal-plane (DoFP) polarimeter, demonstrating higher image reconstruction performance than other methods.",
        "tldr_zh": "本文介绍了使用分焦面极化计量仪（DoFP）进行极化去噪和去马赛克的数据集和方法，展示了比其他方法更高的图像重建性能。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "Efficient and Accurate Downfacing Visual Inertial Odometry",
        "summary": "Visual Inertial Odometry (VIO) is a widely used computer vision method that\ndetermines an agent's movement through a camera and an IMU sensor. This paper\npresents an efficient and accurate VIO pipeline optimized for applications on\nmicro- and nano-UAVs. The proposed design incorporates state-of-the-art feature\ndetection and tracking methods (SuperPoint, PX4FLOW, ORB), all optimized and\nquantized for emerging RISC-V-based ultra-low-power parallel systems on chips\n(SoCs). Furthermore, by employing a rigid body motion model, the pipeline\nreduces estimation errors and achieves improved accuracy in planar motion\nscenarios. The pipeline's suitability for real-time VIO is assessed on an\nultra-low-power SoC in terms of compute requirements and tracking accuracy\nafter quantization. The pipeline, including the three feature tracking methods,\nwas implemented on the SoC for real-world validation. This design bridges the\ngap between high-accuracy VIO pipelines that are traditionally run on\ncomputationally powerful systems and lightweight implementations suitable for\nmicrocontrollers. The optimized pipeline on the GAP9 low-power SoC demonstrates\nan average reduction in RMSE of up to a factor of 3.65x over the baseline\npipeline when using the ORB feature tracker. The analysis of the computational\ncomplexity of the feature trackers further shows that PX4FLOW achieves on-par\ntracking accuracy with ORB at a lower runtime for movement speeds below 24\npixels/frame.",
        "url": "http://arxiv.org/abs/2509.10021v1",
        "published_date": "2025-09-12T07:30:24+00:00",
        "updated_date": "2025-09-12T07:30:24+00:00",
        "categories": [
            "cs.CV",
            "cs.RO",
            "eess.IV"
        ],
        "authors": [
            "Jonas Kühne",
            "Christian Vogt",
            "Michele Magno",
            "Luca Benini"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "This paper introduces an efficient and accurate Visual Inertial Odometry pipeline optimized for micro- and nano-UAV applications, achieving improved accuracy in planar motion scenarios.",
        "tldr_zh": "本文介绍了一种针对微型和纳米无人机应用优化的高效准确的视觉惯性测距管线，提高了在平面运动场景中的准确性。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "An HMM-based framework for identity-aware long-term multi-object tracking from sparse and uncertain identification: use case on long-term tracking in livestock",
        "summary": "The need for long-term multi-object tracking (MOT) is growing due to the\ndemand for analyzing individual behaviors in videos that span several minutes.\nUnfortunately, due to identity switches between objects, the tracking\nperformance of existing MOT approaches decreases over time, making them\ndifficult to apply for long-term tracking. However, in many real-world\napplications, such as in the livestock sector, it is possible to obtain\nsporadic identifications for some of the animals from sources like feeders. To\naddress the challenges of long-term MOT, we propose a new framework that\ncombines both uncertain identities and tracking using a Hidden Markov Model\n(HMM) formulation. In addition to providing real-world identities to animals,\nour HMM framework improves the F1 score of ByteTrack, a leading MOT approach\neven with re-identification, on a 10 minute pig tracking dataset with 21\nidentifications at the pen's feeding station. We also show that our approach is\nrobust to the uncertainty of identifications, with performance increasing as\nidentities are provided more frequently. The improved performance of our HMM\nframework was also validated on the MOT17 and MOT20 benchmark datasets using\nboth ByteTrack and FairMOT. The code for this new HMM framework and the new\n10-minute pig tracking video dataset are available at:\nhttps://github.com/ngobibibnbe/uncertain-identity-aware-tracking",
        "url": "http://arxiv.org/abs/2509.09962v1",
        "published_date": "2025-09-12T04:39:38+00:00",
        "updated_date": "2025-09-12T04:39:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Anne Marthe Sophie Ngo Bibinbe",
            "Chiron Bang",
            "Patrick Gagnon",
            "Jamie Ahloy-Dallaire",
            "Eric R. Paquet"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC",
            "Transformer"
        ],
        "tldr": "The paper proposes a new framework using Hidden Markov Models for long-term multi-object tracking with uncertain identities, showing improved performance in livestock tracking.",
        "tldr_zh": "该论文提出了一个使用隐马尔可夫模型进行长期多目标追踪的新框架，展示了在牲畜追踪中提高性能的结果。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "Investigating the Impact of Various Loss Functions and Learnable Wiener Filter for Laparoscopic Image Desmoking",
        "summary": "To rigorously assess the effectiveness and necessity of individual components\nwithin the recently proposed ULW framework for laparoscopic image desmoking,\nthis paper presents a comprehensive ablation study. The ULW approach combines a\nU-Net based backbone with a compound loss function that comprises mean squared\nerror (MSE), structural similarity index (SSIM) loss, and perceptual loss. The\nframework also incorporates a differentiable, learnable Wiener filter module.\nIn this study, each component is systematically ablated to evaluate its\nspecific contribution to the overall performance of the whole framework. The\nanalysis includes: (1) removal of the learnable Wiener filter, (2) selective\nuse of individual loss terms from the composite loss function. All variants are\nbenchmarked on a publicly available paired laparoscopic images dataset using\nquantitative metrics (SSIM, PSNR, MSE and CIEDE-2000) alongside qualitative\nvisual comparisons.",
        "url": "http://arxiv.org/abs/2509.09849v1",
        "published_date": "2025-09-11T20:58:52+00:00",
        "updated_date": "2025-09-11T20:58:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chengyu Yang",
            "Chengjun Liu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper presents an ablation study on a framework for laparoscopic image desmoking, assessing the impact of different components like loss functions and Wiener filter.",
        "tldr_zh": "该论文对腹腔镜图像去烟雾的框架进行消融研究，评估了不同组件如损失函数和维纳滤波器的影响。",
        "relevance_score": 5,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]