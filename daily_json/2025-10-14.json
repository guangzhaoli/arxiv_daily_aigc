[
    {
        "title": "ContextGen: Contextual Layout Anchoring for Identity-Consistent Multi-Instance Generation",
        "summary": "Multi-instance image generation (MIG) remains a significant challenge for\nmodern diffusion models due to key limitations in achieving precise control\nover object layout and preserving the identity of multiple distinct subjects.\nTo address these limitations, we introduce ContextGen, a novel Diffusion\nTransformer framework for multi-instance generation that is guided by both\nlayout and reference images. Our approach integrates two key technical\ncontributions: a Contextual Layout Anchoring (CLA) mechanism that incorporates\nthe composite layout image into the generation context to robustly anchor the\nobjects in their desired positions, and Identity Consistency Attention (ICA),\nan innovative attention mechanism that leverages contextual reference images to\nensure the identity consistency of multiple instances. Recognizing the lack of\nlarge-scale, hierarchically-structured datasets for this task, we introduce\nIMIG-100K, the first dataset with detailed layout and identity annotations.\nExtensive experiments demonstrate that ContextGen sets a new state-of-the-art,\noutperforming existing methods in control precision, identity fidelity, and\noverall visual quality.",
        "url": "http://arxiv.org/abs/2510.11000v1",
        "published_date": "2025-10-13T04:21:19+00:00",
        "updated_date": "2025-10-13T04:21:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruihang Xu",
            "Dewei Zhou",
            "Fan Ma",
            "Yi Yang"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "LoRA",
            "Diffusion",
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "ContextGen introduces a novel Diffusion Transformer framework for multi-instance image generation, addressing challenges in layout control and identity preservation. It outperforms existing methods in control precision and identity fidelity.",
        "tldr_zh": "ContextGen引入了一种新颖的扩散变换框架，用于多实例图像生成，解决了布局控制和身份保留方面的挑战。它在控制精度和身份保真度方面优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation",
        "summary": "Existing vision language models (VLMs), including GPT-4 and DALL-E, often\nstruggle to preserve logic, object identity, and style in multimodal image-text\ngeneration. This limitation significantly hinders the generalization capability\nof VLMs in complex image-text input-output scenarios. To address this issue, we\npropose IUT-Plug, a module grounded in an Image Understanding Tree (IUT), which\nenhances existing interleaved VLMs through explicit structured reasoning,\nthereby mitigating context drift in logic, entity identity, and style. The\nproposed framework operates in two stages. (1) A dynamic IUT-Plug extraction\nmodule parses visual scenes into hierarchical symbolic structures. (2) A\ncoordinated narrative-flow and image synthesis mechanism ensures cross-modal\nconsistency. To evaluate our approach, we construct a novel benchmark based on\n3,000 real human-generated question-answer pairs over fine-tuned large models,\nintroducing a dynamic evaluation protocol for quantifying context drift in\ninterleaved VLMs. Experimental results demonstrate that IUT-Plug not only\nimproves accuracy on established benchmarks but also effectively alleviates the\nthree critical forms of context drift across diverse multimodal question\nanswering (QA) scenarios.",
        "url": "http://arxiv.org/abs/2510.10969v1",
        "published_date": "2025-10-13T03:19:45+00:00",
        "updated_date": "2025-10-13T03:19:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zeteng Lin",
            "Xingxing Li",
            "Wen You",
            "Xiaoyang Li",
            "Zehan Lu",
            "Yujun Cai",
            "Jing Tang"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "IUT-Plug is a module that enhances existing vision language models by explicitly reasoning about logic, object identity, and style in multimodal image-text generation.",
        "tldr_zh": "IUT-Plug 是一个模块，通过显式推理来增强现有的视觉语言模型，在多模态图像文本生成中处理逻辑、物体标识和样式。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "ExpVid: A Benchmark for Experiment Video Understanding & Reasoning",
        "summary": "Multimodal Large Language Models (MLLMs) hold promise for accelerating\nscientific discovery by interpreting complex experimental procedures. However,\ntheir true capabilities are poorly understood, as existing benchmarks neglect\nthe fine-grained and long-horizon nature of authentic laboratory work,\nespecially in wet-lab settings. To bridge this gap, we introduce ExpVid, the\nfirst benchmark designed to systematically evaluate MLLMs on scientific\nexperiment videos. Curated from peer-reviewed video publications, ExpVid\nfeatures a new three-level task hierarchy that mirrors the scientific process:\n(1) Fine-grained Perception of tools, materials, and actions; (2) Procedural\nUnderstanding of step order and completeness; and (3) Scientific Reasoning that\nconnects the full experiment to its published conclusions. Our vision-centric\nannotation pipeline, combining automated generation with multi-disciplinary\nexpert validation, ensures that tasks require visual grounding. We evaluate 19\nleading MLLMs on ExpVid and find that while they excel at coarse-grained\nrecognition, they struggle with disambiguating fine details, tracking state\nchanges over time, and linking experimental procedures to scientific outcomes.\nOur results reveal a notable performance gap between proprietary and\nopen-source models, particularly in high-order reasoning. ExpVid not only\nprovides a diagnostic tool but also charts a roadmap for developing MLLMs\ncapable of becoming trustworthy partners in scientific experimentation.",
        "url": "http://arxiv.org/abs/2510.11606v1",
        "published_date": "2025-10-13T16:45:28+00:00",
        "updated_date": "2025-10-13T16:45:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yicheng Xu",
            "Yue Wu",
            "Jiashuo Yu",
            "Ziang Yan",
            "Tianxiang Jiang",
            "Yinan He",
            "Qingsong Zhao",
            "Kai Chen",
            "Yu Qiao",
            "Limin Wang",
            "Manabu Okumura",
            "Yi Wang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces ExpVid, a benchmark for evaluating Multimodal Large Language Models (MLLMs) on scientific experiment videos, highlighting their capabilities and limitations.",
        "tldr_zh": "该论文介绍了ExpVid，这是一个用于评估多模态大型语言模型（MLLMs）在科学实验视频上的基准，突出了它们的能力和局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "mmWalk: Towards Multi-modal Multi-view Walking Assistance",
        "summary": "Walking assistance in extreme or complex environments remains a significant\nchallenge for people with blindness or low vision (BLV), largely due to the\nlack of a holistic scene understanding. Motivated by the real-world needs of\nthe BLV community, we build mmWalk, a simulated multi-modal dataset that\nintegrates multi-view sensor and accessibility-oriented features for outdoor\nsafe navigation. Our dataset comprises 120 manually controlled,\nscenario-categorized walking trajectories with 62k synchronized frames. It\ncontains over 559k panoramic images across RGB, depth, and semantic modalities.\nFurthermore, to emphasize real-world relevance, each trajectory involves\noutdoor corner cases and accessibility-specific landmarks for BLV users.\nAdditionally, we generate mmWalkVQA, a VQA benchmark with over 69k visual\nquestion-answer triplets across 9 categories tailored for safe and informed\nwalking assistance. We evaluate state-of-the-art Vision-Language Models (VLMs)\nusing zero- and few-shot settings and found they struggle with our risk\nassessment and navigational tasks. We validate our mmWalk-finetuned model on\nreal-world datasets and show the effectiveness of our dataset for advancing\nmulti-modal walking assistance.",
        "url": "http://arxiv.org/abs/2510.11520v1",
        "published_date": "2025-10-13T15:25:52+00:00",
        "updated_date": "2025-10-13T15:25:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kedi Ying",
            "Ruiping Liu",
            "Chongyan Chen",
            "Mingzhe Tao",
            "Hao Shi",
            "Kailun Yang",
            "Jiaming Zhang",
            "Rainer Stiefelhagen"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces mmWalk, a simulated multi-modal dataset for outdoor safe navigation for people with blindness or low vision. It includes walking trajectories with panoramic images and a VQA benchmark tailored for safe walking assistance.",
        "tldr_zh": "该论文介绍了mmWalk，这是一个针对盲人或低视力人群的户外安全导航的模拟多模态数据集。它包括具有全景图像的行走轨迹和一个针对安全行走援助定制的VQA基准。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "GIR-Bench: Versatile Benchmark for Generating Images with Reasoning",
        "summary": "Unified multimodal models integrate the reasoning capacity of large language\nmodels with both image understanding and generation, showing great promise for\nadvanced multimodal intelligence. However, the community still lacks a rigorous\nreasoning-centric benchmark to systematically evaluate the alignment between\nunderstanding and generation, and their generalization potential in complex\nvisual tasks. To this end, we introduce \\textbf{GIR-Bench}, a comprehensive\nbenchmark that evaluates unified models across three complementary\nperspectives. Firstly, we investigate understanding-generation consistency\n(GIR-Bench-UGC), asking whether models can consistently leverage the same\nknowledge in both understanding and generation tasks. Secondly, we investigate\nwhether models can perform reasoning-centric text-to-image generation that\nrequires applying logical constraints and implicit knowledge to generate\nfaithful visual content (GIR-Bench-T2I). Thirdly, we evaluate whether models\ncan handle multi-step reasoning in editing (GIR-Bench-Edit). For each subset,\nwe carefully design different task-specific evaluation pipelines tailored for\neach task. This enables fine-grained and interpretable evaluation while\nmitigating biases from the prevalent MLLM-as-a-Judge paradigm. Extensive\nablations over various unified models and generation-only systems have shown\nthat: Although unified models are more capable of reasoning-driven visual\ntasks, they still exhibit a persistent gap between understanding and\ngeneration. The data and code for GIR-Bench are available at\n\\href{https://hkust-longgroup.github.io/GIR-Bench}{https://hkust-longgroup.github.io/GIR-Bench}.",
        "url": "http://arxiv.org/abs/2510.11026v1",
        "published_date": "2025-10-13T05:50:44+00:00",
        "updated_date": "2025-10-13T05:50:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hongxiang Li",
            "Yaowei Li",
            "Bin Lin",
            "Yuwei Niu",
            "Yuhang Yang",
            "Xiaoshuang Huang",
            "Jiayin Cai",
            "Xiaolong Jiang",
            "Yao Hu",
            "Long Chen"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "GIR-Bench introduces a benchmark to evaluate unified multimodal models in reasoning-driven visual tasks, highlighting a gap between understanding and generation capabilities.",
        "tldr_zh": "GIR-Bench提出了一个基准来评估统一多模态模型在推理驱动的视觉任务中的表现，突出了理解和生成能力之间的差距。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Into the Unknown: Towards using Generative Models for Sampling Priors of Environment Uncertainty for Planning in Configuration Spaces",
        "summary": "Priors are vital for planning under partial observability, yet difficult to\nobtain in practice. We present a sampling-based pipeline that leverages\nlarge-scale pretrained generative models to produce probabilistic priors\ncapturing environmental uncertainty and spatio-semantic relationships in a\nzero-shot manner. Conditioned on partial observations, the pipeline recovers\ncomplete RGB-D point cloud samples with occupancy and target semantics,\nformulated to be directly useful in configuration-space planning. We establish\na Matterport3D benchmark of rooms partially visible through doorways, where a\nrobot must navigate to an unobserved target object. Effective priors for this\nsetting must represent both occupancy and target-location uncertainty in\nunobserved regions. Experiments show that our approach recovers commonsense\nspatial semantics consistent with ground truth, yielding diverse, clean 3D\npoint clouds usable in motion planning, highlight the promise of generative\nmodels as a rich source of priors for robotic planning.",
        "url": "http://arxiv.org/abs/2510.11014v1",
        "published_date": "2025-10-13T05:08:48+00:00",
        "updated_date": "2025-10-13T05:08:48+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Subhransu S. Bhattacharjee",
            "Hao Lu",
            "Dylan Campbell",
            "Rahul Shome"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a pipeline using generative models to sample priors for planning in uncertain environments. It focuses on producing probabilistic priors that capture environmental uncertainty and spatial relationships to aid in robotic navigation.",
        "tldr_zh": "本文提出了一种使用生成模型进行先验采样的管道，旨在产生能捕获环境不确定性和空间关系的概率先验，以帮助机器人导航。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Perspective-aware 3D Gaussian Inpainting with Multi-view Consistency",
        "summary": "3D Gaussian inpainting, a critical technique for numerous applications in\nvirtual reality and multimedia, has made significant progress with pretrained\ndiffusion models. However, ensuring multi-view consistency, an essential\nrequirement for high-quality inpainting, remains a key challenge. In this work,\nwe present PAInpainter, a novel approach designed to advance 3D Gaussian\ninpainting by leveraging perspective-aware content propagation and consistency\nverification across multi-view inpainted images. Our method iteratively refines\ninpainting and optimizes the 3D Gaussian representation with multiple views\nadaptively sampled from a perspective graph. By propagating inpainted images as\nprior information and verifying consistency across neighboring views,\nPAInpainter substantially enhances global consistency and texture fidelity in\nrestored 3D scenes. Extensive experiments demonstrate the superiority of\nPAInpainter over existing methods. Our approach achieves superior 3D inpainting\nquality, with PSNR scores of 26.03 dB and 29.51 dB on the SPIn-NeRF and\nNeRFiller datasets, respectively, highlighting its effectiveness and\ngeneralization capability.",
        "url": "http://arxiv.org/abs/2510.10993v1",
        "published_date": "2025-10-13T04:10:39+00:00",
        "updated_date": "2025-10-13T04:10:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuxin Cheng",
            "Binxiao Huang",
            "Taiqiang Wu",
            "Wenyong Zhou",
            "Chenchen Ding",
            "Zhengwu Liu",
            "Graziano Chesi",
            "Ngai Wong"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality"
        ],
        "tldr": "The paper presents a novel method, PAInpainter, for 3D Gaussian inpainting using perspective-aware content propagation and multi-view consistency, showing superior results compared to existing methods.",
        "tldr_zh": "本文提出了一种新方法PAInpainter，利用透视感知内容传播和多视角一致性进行3D高斯修补，结果优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "Mixup Helps Understanding Multimodal Video Better",
        "summary": "Multimodal video understanding plays a crucial role in tasks such as action\nrecognition and emotion classification by combining information from different\nmodalities. However, multimodal models are prone to overfitting strong\nmodalities, which can dominate learning and suppress the contributions of\nweaker ones. To address this challenge, we first propose Multimodal Mixup (MM),\nwhich applies the Mixup strategy at the aggregated multimodal feature level to\nmitigate overfitting by generating virtual feature-label pairs. While MM\neffectively improves generalization, it treats all modalities uniformly and\ndoes not account for modality imbalance during training. Building on MM, we\nfurther introduce Balanced Multimodal Mixup (B-MM), which dynamically adjusts\nthe mixing ratios for each modality based on their relative contributions to\nthe learning objective. Extensive experiments on several datasets demonstrate\nthe effectiveness of our methods in improving generalization and multimodal\nrobustness.",
        "url": "http://arxiv.org/abs/2510.10986v1",
        "published_date": "2025-10-13T03:53:25+00:00",
        "updated_date": "2025-10-13T03:53:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoyu Ma",
            "Ding Ding",
            "Hao Chen"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes Multimodal Mixup (MM) and Balanced Multimodal Mixup (B-MM) methods to improve generalization and multimodal robustness by addressing overfitting and modality imbalance in multimodal video understanding.",
        "tldr_zh": "本论文提出了Multimodal Mixup（MM）和Balanced Multimodal Mixup（B-MM）方法，通过解决多模态视频理解中的过拟合和模态不平衡问题，提高了泛化性能和多模态鲁棒性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "DISC-GAN: Disentangling Style and Content for Cluster-Specific Synthetic Underwater Image Generation",
        "summary": "In this paper, we propose a novel framework, Disentangled Style-Content GAN\n(DISC-GAN), which integrates style-content disentanglement with a\ncluster-specific training strategy towards photorealistic underwater image\nsynthesis. The quality of synthetic underwater images is challenged by optical\ndue to phenomena such as color attenuation and turbidity. These phenomena are\nrepresented by distinct stylistic variations across different waterbodies, such\nas changes in tint and haze. While generative models are well-suited to capture\ncomplex patterns, they often lack the ability to model the non-uniform\nconditions of diverse underwater environments. To address these challenges, we\nemploy K-means clustering to partition a dataset into style-specific domains.\nWe use separate encoders to get latent spaces for style and content; we further\nintegrate these latent representations via Adaptive Instance Normalization\n(AdaIN) and decode the result to produce the final synthetic image. The model\nis trained independently on each style cluster to preserve domain-specific\ncharacteristics. Our framework demonstrates state-of-the-art performance,\nobtaining a Structural Similarity Index (SSIM) of 0.9012, an average Peak\nSignal-to-Noise Ratio (PSNR) of 32.5118 dB, and a Frechet Inception Distance\n(FID) of 13.3728.",
        "url": "http://arxiv.org/abs/2510.10782v1",
        "published_date": "2025-10-12T19:56:20+00:00",
        "updated_date": "2025-10-12T19:56:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sneha Varur",
            "Anirudh R Hanchinamani",
            "Tarun S Bagewadi",
            "Uma Mudenagudi",
            "Chaitra D Desai",
            "Sujata C",
            "Padmashree Desai",
            "Sumit Meharwade"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces DISC-GAN, a framework for generating photorealistic underwater images by separating style and content using cluster-specific training.",
        "tldr_zh": "本文介绍了DISC-GAN，这是一个用于生成逼真水下图像的框架，通过聚类特定的培训分离风格和内容。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images",
        "summary": "Recent advances in Large Language Models (LLMs) and Vision Language Models\n(VLMs) have shown significant progress in mathematical reasoning, yet they\nstill face a critical bottleneck with problems requiring visual assistance,\nsuch as drawing auxiliary lines or plotting functions to solve the problems.\nMost LLMs and VLMs are constrained to text-only reasoning chains, while\nmultimodal unified models that can generate interleaved text and images lack\nthe necessary precision and controllability for such tasks. To address this, we\npropose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for \"thinking\nwith images\" in mathematics. Our approach leverages the VLM to generate text\nreasoning as well as executable plotting code, which is then rendered into\nimages as \"visual thought\", to solve mathematical problems. To achieve this, we\nfirst construct Math-VR, the first large-scale, bilingual dataset and benchmark\nfor Mathematics problems with Visual Reasoning, comprising 178K samples.\nSecond, to create high-quality training data, we develop a state-of-the-art\nimage-to-code converter specialized for parsing complex mathematical figures\ninto codes. Finally, using these training data, we train the CodePlot-CoT model\nfor solving mathematical problems. Experimental results show that our model\nachieves up to 21% increase over base model on our new benchmark, fully\nvalidating the efficacy of our proposed code-driven reasoning paradigm. Our\nwork opens a new direction for multimodal mathematical reasoning and provides\nthe community with the first large-scale dataset, comprehensive benchmark, and\nstrong approach for such problems. To facilitate future research, we make our\ndatasets, code, and pretrained models publicly available at\nhttps://github.com/HKU-MMLab/Math-VR-CodePlot-CoT.",
        "url": "http://arxiv.org/abs/2510.11718v1",
        "published_date": "2025-10-13T17:59:55+00:00",
        "updated_date": "2025-10-13T17:59:55+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Chengqi Duan",
            "Kaiyue Sun",
            "Rongyao Fang",
            "Manyuan Zhang",
            "Yan Feng",
            "Ying Luo",
            "Yufang Liu",
            "Ke Wang",
            "Peng Pei",
            "Xunliang Cai",
            "Hongsheng Li",
            "Yi Ma",
            "Xihui Liu"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes CodePlot-CoT, a code-driven paradigm for mathematical reasoning with images, achieving significant improvements over base models.",
        "tldr_zh": "本文提出了CodePlot-CoT，一种代码驱动的数学推理范式，相对基准模型取得了显著的改进。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "InfiniHuman: Infinite 3D Human Creation with Precise Control",
        "summary": "Generating realistic and controllable 3D human avatars is a long-standing\nchallenge, particularly when covering broad attribute ranges such as ethnicity,\nage, clothing styles, and detailed body shapes. Capturing and annotating\nlarge-scale human datasets for training generative models is prohibitively\nexpensive and limited in scale and diversity. The central question we address\nin this paper is: Can existing foundation models be distilled to generate\ntheoretically unbounded, richly annotated 3D human data? We introduce\nInfiniHuman, a framework that synergistically distills these models to produce\nrichly annotated human data at minimal cost and with theoretically unlimited\nscalability. We propose InfiniHumanData, a fully automatic pipeline that\nleverages vision-language and image generation models to create a large-scale\nmulti-modal dataset. User study shows our automatically generated identities\nare undistinguishable from scan renderings. InfiniHumanData contains 111K\nidentities spanning unprecedented diversity. Each identity is annotated with\nmulti-granularity text descriptions, multi-view RGB images, detailed clothing\nimages, and SMPL body-shape parameters. Building on this dataset, we propose\nInfiniHumanGen, a diffusion-based generative pipeline conditioned on text, body\nshape, and clothing assets. InfiniHumanGen enables fast, realistic, and\nprecisely controllable avatar generation. Extensive experiments demonstrate\nsignificant improvements over state-of-the-art methods in visual quality,\ngeneration speed, and controllability. Our approach enables high-quality avatar\ngeneration with fine-grained control at effectively unbounded scale through a\npractical and affordable solution. We will publicly release the automatic data\ngeneration pipeline, the comprehensive InfiniHumanData dataset, and the\nInfiniHumanGen models at https://yuxuan-xue.com/infini-human.",
        "url": "http://arxiv.org/abs/2510.11650v1",
        "published_date": "2025-10-13T17:29:55+00:00",
        "updated_date": "2025-10-13T17:29:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuxuan Xue",
            "Xianghui Xie",
            "Margaret Kostyrko",
            "Gerard Pons-Moll"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces InfiniHuman, a framework for generating diverse and controllable 3D human avatars using models that distill large-scale human datasets at minimal cost. It achieves high realism and scalability.",
        "tldr_zh": "该论文介绍了InfiniHuman，这是一个利用模型以最小成本产生多样化可控的3D人类化身的框架。它实现了高度的逼真度和可伸缩性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "PhySIC: Physically Plausible 3D Human-Scene Interaction and Contact from a Single Image",
        "summary": "Reconstructing metrically accurate humans and their surrounding scenes from a\nsingle image is crucial for virtual reality, robotics, and comprehensive 3D\nscene understanding. However, existing methods struggle with depth ambiguity,\nocclusions, and physically inconsistent contacts. To address these challenges,\nwe introduce PhySIC, a framework for physically plausible Human-Scene\nInteraction and Contact reconstruction. PhySIC recovers metrically consistent\nSMPL-X human meshes, dense scene surfaces, and vertex-level contact maps within\na shared coordinate frame from a single RGB image. Starting from coarse\nmonocular depth and body estimates, PhySIC performs occlusion-aware inpainting,\nfuses visible depth with unscaled geometry for a robust metric scaffold, and\nsynthesizes missing support surfaces like floors. A confidence-weighted\noptimization refines body pose, camera parameters, and global scale by jointly\nenforcing depth alignment, contact priors, interpenetration avoidance, and 2D\nreprojection consistency. Explicit occlusion masking safeguards invisible\nregions against implausible configurations. PhySIC is efficient, requiring only\n9 seconds for joint human-scene optimization and under 27 seconds end-to-end.\nIt naturally handles multiple humans, enabling reconstruction of diverse\ninteractions. Empirically, PhySIC outperforms single-image baselines, reducing\nmean per-vertex scene error from 641 mm to 227 mm, halving PA-MPJPE to 42 mm,\nand improving contact F1 from 0.09 to 0.51. Qualitative results show realistic\nfoot-floor interactions, natural seating, and plausible reconstructions of\nheavily occluded furniture. By converting a single image into a physically\nplausible 3D human-scene pair, PhySIC advances scalable 3D scene understanding.\nOur implementation is publicly available at https://yuxuan-xue.com/physic.",
        "url": "http://arxiv.org/abs/2510.11649v1",
        "published_date": "2025-10-13T17:29:51+00:00",
        "updated_date": "2025-10-13T17:29:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pradyumna Yalandur Muralidhar",
            "Yuxuan Xue",
            "Xianghui Xie",
            "Margaret Kostyrko",
            "Gerard Pons-Moll"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces PhySIC, a framework for reconstructing physically plausible 3D human-scene interactions and contacts from a single image.",
        "tldr_zh": "该论文介绍了 PhySIC，一个从单个图像重建物理上合理的3D人-场景交互和接触的框架。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "FlexAC: Towards Flexible Control of Associative Reasoning in Multimodal Large Language Models",
        "summary": "Multimodal large language models (MLLMs) face an inherent trade-off between\nfaithfulness and creativity, as different tasks require varying degrees of\nassociative reasoning. However, existing methods lack the flexibility to\nmodulate this reasoning strength, limiting MLLMs' adaptability across factual\nand creative scenarios. To bridge this gap, we propose equipping MLLMs with\nmechanisms that enable flexible control over associative reasoning. We begin by\ninvestigating the internal mechanisms underlying associative behavior in MLLMs\nand find that: (1) middle layers play a pivotal role in shaping model's\nassociative tendencies, (2) modifying representations in these layers\neffectively regulates associative reasoning strength, and (3) hallucinations\ncan be exploited to derive steering vectors that guide this modulation.\nBuilding on these findings, we introduce Flexible Association Control (FlexAC),\na lightweight and training-free framework for modulating associative behavior\nin MLLMs. FlexAC first induces hallucination-guided intermediate\nrepresentations to encode associative directions. Then, it selects\nhigh-association instances to construct effective associative steering vectors,\nwhose strengths are adaptively calibrated to balance creative guidance with\noutput stability. Finally, recognizing the multi-dimensional nature of\nassociative reasoning, FlexAC incorporates task-specific associative vectors\nderived from a forward pass on a few target-domain samples, enabling models to\nfollow diverse associative directions and better adapt to creative tasks.\nNotably, our method achieves up to a 5.8x improvement in creativity on\nCreation-MMBench and a 29% reduction in hallucination rate on CHAIR, surpassing\nexisting baselines and demonstrating its effectiveness in enabling flexible\ncontrol over associative reasoning in MLLMs. Our code is available at\nhttps://github.com/ylhz/FlexAC.",
        "url": "http://arxiv.org/abs/2510.11190v1",
        "published_date": "2025-10-13T09:22:12+00:00",
        "updated_date": "2025-10-13T09:22:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shengming Yuan",
            "Xinyu Lyu",
            "Shuailong Wang",
            "Beitao Chen",
            "Jingkuan Song",
            "Lianli Gao"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer"
        ],
        "tldr": "FlexAC proposes a framework for flexible control of associative reasoning in multimodal large language models, achieving significant improvements in creativity and reducing hallucination rate.",
        "tldr_zh": "FlexAC 提出了一种用于在多模式大语言模型中灵活控制关联推理的框架，显著提高了创造性并减少了幻觉率。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams",
        "summary": "Event cameras offer various advantages for novel view rendering compared to\nsynchronously operating RGB cameras, and efficient event-based techniques\nsupporting rigid scenes have been recently demonstrated in the literature. In\nthe case of non-rigid objects, however, existing approaches additionally\nrequire sparse RGB inputs, which can be a substantial practical limitation; it\nremains unknown if similar models could be learned from event streams only.\nThis paper sheds light on this challenging open question and introduces Ev4DGS,\ni.e., the first approach for novel view rendering of non-rigidly deforming\nobjects in the explicit observation space (i.e., as RGB or greyscale images)\nfrom monocular event streams. Our method regresses a deformable 3D Gaussian\nSplatting representation through 1) a loss relating the outputs of the\nestimated model with the 2D event observation space, and 2) a coarse 3D\ndeformation model trained from binary masks generated from events. We perform\nexperimental comparisons on existing synthetic and newly recorded real datasets\nwith non-rigid objects. The results demonstrate the validity of Ev4DGS and its\nsuperior performance compared to multiple naive baselines that can be applied\nin our setting. We will release our models and the datasets used in the\nevaluation for research purposes; see the project webpage:\nhttps://4dqv.mpi-inf.mpg.de/Ev4DGS/.",
        "url": "http://arxiv.org/abs/2510.11717v1",
        "published_date": "2025-10-13T17:59:55+00:00",
        "updated_date": "2025-10-13T17:59:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Takuya Nakabayashi",
            "Navami Kairanda",
            "Hideo Saito",
            "Vladislav Golyanik"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "This paper introduces Ev4DGS, a method for novel view rendering of non-rigid objects from monocular event streams, outperforming baselines in both synthetic and real datasets.",
        "tldr_zh": "本文介绍了Ev4DGS，一种从单眼事件流中渲染非刚性物体的方法，在合成和真实数据集中优于基线方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Point Prompting: Counterfactual Tracking with Video Diffusion Models",
        "summary": "Trackers and video generators solve closely related problems: the former\nanalyze motion, while the latter synthesize it. We show that this connection\nenables pretrained video diffusion models to perform zero-shot point tracking\nby simply prompting them to visually mark points as they move over time. We\nplace a distinctively colored marker at the query point, then regenerate the\nrest of the video from an intermediate noise level. This propagates the marker\nacross frames, tracing the point's trajectory. To ensure that the marker\nremains visible in this counterfactual generation, despite such markers being\nunlikely in natural videos, we use the unedited initial frame as a negative\nprompt. Through experiments with multiple image-conditioned video diffusion\nmodels, we find that these \"emergent\" tracks outperform those of prior\nzero-shot methods and persist through occlusions, often obtaining performance\nthat is competitive with specialized self-supervised models.",
        "url": "http://arxiv.org/abs/2510.11715v1",
        "published_date": "2025-10-13T17:59:46+00:00",
        "updated_date": "2025-10-13T17:59:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ayush Shrivastava",
            "Sanyam Mehta",
            "Daniel Geng",
            "Andrew Owens"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN"
        ],
        "tldr": "The paper introduces a method called point prompting for zero-shot point tracking using pretrained video diffusion models, outperforming prior methods and achieving competitive performance with specialized self-supervised models.",
        "tldr_zh": "本文引入了一种称为点提示的方法，利用预训练的视频扩散模型进行零-shot点追踪，优于先前的方法，并获得与专门的自监督模型竞争性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training",
        "summary": "In this work, we propose DiT360, a DiT-based framework that performs hybrid\ntraining on perspective and panoramic data for panoramic image generation. For\nthe issues of maintaining geometric fidelity and photorealism in generation\nquality, we attribute the main reason to the lack of large-scale, high-quality,\nreal-world panoramic data, where such a data-centric view differs from prior\nmethods that focus on model design. Basically, DiT360 has several key modules\nfor inter-domain transformation and intra-domain augmentation, applied at both\nthe pre-VAE image level and the post-VAE token level. At the image level, we\nincorporate cross-domain knowledge through perspective image guidance and\npanoramic refinement, which enhance perceptual quality while regularizing\ndiversity and photorealism. At the token level, hybrid supervision is applied\nacross multiple modules, which include circular padding for boundary\ncontinuity, yaw loss for rotational robustness, and cube loss for distortion\nawareness. Extensive experiments on text-to-panorama, inpainting, and\noutpainting tasks demonstrate that our method achieves better boundary\nconsistency and image fidelity across eleven quantitative metrics. Our code is\navailable at https://github.com/Insta360-Research-Team/DiT360.",
        "url": "http://arxiv.org/abs/2510.11712v1",
        "published_date": "2025-10-13T17:59:15+00:00",
        "updated_date": "2025-10-13T17:59:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoran Feng",
            "Dizhe Zhang",
            "Xiangtai Li",
            "Bo Du",
            "Lu Qi"
        ],
        "ai_categories": [
            "GAN",
            "Transformer"
        ],
        "tldr": "DiT360 proposes a framework for high-fidelity panoramic image generation using hybrid training on perspective and panoramic data, achieving better boundary consistency and image fidelity across various tasks.",
        "tldr_zh": "DiT360提出了一个框架，通过在透视和全景数据上进行混合训练，实现高保真全景图像生成，在各种任务中获得更好的边界一致性和图像保真度。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Bayesian Topological Convolutional Neural Nets",
        "summary": "Convolutional neural networks (CNNs) have been established as the main\nworkhorse in image data processing; nonetheless, they require large amounts of\ndata to train, often produce overconfident predictions, and frequently lack the\nability to quantify the uncertainty of their predictions. To address these\nconcerns, we propose a new Bayesian topological CNN that promotes a novel\ninterplay between topology-aware learning and Bayesian sampling. Specifically,\nit utilizes information from important manifolds to accelerate training while\nreducing calibration error by placing prior distributions on network parameters\nand properly learning appropriate posteriors. One important contribution of our\nwork is the inclusion of a consistency condition in the learning cost, which\ncan effectively modify the prior distributions to improve the performance of\nour novel network architecture. We evaluate the model on benchmark image\nclassification datasets and demonstrate its superiority over conventional CNNs,\nBayesian neural networks (BNNs), and topological CNNs. In particular, we supply\nevidence that our method provides an advantage in situations where training\ndata is limited or corrupted. Furthermore, we show that the new model allows\nfor better uncertainty quantification than standard BNNs since it can more\nreadily identify examples of out-of-distribution data on which it has not been\ntrained. Our results highlight the potential of our novel hybrid approach for\nmore efficient and robust image classification.",
        "url": "http://arxiv.org/abs/2510.11704v1",
        "published_date": "2025-10-13T17:57:43+00:00",
        "updated_date": "2025-10-13T17:57:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sarah Harkins Dayton",
            "Hayden Everett",
            "Ioannis Schizas",
            "David L. Boothe Jr.",
            "Vasileios Maroulas"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a Bayesian topological CNN that combines topology-aware learning and Bayesian sampling to improve image classification performance and uncertainty quantification.",
        "tldr_zh": "本文引入了一种贝叶斯拓扑卷积神经网络，结合了拓扑感知学习和贝叶斯抽样，以提高图像分类性能和不确定性量化。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Scaling Language-Centric Omnimodal Representation Learning",
        "summary": "Recent multimodal embedding approaches leveraging multimodal large language\nmodels (MLLMs) fine-tuned with contrastive learning (CL) have shown promising\nresults, yet the underlying reasons behind their superiority remain\nunderexplored. This work argues that a crucial advantage of MLLM-based\napproaches stems from implicit cross-modal alignment achieved during generative\npretraining, where the language decoder learns to exploit multimodal signals\nwithin a shared representation space for generating unimodal outputs. Through\nanalysis of anisotropy and kernel similarity structure, we empirically confirm\nthat latent alignment emerges within MLLM representations, allowing CL to serve\nas a lightweight refinement stage. Leveraging this insight, we propose a\nLanguage-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive\nexperiments across diverse backbones and benchmarks demonstrate its\neffectiveness, achieving state-of-the-art performance across modalities.\nFurthermore, we identify a Generation-Representation Scaling Law (GRSL),\nshowing that the representational capabilities gained through contrastive\nrefinement scales positively with the MLLM's generative capabilities. This\nsuggests that improving generative abilities evolves as an effective paradigm\nfor enhancing representation quality. We provide a theoretical explanation of\nGRSL, which formally links the MLLM's generative quality to the upper bound on\nits representation performance, and validate it on a challenging, low-resource\nvisual-document retrieval task, showing that continual generative pretraining\nbefore CL can further enhance the potential of a model's embedding\ncapabilities. Codes, models, and resources are available at\nhttps://github.com/LCO-Embedding/LCO-Embedding.",
        "url": "http://arxiv.org/abs/2510.11693v1",
        "published_date": "2025-10-13T17:53:52+00:00",
        "updated_date": "2025-10-13T17:53:52+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Chenghao Xiao",
            "Hou Pong Chan",
            "Hao Zhang",
            "Weiwen Xu",
            "Mahani Aljunied",
            "Yu Rong"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "This paper introduces a Language-Centric Omnimodal Embedding framework leveraging language models for multimodal representation learning, showing improved performance across modalities.",
        "tldr_zh": "本文介绍了一种利用语言模型进行多模态表示学习的语言中心全模态嵌入框架，展示了跨模态性能的提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Diffusion Transformers with Representation Autoencoders",
        "summary": "Latent generative modeling, where a pretrained autoencoder maps pixels into a\nlatent space for the diffusion process, has become the standard strategy for\nDiffusion Transformers (DiT); however, the autoencoder component has barely\nevolved. Most DiTs continue to rely on the original VAE encoder, which\nintroduces several limitations: outdated backbones that compromise\narchitectural simplicity, low-dimensional latent spaces that restrict\ninformation capacity, and weak representations that result from purely\nreconstruction-based training and ultimately limit generative quality. In this\nwork, we explore replacing the VAE with pretrained representation encoders\n(e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term\nRepresentation Autoencoders (RAEs). These models provide both high-quality\nreconstructions and semantically rich latent spaces, while allowing for a\nscalable transformer-based architecture. Since these latent spaces are\ntypically high-dimensional, a key challenge is enabling diffusion transformers\nto operate effectively within them. We analyze the sources of this difficulty,\npropose theoretically motivated solutions, and validate them empirically. Our\napproach achieves faster convergence without auxiliary representation alignment\nlosses. Using a DiT variant equipped with a lightweight, wide DDT head, we\nachieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no\nguidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers\nclear advantages and should be the new default for diffusion transformer\ntraining.",
        "url": "http://arxiv.org/abs/2510.11690v1",
        "published_date": "2025-10-13T17:51:39+00:00",
        "updated_date": "2025-10-13T17:51:39+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Boyang Zheng",
            "Nanye Ma",
            "Shengbang Tong",
            "Saining Xie"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes using Representation Autoencoders (RAEs) instead of traditional autoencoders in Diffusion Transformers (DiT) for image generation, achieving better results.",
        "tldr_zh": "本文提出在扩散变换器（DiT）中使用表示自动编码器（RAEs）而不是传统自动编码器，以实现更好的图像生成结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Beyond 'Templates': Category-Agnostic Object Pose, Size, and Shape Estimation from a Single View",
        "summary": "Estimating an object's 6D pose, size, and shape from visual input is a\nfundamental problem in computer vision, with critical applications in robotic\ngrasping and manipulation. Existing methods either rely on object-specific\npriors such as CAD models or templates, or suffer from limited generalization\nacross categories due to pose-shape entanglement and multi-stage pipelines. In\nthis work, we propose a unified, category-agnostic framework that\nsimultaneously predicts 6D pose, size, and dense shape from a single RGB-D\nimage, without requiring templates, CAD models, or category labels at test\ntime. Our model fuses dense 2D features from vision foundation models with\npartial 3D point clouds using a Transformer encoder enhanced by a\nMixture-of-Experts, and employs parallel decoders for pose-size estimation and\nshape reconstruction, achieving real-time inference at 28 FPS. Trained solely\non synthetic data from 149 categories in the SOPE dataset, our framework is\nevaluated on four diverse benchmarks SOPE, ROPE, ObjaversePose, and HANDAL,\nspanning over 300 categories. It achieves state-of-the-art accuracy on seen\ncategories while demonstrating remarkably strong zero-shot generalization to\nunseen real-world objects, establishing a new standard for open-set 6D\nunderstanding in robotics and embodied AI.",
        "url": "http://arxiv.org/abs/2510.11687v1",
        "published_date": "2025-10-13T17:49:15+00:00",
        "updated_date": "2025-10-13T17:49:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinyu Zhang",
            "Haitao Lin",
            "Jiashu Hou",
            "Xiangyang Xue",
            "Yanwei Fu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a category-agnostic framework that predicts object pose, size, and shape from a single RGB-D image without using templates or CAD models. It achieves state-of-the-art accuracy on known categories and strong generalization to unseen objects.",
        "tldr_zh": "本文介绍了一种类别无关的框架，可以从单个RGB-D图像中预测物体的姿态、大小和形状，无需使用模板或CAD模型。它在已知类别上达到了最先进的准确性，并对未知对象表现出强大的泛化能力。",
        "relevance_score": 1,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "FACE: Faithful Automatic Concept Extraction",
        "summary": "Interpreting deep neural networks through concept-based explanations offers a\nbridge between low-level features and high-level human-understandable\nsemantics. However, existing automatic concept discovery methods often fail to\nalign these extracted concepts with the model's true decision-making process,\nthereby compromising explanation faithfulness. In this work, we propose FACE\n(Faithful Automatic Concept Extraction), a novel framework that augments\nNon-negative Matrix Factorization (NMF) with a Kullback-Leibler (KL) divergence\nregularization term to ensure alignment between the model's original and\nconcept-based predictions. Unlike prior methods that operate solely on encoder\nactivations, FACE incorporates classifier supervision during concept learning,\nenforcing predictive consistency and enabling faithful explanations. We provide\ntheoretical guarantees showing that minimizing the KL divergence bounds the\ndeviation in predictive distributions, thereby promoting faithful local\nlinearity in the learned concept space. Systematic evaluations on ImageNet,\nCOCO, and CelebA datasets demonstrate that FACE outperforms existing methods\nacross faithfulness and sparsity metrics.",
        "url": "http://arxiv.org/abs/2510.11675v1",
        "published_date": "2025-10-13T17:44:45+00:00",
        "updated_date": "2025-10-13T17:44:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Dipkamal Bhusal",
            "Michael Clifford",
            "Sara Rampazzi",
            "Nidhi Rastogi"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces FACE, a framework that improves concept extraction from deep neural networks by ensuring alignment between the model's original and concept-based predictions.",
        "tldr_zh": "本文介绍了FACE，这是一个框架，通过确保模型原始预测与基于概念的预测之间的对齐，改善了从深度神经网络中提取概念。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "IVEBench: Modern Benchmark Suite for Instruction-Guided Video Editing Assessment",
        "summary": "Instruction-guided video editing has emerged as a rapidly advancing research\ndirection, offering new opportunities for intuitive content transformation\nwhile also posing significant challenges for systematic evaluation. Existing\nvideo editing benchmarks fail to support the evaluation of instruction-guided\nvideo editing adequately and further suffer from limited source diversity,\nnarrow task coverage and incomplete evaluation metrics. To address the above\nlimitations, we introduce IVEBench, a modern benchmark suite specifically\ndesigned for instruction-guided video editing assessment. IVEBench comprises a\ndiverse database of 600 high-quality source videos, spanning seven semantic\ndimensions, and covering video lengths ranging from 32 to 1,024 frames. It\nfurther includes 8 categories of editing tasks with 35 subcategories, whose\nprompts are generated and refined through large language models and expert\nreview. Crucially, IVEBench establishes a three-dimensional evaluation protocol\nencompassing video quality, instruction compliance and video fidelity,\nintegrating both traditional metrics and multimodal large language model-based\nassessments. Extensive experiments demonstrate the effectiveness of IVEBench in\nbenchmarking state-of-the-art instruction-guided video editing methods, showing\nits ability to provide comprehensive and human-aligned evaluation outcomes.",
        "url": "http://arxiv.org/abs/2510.11647v1",
        "published_date": "2025-10-13T17:27:08+00:00",
        "updated_date": "2025-10-13T17:27:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yinan Chen",
            "Jiangning Zhang",
            "Teng Hu",
            "Yuxiang Zeng",
            "Zhucun Xue",
            "Qingdong He",
            "Chengjie Wang",
            "Yong Liu",
            "Xiaobin Hu",
            "Shuicheng Yan"
        ],
        "ai_categories": [
            "Dataset",
            "Multimodality"
        ],
        "tldr": "IVEBench is a benchmark suite for instruction-guided video editing assessment, featuring a diverse database of videos and thorough evaluation protocols.",
        "tldr_zh": "IVEBench是一个用于指导视频编辑评估的基准套件，具有多样化的视频数据库和完善的评估协议。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EvoCAD: Evolutionary CAD Code Generation with Vision Language Models",
        "summary": "Combining large language models with evolutionary computation algorithms\nrepresents a promising research direction leveraging the remarkable generative\nand in-context learning capabilities of LLMs with the strengths of evolutionary\nalgorithms. In this work, we present EvoCAD, a method for generating\ncomputer-aided design (CAD) objects through their symbolic representations\nusing vision language models and evolutionary optimization. Our method samples\nmultiple CAD objects, which are then optimized using an evolutionary approach\nwith vision language and reasoning language models. We assess our method using\nGPT-4V and GPT-4o, evaluating it on the CADPrompt benchmark dataset and\ncomparing it to prior methods. Additionally, we introduce two new metrics based\non topological properties defined by the Euler characteristic, which capture a\nform of semantic similarity between 3D objects. Our results demonstrate that\nEvoCAD outperforms previous approaches on multiple metrics, particularly in\ngenerating topologically correct objects, which can be efficiently evaluated\nusing our two novel metrics that complement existing spatial metrics.",
        "url": "http://arxiv.org/abs/2510.11631v1",
        "published_date": "2025-10-13T17:12:02+00:00",
        "updated_date": "2025-10-13T17:12:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.NE"
        ],
        "authors": [
            "Tobias Preintner",
            "Weixuan Yuan",
            "Adrian König",
            "Thomas Bäck",
            "Elena Raponi",
            "Niki van Stein"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "EvoCAD combines language models with evolutionary algorithms to generate CAD objects, outperforming previous methods with novel topological metrics.",
        "tldr_zh": "EvoCAD将语言模型与进化算法结合以生成CAD对象，在使用新颖的拓扑度量标准时表现优于先前的方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "High-resolution Photo Enhancement in Real-time: A Laplacian Pyramid Network",
        "summary": "Photo enhancement plays a crucial role in augmenting the visual aesthetics of\na photograph. In recent years, photo enhancement methods have either focused on\nenhancement performance, producing powerful models that cannot be deployed on\nedge devices, or prioritized computational efficiency, resulting in inadequate\nperformance for real-world applications. To this end, this paper introduces a\npyramid network called LLF-LUT++, which integrates global and local operators\nthrough closed-form Laplacian pyramid decomposition and reconstruction. This\napproach enables fast processing of high-resolution images while also achieving\nexcellent performance. Specifically, we utilize an image-adaptive 3D LUT that\ncapitalizes on the global tonal characteristics of downsampled images, while\nincorporating two distinct weight fusion strategies to achieve coarse global\nimage enhancement. To implement this strategy, we designed a spatial-frequency\ntransformer weight predictor that effectively extracts the desired distinct\nweights by leveraging frequency features. Additionally, we apply local\nLaplacian filters to adaptively refine edge details in high-frequency\ncomponents. After meticulously redesigning the network structure and\ntransformer model, LLF-LUT++ not only achieves a 2.64 dB improvement in PSNR on\nthe HDR+ dataset, but also further reduces runtime, with 4K resolution images\nprocessed in just 13 ms on a single GPU. Extensive experimental results on two\nbenchmark datasets further show that the proposed approach performs favorably\ncompared to state-of-the-art methods. The source code will be made publicly\navailable at https://github.com/fengzhang427/LLF-LUT.",
        "url": "http://arxiv.org/abs/2510.11613v1",
        "published_date": "2025-10-13T16:52:32+00:00",
        "updated_date": "2025-10-13T16:52:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Feng Zhang",
            "Haoyou Deng",
            "Zhiqiang Li",
            "Lida Li",
            "Bin Xu",
            "Qingbo Lu",
            "Zisheng Cao",
            "Minchen Wei",
            "Changxin Gao",
            "Nong Sang",
            "Xiang Bai"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a Laplacian Pyramid Network called LLF-LUT++ for high-resolution photo enhancement in real-time, achieving excellent performance and fast processing.",
        "tldr_zh": "本文介绍了一种称为LLF-LUT++的Laplacian金字塔网络，用于实时高分辨率照片增强，实现了卓越的性能和快速处理。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MS-Mix: Unveiling the Power of Mixup for Multimodal Sentiment Analysis",
        "summary": "Multimodal Sentiment Analysis (MSA) aims to identify and interpret human\nemotions by integrating information from heterogeneous data sources such as\ntext, video, and audio. While deep learning models have advanced in network\narchitecture design, they remain heavily limited by scarce multimodal annotated\ndata. Although Mixup-based augmentation improves generalization in unimodal\ntasks, its direct application to MSA introduces critical challenges: random\nmixing often amplifies label ambiguity and semantic inconsistency due to the\nlack of emotion-aware mixing mechanisms. To overcome these issues, we propose\nMS-Mix, an adaptive, emotion-sensitive augmentation framework that\nautomatically optimizes sample mixing in multimodal settings. The key\ncomponents of MS-Mix include: (1) a Sentiment-Aware Sample Selection (SASS)\nstrategy that effectively prevents semantic confusion caused by mixing samples\nwith contradictory emotions. (2) a Sentiment Intensity Guided (SIG) module\nusing multi-head self-attention to compute modality-specific mixing ratios\ndynamically based on their respective emotional intensities. (3) a Sentiment\nAlignment Loss (SAL) that aligns the prediction distributions across\nmodalities, and incorporates the Kullback-Leibler-based loss as an additional\nregularization term to train the emotion intensity predictor and the backbone\nnetwork jointly. Extensive experiments on three benchmark datasets with six\nstate-of-the-art backbones confirm that MS-Mix consistently outperforms\nexisting methods, establishing a new standard for robust multimodal sentiment\naugmentation. The source code is available at:\nhttps://github.com/HongyuZhu-s/MS-Mix.",
        "url": "http://arxiv.org/abs/2510.11579v1",
        "published_date": "2025-10-13T16:23:32+00:00",
        "updated_date": "2025-10-13T16:23:32+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Hongyu Zhu",
            "Lin Chen",
            "Mounim A. El-Yacoubi",
            "Mingsheng Shang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes MS-Mix, an adaptive augmentation framework for multimodal sentiment analysis that outperforms existing methods by optimizing sample mixing and emotion-aware mechanisms.",
        "tldr_zh": "该论文提出了MS-Mix，一个用于多模态情感分析的自适应增强框架，通过优化样本混合和情感感知机制，胜过了现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Framework for Low-Effort Training Data Generation for Urban Semantic Segmentation",
        "summary": "Synthetic datasets are widely used for training urban scene recognition\nmodels, but even highly realistic renderings show a noticeable gap to real\nimagery. This gap is particularly pronounced when adapting to a specific target\ndomain, such as Cityscapes, where differences in architecture, vegetation,\nobject appearance, and camera characteristics limit downstream performance.\nClosing this gap with more detailed 3D modelling would require expensive asset\nand scene design, defeating the purpose of low-cost labelled data. To address\nthis, we present a new framework that adapts an off-the-shelf diffusion model\nto a target domain using only imperfect pseudo-labels. Once trained, it\ngenerates high-fidelity, target-aligned images from semantic maps of any\nsynthetic dataset, including low-effort sources created in hours rather than\nmonths. The method filters suboptimal generations, rectifies image-label\nmisalignments, and standardises semantics across datasets, transforming weak\nsynthetic data into competitive real-domain training sets. Experiments on five\nsynthetic datasets and two real target datasets show segmentation gains of up\nto +8.0%pt. mIoU over state-of-the-art translation methods, making rapidly\nconstructed synthetic datasets as effective as high-effort, time-intensive\nsynthetic datasets requiring extensive manual design. This work highlights a\nvaluable collaborative paradigm where fast semantic prototyping, combined with\ngenerative models, enables scalable, high-quality training data creation for\nurban scene understanding.",
        "url": "http://arxiv.org/abs/2510.11567v1",
        "published_date": "2025-10-13T16:12:29+00:00",
        "updated_date": "2025-10-13T16:12:29+00:00",
        "categories": [
            "cs.CV",
            "cs.GR",
            "cs.LG"
        ],
        "authors": [
            "Denis Zavadski",
            "Damjan Kalšan",
            "Tim Küchler",
            "Haebom Lee",
            "Stefan Roth",
            "Carsten Rother"
        ],
        "ai_categories": [
            "Dataset",
            "Diffusion",
            "Transformer"
        ],
        "tldr": "The paper introduces a framework for generating high-quality training data for urban semantic segmentation using synthetic datasets and pseudo-labels, achieving competitive results with real-domain datasets.",
        "tldr_zh": "该论文介绍了一个使用合成数据集和伪标签生成城市语义分割高质量训练数据的框架，实现了与真实数据集竞争性结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ODI-Bench: Can MLLMs Understand Immersive Omnidirectional Environments?",
        "summary": "Omnidirectional images (ODIs) provide full 360x180 view which are widely\nadopted in VR, AR and embodied intelligence applications. While multi-modal\nlarge language models (MLLMs) have demonstrated remarkable performance on\nconventional 2D image and video understanding benchmarks, their ability to\ncomprehend the immersive environments captured by ODIs remains largely\nunexplored. To address this gap, we first present ODI-Bench, a novel\ncomprehensive benchmark specifically designed for omnidirectional image\nunderstanding. ODI-Bench contains 2,000 high-quality omnidirectional images and\nover 4,000 manually annotated question-answering (QA) pairs across 10\nfine-grained tasks, covering both general-level and spatial-level ODI\nunderstanding. Extensive experiments are conducted to benchmark 20\nrepresentative MLLMs, including proprietary and open-source models, under both\nclose-ended and open-ended settings. Experimental results reveal that current\nMLLMs still struggle to capture the immersive context provided by ODIs. To this\nend, we further introduce Omni-CoT, a training-free method which significantly\nenhances MLLMs' comprehension ability in the omnidirectional environment\nthrough chain-of-thought reasoning across both textual information and visual\ncues. Both the benchmark and the code will be released upon the publication.",
        "url": "http://arxiv.org/abs/2510.11549v1",
        "published_date": "2025-10-13T15:51:47+00:00",
        "updated_date": "2025-10-13T15:51:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Liu Yang",
            "Huiyu Duan",
            "Ran Tao",
            "Juntao Cheng",
            "Sijing Wu",
            "Yunhao Li",
            "Jing Liu",
            "Xiongkuo Min",
            "Guangtao Zhai"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces ODI-Bench, a benchmark for understanding omnidirectional images using MLLMs. Results show current models struggle, but a new method, Omni-CoT, improves comprehension.",
        "tldr_zh": "本文介绍了针对MLLMs理解全方位图像的ODI-Bench基准，结果表明当前模型存在困难，但一种名为Omni-CoT的新方法提高了理解能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Massive Activations are the Key to Local Detail Synthesis in Diffusion Transformers",
        "summary": "Diffusion Transformers (DiTs) have recently emerged as a powerful backbone\nfor visual generation. Recent observations reveal \\emph{Massive Activations}\n(MAs) in their internal feature maps, yet their function remains poorly\nunderstood. In this work, we systematically investigate these activations to\nelucidate their role in visual generation. We found that these massive\nactivations occur across all spatial tokens, and their distribution is\nmodulated by the input timestep embeddings. Importantly, our investigations\nfurther demonstrate that these massive activations play a key role in local\ndetail synthesis, while having minimal impact on the overall semantic content\nof output. Building on these insights, we propose \\textbf{D}etail\n\\textbf{G}uidance (\\textbf{DG}), a MAs-driven, training-free self-guidance\nstrategy to explicitly enhance local detail fidelity for DiTs. Specifically, DG\nconstructs a degraded ``detail-deficient'' model by disrupting MAs and\nleverages it to guide the original network toward higher-quality detail\nsynthesis. Our DG can seamlessly integrate with Classifier-Free Guidance (CFG),\nenabling further refinements of fine-grained details. Extensive experiments\ndemonstrate that our DG consistently improves fine-grained detail quality\nacross various pre-trained DiTs (\\eg, SD3, SD3.5, and Flux).",
        "url": "http://arxiv.org/abs/2510.11538v1",
        "published_date": "2025-10-13T15:39:13+00:00",
        "updated_date": "2025-10-13T15:39:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chaofan Gan",
            "Zicheng Zhao",
            "Yuanpeng Tu",
            "Xi Chen",
            "Ziran Qin",
            "Tieyuan Chen",
            "Mehrtash Harandi",
            "Weiyao Lin"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper explores Massive Activations in Diffusion Transformers for visual generation, proposing a Detail Guidance strategy to enhance local detail quality.",
        "tldr_zh": "本文研究了扩散变压器中的大规模激活，提出了一种详细指导策略来增强本地细节质量。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion Models via Likelihood Preference",
        "summary": "Intuitive physics understanding in video diffusion models plays an essential\nrole in building general-purpose physically plausible world simulators, yet\naccurately evaluating such capacity remains a challenging task due to the\ndifficulty in disentangling physics correctness from visual appearance in\ngeneration. To the end, we introduce LikePhys, a training-free method that\nevaluates intuitive physics in video diffusion models by distinguishing\nphysically valid and impossible videos using the denoising objective as an\nELBO-based likelihood surrogate on a curated dataset of valid-invalid pairs. By\ntesting on our constructed benchmark of twelve scenarios spanning over four\nphysics domains, we show that our evaluation metric, Plausibility Preference\nError (PPE), demonstrates strong alignment with human preference, outperforming\nstate-of-the-art evaluator baselines. We then systematically benchmark\nintuitive physics understanding in current video diffusion models. Our study\nfurther analyses how model design and inference settings affect intuitive\nphysics understanding and highlights domain-specific capacity variations across\nphysical laws. Empirical results show that, despite current models struggling\nwith complex and chaotic dynamics, there is a clear trend of improvement in\nphysics understanding as model capacity and inference settings scale.",
        "url": "http://arxiv.org/abs/2510.11512v1",
        "published_date": "2025-10-13T15:19:07+00:00",
        "updated_date": "2025-10-13T15:19:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jianhao Yuan",
            "Fabio Pizzati",
            "Francesco Pinto",
            "Lars Kunze",
            "Ivan Laptev",
            "Paul Newman",
            "Philip Torr",
            "Daniele De Martini"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper introduces a method called LikePhys to evaluate intuitive physics understanding in video diffusion models, showing strong alignment with human preference and highlighting improvements in physics understanding as model capacity and settings scale.",
        "tldr_zh": "本文介绍了一种名为LikePhys的方法，用于评估视频扩散模型中的直观物理理解，显示出与人类偏好的强烈一致性，并突出了模型容量和设置扩展时对物理理解的改进。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Fast and Scalable Normal Integration using Continuous Components",
        "summary": "Surface normal integration is a fundamental problem in computer vision,\ndealing with the objective of reconstructing a surface from its corresponding\nnormal map. Existing approaches require an iterative global optimization to\njointly estimate the depth of each pixel, which scales poorly to larger normal\nmaps. In this paper, we address this problem by recasting normal integration as\nthe estimation of relative scales of continuous components. By constraining\npixels belonging to the same component to jointly vary their scale, we\ndrastically reduce the number of optimization variables. Our framework includes\na heuristic to accurately estimate continuous components from the start, a\nstrategy to rebalance optimization terms, and a technique to iteratively merge\ncomponents to further reduce the size of the problem. Our method achieves\nstate-of-the-art results on the standard normal integration benchmark in as\nlittle as a few seconds and achieves one-order-of-magnitude speedup over\npixel-level approaches on large-resolution normal maps.",
        "url": "http://arxiv.org/abs/2510.11508v1",
        "published_date": "2025-10-13T15:17:16+00:00",
        "updated_date": "2025-10-13T15:17:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Francesco Milano",
            "Jen Jen Chung",
            "Lionel Ott",
            "Roland Siegwart"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper proposes a method to speed up surface normal integration for computer vision tasks by estimating relative scales of continuous components, achieving state-of-the-art results in seconds.",
        "tldr_zh": "该论文提出了一种方法，通过估计连续组件的相对尺度来加速计算机视觉任务中的表面法线集成，仅几秒钟即可获得最新结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model",
        "summary": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,\nGemini, and Claude Sonnet have demonstrated outstanding performance with\nenormous model sizes reaching hundreds of billions of parameters, they\nsignificantly surpass the limitations in memory, power consumption, and\ncomputing capacity of edge devices such as mobile phones. This paper introduces\nAndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on\nQwen3's LLM and various visual encoders. We comprehensively outline the model\narchitectures, training pipeline, and training data of AndesVL, which achieves\nfirst-tier performance across a wide range of open-source benchmarks, including\nfields such as text-rich image understanding, reasoning and math, multi-image\ncomprehension, general VQA, hallucination mitigation, multilingual\nunderstanding, and GUI-related tasks when compared with state-of-the-art models\nof a similar scale. Furthermore, we introduce a 1+N LoR",
        "url": "http://arxiv.org/abs/2510.11496v1",
        "published_date": "2025-10-13T15:04:38+00:00",
        "updated_date": "2025-10-13T15:04:38+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhiwei Jin",
            "Xiaohui Song",
            "Nan Wang",
            "Yafei Liu",
            "Chao Li",
            "Xin Li",
            "Ruichen Wang",
            "Zhihao Li",
            "Qi Qi",
            "Long Cheng",
            "Dongze Hao",
            "Quanlong Zheng",
            "Yanhao Zhang",
            "Haobo Ji",
            "Jian Ma",
            "Zhitong Zheng",
            "Zhenyi Lin",
            "Haolin Deng",
            "Xin Zou",
            "Xiaojie Yin",
            "Ruilin Wang",
            "Liankai Cai",
            "Haijing Liu",
            "Yuqing Qiu",
            "Ke Chen",
            "Zixian Li",
            "Chi Xie",
            "Huafei Li",
            "Chenxing Li",
            "Chuangchuang Wang",
            "Kai Tang",
            "Zhiguang Zhu",
            "Kai Tang",
            "Wenmei Gao",
            "Rui Wang",
            "Jun Wu",
            "Chao Liu",
            "Qin Xie",
            "Chen Chen",
            "Haonan Lu"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces AndesVL, a suite of mobile-side Large Language Models with 0.6B to 4B parameters that outperform existing models in various tasks.",
        "tldr_zh": "本文介绍了AndesVL，一个具有0.6B至4B参数的移动端大型语言模型套件，优于各种任务中的现有模型。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VA-GS: Enhancing the Geometric Representation of Gaussian Splatting via View Alignment",
        "summary": "3D Gaussian Splatting has recently emerged as an efficient solution for\nhigh-quality and real-time novel view synthesis. However, its capability for\naccurate surface reconstruction remains underexplored. Due to the discrete and\nunstructured nature of Gaussians, supervision based solely on image rendering\nloss often leads to inaccurate geometry and inconsistent multi-view alignment.\nIn this work, we propose a novel method that enhances the geometric\nrepresentation of 3D Gaussians through view alignment (VA). Specifically, we\nincorporate edge-aware image cues into the rendering loss to improve surface\nboundary delineation. To enforce geometric consistency across views, we\nintroduce a visibility-aware photometric alignment loss that models occlusions\nand encourages accurate spatial relationships among Gaussians. To further\nmitigate ambiguities caused by lighting variations, we incorporate normal-based\nconstraints to refine the spatial orientation of Gaussians and improve local\nsurface estimation. Additionally, we leverage deep image feature embeddings to\nenforce cross-view consistency, enhancing the robustness of the learned\ngeometry under varying viewpoints and illumination. Extensive experiments on\nstandard benchmarks demonstrate that our method achieves state-of-the-art\nperformance in both surface reconstruction and novel view synthesis. The source\ncode is available at https://github.com/LeoQLi/VA-GS.",
        "url": "http://arxiv.org/abs/2510.11473v1",
        "published_date": "2025-10-13T14:44:50+00:00",
        "updated_date": "2025-10-13T14:44:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qing Li",
            "Huifang Feng",
            "Xun Gong",
            "Yu-Shen Liu"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a method to enhance the geometric representation of 3D Gaussians for improved surface reconstruction and novel view synthesis, achieving state-of-the-art performance.",
        "tldr_zh": "本文提出了一种方法，通过视图对齐增强3D高斯的几何表示，以改善表面重建和新视图合成的性能，实现了最先进的表现。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Coupled Degradation Modeling and Fusion: A VLM-Guided Degradation-Coupled Network for Degradation-Aware Infrared and Visible Image Fusion",
        "summary": "Existing Infrared and Visible Image Fusion (IVIF) methods typically assume\nhigh-quality inputs. However, when handing degraded images, these methods\nheavily rely on manually switching between different pre-processing techniques.\nThis decoupling of degradation handling and image fusion leads to significant\nperformance degradation. In this paper, we propose a novel VLM-Guided\nDegradation-Coupled Fusion network (VGDCFusion), which tightly couples\ndegradation modeling with the fusion process and leverages vision-language\nmodels (VLMs) for degradation-aware perception and guided suppression.\nSpecifically, the proposed Specific-Prompt Degradation-Coupled Extractor\n(SPDCE) enables modality-specific degradation awareness and establishes a joint\nmodeling of degradation suppression and intra-modal feature extraction. In\nparallel, the Joint-Prompt Degradation-Coupled Fusion (JPDCF) facilitates\ncross-modal degradation perception and couples residual degradation filtering\nwith complementary cross-modal feature fusion. Extensive experiments\ndemonstrate that our VGDCFusion significantly outperforms existing\nstate-of-the-art fusion approaches under various degraded image scenarios. Our\ncode is available at https://github.com/Lmmh058/VGDCFusion.",
        "url": "http://arxiv.org/abs/2510.11456v1",
        "published_date": "2025-10-13T14:26:33+00:00",
        "updated_date": "2025-10-13T14:26:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianpei Zhang",
            "Jufeng Zhao",
            "Yiming Zhu",
            "Guangmang Cui"
        ],
        "ai_categories": [
            "GAN",
            "Other"
        ],
        "tldr": "The paper proposes a novel network for Infrared and Visible Image Fusion that tightly couples degradation modeling with the fusion process, achieving better performance on degraded images.",
        "tldr_zh": "本文提出了一种新颖的网络，用于红外和可见图像融合，紧密耦合降解建模与融合过程，在降解图像上取得更好的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MaterialRefGS: Reflective Gaussian Splatting with Multi-view Consistent Material Inference",
        "summary": "Modeling reflections from 2D images is essential for photorealistic rendering\nand novel view synthesis. Recent approaches enhance Gaussian primitives with\nreflection-related material attributes to enable physically based rendering\n(PBR) with Gaussian Splatting. However, the material inference often lacks\nsufficient constraints, especially under limited environment modeling,\nresulting in illumination aliasing and reduced generalization. In this work, we\nrevisit the problem from a multi-view perspective and show that multi-view\nconsistent material inference with more physically-based environment modeling\nis key to learning accurate reflections with Gaussian Splatting. To this end,\nwe enforce 2D Gaussians to produce multi-view consistent material maps during\ndeferred shading. We also track photometric variations across views to identify\nhighly reflective regions, which serve as strong priors for reflection strength\nterms. To handle indirect illumination caused by inter-object occlusions, we\nfurther introduce an environment modeling strategy through ray tracing with\n2DGS, enabling photorealistic rendering of indirect radiance. Experiments on\nwidely used benchmarks show that our method faithfully recovers both\nillumination and geometry, achieving state-of-the-art rendering quality in\nnovel views synthesis.",
        "url": "http://arxiv.org/abs/2510.11387v1",
        "published_date": "2025-10-13T13:29:20+00:00",
        "updated_date": "2025-10-13T13:29:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenyuan Zhang",
            "Jimin Tang",
            "Weiqi Zhang",
            "Yi Fang",
            "Yu-Shen Liu",
            "Zhizhong Han"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper presents a method for accurate reflection modeling in photorealistic rendering and novel view synthesis through multi-view consistent material inference with Gaussian Splatting.",
        "tldr_zh": "本文提出了一种通过高斯飞溅实现多视角一致性材料推断的方法，用于准确建模光线反射和新视图合成。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Uncertainty-Aware ControlNet: Bridging Domain Gaps with Synthetic Image Generation",
        "summary": "Generative Models are a valuable tool for the controlled creation of\nhigh-quality image data. Controlled diffusion models like the ControlNet have\nallowed the creation of labeled distributions. Such synthetic datasets can\naugment the original training distribution when discriminative models, like\nsemantic segmentation, are trained. However, this augmentation effect is\nlimited since ControlNets tend to reproduce the original training distribution.\n  This work introduces a method to utilize data from unlabeled domains to train\nControlNets by introducing the concept of uncertainty into the control\nmechanism. The uncertainty indicates that a given image was not part of the\ntraining distribution of a downstream task, e.g., segmentation. Thus, two types\nof control are engaged in the final network: an uncertainty control from an\nunlabeled dataset and a semantic control from the labeled dataset. The\nresulting ControlNet allows us to create annotated data with high uncertainty\nfrom the target domain, i.e., synthetic data from the unlabeled distribution\nwith labels. In our scenario, we consider retinal OCTs, where typically\nhigh-quality Spectralis images are available with given ground truth\nsegmentations, enabling the training of segmentation networks. The recent\ndevelopment in Home-OCT devices, however, yields retinal OCTs with lower\nquality and a large domain shift, such that out-of-the-pocket segmentation\nnetworks cannot be applied for this type of data. Synthesizing annotated images\nfrom the Home-OCT domain using the proposed approach closes this gap and leads\nto significantly improved segmentation results without adding any further\nsupervision. The advantage of uncertainty-guidance becomes obvious when\ncompared to style transfer: it enables arbitrary domain shifts without any\nstrict learning of an image style. This is also demonstrated in a traffic scene\nexperiment.",
        "url": "http://arxiv.org/abs/2510.11346v1",
        "published_date": "2025-10-13T12:41:28+00:00",
        "updated_date": "2025-10-13T12:41:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Joshua Niemeijer",
            "Jan Ehrhardt",
            "Heinz Handels",
            "Hristina Uzunova"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "This paper introduces Uncertainty-Aware ControlNet, a method that utilizes data from unlabeled domains to train ControlNets by incorporating uncertainty into the control mechanism, resulting in improved segmentation results without additional supervision.",
        "tldr_zh": "本文介绍了不确定性感知控制网络，一种利用未标记域数据来训练控制网络的方法，通过将不确定性纳入控制机制，在不增加额外监督的情况下改善分割结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "InternSVG: Towards Unified SVG Tasks with Multimodal Large Language Models",
        "summary": "General SVG modeling remains challenging due to fragmented datasets, limited\ntransferability of methods across tasks, and the difficulty of handling\nstructural complexity. In response, we leverage the strong transfer and\ngeneralization capabilities of multimodal large language models (MLLMs) to\nachieve unified modeling for SVG understanding, editing, and generation. We\npresent the InternSVG family, an integrated data-benchmark-model suite. At its\ncore is SAgoge, the largest and most comprehensive multimodal dataset for SVG\ntasks, encompassing both static graphics and dynamic animations. It covers\nicons, long-sequence illustrations, scientific diagrams, and dynamic\nanimations, supporting tasks of varied difficulty levels and providing deeper\nhierarchies with richer attributes compared to previous datasets. Based on this\nresource, we introduce SArena, a companion benchmark with comprehensive task\ndefinitions and standardized evaluation that aligns with the domains and\ndifficulty spectrum covered by SAgoge. Building on these foundations, we\npropose InternSVG, a unified MLLM for SVG understanding, editing, and\ngeneration with SVG-specific special tokens, subword-based embedding\ninitialization, and a two-stage training strategy that progresses from short\nstatic SVGs to long-sequence illustrations and complex animations. This unified\nformulation induces positive transfer and improves overall performance.\nExperiments on SArena and prior benchmark confirm that InternSVG achieves\nsubstantial gains and consistently outperforms leading open and proprietary\ncounterparts.",
        "url": "http://arxiv.org/abs/2510.11341v1",
        "published_date": "2025-10-13T12:38:04+00:00",
        "updated_date": "2025-10-13T12:38:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haomin Wang",
            "Jinhui Yin",
            "Qi Wei",
            "Wenguang Zeng",
            "Lixin Gu",
            "Shenglong Ye",
            "Zhangwei Gao",
            "Yaohui Wang",
            "Yanting Zhang",
            "Yuanqi Li",
            "Yanwen Guo",
            "Wenhai Wang",
            "Kai Chen",
            "Yu Qiao",
            "Hongjie Zhang"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces InternSVG, a unified MLLM for SVG tasks with a comprehensive dataset and benchmark. It outperforms other models in SVG understanding, editing, and generation tasks.",
        "tldr_zh": "该论文介绍了InternSVG，它是一个统一的MLLM，用于SVG任务，提供了一个全面的数据集和基准。在SVG理解、编辑和生成任务中，其表现优于其他模型。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Human Uncertainty-Aware Data Selection and Automatic Labeling in Visual Question Answering",
        "summary": "Large vision-language models (VLMs) achieve strong performance in Visual\nQuestion Answering but still rely heavily on supervised fine-tuning (SFT) with\nmassive labeled datasets, which is costly due to human annotations. Crucially,\nreal-world datasets often exhibit human uncertainty (HU) -- variation in human\nconfidence across annotations -- but standard SFT simply optimizes toward the\nmost frequent label, disregarding HU distributions. This leaves two open\nquestions: How does HU affect SFT, and how can HU be effectively leveraged in\ntraining? In this work, we first conduct a systematic evaluation of VLMs across\nvarying HU levels. We have two key findings: (i) surprisingly, high-HU samples\ncontribute little or even degrade model performance, and (ii) naively training\non the full dataset yields under-calibrated models that fail to capture HU\ndistributions. Motivated by these findings, we introduce HaDola, a human\nuncertainty-aware data selection and automatic labeling framework. HaDola\noperates in four stages -- discriminate, self-annotate, error trigger, and\ntraining -- to iteratively identify harmful samples, prioritize informative\nones, and bootstrap from a small seed set (5\\% of data). Our approach\nsubstantially reduces reliance on costly HU annotations and makes VLMs more\naccurate and better calibrated. Extensive experiments on VQAv2 and VizWiz\ndatasets demonstrate that HaDola consistently matches or outperforms\nstate-of-the-art baselines with less training data. Our work highlights the\nimportance of explicitly modeling HU in SFT, suggesting that better utilization\nof HU is more effective than merely scaling up dataset size.",
        "url": "http://arxiv.org/abs/2510.11295v1",
        "published_date": "2025-10-13T11:35:30+00:00",
        "updated_date": "2025-10-13T11:35:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jian Lan",
            "Zhicheng Liu",
            "Udo Schlegel",
            "Raoyuan Zhao",
            "Yihong Liu",
            "Hinrich Schütze",
            "Michael A. Hedderich",
            "Thomas Seidl"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces HaDola, a framework that leverages human uncertainty in data selection and labeling to improve model performance in Visual Question Answering.",
        "tldr_zh": "本文介绍了HaDola，这是一个利用人类不确定性进行数据选择和标记的框架，旨在提高视觉问答中的模型性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Exploring and Leveraging Class Vectors for Classifier Editing",
        "summary": "Image classifiers play a critical role in detecting diseases in medical\nimaging and identifying anomalies in manufacturing processes. However, their\npredefined behaviors after extensive training make post hoc model editing\ndifficult, especially when it comes to forgetting specific classes or adapting\nto distribution shifts. Existing classifier editing methods either focus\nnarrowly on correcting errors or incur extensive retraining costs, creating a\nbottleneck for flexible editing. Moreover, such editing has seen limited\ninvestigation in image classification. To overcome these challenges, we\nintroduce Class Vectors, which capture class-specific representation\nadjustments during fine-tuning. Whereas task vectors encode task-level changes\nin weight space, Class Vectors disentangle each class's adaptation in the\nlatent space. We show that Class Vectors capture each class's semantic shift\nand that classifier editing can be achieved either by steering latent features\nalong these vectors or by mapping them into weight space to update the decision\nboundaries. We also demonstrate that the inherent linearity and orthogonality\nof Class Vectors support efficient, flexible, and high-level concept editing\nvia simple class arithmetic. Finally, we validate their utility in applications\nsuch as unlearning, environmental adaptation, adversarial defense, and\nadversarial trigger optimization.",
        "url": "http://arxiv.org/abs/2510.11268v1",
        "published_date": "2025-10-13T10:57:51+00:00",
        "updated_date": "2025-10-13T10:57:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jaeik Kim",
            "Jaeyoung Do"
        ],
        "ai_categories": [
            "AIGC",
            "GAN"
        ],
        "tldr": "The paper introduces Class Vectors to facilitate classifier editing in image classification tasks, enabling efficient and flexible concept editing.",
        "tldr_zh": "本文引入了类向量，以促进图像分类任务中的分类器编辑，在实现高效和灵活的概念编辑方面发挥作用。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Large-Language-Model Assisted Automated Scale Bar Detection and Extraction Framework for Scanning Electron Microscopic Images",
        "summary": "Microscopic characterizations, such as Scanning Electron Microscopy (SEM),\nare widely used in scientific research for visualizing and analyzing\nmicrostructures. Determining the scale bars is an important first step of\naccurate SEM analysis; however, currently, it mainly relies on manual\noperations, which is both time-consuming and prone to errors. To address this\nissue, we propose a multi-modal and automated scale bar detection and\nextraction framework that provides concurrent object detection, text detection\nand text recognition with a Large Language Model (LLM) agent. The proposed\nframework operates in four phases; i) Automatic Dataset Generation (Auto-DG)\nmodel to synthesize a diverse dataset of SEM images ensuring robust training\nand high generalizability of the model, ii) scale bar object detection, iii)\ninformation extraction using a hybrid Optical Character Recognition (OCR)\nsystem with DenseNet and Convolutional Recurrent Neural Network (CRNN) based\nalgorithms, iv) an LLM agent to analyze and verify accuracy of the results. The\nproposed model demonstrates a strong performance in object detection and\naccurate localization with a precision of 100%, recall of 95.8%, and a mean\nAverage Precision (mAP) of 99.2% at IoU=0.5 and 69.1% at IoU=0.5:0.95. The\nhybrid OCR system achieved 89% precision, 65% recall, and a 75% F1 score on the\nAuto-DG dataset, significantly outperforming several mainstream standalone\nengines, highlighting its reliability for scientific image analysis. The LLM is\nintroduced as a reasoning engine as well as an intelligent assistant that\nsuggests follow-up steps and verifies the results. This automated method\npowered by an LLM agent significantly enhances the efficiency and accuracy of\nscale bar detection and extraction in SEM images, providing a valuable tool for\nmicroscopic analysis and advancing the field of scientific imaging.",
        "url": "http://arxiv.org/abs/2510.11260v1",
        "published_date": "2025-10-13T10:50:54+00:00",
        "updated_date": "2025-10-13T10:50:54+00:00",
        "categories": [
            "cs.CV",
            "cond-mat.mtrl-sci",
            "cs.AI",
            "physics.data-an"
        ],
        "authors": [
            "Yuxuan Chen",
            "Ruotong Yang",
            "Zhengyang Zhang",
            "Mehreen Ahmed",
            "Yanming Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Multimodality",
            "AIGC"
        ],
        "tldr": "The paper presents an automated framework for detecting and extracting scale bars in SEM images using a Large Language Model, improving efficiency and accuracy.",
        "tldr_zh": "本文提出了一种利用大型语言模型检测和提取SEM图像中比例尺的自动化框架，提高了效率和准确性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LightPneumoNet: Lightweight Pneumonia Classifier",
        "summary": "Effective pneumonia diagnosis is often challenged by the difficulty of\ndeploying large, computationally expensive deep learning models in\nresource-limited settings. This study introduces LightPneumoNet, an efficient,\nlightweight convolutional neural network (CNN) built from scratch to provide an\naccessible and accurate diagnostic solution for pneumonia detection from chest\nX-rays. Our model was trained on a public dataset of 5,856 chest X-ray images.\nPreprocessing included image resizing to 224x224, grayscale conversion, and\npixel normalization, with data augmentation (rotation, zoom, shear) to prevent\noverfitting. The custom architecture features four blocks of stacked\nconvolutional layers and contains only 388,082 trainable parameters, resulting\nin a minimal 1.48 MB memory footprint. On the independent test set, our model\ndelivered exceptional performance, achieving an overall accuracy of 0.942,\nprecision of 0.92, and an F1-Score of 0.96. Critically, it obtained a\nsensitivity (recall) of 0.99, demonstrating a near-perfect ability to identify\ntrue pneumonia cases and minimize clinically significant false negatives.\nNotably, LightPneumoNet achieves this high recall on the same dataset where\nexisting approaches typically require significantly heavier architectures or\nfail to reach comparable sensitivity levels. The model's efficiency enables\ndeployment on low-cost hardware, making advanced computer-aided diagnosis\naccessible in underserved clinics and serving as a reliable second-opinion tool\nto improve patient outcomes.",
        "url": "http://arxiv.org/abs/2510.11232v1",
        "published_date": "2025-10-13T10:14:17+00:00",
        "updated_date": "2025-10-13T10:14:17+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Neilansh Chauhan",
            "Piyush Kumar Gupta",
            "Faraz Doja"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "LightPneumoNet is a lightweight convolutional neural network designed for efficient pneumonia detection from chest X-rays, achieving high accuracy and sensitivity while maintaining a small model size.",
        "tldr_zh": "LightPneumoNet是一种轻量级卷积神经网络，专为从胸部X光片有效地检测肺炎而设计，实现了高准确度和敏感性，同时保持较小的模型尺寸。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Evaluating Reasoning Faithfulness in Medical Vision-Language Models using Multimodal Perturbations",
        "summary": "Vision-language models (VLMs) often produce chain-of-thought (CoT)\nexplanations that sound plausible yet fail to reflect the underlying decision\nprocess, undermining trust in high-stakes clinical use. Existing evaluations\nrarely catch this misalignment, prioritizing answer accuracy or adherence to\nformats. We present a clinically grounded framework for chest X-ray visual\nquestion answering (VQA) that probes CoT faithfulness via controlled text and\nimage modifications across three axes: clinical fidelity, causal attribution,\nand confidence calibration. In a reader study (n=4), evaluator-radiologist\ncorrelations fall within the observed inter-radiologist range for all axes,\nwith strong alignment for attribution (Kendall's $\\tau_b=0.670$), moderate\nalignment for fidelity ($\\tau_b=0.387$), and weak alignment for confidence tone\n($\\tau_b=0.091$), which we report with caution. Benchmarking six VLMs shows\nthat answer accuracy and explanation quality are decoupled, acknowledging\ninjected cues does not ensure grounding, and text cues shift explanations more\nthan visual cues. While some open-source models match final answer accuracy,\nproprietary models score higher on attribution (25.0% vs. 1.4%) and often on\nfidelity (36.1% vs. 31.7%), highlighting deployment risks and the need to\nevaluate beyond final answer accuracy.",
        "url": "http://arxiv.org/abs/2510.11196v1",
        "published_date": "2025-10-13T09:28:22+00:00",
        "updated_date": "2025-10-13T09:28:22+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Johannes Moll",
            "Markus Graf",
            "Tristan Lemke",
            "Nicolas Lenhart",
            "Daniel Truhn",
            "Jean-Benoit Delbrouck",
            "Jiazhen Pan",
            "Daniel Rueckert",
            "Lisa C. Adams",
            "Keno K. Bressem"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a framework to evaluate reasoning faithfulness in medical vision-language models using controlled text and image modifications, highlighting the disconnect between answer accuracy and explanation quality.",
        "tldr_zh": "本文介绍了一个框架，通过控制文本和图像修改来评估医疗视觉语言模型中的推理忠实度，突出了答案准确性和解释质量之间的脱节。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Generalisation of automatic tumour segmentation in histopathological whole-slide images across multiple cancer types",
        "summary": "Deep learning is expected to aid pathologists by automating tasks such as\ntumour segmentation. We aimed to develop one universal tumour segmentation\nmodel for histopathological images and examine its performance in different\ncancer types. The model was developed using over 20 000 whole-slide images from\nover 4 000 patients with colorectal, endometrial, lung, or prostate carcinoma.\nPerformance was validated in pre-planned analyses on external cohorts with over\n3 000 patients across six cancer types. Exploratory analyses included over 1\n500 additional patients from The Cancer Genome Atlas. Average Dice coefficient\nwas over 80% in all validation cohorts with en bloc resection specimens and in\nThe Cancer Genome Atlas cohorts. No loss of performance was observed when\ncomparing the universal model with models specialised on single cancer types.\nIn conclusion, extensive and rigorous evaluations demonstrate that generic\ntumour segmentation by a single model is possible across cancer types, patient\npopulations, sample preparations, and slide scanners.",
        "url": "http://arxiv.org/abs/2510.11182v1",
        "published_date": "2025-10-13T09:18:15+00:00",
        "updated_date": "2025-10-13T09:18:15+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Ole-Johan Skrede",
            "Manohar Pradhan",
            "Maria Xepapadakis Isaksen",
            "Tarjei Sveinsgjerd Hveem",
            "Ljiljana Vlatkovic",
            "Arild Nesbakken",
            "Kristina Lindemann",
            "Gunnar B Kristensen",
            "Jenneke Kasius",
            "Alain G Zeimet",
            "Odd Terje Brustugun",
            "Lill-Tove Rasmussen Busund",
            "Elin H Richardsen",
            "Erik Skaaheim Haug",
            "Bjørn Brennhovd",
            "Emma Rewcastle",
            "Melinda Lillesand",
            "Vebjørn Kvikstad",
            "Emiel Janssen",
            "David J Kerr",
            "Knut Liestøl",
            "Fritz Albregtsen",
            "Andreas Kleppe"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper presents a universal tumour segmentation model for histopathological images across multiple cancer types, achieving high performance without loss of accuracy compared to specialized models.",
        "tldr_zh": "该论文提出了一个针对多种癌症类型的组织病理图像的通用肿瘤分割模型，在不损失精度的情况下，实现了高性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "BLEnD-Vis: Benchmarking Multimodal Cultural Understanding in Vision Language Models",
        "summary": "As vision-language models (VLMs) are deployed globally, their ability to\nunderstand culturally situated knowledge becomes essential. Yet, existing\nevaluations largely assess static recall or isolated visual grounding, leaving\nunanswered whether VLMs possess robust and transferable cultural understanding.\nWe introduce BLEnD-Vis, a multimodal, multicultural benchmark designed to\nevaluate the robustness of everyday cultural knowledge in VLMs across\nlinguistic rephrasings and visual modalities. Building on the BLEnD dataset,\nBLEnD-Vis constructs 313 culturally grounded question templates spanning 16\nregions and generates three aligned multiple-choice formats: (i) a text-only\nbaseline querying from Region $\\to$ Entity, (ii) an inverted text-only variant\n(Entity $\\to$ Region), and (iii) a VQA-style version of (ii) with generated\nimages. The resulting benchmark comprises 4,916 images and over 21,000\nmultiple-choice question (MCQ) instances, validated through human annotation.\nBLEnD-Vis reveals significant fragility in current VLM cultural knowledge;\nmodels exhibit performance drops under linguistic rephrasing and, whilst visual\ncues often aid performance, low cross-modal consistency highlights challenges\nin robustly integrating textual and visual understanding, particularly for\nlower-resource regions. BLEnD-Vis thus provides a crucial testbed for\nsystematically analysing cultural robustness and multimodal grounding, exposing\nlimitations and guiding the development of more culturally competent VLMs.",
        "url": "http://arxiv.org/abs/2510.11178v1",
        "published_date": "2025-10-13T09:10:05+00:00",
        "updated_date": "2025-10-13T09:10:05+00:00",
        "categories": [
            "cs.CV",
            "cs.CY"
        ],
        "authors": [
            "Bryan Chen Zhengyu Tan",
            "Zheng Weihua",
            "Zhengyuan Liu",
            "Nancy F. Chen",
            "Hwaran Lee",
            "Kenny Tsu Wei Choo",
            "Roy Ka-Wei Lee"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "BLEnD-Vis introduces a benchmark to evaluate cultural understanding in vision-language models, revealing weaknesses in current models and providing insights for development.",
        "tldr_zh": "BLEnD-Vis提出了一个基准来评估视觉语言模型中的文化理解，在当前模型中揭示了弱点并为发展提供了见解。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation Models via Knowledge Distillation",
        "summary": "Recent studies in pathology foundation models have shown that scaling\ntraining data, diversifying cancer types, and increasing model size\nconsistently improve their performance. However, giga-scale foundation models,\nwhich are trained on hundreds of thousands of slides covering tens of cancer\ntypes and contain billions of parameters, pose significant challenges for\npractical use due to their tremendous computational costs in both development\nand deployment. In this work, we present a novel strategy, named the G2L\nframework, to increase the performance of large-scale foundation models, which\nconsist of only $15\\%$ of the parameters of giga-scale models, to a comparable\nperformance level of giga-scale models in cancer-specific tasks. Our approach\napplies knowledge distillation, transferring the capabilities of a giga-scale\nmodel to a large-scale model, using just 1K pathology slides of a target cancer\n(e.g., breast, prostate, etc.). The resulting distilled model not only\noutperformed state-of-the-art models of the same size (i.e., large-scale)\nacross several benchmarks but also, interestingly, surpassed the giga-scale\nteacher and huge-scale models in some benchmarks. In addition, the distilled\nmodel exhibited a higher robustness index, indicating improved resilience to\nimage variations originating from multiple institutions. These findings suggest\nthat the proposed distillation approach for a large-scale model is a data- and\nparameter-efficient way to achieve giga-scale-level performance for\ncancer-specific applications without prohibitive computational burden.",
        "url": "http://arxiv.org/abs/2510.11176v1",
        "published_date": "2025-10-13T09:08:59+00:00",
        "updated_date": "2025-10-13T09:08:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yesung Cho",
            "Sungmin Lee",
            "Geongyu Lee",
            "Minkyung Lee",
            "Jongbae Park",
            "Dongmyung Shin"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper proposes a novel method, G2L, to distill knowledge from giga-scale pathology models to large-scale models for better performance in cancer-specific tasks using just 1K slides.",
        "tldr_zh": "本文提出了一种新方法，G2L，通过只使用1000张幻灯片，从基因组规模病理模型中提炼知识到大规模模型，以在癌症特定任务中获得更好的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Reliable Cross-modal Alignment via Prototype Iterative Construction",
        "summary": "Cross-modal alignment is an important multi-modal task, aiming to bridge the\nsemantic gap between different modalities. The most reliable fundamention for\nachieving this objective lies in the semantic consistency between matched\npairs. Conventional methods implicitly assume embeddings contain solely\nsemantic information, ignoring the impact of non-semantic information during\nalignment, which inevitably leads to information bias or even loss. These\nnon-semantic information primarily manifest as stylistic variations in the\ndata, which we formally define as style information. An intuitive approach is\nto separate style from semantics, aligning only the semantic information.\nHowever, most existing methods distinguish them based on feature columns, which\ncannot represent the complex coupling relationship between semantic and style\ninformation. In this paper, we propose PICO, a novel framework for suppressing\nstyle interference during embedding interaction. Specifically, we quantify the\nprobability of each feature column representing semantic information, and\nregard it as the weight during the embedding interaction. To ensure the\nreliability of the semantic probability, we propose a prototype iterative\nconstruction method. The key operation of this method is a performance\nfeedback-based weighting function, and we have theoretically proven that the\nfunction can assign higher weight to prototypes that bring higher performance\nimprovements. Extensive experiments on various benchmarks and model backbones\ndemonstrate the superiority of PICO, outperforming state-of-the-art methods by\n5.2\\%-14.1\\%.",
        "url": "http://arxiv.org/abs/2510.11175v1",
        "published_date": "2025-10-13T09:08:27+00:00",
        "updated_date": "2025-10-13T09:08:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiang Ma",
            "Litian Xu",
            "Lexin Fang",
            "Caiming Zhang",
            "Lizhen Cui"
        ],
        "ai_categories": [
            "Multimodality"
        ],
        "tldr": "The paper introduces a new framework called PICO to suppress style interference during cross-modal alignment by quantifying semantic probability and using prototype iterative construction.",
        "tldr_zh": "本文介绍了一种名为PICO的新框架，通过量化语义概率和使用原型迭代构建来抑制跨模态对齐过程中的风格干扰。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CoPRS: Learning Positional Prior from Chain-of-Thought for Reasoning Segmentation",
        "summary": "Existing works on reasoning segmentation either connect hidden features from\na language model directly to a mask decoder or represent positions in text,\nwhich limits interpretability and semantic detail. To solve this, we present\nCoPRS, a Multi-modal Chain-of-Thought (MCoT)-based positional perception model\nthat bridges language reasoning to segmentation through a differentiable and\ninterpretable positional prior instantiated as a heatmap. By making the\nreasoning process clear via MCoT and expressing it as a dense, differentiable\nheatmap, this interface enhances interpretability and diagnostic analysis and\nyields more concentrated evidence on the target. A learnable concentration\ntoken aggregates features of the image and reasoning text to generate this\npositional prior, which is decoded to precise masks through a lightweight\ndecoder, providing a direct connection between reasoning and segmentation.\nAcross the RefCOCO series and ReasonSeg, CoPRS matches or surpasses the best\nreported metrics on each standard split under comparable protocols, with\nperformance at or above prior state of the art across both validation and test\npartitions. Extensive experiments reveal that the quality of the heatmap\nstrongly influences the resulting mask quality, supporting a consistent\nassociation between the reasoning output and downstream mask generation.\nCollectively, these findings support the utility of this paradigm in bridging\nreasoning and segmentation and show advantages in concentration driven by\nreasoning and predicting masks more precisely. Code, checkpoints and logs are\nreleased at https://github.com/ZhenyuLU-Heliodore/CoPRS.git.",
        "url": "http://arxiv.org/abs/2510.11173v1",
        "published_date": "2025-10-13T09:07:54+00:00",
        "updated_date": "2025-10-13T09:07:54+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Zhenyu Lu",
            "Liupeng Li",
            "Jinpeng Wang",
            "Yan Feng",
            "Bin Chen",
            "Ke Chen",
            "Yaowei Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "CoPRS introduces a positional perception model that bridges language reasoning to segmentation through a differentiable heatmap, improving interpretability and diagnostic analysis in reasoning segmentation tasks.",
        "tldr_zh": "CoPRS引入了一个位置感知模型，通过可微的热图将语言推理与分割相连接，提高了推理分割任务中的可解释性和诊断分析。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "video-SALMONN S: Streaming Audio-Visual LLMs Beyond Length Limits via Memory",
        "summary": "Continuous, high-frame-rate, high-resolution processing of long video streams\nis critical for future AI agents, yet current video-understanding LLMs struggle\nto scale. Offline, fixed-frame-number methods require the stream length to\nadapt frame rates; streaming methods constrain memory by merging or discarding\ntokens, losing information. We propose video-SALMONN S, a streaming\naudio-visual LLM that, to our knowledge, is the first to process 3-hour videos\nat 1 FPS and 360p resolution under a fixed memory budget. Our model introduces\n(i) a test-time-training (TTT) memory module that continually updates token\nrepresentations to capture long-range dependencies by replacing token merging,\nand (ii) a prompt-dependent memory reader that selectively retrieves\ncontext-relevant content from fixed-size memory. The TTT module is optimised\nwith a Hessian-free conjugate-gradient procedure (TTT_HF) for efficient\nadaptation. On long-video benchmarks (Video-MME, LVBench, VideoEvalPro),\nvideo-SALMONN S sustains high-quality understanding on multi-hour videos with\n10k frames and 1M tokens. Our 8B-parameter model achieves 74.2% overall and\n67.8% on the Video-MME long split, outperforming both offline and streaming\nbaselines.",
        "url": "http://arxiv.org/abs/2510.11129v1",
        "published_date": "2025-10-13T08:20:15+00:00",
        "updated_date": "2025-10-13T08:20:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Guangzhi Sun",
            "Yixuan Li",
            "Xiaodong Wu",
            "Yudong Yang",
            "Wei Li",
            "Zejun Ma",
            "Chao Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper proposes a streaming audio-visual model that processes long video streams efficiently under a fixed memory budget.",
        "tldr_zh": "本文提出了一种流式音视频模型，能够在固定内存预算下高效处理长视频流。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Lightweight Facial Landmark Detection in Thermal Images via Multi-Level Cross-Modal Knowledge Transfer",
        "summary": "Facial Landmark Detection (FLD) in thermal imagery is critical for\napplications in challenging lighting conditions, but it is hampered by the lack\nof rich visual cues. Conventional cross-modal solutions, like feature fusion or\nimage translation from RGB data, are often computationally expensive or\nintroduce structural artifacts, limiting their practical deployment. To address\nthis, we propose Multi-Level Cross-Modal Knowledge Distillation (MLCM-KD), a\nnovel framework that decouples high-fidelity RGB-to-thermal knowledge transfer\nfrom model compression to create both accurate and efficient thermal FLD\nmodels. A central challenge during knowledge transfer is the profound modality\ngap between RGB and thermal data, where traditional unidirectional distillation\nfails to enforce semantic consistency across disparate feature spaces. To\novercome this, we introduce Dual-Injected Knowledge Distillation (DIKD), a\nbidirectional mechanism designed specifically for this task. DIKD establishes a\nconnection between modalities: it not only guides the thermal student with rich\nRGB features but also validates the student's learned representations by\nfeeding them back into the frozen teacher's prediction head. This closed-loop\nsupervision forces the student to learn modality-invariant features that are\nsemantically aligned with the teacher, ensuring a robust and profound knowledge\ntransfer. Experiments show that our approach sets a new state-of-the-art on\npublic thermal FLD benchmarks, notably outperforming previous methods while\ndrastically reducing computational overhead.",
        "url": "http://arxiv.org/abs/2510.11128v1",
        "published_date": "2025-10-13T08:19:56+00:00",
        "updated_date": "2025-10-13T08:19:56+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Qiyi Tong",
            "Olivia Nocentini",
            "Marta Lagomarsino",
            "Kuanqi Cai",
            "Marta Lorenzini",
            "Arash Ajoudani"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel framework called MLCM-KD for lightweight facial landmark detection in thermal images, outperforming previous methods while reducing computational overhead.",
        "tldr_zh": "该论文提出了一种名为MLCM-KD的轻量级热成像人脸标记检测框架，优于先前的方法，同时减少了计算开销。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Demystifying Numerosity in Diffusion Models -- Limitations and Remedies",
        "summary": "Numerosity remains a challenge for state-of-the-art text-to-image generation\nmodels like FLUX and GPT-4o, which often fail to accurately follow counting\ninstructions in text prompts. In this paper, we aim to study a fundamental yet\noften overlooked question: Can diffusion models inherently generate the correct\nnumber of objects specified by a textual prompt simply by scaling up the\ndataset and model size? To enable rigorous and reproducible evaluation, we\nconstruct a clean synthetic numerosity benchmark comprising two complementary\ndatasets: GrayCount250 for controlled scaling studies, and NaturalCount6\nfeaturing complex naturalistic scenes. Second, we empirically show that the\nscaling hypothesis does not hold: larger models and datasets alone fail to\nimprove counting accuracy on our benchmark. Our analysis identifies a key\nreason: diffusion models tend to rely heavily on the noise initialization\nrather than the explicit numerosity specified in the prompt. We observe that\nnoise priors exhibit biases toward specific object counts. In addition, we\npropose an effective strategy for controlling numerosity by injecting\ncount-aware layout information into the noise prior. Our method achieves\nsignificant gains, improving accuracy on GrayCount250 from 20.0\\% to 85.3\\% and\non NaturalCount6 from 74.8\\% to 86.3\\%, demonstrating effective generalization\nacross settings.",
        "url": "http://arxiv.org/abs/2510.11117v1",
        "published_date": "2025-10-13T08:07:24+00:00",
        "updated_date": "2025-10-13T08:07:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yaqi Zhao",
            "Xiaochen Wang",
            "Li Dong",
            "Wentao Zhang",
            "Yuhui Yuan"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper investigates the challenge of accurately generating the correct number of objects in text-to-image models. It introduces a synthetic benchmark and proposes a method to improve counting accuracy by controlling numerosity using count-aware layout information.",
        "tldr_zh": "本文研究了在文本到图像模型中准确生成正确数量对象的难题。它引入了一个合成基准，并提出了一种通过使用计数感知布局信息来改善计数准确性的方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Connecting Giants: Synergistic Knowledge Transfer of Large Multimodal Models for Few-Shot Learning",
        "summary": "Few-shot learning (FSL) addresses the challenge of classifying novel classes\nwith limited training samples. While some methods leverage semantic knowledge\nfrom smaller-scale models to mitigate data scarcity, these approaches often\nintroduce noise and bias due to the data's inherent simplicity. In this paper,\nwe propose a novel framework, Synergistic Knowledge Transfer (SynTrans), which\neffectively transfers diverse and complementary knowledge from large multimodal\nmodels to empower the off-the-shelf few-shot learner. Specifically, SynTrans\nemploys CLIP as a robust teacher and uses a few-shot vision encoder as a weak\nstudent, distilling semantic-aligned visual knowledge via an unsupervised proxy\ntask. Subsequently, a training-free synergistic knowledge mining module\nfacilitates collaboration among large multimodal models to extract high-quality\nsemantic knowledge. Building upon this, a visual-semantic bridging module\nenables bi-directional knowledge transfer between visual and semantic spaces,\ntransforming explicit visual and implicit semantic knowledge into\ncategory-specific classifier weights. Finally, SynTrans introduces a visual\nweight generator and a semantic weight reconstructor to adaptively construct\noptimal multimodal FSL classifiers. Experimental results on four FSL datasets\ndemonstrate that SynTrans, even when paired with a simple few-shot vision\nencoder, significantly outperforms current state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2510.11115v1",
        "published_date": "2025-10-13T08:06:23+00:00",
        "updated_date": "2025-10-13T08:06:23+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Hao Tang",
            "Shengfeng He",
            "Jing Qin"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes a framework called SynTrans for transferring knowledge from large multimodal models to improve few-shot learning, achieving better results compared to current methods.",
        "tldr_zh": "本文提出了一种名为SynTrans的框架，用于从大型多模态模型中转移知识，以改进少样本学习，在结果上比目前的方法表现更好。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Multimodal Disease Progression Modeling via Spatiotemporal Disentanglement and Multiscale Alignment",
        "summary": "Longitudinal multimodal data, including electronic health records (EHR) and\nsequential chest X-rays (CXRs), is critical for modeling disease progression,\nyet remains underutilized due to two key challenges: (1) redundancy in\nconsecutive CXR sequences, where static anatomical regions dominate over\nclinically-meaningful dynamics, and (2) temporal misalignment between sparse,\nirregular imaging and continuous EHR data. We introduce $\\texttt{DiPro}$, a\nnovel framework that addresses these challenges through region-aware\ndisentanglement and multi-timescale alignment. First, we disentangle static\n(anatomy) and dynamic (pathology progression) features in sequential CXRs,\nprioritizing disease-relevant changes. Second, we hierarchically align these\nstatic and dynamic CXR features with asynchronous EHR data via local (pairwise\ninterval-level) and global (full-sequence) synchronization to model coherent\nprogression pathways. Extensive experiments on the MIMIC dataset demonstrate\nthat $\\texttt{DiPro}$ could effectively extract temporal clinical dynamics and\nachieve state-of-the-art performance on both disease progression identification\nand general ICU prediction tasks.",
        "url": "http://arxiv.org/abs/2510.11112v1",
        "published_date": "2025-10-13T08:02:36+00:00",
        "updated_date": "2025-10-13T08:02:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chen Liu",
            "Wenfang Yao",
            "Kejing Yin",
            "William K. Cheung",
            "Jing Qin"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces DiPro, a framework for modeling disease progression using multimodal data like electronic health records and sequential chest X-rays. It prioritizes disease-relevant changes and aligns static and dynamic features to model coherent progression pathways.",
        "tldr_zh": "本文介绍了DiPro，这是一个利用多模态数据如电子健康记录和序列胸部X射线来建模疾病进展的框架。它优先考虑与疾病相关的变化，并对静态和动态特征进行对齐，以建模连贯的进展路径。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MoMaps: Semantics-Aware Scene Motion Generation with Motion Maps",
        "summary": "This paper addresses the challenge of learning semantically and functionally\nmeaningful 3D motion priors from real-world videos, in order to enable\nprediction of future 3D scene motion from a single input image. We propose a\nnovel pixel-aligned Motion Map (MoMap) representation for 3D scene motion,\nwhich can be generated from existing generative image models to facilitate\nefficient and effective motion prediction. To learn meaningful distributions\nover motion, we create a large-scale database of MoMaps from over 50,000 real\nvideos and train a diffusion model on these representations. Our motion\ngeneration not only synthesizes trajectories in 3D but also suggests a new\npipeline for 2D video synthesis: first generate a MoMap, then warp an image\naccordingly and complete the warped point-based renderings. Experimental\nresults demonstrate that our approach generates plausible and semantically\nconsistent 3D scene motion.",
        "url": "http://arxiv.org/abs/2510.11107v1",
        "published_date": "2025-10-13T07:56:19+00:00",
        "updated_date": "2025-10-13T07:56:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahui Lei",
            "Kyle Genova",
            "George Kopanas",
            "Noah Snavely",
            "Leonidas Guibas"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Diffusion"
        ],
        "tldr": "This paper introduces MoMaps, a representation for 3D scene motion to predict future motion from a single image. It demonstrates generating realistic 3D scene motion and suggests a pipeline for 2D video synthesis using MoMaps.",
        "tldr_zh": "本文提出了MoMaps，这是一种用于表示3D场景运动的方法，可以从单个图像预测未来的运动。它展示了生成逼真的3D场景运动，并提出了使用MoMaps进行2D视频合成的流程。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Compositional Zero-Shot Learning: A Survey",
        "summary": "Compositional Zero-Shot Learning (CZSL) is a critical task in computer vision\nthat enables models to recognize unseen combinations of known attributes and\nobjects during inference, addressing the combinatorial challenge of requiring\ntraining data for every possible composition. This is particularly challenging\nbecause the visual appearance of primitives is highly contextual; for example,\n``small'' cats appear visually distinct from ``older'' ones, and ``wet'' cars\ndiffer significantly from ``wet'' cats. Effectively modeling this contextuality\nand the inherent compositionality is crucial for robust compositional zero-shot\nrecognition. This paper presents, to our knowledge, the first comprehensive\nsurvey specifically focused on Compositional Zero-Shot Learning. We\nsystematically review the state-of-the-art CZSL methods, introducing a taxonomy\ngrounded in disentanglement, with four families of approaches: no explicit\ndisentanglement, textual disentanglement, visual disentanglement, and\ncross-modal disentanglement. We provide a detailed comparative analysis of\nthese methods, highlighting their core advantages and limitations in different\nproblem settings, such as closed-world and open-world CZSL. Finally, we\nidentify the most significant open challenges and outline promising future\nresearch directions. This survey aims to serve as a foundational resource to\nguide and inspire further advancements in this fascinating and important field.\nPapers studied in this survey with their official code are available on our\ngithub: https://github.com/ans92/Compositional-Zero-Shot-Learning",
        "url": "http://arxiv.org/abs/2510.11106v1",
        "published_date": "2025-10-13T07:54:47+00:00",
        "updated_date": "2025-10-13T07:54:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ans Munir",
            "Faisal Z. Qureshi",
            "Mohsen Ali",
            "Muhammad Haris Khan"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper presents a survey on Compositional Zero-Shot Learning (CZSL) in computer vision, exploring different disentanglement approaches and identifying open challenges for future research.",
        "tldr_zh": "这篇论文介绍了关于计算机视觉中的组合零样本学习（CZSL）的调查，探索了不同的解缠方法，并确定了未来研究的挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CoDefend: Cross-Modal Collaborative Defense via Diffusion Purification and Prompt Optimization",
        "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\ntasks such as image captioning, visual question answering, and cross-modal\nreasoning by integrating visual and textual modalities. However, their\nmultimodal nature also exposes them to adversarial threats, where attackers can\nperturb either modality or both jointly to induce harmful, misleading, or\npolicy violating outputs. Existing defense strategies, such as adversarial\ntraining and input purification, face notable limitations: adversarial training\ntypically improves robustness only against known attacks while incurring high\ncomputational costs, whereas conventional purification approaches often suffer\nfrom degraded image quality and insufficient generalization to complex\nmultimodal tasks.\n  In this work, we focus on defending the visual modality, which frequently\nserves as the primary entry point for adversarial manipulation. We propose a\nsupervised diffusion based denoising framework that leverages paired\nadversarial clean image datasets to fine-tune diffusion models with\ndirectional, task specific guidance. Unlike prior unsupervised purification\nmethods such as DiffPure, our approach achieves higher quality reconstructions\nwhile significantly improving defense robustness in multimodal tasks.\nFurthermore, we incorporate prompt optimization as a complementary defense\nmechanism, enhancing resistance against diverse and unseen attack strategies.\n  Extensive experiments on image captioning and visual question answering\ndemonstrate that our method not only substantially improves robustness but also\nexhibits strong transferability to unknown adversarial attacks. These results\nhighlight the effectiveness of supervised diffusion based denoising for\nmultimodal defense, paving the way for more reliable and secure deployment of\nMLLMs in real world applications.",
        "url": "http://arxiv.org/abs/2510.11096v1",
        "published_date": "2025-10-13T07:44:54+00:00",
        "updated_date": "2025-10-13T07:44:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fengling Zhu",
            "Boshi Liu",
            "Jingyu Hua",
            "Sheng Zhong"
        ],
        "ai_categories": [
            "Multimodality",
            "Diffusion"
        ],
        "tldr": "The paper proposes a defense mechanism for multimodal large language models to protect against adversarial attacks by improving the quality of reconstructions and enhancing resistance to diverse attack strategies.",
        "tldr_zh": "本文提出了一种针对多模态大型语言模型的防御机制，通过提高重构质量和增强对多样攻击策略的抵抗力来保护免受对抗性攻击。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer",
        "summary": "Patient face images provide a convenient mean for evaluating eye diseases,\nwhile also raising privacy concerns. Here, we introduce ROFI, a deep\nlearning-based privacy protection framework for ophthalmology. Using weakly\nsupervised learning and neural identity translation, ROFI anonymizes facial\nfeatures while retaining disease features (over 98\\% accuracy, $\\kappa >\n0.90$). It achieves 100\\% diagnostic sensitivity and high agreement ($\\kappa >\n0.90$) across eleven eye diseases in three cohorts, anonymizing over 95\\% of\nimages. ROFI works with AI systems, maintaining original diagnoses ($\\kappa >\n0.80$), and supports secure image reversal (over 98\\% similarity), enabling\naudits and long-term care. These results show ROFI's effectiveness of\nprotecting patient privacy in the digital medicine era.",
        "url": "http://arxiv.org/abs/2510.11073v1",
        "published_date": "2025-10-13T07:12:23+00:00",
        "updated_date": "2025-10-13T07:12:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuan Tian",
            "Min Zhou",
            "Yitong Chen",
            "Fang Li",
            "Lingzi Qi",
            "Shuo Wang",
            "Xieyang Xu",
            "Yu Yu",
            "Shiqiong Xu",
            "Chaoyu Lei",
            "Yankai Jiang",
            "Rongzhao Zhang",
            "Jia Tan",
            "Li Wu",
            "Hong Chen",
            "Xiaowei Liu",
            "Wei Lu",
            "Lin Li",
            "Huifang Zhou",
            "Xuefei Song",
            "Guangtao Zhai",
            "Xianqun Fan"
        ],
        "ai_categories": [
            "GAN",
            "AIGC"
        ],
        "tldr": "ROFI is a deep learning-based framework for anonymizing patient face images in ophthalmology while preserving disease features and supporting secure image reversal.",
        "tldr_zh": "ROFI是一个基于深度学习的框架，用于在眼科医学中匿名化患者面部图像，同时保留疾病特征并支持安全的图像反转。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Zero-shot Face Editing via ID-Attribute Decoupled Inversion",
        "summary": "Recent advancements in text-guided diffusion models have shown promise for\ngeneral image editing via inversion techniques, but often struggle to maintain\nID and structural consistency in real face editing tasks. To address this\nlimitation, we propose a zero-shot face editing method based on ID-Attribute\nDecoupled Inversion. Specifically, we decompose the face representation into ID\nand attribute features, using them as joint conditions to guide both the\ninversion and the reverse diffusion processes. This allows independent control\nover ID and attributes, ensuring strong ID preservation and structural\nconsistency while enabling precise facial attribute manipulation. Our method\nsupports a wide range of complex multi-attribute face editing tasks using only\ntext prompts, without requiring region-specific input, and operates at a speed\ncomparable to DDIM inversion. Comprehensive experiments demonstrate its\npracticality and effectiveness.",
        "url": "http://arxiv.org/abs/2510.11050v1",
        "published_date": "2025-10-13T06:34:40+00:00",
        "updated_date": "2025-10-13T06:34:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yang Hou",
            "Minggu Wang",
            "Jianjun Zhao"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a zero-shot face editing method using ID-Attribute Decoupled Inversion, allowing independent control over ID and attributes for precise facial editing.",
        "tldr_zh": "本文提出了一种零击中人脸编辑方法，使用ID-属性解耦反转，允许独立控制ID和属性以进行精细的面部编辑。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "COCO-Tree: Compositional Hierarchical Concept Trees for Enhanced Reasoning in Vision Language Models",
        "summary": "Compositional reasoning remains a persistent weakness of modern vision\nlanguage models (VLMs): they often falter when a task hinges on understanding\nhow multiple objects, attributes, and relations interact within an image.\nMultiple research works have attempted to improve compositionality performance\nby creative tricks such as improving prompt structure, chain of thought\nreasoning, etc. A more recent line of work attempts to impart additional\nreasoning in VLMs using well-trained Large Language Models (LLMs), which are\nfar superior in linguistic understanding than VLMs to compensate for the\nlimited linguistic prowess of VLMs. However, these approaches are either\nresource-intensive or do not provide an interpretable reasoning process. In\nthis paper, we present 'COCO-Tree' - a novel approach that augments VLM outputs\nwith carefully designed neurosymbolic concept trees learned from LLMs to\nimprove VLM's linguistic reasoning. COCO-Tree's beam search-inspired reasoning\nprocess boosts compositionality performance and provides a rationale behind VLM\npredictions. Empirical results on four compositionality benchmarks, Winoground,\nEqBench, ColorSwap, and SugarCrepe, in seven different open-source VLMs with\nvarying sizes, demonstrate that COCO-Tree significantly improves compositional\ngeneralization by 5-10% over baselines.",
        "url": "http://arxiv.org/abs/2510.11012v1",
        "published_date": "2025-10-13T05:07:13+00:00",
        "updated_date": "2025-10-13T05:07:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sanchit Sinha",
            "Guangzhi Xiong",
            "Aidong Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "AIGC"
        ],
        "tldr": "The paper introduces COCO-Tree, a novel approach that enhances vision language models' compositional reasoning by incorporating neurosymbolic concept trees learned from Large Language Models.",
        "tldr_zh": "本文介绍了COCO-Tree，这是一种通过将从大型语言模型中学习到的神经符号概念树集成到其中，来增强视觉语言模型的组成推理能力的新方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Frequency Domain Unlocks New Perspectives for Abdominal Medical Image Segmentation",
        "summary": "Accurate segmentation of tumors and adjacent normal tissues in medical images\nis essential for surgical planning and tumor staging. Although foundation\nmodels generally perform well in segmentation tasks, they often struggle to\nfocus on foreground areas in complex, low-contrast backgrounds, where some\nmalignant tumors closely resemble normal organs, complicating contextual\ndifferentiation. To address these challenges, we propose the Foreground-Aware\nSpectrum Segmentation (FASS) framework. First, we introduce a foreground-aware\nmodule to amplify the distinction between background and the entire volume\nspace, allowing the model to concentrate more effectively on target areas.\nNext, a feature-level frequency enhancement module, based on wavelet transform,\nextracts discriminative high-frequency features to enhance boundary recognition\nand detail perception. Eventually, we introduce an edge constraint module to\npreserve geometric continuity in segmentation boundaries. Extensive experiments\non multiple medical datasets demonstrate superior performance across all\nmetrics, validating the effectiveness of our framework, particularly in\nrobustness under complex conditions and fine structure recognition. Our\nframework significantly enhances segmentation of low-contrast images, paving\nthe way for applications in more diverse and complex medical imaging scenarios.",
        "url": "http://arxiv.org/abs/2510.11005v1",
        "published_date": "2025-10-13T04:44:43+00:00",
        "updated_date": "2025-10-13T04:44:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kai Han",
            "Siqi Ma",
            "Chengxuan Qian",
            "Jun Chen",
            "Chongwen Lyu",
            "Yuqing Song",
            "Zhe Liu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a new framework, FASS, for abdominal medical image segmentation, emphasizing foreground areas and utilizing frequency domain features for better accuracy in tumor identification.",
        "tldr_zh": "本文介绍了一种新的框架FASS，用于腹部医学图像分割，强调前景区域，并利用频域特征更准确地识别肿瘤。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "On the Optimal Representation Efficiency of Barlow Twins: An Information-Geometric Interpretation",
        "summary": "Self-supervised learning (SSL) has achieved remarkable success by learning\nmeaningful representations without labeled data. However, a unified theoretical\nframework for understanding and comparing the efficiency of different SSL\nparadigms remains elusive. In this paper, we introduce a novel\ninformation-geometric framework to quantify representation efficiency. We\ndefine representation efficiency $\\eta$ as the ratio between the effective\nintrinsic dimension of the learned representation space and its ambient\ndimension, where the effective dimension is derived from the spectral\nproperties of the Fisher Information Matrix (FIM) on the statistical manifold\ninduced by the encoder. Within this framework, we present a theoretical\nanalysis of the Barlow Twins method. Under specific but natural assumptions, we\nprove that Barlow Twins achieves optimal representation efficiency ($\\eta = 1$)\nby driving the cross-correlation matrix of representations towards the identity\nmatrix, which in turn induces an isotropic FIM. This work provides a rigorous\ntheoretical foundation for understanding the effectiveness of Barlow Twins and\noffers a new geometric perspective for analyzing SSL algorithms.",
        "url": "http://arxiv.org/abs/2510.10980v1",
        "published_date": "2025-10-13T03:41:27+00:00",
        "updated_date": "2025-10-13T03:41:27+00:00",
        "categories": [
            "cs.LG",
            "cs.CV",
            "cs.IT",
            "math.IT",
            "math.ST",
            "stat.ML",
            "stat.TH",
            "68T07, 62B11, 94A17, 53B12",
            "I.2.6; I.5.1; G.3; H.1.1"
        ],
        "authors": [
            "Di Zhang"
        ],
        "ai_categories": [
            "Other"
        ],
        "tldr": "The paper introduces an information-geometric framework to quantify representation efficiency in self-supervised learning, focusing on the Barlow Twins method.",
        "tldr_zh": "本文引入了一个信息几何框架来量化自监督学习中的表示效率，着重介绍了Barlow Twins方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Distribution-Shift Uncertainty Estimation for Inverse Problems with Generative Priors",
        "summary": "Generative models have shown strong potential as data-driven priors for\nsolving inverse problems such as reconstructing medical images from\nundersampled measurements. While these priors improve reconstruction quality\nwith fewer measurements, they risk hallucinating features when test images lie\noutside the training distribution. Existing uncertainty quantification methods\nin this setting (i) require an in-distribution calibration dataset, which may\nnot be available, (ii) provide heuristic rather than statistical estimates, or\n(iii) quantify uncertainty from model capacity or limited measurements rather\nthan distribution shift. We propose an instance-level, calibration-free\nuncertainty indicator that is sensitive to distribution shift, requires no\nknowledge of the training distribution, and incurs no retraining cost. Our key\nhypothesis is that reconstructions of in-distribution images remain stable\nunder random measurement variations, while reconstructions of\nout-of-distribution (OOD) images exhibit greater instability. We use this\nstability as a proxy for detecting distribution shift. Our proposed OOD\nindicator is efficiently computable for any computational imaging inverse\nproblem; we demonstrate it on tomographic reconstruction of MNIST digits, where\na learned proximal network trained only on digit \"0\" is evaluated on all ten\ndigits. Reconstructions of OOD digits show higher variability and\ncorrespondingly higher reconstruction error, validating this indicator. These\nresults suggest a deployment strategy that pairs generative priors with\nlightweight guardrails, enabling aggressive measurement reduction for\nin-distribution cases while automatically warning when priors are applied out\nof distribution.",
        "url": "http://arxiv.org/abs/2510.10947v1",
        "published_date": "2025-10-13T02:58:26+00:00",
        "updated_date": "2025-10-13T02:58:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Namhoon Kim",
            "Sara Fridovich-Keil"
        ],
        "ai_categories": [
            "GAN",
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper proposes a new method to estimate uncertainty in solving inverse problems using generative priors, focusing on detecting distribution shift without requiring a calibration dataset or retraining.",
        "tldr_zh": "本文提出了一种新方法，用于使用生成先验解决逆问题时估计不确定性，重点是在不需要校准数据集或重新训练的情况下检测分布偏移。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model",
        "summary": "Fine-grained vision-language understanding requires precise alignment between\nvisual content and linguistic descriptions, a capability that remains limited\nin current models, particularly in non-English settings. While models like CLIP\nperform well on global alignment, they often struggle to capture fine-grained\ndetails in object attributes, spatial relations, and linguistic expressions,\nwith limited support for bilingual comprehension. To address these challenges,\nwe introduce FG-CLIP 2, a bilingual vision-language model designed to advance\nfine-grained alignment for both English and Chinese. Our approach leverages\nrich fine-grained supervision, including region-text matching and long-caption\nmodeling, alongside multiple discriminative objectives. We further introduce\nthe Textual Intra-modal Contrastive (TIC) loss to better distinguish\nsemantically similar captions. Trained on a carefully curated mixture of\nlarge-scale English and Chinese data, FG-CLIP 2 achieves powerful bilingual\nperformance. To enable rigorous evaluation, we present a new benchmark for\nChinese multimodal understanding, featuring long-caption retrieval and bounding\nbox classification. Extensive experiments on 29 datasets across 8 tasks show\nthat FG-CLIP 2 outperforms existing methods, achieving state-of-the-art results\nin both languages. We release the model, code, and benchmark to facilitate\nfuture research on bilingual fine-grained alignment.",
        "url": "http://arxiv.org/abs/2510.10921v1",
        "published_date": "2025-10-13T02:32:07+00:00",
        "updated_date": "2025-10-13T02:32:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Chunyu Xie",
            "Bin Wang",
            "Fanjing Kong",
            "Jincheng Li",
            "Dawei Liang",
            "Ji Ao",
            "Dawei Leng",
            "Yuhui Yin"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces FG-CLIP 2, a bilingual fine-grained vision-language alignment model that improves alignment between visual content and linguistic descriptions in English and Chinese.",
        "tldr_zh": "该论文介绍了FG-CLIP 2，一种双语精细视觉-语言对齐模型，可以改进英语和中文中视觉内容与语言描述之间的对齐。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DreamMakeup: Face Makeup Customization using Latent Diffusion Models",
        "summary": "The exponential growth of the global makeup market has paralleled\nadvancements in virtual makeup simulation technology. Despite the progress led\nby GANs, their application still encounters significant challenges, including\ntraining instability and limited customization capabilities. Addressing these\nchallenges, we introduce DreamMakup - a novel training-free Diffusion model\nbased Makeup Customization method, leveraging the inherent advantages of\ndiffusion models for superior controllability and precise real-image editing.\nDreamMakeup employs early-stopped DDIM inversion to preserve the facial\nstructure and identity while enabling extensive customization through various\nconditioning inputs such as reference images, specific RGB colors, and textual\ndescriptions. Our model demonstrates notable improvements over existing\nGAN-based and recent diffusion-based frameworks - improved customization,\ncolor-matching capabilities, identity preservation and compatibility with\ntextual descriptions or LLMs with affordable computational costs.",
        "url": "http://arxiv.org/abs/2510.10918v1",
        "published_date": "2025-10-13T02:29:23+00:00",
        "updated_date": "2025-10-13T02:29:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Geon Yeong Park",
            "Inhwa Han",
            "Serin Yang",
            "Yeobin Hong",
            "Seongmin Jeong",
            "Heechan Jeon",
            "Myeongjin Goh",
            "Sung Won Yi",
            "Jin Nam",
            "Jong Chul Ye"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN",
            "Multimodality"
        ],
        "tldr": "DreamMakeup introduces a training-free diffusion model for face makeup customization, offering improved customization, color-matching capabilities, and identity preservation.",
        "tldr_zh": "DreamMakeup引入了一种无需训练的扩散模型，用于脸部化妆定制，提供了改进的化妆、颜色匹配能力和身份保留。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model",
        "summary": "With the rapid development of diffusion models, style transfer has made\nremarkable progress. However, flexible and localized style editing for scene\ntext remains an unsolved challenge. Although existing scene text editing\nmethods have achieved text region editing, they are typically limited to\ncontent replacement and simple styles, which lack the ability of free-style\ntransfer. In this paper, we introduce SceneTextStylizer, a novel training-free\ndiffusion-based framework for flexible and high-fidelity style transfer of text\nin scene images. Unlike prior approaches that either perform global style\ntransfer or focus solely on textual content modification, our method enables\nprompt-guided style transformation specifically for text regions, while\npreserving both text readability and stylistic consistency. To achieve this, we\ndesign a feature injection module that leverages diffusion model inversion and\nself-attention to transfer style features effectively. Additionally, a region\ncontrol mechanism is introduced by applying a distance-based changing mask at\neach denoising step, enabling precise spatial control. To further enhance\nvisual quality, we incorporate a style enhancement module based on the Fourier\ntransform to reinforce stylistic richness. Extensive experiments demonstrate\nthat our method achieves superior performance in scene text style\ntransformation, outperforming existing state-of-the-art methods in both visual\nfidelity and text preservation.",
        "url": "http://arxiv.org/abs/2510.10910v1",
        "published_date": "2025-10-13T02:11:57+00:00",
        "updated_date": "2025-10-13T02:11:57+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Honghui Yuan",
            "Keiji Yanai"
        ],
        "ai_categories": [
            "Diffusion"
        ],
        "tldr": "The paper introduces SceneTextStylizer, a training-free framework for flexible and high-fidelity style transfer of text in scene images, outperforming existing methods in both visual fidelity and text preservation.",
        "tldr_zh": "本文介绍了SceneTextStylizer，这是一个无需训练的框架，用于在场景图像中对文本进行灵活高保真度的风格转移，在视觉保真度和文本保留方面优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Topological Alignment of Shared Vision-Language Embedding Space",
        "summary": "Contrastive Vision-Language Models (VLMs) have demonstrated strong zero-shot\ncapabilities. However, their cross-modal alignment remains biased toward\nEnglish due to limited multilingual multimodal data. Recent multilingual\nextensions have alleviated this gap but enforce instance-level alignment while\nneglecting the global geometry of the shared embedding space. We address this\nproblem by introducing ToMCLIP (Topological Alignment for Multilingual CLIP), a\ntopology-aware framework aligning embedding spaces with topology-preserving\nconstraints. The proposed method applies persistent homology to define a\ntopological alignment loss and approximates persistence diagram with\ntheoretical error bounds using graph sparsification strategy. This work\nvalidates the proposed approach, showing enhanced structural coherence of\nmultilingual representations, higher zero-shot accuracy on the CIFAR-100, and\nstronger multilingual retrieval performance on the xFlickr&CO. Beyond VLMs, the\nproposed approach provides a general method for incorporating topological\nalignment into representation learning.",
        "url": "http://arxiv.org/abs/2510.10889v1",
        "published_date": "2025-10-13T01:36:38+00:00",
        "updated_date": "2025-10-13T01:36:38+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Junwon You",
            "Dasol Kang",
            "Jae-Hun Jung"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces ToMCLIP, a framework for aligning embedding spaces in a multilingual context, focusing on topological constraints to enhance representation coherence and retrieval performance.",
        "tldr_zh": "本文引入了ToMCLIP，一个框架用于在多语言环境下对齐嵌入空间，专注于拓扑约束以提高表示一致性和检索性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EGD-YOLO: A Lightweight Multimodal Framework for Robust Drone-Bird Discrimination via Ghost-Enhanced YOLOv8n and EMA Attention under Adverse Condition",
        "summary": "Identifying drones and birds correctly is essential for keeping the skies\nsafe and improving security systems. Using the VIP CUP 2025 dataset, which\nprovides both RGB and infrared (IR) images, this study presents EGD-YOLOv8n, a\nnew lightweight yet powerful model for object detection. The model improves how\nimage features are captured and understood, making detection more accurate and\nefficient. It uses smart design changes and attention layers to focus on\nimportant details while reducing the amount of computation needed. A special\ndetection head helps the model adapt to objects of different shapes and sizes.\nWe trained three versions: one using RGB images, one using IR images, and one\ncombining both. The combined model achieved the best accuracy and reliability\nwhile running fast enough for real-time use on common GPUs.",
        "url": "http://arxiv.org/abs/2510.10765v1",
        "published_date": "2025-10-12T19:05:16+00:00",
        "updated_date": "2025-10-12T19:05:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sudipto Sarkar",
            "Mohammad Asif Hasan",
            "Khondokar Ashik Shahriar",
            "Fablia Labiba",
            "Nahian Tasnim",
            "Sheikh Anawarul Haq Fattah"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces EGD-YOLO, a lightweight multimodal framework for drone-bird discrimination using RGB and IR images, achieving high accuracy and real-time performance.",
        "tldr_zh": "本文介绍了EGD-YOLO，一个使用RGB和IR图像进行无人机-鸟类区分的轻量级多模态框架，实现了高准确性和实时性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Optimally Deep Networks -- Adapting Model Depth to Datasets for Superior Efficiency",
        "summary": "Deep neural networks (DNNs) have provided brilliant performance across\nvarious tasks. However, this success often comes at the cost of unnecessarily\nlarge model sizes, high computational demands, and substantial memory\nfootprints. Typically, powerful architectures are trained at full depths but\nnot all datasets or tasks require such high model capacity. Training very deep\narchitectures on relatively low-complexity datasets frequently leads to wasted\ncomputation, unnecessary energy consumption, and excessive memory usage, which\nin turn makes deployment of models on resource-constrained devices impractical.\nTo address this problem, we introduce Optimally Deep Networks (ODNs), which\nprovide a balance between model depth and task complexity. Specifically, we\npropose a NAS like training strategy called progressive depth expansion, which\nbegins by training deep networks at shallower depths and incrementally\nincreases their depth as the earlier blocks converge, continuing this process\nuntil the target accuracy is reached. ODNs use only the optimal depth for the\ngiven datasets, removing redundant layers. This cuts down future training and\ninference costs, lowers the memory footprint, enhances computational\nefficiency, and facilitates deployment on edge devices. Empirical results show\nthat the optimal depths of ResNet-18 and ResNet-34 for MNIST and SVHN, achieve\nup to 98.64 % and 96.44 % reduction in memory footprint, while maintaining a\ncompetitive accuracy of 99.31 % and 96.08 %, respectively.",
        "url": "http://arxiv.org/abs/2510.10764v1",
        "published_date": "2025-10-12T19:05:04+00:00",
        "updated_date": "2025-10-12T19:05:04+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Shaharyar Ahmed Khan Tareen",
            "Filza Khan Tareen"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces Optimally Deep Networks (ODNs) to adapt model depth to datasets, reducing memory usage and enhancing efficiency. Empirical results show significant memory footprint reductions with competitive accuracy.",
        "tldr_zh": "这篇论文介绍了优化深度网络（ODNs），以调整模型深度来减少内存使用量并提高效率。实证结果显示了显著的内存占用减少和竞争性的准确率。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Uncovering Anomalous Events for Marine Environmental Monitoring via Visual Anomaly Detection",
        "summary": "Underwater video monitoring is a promising strategy for assessing marine\nbiodiversity, but the vast volume of uneventful footage makes manual inspection\nhighly impractical. In this work, we explore the use of visual anomaly\ndetection (VAD) based on deep neural networks to automatically identify\ninteresting or anomalous events. We introduce AURA, the first multi-annotator\nbenchmark dataset for underwater VAD, and evaluate four VAD models across two\nmarine scenes. We demonstrate the importance of robust frame selection\nstrategies to extract meaningful video segments. Our comparison against\nmultiple annotators reveals that VAD performance of current models varies\ndramatically and is highly sensitive to both the amount of training data and\nthe variability in visual content that defines \"normal\" scenes. Our results\nhighlight the value of soft and consensus labels and offer a practical approach\nfor supporting scientific exploration and scalable biodiversity monitoring.",
        "url": "http://arxiv.org/abs/2510.10750v1",
        "published_date": "2025-10-12T18:39:34+00:00",
        "updated_date": "2025-10-12T18:39:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Laura Weihl",
            "Nejc Novak",
            "Stefan H. Bengtson",
            "Malte Pedersen"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a method using visual anomaly detection based on deep neural networks to automatically identify interesting events in underwater videos. It also presents a benchmark dataset and highlights the importance of robust frame selection strategies.",
        "tldr_zh": "本文介绍了一种利用深度神经网络的视觉异常检测方法，自动识别水下视频中的有趣事件。同时提出了一个基准数据集，强调了稳健的帧选择策略的重要性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation",
        "summary": "This report presents an overview of the 7th Large-scale Video Object\nSegmentation (LSVOS) Challenge held in conjunction with ICCV 2025. Besides the\ntwo traditional tracks of LSVOS that jointly target robustness in realistic\nvideo scenarios: Classic VOS (VOS), and Referring VOS (RVOS), the 2025 edition\nfeatures a newly introduced track, Complex VOS (MOSEv2). Building upon prior\ninsights, MOSEv2 substantially increases difficulty, introducing more\nchallenging but realistic scenarios including denser small objects, frequent\ndisappear/reappear events, severe occlusions, adverse weather and lighting,\netc., pushing long-term consistency and generalization beyond curated\nbenchmarks. The challenge retains standard ${J}$, $F$, and ${J\\&F}$ metrics for\nVOS and RVOS, while MOSEv2 adopts ${J\\&\\dot{F}}$ as the primary ranking metric\nto better evaluate objects across scales and disappearance cases. We summarize\ndatasets and protocols, highlight top-performing solutions, and distill\nemerging trends, such as the growing role of LLM/MLLM components and\nmemory-aware propagation, aiming to chart future directions for resilient,\nlanguage-aware video segmentation in the wild.",
        "url": "http://arxiv.org/abs/2510.11063v1",
        "published_date": "2025-10-13T07:02:09+00:00",
        "updated_date": "2025-10-13T07:02:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chang Liu",
            "Henghui Ding",
            "Kaining Ying",
            "Lingyi Hong",
            "Ning Xu",
            "Linjie Yang",
            "Yuchen Fan",
            "Mingqi Gao",
            "Jingkun Chen",
            "Yunqi Miao",
            "Gengshen Wu",
            "Zhijin Qin",
            "Jungong Han",
            "Zhixiong Zhang",
            "Shuangrui Ding",
            "Xiaoyi Dong",
            "Yuhang Zang",
            "Yuhang Cao",
            "Jiaqi Wang",
            "Chang Soo Lim",
            "Joonyoung Moon",
            "Donghyeon Cho",
            "Tingmin Li",
            "Yixuan Li",
            "Yang Yang",
            "An Yan",
            "Leilei Cao",
            "Feng Lu",
            "Ran Hong",
            "Youhai Jiang",
            "Fengjie Zhu",
            "Yujie Xie",
            "Hongyang Zhang",
            "Zhihui Liu",
            "Shihai Ruan",
            "Quanzhu Niu",
            "Dengxian Gong",
            "Shihao Chen",
            "Tao Zhang",
            "Yikang Zhou",
            "Haobo Yuan",
            "Lu Qi",
            "Xiangtai Li",
            "Shunping Ji",
            "Ran Hong",
            "Feng Lu",
            "Leilei Cao",
            "An Yan",
            "Alexey Nekrasov",
            "Ali Athar",
            "Daan de Geus",
            "Alexander Hermans",
            "Bastian Leibe"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "This report discusses the 7th LSVOS Challenge, introducing a new track focusing on complex video object segmentation with challenging scenarios, aiming to improve long-term consistency and generalization beyond existing benchmarks.",
        "tldr_zh": "本报告讨论了第七届LSVOS挑战赛，引入了一个新的赛道，专注于复杂视频对象分割，涉及具有挑战性的场景，旨在提高长期一致性和泛化能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.75
    },
    {
        "title": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs",
        "summary": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for\nlarge language models (LLMs). While RL is essential for LLMs' reasoning\ncapabilities, it is resource-intensive, requiring substantial GPU memory and\nlong rollout durations. QeRL addresses these issues by combining NVFP4\nquantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL\nwhile reducing memory overhead. Beyond efficiency, our findings show that\nquantization noise increases policy entropy, enhancing exploration, and\nenabling the discovery of better strategies during RL. To further optimize\nexploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,\nwhich dynamically adjusts noise during training. Experiments demonstrate that\nQeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is\nthe first framework to enable RL training of a 32B LLM on a single H100 80GB\nGPU, while delivering overall speedups for RL training. It also achieves faster\nreward growth and higher final accuracy than 16-bit LoRA and QLoRA, while\nmatching the performance of full-parameter fine-tuning on mathematical\nbenchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These\nresults establish QeRL as an efficient and effective framework for RL training\nin LLMs.",
        "url": "http://arxiv.org/abs/2510.11696v1",
        "published_date": "2025-10-13T17:55:09+00:00",
        "updated_date": "2025-10-13T17:55:09+00:00",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Wei Huang",
            "Yi Ge",
            "Shuai Yang",
            "Yicheng Xiao",
            "Huizi Mao",
            "Yujun Lin",
            "Hanrong Ye",
            "Sifei Liu",
            "Ka Chun Cheung",
            "Hongxu Yin",
            "Yao Lu",
            "Xiaojuan Qi",
            "Song Han",
            "Yukang Chen"
        ],
        "ai_categories": [
            "AIGC",
            "LoRA",
            "Transformer"
        ],
        "tldr": "QeRL is a framework that enhances Reinforcement Learning for large language models by using quantization techniques to improve efficiency and exploration in training.",
        "tldr_zh": "QeRL是一个通过量化技术来增强大型语言模型的强化学习框架，以提高效率和探索性能。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "ACE-G: Improving Generalization of Scene Coordinate Regression Through Query Pre-Training",
        "summary": "Scene coordinate regression (SCR) has established itself as a promising\nlearning-based approach to visual relocalization. After mere minutes of\nscene-specific training, SCR models estimate camera poses of query images with\nhigh accuracy. Still, SCR methods fall short of the generalization capabilities\nof more classical feature-matching approaches. When imaging conditions of query\nimages, such as lighting or viewpoint, are too different from the training\nviews, SCR models fail. Failing to generalize is an inherent limitation of\nprevious SCR frameworks, since their training objective is to encode the\ntraining views in the weights of the coordinate regressor itself. The regressor\nessentially overfits to the training views, by design. We propose to separate\nthe coordinate regressor and the map representation into a generic transformer\nand a scene-specific map code. This separation allows us to pre-train the\ntransformer on tens of thousands of scenes. More importantly, it allows us to\ntrain the transformer to generalize from mapping images to unseen query images\nduring pre-training. We demonstrate on multiple challenging relocalization\ndatasets that our method, ACE-G, leads to significantly increased robustness\nwhile keeping the computational footprint attractive.",
        "url": "http://arxiv.org/abs/2510.11605v1",
        "published_date": "2025-10-13T16:45:17+00:00",
        "updated_date": "2025-10-13T16:45:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Leonard Bruns",
            "Axel Barroso-Laguna",
            "Tommaso Cavallari",
            "Áron Monszpart",
            "Sowmya Munukutla",
            "Victor Adrian Prisacariu",
            "Eric Brachmann"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a method, ACE-G, to improve the generalization of scene coordinate regression for visual relocalization by pre-training a transformer on tens of thousands of scenes.",
        "tldr_zh": "本文提出了一种名为ACE-G的方法，通过在数万个场景上进行预训练，改善视觉重定位中场景坐标回归的泛化能力。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Situat3DChange: Situated 3D Change Understanding Dataset for Multimodal Large Language Model",
        "summary": "Physical environments and circumstances are fundamentally dynamic, yet\ncurrent 3D datasets and evaluation benchmarks tend to concentrate on either\ndynamic scenarios or dynamic situations in isolation, resulting in incomplete\ncomprehension. To overcome these constraints, we introduce Situat3DChange, an\nextensive dataset supporting three situation-aware change understanding tasks\nfollowing the perception-action model: 121K question-answer pairs, 36K change\ndescriptions for perception tasks, and 17K rearrangement instructions for the\naction task. To construct this large-scale dataset, Situat3DChange leverages\n11K human observations of environmental changes to establish shared mental\nmodels and shared situational awareness for human-AI collaboration. These\nobservations, enriched with egocentric and allocentric perspectives as well as\ncategorical and coordinate spatial relations, are integrated using an LLM to\nsupport understanding of situated changes. To address the challenge of\ncomparing pairs of point clouds from the same scene with minor changes, we\npropose SCReasoner, an efficient 3D MLLM approach that enables effective point\ncloud comparison with minimal parameter overhead and no additional tokens\nrequired for the language decoder. Comprehensive evaluation on Situat3DChange\ntasks highlights both the progress and limitations of MLLMs in dynamic scene\nand situation understanding. Additional experiments on data scaling and\ncross-domain transfer demonstrate the task-agnostic effectiveness of using\nSituat3DChange as a training dataset for MLLMs.",
        "url": "http://arxiv.org/abs/2510.11509v1",
        "published_date": "2025-10-13T15:17:18+00:00",
        "updated_date": "2025-10-13T15:17:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruiping Liu",
            "Junwei Zheng",
            "Yufan Chen",
            "Zirui Wang",
            "Kunyu Peng",
            "Kailun Yang",
            "Jiaming Zhang",
            "Marc Pollefeys",
            "Rainer Stiefelhagen"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces Situat3DChange, a dataset for understanding dynamic 3D environments through questions, change descriptions, and rearrangement instructions, evaluated with an efficient 3D MLLM approach.",
        "tldr_zh": "本文介绍了Situa3DChange数据集，通过问题、变化描述和重新排列指令来理解动态3D环境，并使用高效的3D MLLM方法进行评估。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Reasoning as Representation: Rethinking Visual Reinforcement Learning in Image Quality Assessment",
        "summary": "Reasoning-based image quality assessment (IQA) models trained through\nreinforcement learning (RL) exhibit exceptional generalization, yet the\nunderlying mechanisms and critical factors driving this capability remain\nunderexplored in current research. Moreover, despite their superior\nperformance, these models incur inference energy usage and latency orders of\nmagnitude higher than their earlier counterparts, restricting their deployment\nin specific scenarios. Through extensive experiments, this paper verifies and\nelaborates that through RL training, MLLMs leverage their reasoning capability\nto convert redundant visual representations into compact, cross-domain aligned\ntext representations. This conversion is precisely the source of the\ngeneralization exhibited by these reasoning-based IQA models. Building on this\nfundamental insight, we propose a novel algorithm, RALI, which employs\ncontrastive learning to directly align images with these generalizable text\nrepresentations learned by RL. This approach eliminates the reliance on\nreasoning processes and even obviates the need to load an LLM. For the quality\nscoring task, this framework achieves generalization performance comparable to\nreasoning-based models while requiring less than 5% of their model parameters\nand inference time.",
        "url": "http://arxiv.org/abs/2510.11369v1",
        "published_date": "2025-10-13T13:11:08+00:00",
        "updated_date": "2025-10-13T13:11:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shijie Zhao",
            "Xuanyu Zhang",
            "Weiqi Li",
            "Junlin Li",
            "Li Zhang",
            "Tianfan Xue",
            "Jian Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper explores how reasoning-based image quality assessment models trained through reinforcement learning convert visual representations into text representations for better generalization. They propose a new algorithm, RALI, that aligns images with text representations, achieving comparable performance with less complexity.",
        "tldr_zh": "本文探讨了基于推理的图像质量评估模型如何通过强化学习将视觉表示转换为文本表示以提高泛化能力。他们提出了一种新算法RALI，将图像与文本表示进行直接对齐，实现了与更少复杂性相比的性能。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "MMAP: A Multi-Magnification and Prototype-Aware Architecture for Predicting Spatial Gene Expression",
        "summary": "Spatial Transcriptomics (ST) enables the measurement of gene expression while\npreserving spatial information, offering critical insights into tissue\narchitecture and disease pathology. Recent developments have explored the use\nof hematoxylin and eosin (H&E)-stained whole-slide images (WSIs) to predict\ntranscriptome-wide gene expression profiles through deep neural networks. This\ntask is commonly framed as a regression problem, where each input corresponds\nto a localized image patch extracted from the WSI. However, predicting spatial\ngene expression from histological images remains a challenging problem due to\nthe significant modality gap between visual features and molecular signals.\nRecent studies have attempted to incorporate both local and global information\ninto predictive models. Nevertheless, existing methods still suffer from two\nkey limitations: (1) insufficient granularity in local feature extraction, and\n(2) inadequate coverage of global spatial context. In this work, we propose a\nnovel framework, MMAP (Multi-MAgnification and Prototype-enhanced\narchitecture), that addresses both challenges simultaneously. To enhance local\nfeature granularity, MMAP leverages multi-magnification patch representations\nthat capture fine-grained histological details. To improve global contextual\nunderstanding, it learns a set of latent prototype embeddings that serve as\ncompact representations of slide-level information. Extensive experimental\nresults demonstrate that MMAP consistently outperforms all existing\nstate-of-the-art methods across multiple evaluation metrics, including Mean\nAbsolute Error (MAE), Mean Squared Error (MSE), and Pearson Correlation\nCoefficient (PCC).",
        "url": "http://arxiv.org/abs/2510.11344v1",
        "published_date": "2025-10-13T12:41:09+00:00",
        "updated_date": "2025-10-13T12:41:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hai Dang Nguyen",
            "Nguyen Dang Huy Pham",
            "The Minh Duc Nguyen",
            "Dac Thai Nguyen",
            "Hang Thi Nguyen",
            "Duong M. Nguyen"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces MMAP, a novel framework for predicting spatial gene expression from histological images by leveraging multi-magnification patch representations and latent prototype embeddings, outperforming existing methods.",
        "tldr_zh": "本文介绍了MMAP，这是一个新颖的框架，通过利用多放大率补丁表示和潜在原型嵌入来预测组织学图像中的空间基因表达，优于现有方法。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Future-Aware End-to-End Driving: Bidirectional Modeling of Trajectory Planning and Scene Evolution",
        "summary": "End-to-end autonomous driving methods aim to directly map raw sensor inputs\nto future driving actions such as planned trajectories, bypassing traditional\nmodular pipelines. While these approaches have shown promise, they often\noperate under a one-shot paradigm that relies heavily on the current scene\ncontext, potentially underestimating the importance of scene dynamics and their\ntemporal evolution. This limitation restricts the model's ability to make\ninformed and adaptive decisions in complex driving scenarios. We propose a new\nperspective: the future trajectory of an autonomous vehicle is closely\nintertwined with the evolving dynamics of its environment, and conversely, the\nvehicle's own future states can influence how the surrounding scene unfolds.\nMotivated by this bidirectional relationship, we introduce SeerDrive, a novel\nend-to-end framework that jointly models future scene evolution and trajectory\nplanning in a closed-loop manner. Our method first predicts future bird's-eye\nview (BEV) representations to anticipate the dynamics of the surrounding scene,\nthen leverages this foresight to generate future-context-aware trajectories.\nTwo key components enable this: (1) future-aware planning, which injects\npredicted BEV features into the trajectory planner, and (2) iterative scene\nmodeling and vehicle planning, which refines both future scene prediction and\ntrajectory generation through collaborative optimization. Extensive experiments\non the NAVSIM and nuScenes benchmarks show that SeerDrive significantly\noutperforms existing state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2510.11092v1",
        "published_date": "2025-10-13T07:41:47+00:00",
        "updated_date": "2025-10-13T07:41:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bozhou Zhang",
            "Nan Song",
            "Jingyu Li",
            "Xiatian Zhu",
            "Jiankang Deng",
            "Li Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces SeerDrive, an end-to-end framework that models future scene evolution and trajectory planning for autonomous driving in a bidirectional manner, outperforming existing methods.",
        "tldr_zh": "该论文介绍了SeerDrive，一种新的端到端框架，以双向方式对自动驾驶的未来场景演变和轨迹规划进行建模，优于现有方法。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "ImHead: A Large-scale Implicit Morphable Model for Localized Head Modeling",
        "summary": "Over the last years, 3D morphable models (3DMMs) have emerged as a\nstate-of-the-art methodology for modeling and generating expressive 3D avatars.\nHowever, given their reliance on a strict topology, along with their linear\nnature, they struggle to represent complex full-head shapes. Following the\nadvent of deep implicit functions, we propose imHead, a novel implicit 3DMM\nthat not only models expressive 3D head avatars but also facilitates localized\nediting of the facial features. Previous methods directly divided the latent\nspace into local components accompanied by an identity encoding to capture the\nglobal shape variations, leading to expensive latent sizes. In contrast, we\nretain a single compact identity space and introduce an intermediate\nregion-specific latent representation to enable local edits. To train imHead,\nwe curate a large-scale dataset of 4K distinct identities, making a\nstep-towards large scale 3D head modeling. Under a series of experiments we\ndemonstrate the expressive power of the proposed model to represent diverse\nidentities and expressions outperforming previous approaches. Additionally, the\nproposed approach provides an interpretable solution for 3D face manipulation,\nallowing the user to make localized edits.",
        "url": "http://arxiv.org/abs/2510.10793v1",
        "published_date": "2025-10-12T20:17:34+00:00",
        "updated_date": "2025-10-12T20:17:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rolandos Alexandros Potamias",
            "Stathis Galanakis",
            "Jiankang Deng",
            "Athanasios Papaioannou",
            "Stefanos Zafeiriou"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces imHead, a new implicit 3D morphable model for localized head modeling, allowing for expressive 3D avatars and easy facial feature editing.",
        "tldr_zh": "该论文介绍了imHead，一种新的隐式三维可塑模型，用于局部头部建模，允许表现丰富的三维头像和轻松的面部特征编辑。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7.5
    },
    {
        "title": "Enhancing Maritime Domain Awareness on Inland Waterways: A YOLO-Based Fusion of Satellite and AIS for Vessel Characterization",
        "summary": "Maritime Domain Awareness (MDA) for inland waterways remains challenged by\ncooperative system vulnerabilities. This paper presents a novel framework that\nfuses high-resolution satellite imagery with vessel trajectory data from the\nAutomatic Identification System (AIS). This work addresses the limitations of\nAIS-based monitoring by leveraging non-cooperative satellite imagery and\nimplementing a fusion approach that links visual detections with AIS data to\nidentify dark vessels, validate cooperative traffic, and support advanced MDA.\nThe You Only Look Once (YOLO) v11 object detection model is used to detect and\ncharacterize vessels and barges by vessel type, barge cover, operational\nstatus, barge count, and direction of travel. An annotated data set of 4,550\ninstances was developed from $5{,}973~\\mathrm{mi}^2$ of Lower Mississippi River\nimagery. Evaluation on a held-out test set demonstrated vessel classification\n(tugboat, crane barge, bulk carrier, cargo ship, and hopper barge) with an F1\nscore of 95.8\\%; barge cover (covered or uncovered) detection yielded an F1\nscore of 91.6\\%; operational status (staged or in motion) classification\nreached an F1 score of 99.4\\%. Directionality (upstream, downstream) yielded\n93.8\\% accuracy. The barge count estimation resulted in a mean absolute error\n(MAE) of 2.4 barges. Spatial transferability analysis across geographically\ndisjoint river segments showed accuracy was maintained as high as 98\\%. These\nresults underscore the viability of integrating non-cooperative satellite\nsensing with AIS fusion. This approach enables near-real-time fleet\ninventories, supports anomaly detection, and generates high-quality data for\ninland waterway surveillance. Future work will expand annotated datasets,\nincorporate temporal tracking, and explore multi-modal deep learning to further\nenhance operational scalability.",
        "url": "http://arxiv.org/abs/2510.11449v1",
        "published_date": "2025-10-13T14:19:58+00:00",
        "updated_date": "2025-10-13T14:19:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Geoffery Agorku",
            "Sarah Hernandez",
            "Hayley Hames",
            "Cade Wagner"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC",
            "Multimodality"
        ],
        "tldr": "The paper presents a framework that combines satellite imagery with AIS data to monitor vessel activities on inland waterways, achieving high accuracy in vessel characterization and classification.",
        "tldr_zh": "本文提出了一种将卫星图像与AIS数据结合起来监测内陆水路上船只活动的框架，实现了对船只的高精度特征和分类。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Robust Ego-Exo Correspondence with Long-Term Memory",
        "summary": "Establishing object-level correspondence between egocentric and exocentric\nviews is essential for intelligent assistants to deliver precise and intuitive\nvisual guidance. However, this task faces numerous challenges, including\nextreme viewpoint variations, occlusions, and the presence of small objects.\nExisting approaches usually borrow solutions from video object segmentation\nmodels, but still suffer from the aforementioned challenges. Recently, the\nSegment Anything Model 2 (SAM 2) has shown strong generalization capabilities\nand excellent performance in video object segmentation. Yet, when simply\napplied to the ego-exo correspondence (EEC) task, SAM 2 encounters severe\ndifficulties due to ineffective ego-exo feature fusion and limited long-term\nmemory capacity, especially for long videos. Addressing these problems, we\npropose a novel EEC framework based on SAM 2 with long-term memories by\npresenting a dual-memory architecture and an adaptive feature routing module\ninspired by Mixture-of-Experts (MoE). Compared to SAM 2, our approach features\n(i) a Memory-View MoE module which consists of a dual-branch routing mechanism\nto adaptively assign contribution weights to each expert feature along both\nchannel and spatial dimensions, and (ii) a dual-memory bank system with a\nsimple yet effective compression strategy to retain critical long-term\ninformation while eliminating redundancy. In the extensive experiments on the\nchallenging EgoExo4D benchmark, our method, dubbed LM-EEC, achieves new\nstate-of-the-art results and significantly outperforms existing methods and the\nSAM 2 baseline, showcasing its strong generalization across diverse scenarios.\nOur code and model are available at https://github.com/juneyeeHu/LM-EEC.",
        "url": "http://arxiv.org/abs/2510.11417v1",
        "published_date": "2025-10-13T13:54:12+00:00",
        "updated_date": "2025-10-13T13:54:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yijun Hu",
            "Bing Fan",
            "Xin Gu",
            "Haiqing Ren",
            "Dongfang Liu",
            "Heng Fan",
            "Libo Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel Ego-Exo Correspondence framework with long-term memories to address challenges in establishing object-level correspondence between egocentric and exocentric views.",
        "tldr_zh": "该论文提出了一种基于长期记忆的新型自我-外部对应框架，以解决建立自我中心和外部中心视图之间物体级对应的挑战。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "DTEA: Dynamic Topology Weaving and Instability-Driven Entropic Attenuation for Medical Image Segmentation",
        "summary": "In medical image segmentation, skip connections are used to merge global\ncontext and reduce the semantic gap between encoder and decoder. Current\nmethods often struggle with limited structural representation and insufficient\ncontextual modeling, affecting generalization in complex clinical scenarios. We\npropose the DTEA model, featuring a new skip connection framework with the\nSemantic Topology Reconfiguration (STR) and Entropic Perturbation Gating (EPG)\nmodules. STR reorganizes multi-scale semantic features into a dynamic\nhypergraph to better model cross-resolution anatomical dependencies, enhancing\nstructural and semantic representation. EPG assesses channel stability after\nperturbation and filters high-entropy channels to emphasize clinically\nimportant regions and improve spatial attention. Extensive experiments on three\nbenchmark datasets show our framework achieves superior segmentation accuracy\nand better generalization across various clinical settings. The code is\navailable at\n\\href{https://github.com/LWX-Research/DTEA}{https://github.com/LWX-Research/DTEA}.",
        "url": "http://arxiv.org/abs/2510.11259v1",
        "published_date": "2025-10-13T10:50:41+00:00",
        "updated_date": "2025-10-13T10:50:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weixuan Li",
            "Quanjun Li",
            "Guang Yu",
            "Song Yang",
            "Zimeng Li",
            "Chi-Man Pun",
            "Yupeng Liu",
            "Xuhang Chen"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces the DTEA model for medical image segmentation, enhancing structural and semantic representation with new skip connection framework and dynamic hypergraph modeling.",
        "tldr_zh": "本文介绍了DTEA模型用于医学图像分割，在新的skip connection框架和动态超图建模方面增强结构和语义表示。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Class Prototypes based Contrastive Learning for Classifying Multi-Label and Fine-Grained Educational Videos",
        "summary": "The recent growth in the consumption of online media by children during early\nchildhood necessitates data-driven tools enabling educators to filter out\nappropriate educational content for young learners. This paper presents an\napproach for detecting educational content in online videos. We focus on two\nwidely used educational content classes: literacy and math. For each class, we\nchoose prominent codes (sub-classes) based on the Common Core Standards. For\nexample, literacy codes include `letter names', `letter sounds', and math codes\ninclude `counting', `sorting'. We pose this as a fine-grained multilabel\nclassification problem as videos can contain multiple types of educational\ncontent and the content classes can get visually similar (e.g., `letter names'\nvs `letter sounds'). We propose a novel class prototypes based supervised\ncontrastive learning approach that can handle fine-grained samples associated\nwith multiple labels. We learn a class prototype for each class and a loss\nfunction is employed to minimize the distances between a class prototype and\nthe samples from the class. Similarly, distances between a class prototype and\nthe samples from other classes are maximized. As the alignment between visual\nand audio cues are crucial for effective comprehension, we consider a\nmultimodal transformer network to capture the interaction between visual and\naudio cues in videos while learning the embedding for videos. For evaluation,\nwe present a dataset, APPROVE, employing educational videos from YouTube\nlabeled with fine-grained education classes by education researchers. APPROVE\nconsists of 193 hours of expert-annotated videos with 19 classes. The proposed\napproach outperforms strong baselines on APPROVE and other benchmarks such as\nYoutube-8M, and COIN. The dataset is available at\nhttps://github.com/rohit-gupta/MMContrast/tree/main/APPROVE",
        "url": "http://arxiv.org/abs/2510.11204v1",
        "published_date": "2025-10-13T09:36:26+00:00",
        "updated_date": "2025-10-13T09:36:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rohit Gupta",
            "Anirban Roy",
            "Claire Christensen",
            "Sujeong Kim",
            "Sarah Gerard",
            "Madeline Cincebeaux",
            "Ajay Divakaran",
            "Todd Grindal",
            "Mubarak Shah"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "This paper introduces a novel approach using class prototypes based contrastive learning for classifying multi-label educational videos, outperforming strong baselines on various datasets.",
        "tldr_zh": "本文引入了一种基于类原型的对比学习方法，用于分类多标签的教育视频，在各个数据集上表现出色。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Text-Enhanced Panoptic Symbol Spotting in CAD Drawings",
        "summary": "With the widespread adoption of Computer-Aided Design(CAD) drawings in\nengineering, architecture, and industrial design, the ability to accurately\ninterpret and analyze these drawings has become increasingly critical. Among\nvarious subtasks, panoptic symbol spotting plays a vital role in enabling\ndownstream applications such as CAD automation and design retrieval. Existing\nmethods primarily focus on geometric primitives within the CAD drawings to\naddress this task, but they face following major problems: they usually\noverlook the rich textual annotations present in CAD drawings and they lack\nexplicit modeling of relationships among primitives, resulting in\nincomprehensive understanding of the holistic drawings. To fill this gap, we\npropose a panoptic symbol spotting framework that incorporates textual\nannotations. The framework constructs unified representations by jointly\nmodeling geometric and textual primitives. Then, using visual features extract\nby pretrained CNN as the initial representations, a Transformer-based backbone\nis employed, enhanced with a type-aware attention mechanism to explicitly model\nthe different types of spatial dependencies between various primitives.\nExtensive experiments on the real-world dataset demonstrate that the proposed\nmethod outperforms existing approaches on symbol spotting tasks involving\ntextual annotations, and exhibits superior robustness when applied to complex\nCAD drawings.",
        "url": "http://arxiv.org/abs/2510.11091v1",
        "published_date": "2025-10-13T07:41:15+00:00",
        "updated_date": "2025-10-13T07:41:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xianlin Liu",
            "Yan Gong",
            "Bohao Li",
            "Jiajing Huang",
            "Bowen Du",
            "Junchen Ye",
            "Liyan Xu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a panoptic symbol spotting framework that incorporates textual annotations in CAD drawings, outperforming existing approaches and showing superior robustness in complex CAD drawings.",
        "tldr_zh": "该论文提出了一种综合符号识别框架，将文本注释融入CAD图纸中，优于现有方法，并在复杂CAD图纸中表现出更强的鲁棒性。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Source-Free Object Detection with Detection Transformer",
        "summary": "Source-Free Object Detection (SFOD) enables knowledge transfer from a source\ndomain to an unsupervised target domain for object detection without access to\nsource data. Most existing SFOD approaches are either confined to conventional\nobject detection (OD) models like Faster R-CNN or designed as general solutions\nwithout tailored adaptations for novel OD architectures, especially Detection\nTransformer (DETR). In this paper, we introduce Feature Reweighting ANd\nContrastive Learning NetworK (FRANCK), a novel SFOD framework specifically\ndesigned to perform query-centric feature enhancement for DETRs. FRANCK\ncomprises four key components: (1) an Objectness Score-based Sample Reweighting\n(OSSR) module that computes attention-based objectness scores on multi-scale\nencoder feature maps, reweighting the detection loss to emphasize\nless-recognized regions; (2) a Contrastive Learning with Matching-based Memory\nBank (CMMB) module that integrates multi-level features into memory banks,\nenhancing class-wise contrastive learning; (3) an Uncertainty-weighted\nQuery-fused Feature Distillation (UQFD) module that improves feature\ndistillation through prediction quality reweighting and query feature fusion;\nand (4) an improved self-training pipeline with a Dynamic Teacher Updating\nInterval (DTUI) that optimizes pseudo-label quality. By leveraging these\ncomponents, FRANCK effectively adapts a source-pre-trained DETR model to a\ntarget domain with enhanced robustness and generalization. Extensive\nexperiments on several widely used benchmarks demonstrate that our method\nachieves state-of-the-art performance, highlighting its effectiveness and\ncompatibility with DETR-based SFOD models.",
        "url": "http://arxiv.org/abs/2510.11090v1",
        "published_date": "2025-10-13T07:35:04+00:00",
        "updated_date": "2025-10-13T07:35:04+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Huizai Yao",
            "Sicheng Zhao",
            "Shuo Lu",
            "Hui Chen",
            "Yangyang Li",
            "Guoping Liu",
            "Tengfei Xing",
            "Chenggang Yan",
            "Jianhua Tao",
            "Guiguang Ding"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a new framework called FRANCK for Source-Free Object Detection with a focus on DETR models, achieving state-of-the-art performance.",
        "tldr_zh": "本文引入了一种名为FRANCK的新框架，用于无源物体检测，重点放在DETR模型上，实现了最先进的性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Benchmarking Deep Learning Models for Laryngeal Cancer Staging Using the LaryngealCT Dataset",
        "summary": "Laryngeal cancer imaging research lacks standardised datasets to enable\nreproducible deep learning (DL) model development. We present LaryngealCT, a\ncurated benchmark of 1,029 computed tomography (CT) scans aggregated from six\ncollections from The Cancer Imaging Archive (TCIA). Uniform 1 mm isotropic\nvolumes of interest encompassing the larynx were extracted using a weakly\nsupervised parameter search framework validated by clinical experts. 3D DL\narchitectures (3D CNN, ResNet18,50,101, DenseNet121) were benchmarked on (i)\nearly (Tis,T1,T2) vs. advanced (T3,T4) and (ii) T4 vs. non-T4 classification\ntasks. 3D CNN (AUC-0.881, F1-macro-0.821) and ResNet18 (AUC-0.892,\nF1-macro-0.646) respectively outperformed the other models in the two tasks.\nModel explainability assessed using 3D GradCAMs with thyroid cartilage overlays\nrevealed greater peri-cartilage attention in non-T4 cases and focal activations\nin T4 predictions. Through open-source data, pretrained models, and integrated\nexplainability tools, LaryngealCT offers a reproducible foundation for\nAI-driven research to support clinical decisions in laryngeal oncology.",
        "url": "http://arxiv.org/abs/2510.11047v1",
        "published_date": "2025-10-13T06:25:19+00:00",
        "updated_date": "2025-10-13T06:25:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nivea Roy",
            "Son Tran",
            "Atul Sajjanhar",
            "K. Devaraja",
            "Prakashini Koteshwara",
            "Yong Xiang",
            "Divya Rao"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper presents the LaryngealCT dataset for laryngeal cancer staging using deep learning models, which outperformed other models in classification tasks. The dataset aims to support AI-driven research in laryngeal oncology.",
        "tldr_zh": "该论文使用深度学习模型提出了LaryngealCT数据集用于喉癌分期，该数据集的模型在分类任务中表现优异。该数据集旨在支持喉部肿瘤学中的人工智能研究。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning",
        "summary": "While significant research has focused on developing embodied reasoning\ncapabilities using Vision-Language Models (VLMs) or integrating advanced VLMs\ninto Vision-Language-Action (VLA) models for end-to-end robot control, few\nstudies directly address the critical gap between upstream VLM-based reasoning\nand downstream VLA policy learning. In this work, we take an initial step\ntoward bridging embodied reasoning with VLA policy learning by introducing\nVlaser - a Vision-Language-Action Model with synergistic embodied reasoning\ncapability, which is a foundational vision-language model designed to integrate\nhigh-level reasoning with low-level control for embodied agents. Built upon the\nhigh-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance\nacross a range of embodied reasoning benchmarks - including spatial reasoning,\nembodied grounding, embodied QA, and task planning. Furthermore, we\nsystematically examine how different VLM initializations affect supervised VLA\nfine-tuning, offering novel insights into mitigating the domain shift between\ninternet-scale pre-training data and embodied-specific policy learning data.\nBased on these insights, our approach achieves state-of-the-art results on the\nWidowX benchmark and competitive performance on the Google Robot benchmark.",
        "url": "http://arxiv.org/abs/2510.11027v1",
        "published_date": "2025-10-13T05:51:22+00:00",
        "updated_date": "2025-10-13T05:51:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ganlin Yang",
            "Tianyi Zhang",
            "Haoran Hao",
            "Weiyun Wang",
            "Yibin Liu",
            "Dehui Wang",
            "Guanzhou Chen",
            "Zijian Cai",
            "Junting Chen",
            "Weijie Su",
            "Wengang Zhou",
            "Yu Qiao",
            "Jifeng Dai",
            "Jiangmiao Pang",
            "Gen Luo",
            "Wenhai Wang",
            "Yao Mu",
            "Zhi Hou"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces Vlaser, a model combining vision, language, and action for embodied agents. It achieves state-of-the-art results in embodied reasoning benchmarks and offers insights into VLM initialization for VLA fine-tuning.",
        "tldr_zh": "该论文介绍了Vlaser模型，结合了视觉、语言和行为，用于具身体代理。它在具身体推理基准测试中取得了最先进的结果，并提供了关于VLM初始化对VLA微调的见解。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "GeoVLMath: Enhancing Geometry Reasoning in Vision-Language Models via Cross-Modal Reward for Auxiliary Line Creation",
        "summary": "Auxiliary lines are essential for solving complex geometric problems but\nremain challenging for large vision-language models (LVLMs). Rather than\nediting diagrams to draw auxiliary lines, which current image editing models\nstruggle to render with geometric precision, we generate textual descriptions\nof auxiliary-line constructions to better align with the representational\nstrengths of LVLMs. To bridge the gap between textual descriptions and spatial\nstructure, we propose a reinforcement learning framework that enhances\ndiagram-text alignment. At the core of our approach is a cross-modal reward\nthat evaluates how well the generated auxiliary-line description for an\noriginal diagram matches a ground-truth auxiliary-line diagram. Built on this\nreward, we present GeoVLMath, an open-source LVLM tailored to auxiliary-line\nreasoning in solid geometry. This fine-grained signal drives a GRPO-based RL\nstage, yielding precise diagram-text alignment. To support training, we develop\na scalable data creation pipeline and construct AuxSolidMath, a dataset of\n3,018 real-exam geometry problems with paired diagrams and aligned textual\nfields. At the 3B and 7B scales, GeoVLMath achieves competitive and often\nsuperior performance compared with strong open-source and proprietary LVLMs on\nauxiliary-line reasoning benchmarks.",
        "url": "http://arxiv.org/abs/2510.11020v1",
        "published_date": "2025-10-13T05:33:51+00:00",
        "updated_date": "2025-10-13T05:33:51+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shasha Guo",
            "Liang Pang",
            "Xi Wang",
            "Yanling Wang",
            "Huawei Shen",
            "Jing Zhang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces GeoVLMath, a model that uses reinforcement learning to improve geometry reasoning in vision-language models by generating textual descriptions of auxiliary lines for solving geometric problems.",
        "tldr_zh": "该论文介绍了GeoVLMath，该模型使用强化学习来通过生成辅助线的文本描述来改进视觉语言模型中的几何推理。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "High-Resolution Spatiotemporal Modeling with Global-Local State Space Models for Video-Based Human Pose Estimation",
        "summary": "Modeling high-resolution spatiotemporal representations, including both\nglobal dynamic contexts (e.g., holistic human motion tendencies) and local\nmotion details (e.g., high-frequency changes of keypoints), is essential for\nvideo-based human pose estimation (VHPE). Current state-of-the-art methods\ntypically unify spatiotemporal learning within a single type of modeling\nstructure (convolution or attention-based blocks), which inherently have\ndifficulties in balancing global and local dynamic modeling and may bias the\nnetwork to one of them, leading to suboptimal performance. Moreover, existing\nVHPE models suffer from quadratic complexity when capturing global\ndependencies, limiting their applicability especially for high-resolution\nsequences. Recently, the state space models (known as Mamba) have demonstrated\nsignificant potential in modeling long-range contexts with linear complexity;\nhowever, they are restricted to 1D sequential data. In this paper, we present a\nnovel framework that extends Mamba from two aspects to separately learn global\nand local high-resolution spatiotemporal representations for VHPE.\nSpecifically, we first propose a Global Spatiotemporal Mamba, which performs 6D\nselective space-time scan and spatial- and temporal-modulated scan merging to\nefficiently extract global representations from high-resolution sequences. We\nfurther introduce a windowed space-time scan-based Local Refinement Mamba to\nenhance the high-frequency details of localized keypoint motions. Extensive\nexperiments on four benchmark datasets demonstrate that the proposed model\noutperforms state-of-the-art VHPE approaches while achieving better\ncomputational trade-offs.",
        "url": "http://arxiv.org/abs/2510.11017v1",
        "published_date": "2025-10-13T05:18:27+00:00",
        "updated_date": "2025-10-13T05:18:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Runyang Feng",
            "Hyung Jin Chang",
            "Tze Ho Elden Tse",
            "Boeun Kim",
            "Yi Chang",
            "Yixing Gao"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel framework for video-based human pose estimation that combines global and local spatiotemporal modeling for better performance and efficiency.",
        "tldr_zh": "本文提出了一种新颖的框架，用于视频人体姿势估计，结合了全局和局部时空建模，以实现更好的性能和效率。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "A Survey on Agentic Multimodal Large Language Models",
        "summary": "With the recent emergence of revolutionary autonomous agentic systems,\nresearch community is witnessing a significant shift from traditional static,\npassive, and domain-specific AI agents toward more dynamic, proactive, and\ngeneralizable agentic AI. Motivated by the growing interest in agentic AI and\nits potential trajectory toward AGI, we present a comprehensive survey on\nAgentic Multimodal Large Language Models (Agentic MLLMs). In this survey, we\nexplore the emerging paradigm of agentic MLLMs, delineating their conceptual\nfoundations and distinguishing characteristics from conventional MLLM-based\nagents. We establish a conceptual framework that organizes agentic MLLMs along\nthree fundamental dimensions: (i) Agentic internal intelligence functions as\nthe system's commander, enabling accurate long-horizon planning through\nreasoning, reflection, and memory; (ii) Agentic external tool invocation,\nwhereby models proactively use various external tools to extend their\nproblem-solving capabilities beyond their intrinsic knowledge; and (iii)\nAgentic environment interaction further situates models within virtual or\nphysical environments, allowing them to take actions, adapt strategies, and\nsustain goal-directed behavior in dynamic real-world scenarios. To further\naccelerate research in this area for the community, we compile open-source\ntraining frameworks, training and evaluation datasets for developing agentic\nMLLMs. Finally, we review the downstream applications of agentic MLLMs and\noutline future research directions for this rapidly evolving field. To\ncontinuously track developments in this rapidly evolving field, we will also\nactively update a public repository at\nhttps://github.com/HJYao00/Awesome-Agentic-MLLMs.",
        "url": "http://arxiv.org/abs/2510.10991v1",
        "published_date": "2025-10-13T04:07:01+00:00",
        "updated_date": "2025-10-13T04:07:01+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Huanjin Yao",
            "Ruifei Zhang",
            "Jiaxing Huang",
            "Jingyi Zhang",
            "Yibo Wang",
            "Bo Fang",
            "Ruolin Zhu",
            "Yongcheng Jing",
            "Shunyu Liu",
            "Guanbin Li",
            "Dacheng Tao"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper surveys Agentic Multimodal Large Language Models (Agentic MLLMs) which are dynamic, proactive AI systems with potential for generalization towards AGI. It establishes a conceptual framework for these models and provides resources to accelerate research in this area.",
        "tldr_zh": "本文调查了具有潜力通向AGI的动态、主动人工智能系统Agentic Multimodal Large Language Models (Agentic MLLMs)。它为这些模型建立了一个概念框架，并提供资源加速这一领域的研究。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "From Detection to Mitigation: Addressing Bias in Deep Learning Models for Chest X-Ray Diagnosis",
        "summary": "Deep learning models have shown promise in improving diagnostic accuracy from\nchest X-rays, but they also risk perpetuating healthcare disparities when\nperformance varies across demographic groups. In this work, we present a\ncomprehensive bias detection and mitigation framework targeting sex, age, and\nrace-based disparities when performing diagnostic tasks with chest X-rays. We\nextend a recent CNN-XGBoost pipeline to support multi-label classification and\nevaluate its performance across four medical conditions. We show that replacing\nthe final layer of CNN with an eXtreme Gradient Boosting classifier improves\nthe fairness of the subgroup while maintaining or improving the overall\npredictive performance. To validate its generalizability, we apply the method\nto different backbones, namely DenseNet-121 and ResNet-50, and achieve\nsimilarly strong performance and fairness outcomes, confirming its\nmodel-agnostic design. We further compare this lightweight adapter training\nmethod with traditional full-model training bias mitigation techniques,\nincluding adversarial training, reweighting, data augmentation, and active\nlearning, and find that our approach offers competitive or superior bias\nreduction at a fraction of the computational cost. Finally, we show that\ncombining eXtreme Gradient Boosting retraining with active learning yields the\nlargest reduction in bias across all demographic subgroups, both in and out of\ndistribution on the CheXpert and MIMIC datasets, establishing a practical and\neffective path toward equitable deep learning deployment in clinical radiology.",
        "url": "http://arxiv.org/abs/2510.10822v1",
        "published_date": "2025-10-12T22:20:08+00:00",
        "updated_date": "2025-10-12T22:20:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Clemence Mottez",
            "Louisa Fay",
            "Maya Varma",
            "Sophie Ostmeier",
            "Curtis Langlotz"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper presents a framework to detect and mitigate bias in deep learning models for chest X-ray diagnosis, improving fairness across demographic groups while maintaining or enhancing predictive performance.",
        "tldr_zh": "本文提出了一个框架，用于检测和减轻深度学习模型在胸部X射线诊断中的偏见，提高了不同族群间的公平性，同时保持或改进了预测性能。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Seeing My Future: Predicting Situated Interaction Behavior in Virtual Reality",
        "summary": "Virtual and augmented reality systems increasingly demand intelligent\nadaptation to user behaviors for enhanced interaction experiences. Achieving\nthis requires accurately understanding human intentions and predicting future\nsituated behaviors - such as gaze direction and object interactions - which is\nvital for creating responsive VR/AR environments and applications like\npersonalized assistants. However, accurate behavioral prediction demands\nmodeling the underlying cognitive processes that drive human-environment\ninteractions. In this work, we introduce a hierarchical, intention-aware\nframework that models human intentions and predicts detailed situated behaviors\nby leveraging cognitive mechanisms. Given historical human dynamics and the\nobservation of scene contexts, our framework first identifies potential\ninteraction targets and forecasts fine-grained future behaviors. We propose a\ndynamic Graph Convolutional Network (GCN) to effectively capture\nhuman-environment relationships. Extensive experiments on challenging\nreal-world benchmarks and live VR environment demonstrate the effectiveness of\nour approach, achieving superior performance across all metrics and enabling\npractical applications for proactive VR systems that anticipate user behaviors\nand adapt virtual environments accordingly.",
        "url": "http://arxiv.org/abs/2510.10742v1",
        "published_date": "2025-10-12T18:29:01+00:00",
        "updated_date": "2025-10-12T18:29:01+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yuan Xu",
            "Zimu Zhang",
            "Xiaoxuan Ma",
            "Wentao Zhu",
            "Yu Qiao",
            "Yizhou Wang"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a framework that predicts human intentions and behaviors in virtual reality environments to enhance user interaction experiences.",
        "tldr_zh": "该论文介绍了一个框架，可以预测虚拟现实环境中的人类意图和行为，以增强用户的交互体验。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Adversarial Attacks Leverage Interference Between Features in Superposition",
        "summary": "Fundamental questions remain about when and why adversarial examples arise in\nneural networks, with competing views characterising them either as artifacts\nof the irregularities in the decision landscape or as products of sensitivity\nto non-robust input features. In this paper, we instead argue that adversarial\nvulnerability can stem from efficient information encoding in neural networks.\nSpecifically, we show how superposition - where networks represent more\nfeatures than they have dimensions - creates arrangements of latent\nrepresentations that adversaries can exploit. We demonstrate that adversarial\nperturbations leverage interference between superposed features, making attack\npatterns predictable from feature arrangements. Our framework provides a\nmechanistic explanation for two known phenomena: adversarial attack\ntransferability between models with similar training regimes and class-specific\nvulnerability patterns. In synthetic settings with precisely controlled\nsuperposition, we establish that superposition suffices to create adversarial\nvulnerability. We then demonstrate that these findings persist in a ViT trained\non CIFAR-10. These findings reveal adversarial vulnerability can be a byproduct\nof networks' representational compression, rather than flaws in the learning\nprocess or non-robust inputs.",
        "url": "http://arxiv.org/abs/2510.11709v1",
        "published_date": "2025-10-13T17:59:02+00:00",
        "updated_date": "2025-10-13T17:59:02+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Edward Stevinson",
            "Lucas Prieto",
            "Melih Barsbey",
            "Tolga Birdal"
        ],
        "ai_categories": [
            "GAN",
            "Transformer"
        ],
        "tldr": "The paper explains how adversarial attacks exploit superposition in neural networks to create vulnerability, suggesting it is a result of networks' efficient information encoding rather than flaws in the learning process or input data.",
        "tldr_zh": "本文解释了对抗攻击如何利用神经网络中的叠加特性来造成脆弱性，表明这是网络高效信息编码的结果，而不是学习过程或输入数据的缺陷。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "NV3D: Leveraging Spatial Shape Through Normal Vector-based 3D Object Detection",
        "summary": "Recent studies in 3D object detection for autonomous vehicles aim to enrich\nfeatures through the utilization of multi-modal setups or the extraction of\nlocal patterns within LiDAR point clouds. However, multi-modal methods face\nsignificant challenges in feature alignment, and gaining features locally can\nbe oversimplified for complex 3D object detection tasks. In this paper, we\npropose a novel model, NV3D, which utilizes local features acquired from voxel\nneighbors, as normal vectors computed per voxel basis using K-nearest neighbors\n(KNN) and principal component analysis (PCA). This informative feature enables\nNV3D to determine the relationship between the surface and pertinent target\nentities, including cars, pedestrians, or cyclists. During the normal vector\nextraction process, NV3D offers two distinct sampling strategies: normal vector\ndensity-based sampling and FOV-aware bin-based sampling, allowing elimination\nof up to 55% of data while maintaining performance. In addition, we applied\nelement-wise attention fusion, which accepts voxel features as the query and\nvalue and normal vector features as the key, similar to the attention\nmechanism. Our method is trained on the KITTI dataset and has demonstrated\nsuperior performance in car and cyclist detection owing to their spatial\nshapes. In the validation set, NV3D without sampling achieves 86.60% and 80.18%\nmean Average Precision (mAP), greater than the baseline Voxel R-CNN by 2.61%\nand 4.23% mAP, respectively. With both samplings, NV3D achieves 85.54% mAP in\ncar detection, exceeding the baseline by 1.56% mAP, despite roughly 55% of\nvoxels being filtered out.",
        "url": "http://arxiv.org/abs/2510.11632v1",
        "published_date": "2025-10-13T17:13:06+00:00",
        "updated_date": "2025-10-13T17:13:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "I.2.6; I.2.9; I.2.10; I.4.8; I.4.10; I.5.1; I.5.4"
        ],
        "authors": [
            "Krittin Chaowakarn",
            "Paramin Sangwongngam",
            "Nang Htet Htet Aung",
            "Chalie Charoenlarpnopparut"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "NV3D introduces a novel 3D object detection model that leverages normal vectors for spatial shape analysis, achieving superior performance in car and cyclist detection tasks.",
        "tldr_zh": "NV3D 提出了一种新颖的 3D 物体检测模型，利用法向量进行空间形状分析，在车辆和骑行者检测任务中表现出色。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Benchmarking foundation models for hyperspectral image classification: Application to cereal crop type mapping",
        "summary": "Foundation models are transforming Earth observation, but their potential for\nhyperspectral crop mapping remains underexplored. This study benchmarks three\nfoundation models for cereal crop mapping using hyperspectral imagery:\nHyperSigma, DOFA, and Vision Transformers pre-trained on the SpectralEarth\ndataset (a large multitemporal hyperspectral archive). Models were fine-tuned\non manually labeled data from a training region and evaluated on an independent\ntest region. Performance was measured with overall accuracy (OA), average\naccuracy (AA), and F1-score. HyperSigma achieved an OA of 34.5% (+/- 1.8%),\nDOFA reached 62.6% (+/- 3.5%), and the SpectralEarth model achieved an OA of\n93.5% (+/- 0.8%). A compact SpectralEarth variant trained from scratch achieved\n91%, highlighting the importance of model architecture for strong\ngeneralization across geographic regions and sensor platforms. These results\nprovide a systematic evaluation of foundation models for operational\nhyperspectral crop mapping and outline directions for future model development.",
        "url": "http://arxiv.org/abs/2510.11576v1",
        "published_date": "2025-10-13T16:21:59+00:00",
        "updated_date": "2025-10-13T16:21:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Walid Elbarz",
            "Mohamed Bourriz",
            "Hicham Hajji",
            "Hamd Ait Abdelali",
            "François Bourzeix"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper benchmarks foundation models for hyperspectral crop mapping and highlights the importance of model architecture for strong generalization across geographic regions and sensor platforms.",
        "tldr_zh": "本文对高光谱农作物图谱的基础模型进行了基准测试，并强调了模型架构对地理区域和传感器平台的强大泛化的重要性。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SNAP: Towards Segmenting Anything in Any Point Cloud",
        "summary": "Interactive 3D point cloud segmentation enables efficient annotation of\ncomplex 3D scenes through user-guided prompts. However, current approaches are\ntypically restricted in scope to a single domain (indoor or outdoor), and to a\nsingle form of user interaction (either spatial clicks or textual prompts).\nMoreover, training on multiple datasets often leads to negative transfer,\nresulting in domain-specific tools that lack generalizability. To address these\nlimitations, we present \\textbf{SNAP} (\\textbf{S}egment a\\textbf{N}ything in\n\\textbf{A}ny \\textbf{P}oint cloud), a unified model for interactive 3D\nsegmentation that supports both point-based and text-based prompts across\ndiverse domains. Our approach achieves cross-domain generalizability by\ntraining on 7 datasets spanning indoor, outdoor, and aerial environments, while\nemploying domain-adaptive normalization to prevent negative transfer. For\ntext-prompted segmentation, we automatically generate mask proposals without\nhuman intervention and match them against CLIP embeddings of textual queries,\nenabling both panoptic and open-vocabulary segmentation. Extensive experiments\ndemonstrate that SNAP consistently delivers high-quality segmentation results.\nWe achieve state-of-the-art performance on 8 out of 9 zero-shot benchmarks for\nspatial-prompted segmentation and demonstrate competitive results on all 5\ntext-prompted benchmarks. These results show that a unified model can match or\nexceed specialized domain-specific approaches, providing a practical tool for\nscalable 3D annotation. Project page is at, https://neu-vi.github.io/SNAP/",
        "url": "http://arxiv.org/abs/2510.11565v1",
        "published_date": "2025-10-13T16:07:00+00:00",
        "updated_date": "2025-10-13T16:07:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aniket Gupta",
            "Hanhui Wang",
            "Charles Saunders",
            "Aruni RoyChowdhury",
            "Hanumant Singh",
            "Huaizu Jiang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces SNAP, a model for interactive 3D point cloud segmentation that can handle diverse domains and user prompts, achieving state-of-the-art results on various benchmarks.",
        "tldr_zh": "本文介绍了SNAP，一种用于交互式3D点云分割的模型，可以处理多样化的领域和用户提示，达到了各种基准测试的最先进结果。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "DocReward: A Document Reward Model for Structuring and Stylizing",
        "summary": "Recent advances in agentic workflows have enabled the automation of tasks\nsuch as professional document generation. However, they primarily focus on\ntextual quality, neglecting visual structure and style, which are crucial for\nreadability and engagement. This gap arises mainly from the absence of suitable\nreward models to guide agentic workflows toward producing documents with\nstronger structural and stylistic quality. To address this, we propose\nDocReward, a document reward model that evaluates documents based on their\nstructure and style. We construct a multi-domain dataset DocPair of 117K paired\ndocuments, covering 32 domains and 267 document types, each including a high-\nand low-professionalism document with identical content but different structure\nand style. This enables the model to evaluate professionalism comprehensively,\nand in a textual-quality-agnostic way. DocReward is trained using the\nBradley-Terry loss to score documents, penalizing predictions that contradict\nthe annotated ranking. To assess the performance of reward models, we create a\ntest dataset containing document bundles ranked by well-educated human\nevaluators. Notably, DocReward outperforms GPT-4o and GPT-5 in accuracy by 30.6\nand 19.4 percentage points, respectively, demonstrating its superiority over\nbaselines. In an extrinsic evaluation of document generation, DocReward\nachieves a significantly higher win rate of 60.8%, compared to GPT-5's 37.7%\nwin rate, demonstrating its utility in guiding generation agents toward\nproducing human-preferred documents.",
        "url": "http://arxiv.org/abs/2510.11391v1",
        "published_date": "2025-10-13T13:36:32+00:00",
        "updated_date": "2025-10-13T13:36:32+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Junpeng Liu",
            "Yuzhong Zhao",
            "Bowen Cao",
            "Jiayu Ding",
            "Yilin Jia",
            "Tengchao Lv",
            "Yupan Huang",
            "Shaohan Huang",
            "Nan Yang",
            "Li Dong",
            "Lei Cui",
            "Tao Ge",
            "Xun Wang",
            "Huitian Jiao",
            "Sun Mao",
            "FNU Kartik",
            "Si-Qing Chen",
            "Wai Lam",
            "Furu Wei"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "DocReward proposes a document reward model to evaluate documents based on structure and style, outperforming GPT-4o and GPT-5 in accuracy by a significant margin in guiding document generation.",
        "tldr_zh": "DocReward 提出了一种文件奖励模型，评估文档的结构和风格，表现优异，在引导文件生成方面优于 GPT-4o 和 GPT-5。",
        "relevance_score": 3,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "sketch2symm: Symmetry-aware sketch-to-shape generation via semantic bridging",
        "summary": "Sketch-based 3D reconstruction remains a challenging task due to the abstract\nand sparse nature of sketch inputs, which often lack sufficient semantic and\ngeometric information. To address this, we propose Sketch2Symm, a two-stage\ngeneration method that produces geometrically consistent 3D shapes from\nsketches. Our approach introduces semantic bridging via sketch-to-image\ntranslation to enrich sparse sketch representations, and incorporates symmetry\nconstraints as geometric priors to leverage the structural regularity commonly\nfound in everyday objects. Experiments on mainstream sketch datasets\ndemonstrate that our method achieves superior performance compared to existing\nsketch-based reconstruction methods in terms of Chamfer Distance, Earth Mover's\nDistance, and F-Score, verifying the effectiveness of the proposed semantic\nbridging and symmetry-aware design.",
        "url": "http://arxiv.org/abs/2510.11303v1",
        "published_date": "2025-10-13T11:49:45+00:00",
        "updated_date": "2025-10-13T11:49:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yan Zhou",
            "Mingji Li",
            "Xiantao Zeng",
            "Jie Lin",
            "Yuexia Zhou"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes Sketch2Symm, a two-stage generation method for producing 3D shapes from sketches by introducing semantic bridging and symmetry constraints, outperforming existing methods in terms of performance metrics.",
        "tldr_zh": "本文提出了Sketch2Symm，一种两阶段生成方法，通过引入语义桥接和对称约束，从草图中产生几何一致的3D形状，在性能指标上优于现有方法。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "$Δ\\mathrm{Energy}$: Optimizing Energy Change During Vision-Language Alignment Improves both OOD Detection and OOD Generalization",
        "summary": "Recent approaches for vision-language models (VLMs) have shown remarkable\nsuccess in achieving fast downstream adaptation. When applied to real-world\ndownstream tasks, VLMs inevitably encounter both the in-distribution (ID) data\nand out-of-distribution (OOD) data. The OOD datasets often include both\ncovariate shifts (e.g., known classes with changes in image styles) and\nsemantic shifts (e.g., test-time unseen classes). This highlights the\nimportance of improving VLMs' generalization ability to covariate-shifted OOD\ndata, while effectively detecting open-set semantic-shifted OOD classes. In\nthis paper, inspired by the substantial energy change observed in closed-set\ndata when re-aligning vision-language modalities (specifically by directly\nreducing the maximum cosine similarity to a low value), we introduce a novel\nOOD score, named {\\Delta}Energy. {\\Delta}Energy significantly outperforms the\nvanilla energy-based OOD score and provides a more reliable approach for OOD\ndetection. Furthermore, {\\Delta}Energy can simultaneously improve OOD\ngeneralization under covariate shifts, which is achieved by lower-bound\nmaximization for {\\Delta}Energy (termed EBM). EBM is theoretically proven to\nnot only enhance OOD detection but also yields a domain-consistent Hessian,\nwhich serves as a strong indicator for OOD generalization. Based on this\nfinding, we developed a unified fine-tuning framework that allows for improving\nVLMs' robustness in both OOD generalization and OOD detection. Extensive\nexperiments on challenging OOD detection and generalization benchmarks\ndemonstrate the superiority of our method, outperforming recent approaches by\n10% to 25% in AUROC.",
        "url": "http://arxiv.org/abs/2510.11296v1",
        "published_date": "2025-10-13T11:36:58+00:00",
        "updated_date": "2025-10-13T11:36:58+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Lin Zhu",
            "Yifeng Yang",
            "Xinbing Wang",
            "Qinying Gu",
            "Nanyang Ye"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel OOD score named Delta Energy to improve OOD detection and generalization in vision-language models, achieving superior performance in experiments.",
        "tldr_zh": "本文介绍了一种名为Delta Energy的新型OOD分数，旨在改善视觉语言模型中的OOD检测和泛化，在实验中取得了卓越表现。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "EEMS: Edge-Prompt Enhanced Medical Image Segmentation Based on Learnable Gating Mechanism",
        "summary": "Medical image segmentation is vital for diagnosis, treatment planning, and\ndisease monitoring but is challenged by complex factors like ambiguous edges\nand background noise. We introduce EEMS, a new model for segmentation,\ncombining an Edge-Aware Enhancement Unit (EAEU) and a Multi-scale Prompt\nGeneration Unit (MSPGU). EAEU enhances edge perception via multi-frequency\nfeature extraction, accurately defining boundaries. MSPGU integrates high-level\nsemantic and low-level spatial features using a prompt-guided approach,\nensuring precise target localization. The Dual-Source Adaptive Gated Fusion\nUnit (DAGFU) merges edge features from EAEU with semantic features from MSPGU,\nenhancing segmentation accuracy and robustness. Tests on datasets like ISIC2018\nconfirm EEMS's superior performance and reliability as a clinical tool.",
        "url": "http://arxiv.org/abs/2510.11287v1",
        "published_date": "2025-10-13T11:21:57+00:00",
        "updated_date": "2025-10-13T11:21:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Han Xia",
            "Quanjun Li",
            "Qian Li",
            "Zimeng Li",
            "Hongbin Ye",
            "Yupeng Liu",
            "Haolun Li",
            "Xuhang Chen"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "EEMS is a new model for medical image segmentation that enhances edge perception and integrates high-level semantic features for precise target localization. It outperforms existing models on datasets like ISIC2018.",
        "tldr_zh": "EEMS是一种新的医学图像分割模型，通过增强边缘感知和整合高级语义特征实现精确的目标定位。在ISIC2018等数据集上表现优异。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Multiview Manifold Evidential Fusion for PolSAR Image Classification",
        "summary": "Polarimetric Synthetic Aperture Radar (PolSAR) covariance matrices and their\nextracted multi-features - such as scattering angle, entropy, texture, and\nboundary descriptors - provide complementary and physically interpretable\ninformation for image classification. Traditional fusion strategies typically\nconcatenate these features or employ deep learning networks to combine them.\nHowever, the covariance matrices and multi-features, as two complementary\nviews, lie on different manifolds with distinct geometric structures. Existing\nfusion methods also overlook the varying importance of different views and\nignore uncertainty, often leading to unreliable predictions. To address these\nissues, we propose a Multiview Manifold Evidential Fusion (MMEFnet) method to\neffectively fuse these two views. It gives a new framework to integrate PolSAR\nmanifold learning and evidence fusion into a unified architecture.\nSpecifically, covariance matrices are represented on the Hermitian Positive\nDefinite (HPD) manifold, while multi-features are modeled on the Grassmann\nmanifold. Two different kernel metric learning networks are constructed to\nlearn their manifold representations. Subsequently, a trusted multiview\nevidence fusion, replacing the conventional softmax classifier, estimates\nbelief mass and quantifies the uncertainty of each view from the learned deep\nfeatures. Finally, a Dempster-Shafer theory-based fusion strategy combines\nevidence, enabling a more reliable and interpretable classification. Extensive\nexperiments on three real-world PolSAR datasets demonstrate that the proposed\nmethod consistently outperforms existing approaches in accuracy, robustness,\nand interpretability.",
        "url": "http://arxiv.org/abs/2510.11171v1",
        "published_date": "2025-10-13T09:05:51+00:00",
        "updated_date": "2025-10-13T09:05:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junfei Shi",
            "Haojia Zhang",
            "Haiyan Jin",
            "Junhuai Li",
            "Xiaogang Song",
            "Yuanfan Guo",
            "Haonan Su",
            "Weisi Lin"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a method to effectively fuse PolSAR covariance matrices and multi-features for image classification, outperforming existing approaches in accuracy, robustness, and interpretability.",
        "tldr_zh": "该论文介绍了一种有效融合PolSAR协方差矩阵和多特征的方法，优于现有方法在准确性、稳健性和可解释性方面。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Validation of an Artificial Intelligence Tool for the Detection of Sperm DNA Fragmentation Using the TUNEL In Situ Hybridization Assay",
        "summary": "Sperm DNA fragmentation (SDF) is a critical parameter in male fertility\nassessment that conventional semen analysis fails to evaluate. This study\npresents the validation of a novel artificial intelligence (AI) tool designed\nto detect SDF through digital analysis of phase contrast microscopy images,\nusing the terminal deoxynucleotidyl transferase dUTP nick end labeling (TUNEL)\nassay as the gold standard reference. Utilising the established link between\nsperm morphology and DNA integrity, the present work proposes a morphology\nassisted ensemble AI model that combines image processing techniques with\nstate-of-the-art transformer based machine learning models (GC-ViT) for the\nprediction of DNA fragmentation in sperm from phase contrast images. The\nensemble model is benchmarked against a pure transformer `vision' model as well\nas a `morphology-only` model. Promising results show the proposed framework is\nable to achieve sensitivity of 60\\% and specificity of 75\\%. This\nnon-destructive methodology represents a significant advancement in\nreproductive medicine by enabling real-time sperm selection based on DNA\nintegrity for clinical diagnostic and therapeutic applications.",
        "url": "http://arxiv.org/abs/2510.11142v1",
        "published_date": "2025-10-13T08:32:11+00:00",
        "updated_date": "2025-10-13T08:32:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Byron Alexander Jacobs",
            "Aqeel Morris",
            "Ifthakaar Shaik",
            "Frando Lin"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper presents a novel AI tool for detecting sperm DNA fragmentation using microscopy images and achieves promising results in real-time sperm selection based on DNA integrity.",
        "tldr_zh": "本文提出了一种新颖的人工智能工具，用于通过显微镜图片检测精子DNA碎裂，并在实时精子选择方面取得了令人期待的结果。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Enhancing Zero-Shot Anomaly Detection: CLIP-SAM Collaboration with Cascaded Prompts",
        "summary": "Recently, the powerful generalization ability exhibited by foundation models\nhas brought forth new solutions for zero-shot anomaly segmentation tasks.\nHowever, guiding these foundation models correctly to address downstream tasks\nremains a challenge. This paper proposes a novel two-stage framework, for\nzero-shot anomaly segmentation tasks in industrial anomaly detection. This\nframework excellently leverages the powerful anomaly localization capability of\nCLIP and the boundary perception ability of SAM.(1) To mitigate SAM's\ninclination towards object segmentation, we propose the Co-Feature Point Prompt\nGeneration (PPG) module. This module collaboratively utilizes CLIP and SAM to\ngenerate positive and negative point prompts, guiding SAM to focus on\nsegmenting anomalous regions rather than the entire object. (2) To further\noptimize SAM's segmentation results and mitigate rough boundaries and isolated\nnoise, we introduce the Cascaded Prompts for SAM (CPS) module. This module\nemploys hybrid prompts cascaded with a lightweight decoder of SAM, achieving\nprecise segmentation of anomalous regions. Across multiple datasets, consistent\nexperimental validation demonstrates that our approach achieves\nstate-of-the-art zero-shot anomaly segmentation results. Particularly\nnoteworthy is our performance on the Visa dataset, where we outperform the\nstate-of-the-art methods by 10.3\\% and 7.7\\% in terms of {$F_1$-max} and AP\nmetrics, respectively.",
        "url": "http://arxiv.org/abs/2510.11028v1",
        "published_date": "2025-10-13T05:53:49+00:00",
        "updated_date": "2025-10-13T05:53:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanning Hou",
            "Ke Xu",
            "Junfa Li",
            "Yanran Ruan",
            "Jianfeng Qiu"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "This paper proposes a two-stage framework for zero-shot anomaly segmentation using CLIP and SAM, achieving state-of-the-art results on multiple datasets.",
        "tldr_zh": "本文提出了一个使用CLIP和SAM的两阶段框架，实现了在多个数据集上的最先进结果。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "The Easy Path to Robustness: Coreset Selection using Sample Hardness",
        "summary": "Designing adversarially robust models from a data-centric perspective\nrequires understanding which input samples are most crucial for learning\nresilient features. While coreset selection provides a mechanism for efficient\ntraining on data subsets, current algorithms are designed for clean accuracy\nand fall short in preserving robustness. To address this, we propose a\nframework linking a sample's adversarial vulnerability to its\n\\textit{hardness}, which we quantify using the average input gradient norm\n(AIGN) over training. We demonstrate that \\textit{easy} samples (with low AIGN)\nare less vulnerable and occupy regions further from the decision boundary.\nLeveraging this insight, we present EasyCore, a coreset selection algorithm\nthat retains only the samples with low AIGN for training. We empirically show\nthat models trained on EasyCore-selected data achieve significantly higher\nadversarial accuracy than those trained with competing coreset methods under\nboth standard and adversarial training. As AIGN is a model-agnostic dataset\nproperty, EasyCore is an efficient and widely applicable data-centric method\nfor improving adversarial robustness. We show that EasyCore achieves up to 7\\%\nand 5\\% improvement in adversarial accuracy under standard training and TRADES\nadversarial training, respectively, compared to existing coreset methods.",
        "url": "http://arxiv.org/abs/2510.11018v1",
        "published_date": "2025-10-13T05:28:16+00:00",
        "updated_date": "2025-10-13T05:28:16+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Pranav Ramesh",
            "Arjun Roy",
            "Deepak Ravikumar",
            "Kaushik Roy",
            "Gopalakrishnan Srinivasan"
        ],
        "ai_categories": [
            "AIGC"
        ],
        "tldr": "The paper proposes a framework called EasyCore for selecting crucial data samples to improve adversarial robustness in models, achieving significant improvements in adversarial accuracy compared to existing methods.",
        "tldr_zh": "本文提出了一种名为EasyCore的框架，用于选择关键的数据样本，从而改善模型的对抗鲁棒性，与现有方法相比，在对抗准确性方面取得了显著进展。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Chart-RVR: Reinforcement Learning with Verifiable Rewards for Explainable Chart Reasoning",
        "summary": "The capabilities of Large Vision-Language Models (LVLMs) have reached\nstate-of-the-art on many visual reasoning tasks, including chart reasoning, yet\nthey still falter on out-of-distribution (OOD) data, and degrade further when\nasked to produce their chain-of-thought (CoT) rationales, limiting\nexplainability. We present Chart-RVR, a general framework that fine-tunes LVLMs\nto be more robust and explainable for chart reasoning by coupling Group\nRelative Policy Optimization (GRPO) with automatically verifiable rewards. Our\nframework comprises of three rewards that maximize: (i) correct chart-type\nclassification, (ii) faithful chart table reconstruction, and (iii) process\nconformity. Applied to 3-billion-parameter LVLMs, Chart-RVR consistently\noutperforms standard supervised fine-tuning (SFT) on both in-distribution and\nout-of-distribution datasets, closing the OOD performance gap while improving\nrationale fidelity. The resulting models, the Chart-RVR-3B series, achieve\nstate-of-the-art results on six chart-reasoning benchmarks spanning in-domain\nand OOD settings, surpassing all existing models of comparable size. Beyond\naccuracy, Chart-RVR yields more interpretable CoT rationales, strengthening\ntrust and reliability - showcasing the power of verifiable rewards with GRPO\nfor training reliable, interpretable chart-reasoning models.",
        "url": "http://arxiv.org/abs/2510.10973v1",
        "published_date": "2025-10-13T03:25:35+00:00",
        "updated_date": "2025-10-13T03:25:35+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Sanchit Sinha",
            "Oana Frunza",
            "Kashif Rasul",
            "Yuriy Nevmyvaka",
            "Aidong Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces Chart-RVR, a framework that enhances the robustness and explainability of Large Vision-Language Models for chart reasoning by using verifiable rewards. It outperforms standard supervised fine-tuning on in-distribution and out-of-distribution datasets, achieving state-of-the-art results on various benchmarks.",
        "tldr_zh": "本文介绍了Chart-RVR，这是一个通过使用可验证的奖励来增强大型视觉语言模型对图表推理的鲁棒性和可解释性的框架。它在各种基准测试中表现优异，超越了标准监督微调的结果。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Where on Earth? A Vision-Language Benchmark for Probing Model Geolocation Skills Across Scales",
        "summary": "Vision-language models (VLMs) have advanced rapidly, yet their capacity for\nimage-grounded geolocation in open-world conditions, a task that is challenging\nand of demand in real life, has not been comprehensively evaluated. We present\nEarthWhere, a comprehensive benchmark for VLM image geolocation that evaluates\nvisual recognition, step-by-step reasoning, and evidence use. EarthWhere\ncomprises 810 globally distributed images across two complementary geolocation\nscales: WhereCountry (i.e., 500 multiple-choice question-answering, with\ncountry-level answer and panoramas) and WhereStreet (i.e., 310 fine-grained\nstreet-level identification tasks requiring multi-step reasoning with optional\nweb search). For evaluation, we adopt the final-prediction metrics: location\naccuracies within k km (Acc@k) for coordinates and hierarchical path scores for\ntextual localization. Beyond this, we propose to explicitly score intermediate\nreasoning chains using human-verified key visual clues and a Shapley-reweighted\nthinking score that attributes credit to each clue's marginal contribution. We\nbenchmark 13 state-of-the-art VLMs with web searching tools on our EarthWhere\nand report different types of final answer accuracies as well as the calibrated\nmodel thinking scores. Overall, Gemini-2.5-Pro achieves the best average\naccuracy at 56.32%, while the strongest open-weight model, GLM-4.5V, reaches\n34.71%. We reveal that web search and reasoning do not guarantee improved\nperformance when visual clues are limited, and models exhibit regional biases,\nachieving up to 42.7% higher scores in certain areas than others. These\nfindings highlight not only the promise but also the persistent challenges of\nmodels to mitigate bias and achieve robust, fine-grained localization. We\nopen-source our benchmark at https://github.com/UCSC-VLAA/EarthWhere.",
        "url": "http://arxiv.org/abs/2510.10880v1",
        "published_date": "2025-10-13T01:12:21+00:00",
        "updated_date": "2025-10-13T01:12:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhaofang Qian",
            "Hardy Chen",
            "Zeyu Wang",
            "Li Zhang",
            "Zijun Wang",
            "Xiaoke Huang",
            "Hui Liu",
            "Xianfeng Tang",
            "Zeyu Zheng",
            "Haoqin Tu",
            "Cihang Xie",
            "Yuyin Zhou"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper introduces a benchmark called EarthWhere to evaluate vision-language models' geolocation skills, highlighting the challenges and biases models face in open-world conditions.",
        "tldr_zh": "本文介绍了一个名为EarthWhere的基准测试，用于评估视觉语言模型在地理定位方面的技能，突出了这些模型在开放世界条件下面临的挑战和偏见。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "FastHMR: Accelerating Human Mesh Recovery via Token and Layer Merging with Diffusion Decoding",
        "summary": "Recent transformer-based models for 3D Human Mesh Recovery (HMR) have\nachieved strong performance but often suffer from high computational cost and\ncomplexity due to deep transformer architectures and redundant tokens. In this\npaper, we introduce two HMR-specific merging strategies: Error-Constrained\nLayer Merging (ECLM) and Mask-guided Token Merging (Mask-ToMe). ECLM\nselectively merges transformer layers that have minimal impact on the Mean Per\nJoint Position Error (MPJPE), while Mask-ToMe focuses on merging background\ntokens that contribute little to the final prediction. To further address the\npotential performance drop caused by merging, we propose a diffusion-based\ndecoder that incorporates temporal context and leverages pose priors learned\nfrom large-scale motion capture datasets. Experiments across multiple\nbenchmarks demonstrate that our method achieves up to 2.3x speed-up while\nslightly improving performance over the baseline.",
        "url": "http://arxiv.org/abs/2510.10868v1",
        "published_date": "2025-10-13T00:23:17+00:00",
        "updated_date": "2025-10-13T00:23:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Soroush Mehraban",
            "Andrea Iaboni",
            "Babak Taati"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "Dataset"
        ],
        "tldr": "This paper introduces two merging strategies to accelerate 3D Human Mesh Recovery models with minimal impact on accuracy and proposes a diffusion-based decoder to improve performance.",
        "tldr_zh": "本文提出两种合并策略来加速3D人体网格恢复模型，同时提出了一种扩散解码器来提高性能。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MSCloudCAM: Cross-Attention with Multi-Scale Context for Multispectral Cloud Segmentation",
        "summary": "Clouds remain a critical challenge in optical satellite imagery, hindering\nreliable analysis for environmental monitoring, land cover mapping, and climate\nresearch. To overcome this, we propose MSCloudCAM, a Cross-Attention with\nMulti-Scale Context Network tailored for multispectral and multi-sensor cloud\nsegmentation. Our framework exploits the spectral richness of Sentinel-2\n(CloudSEN12) and Landsat-8 (L8Biome) data to classify four semantic categories:\nclear sky, thin cloud, thick cloud, and cloud shadow. MSCloudCAM combines a\nSwin Transformer backbone for hierarchical feature extraction with multi-scale\ncontext modules ASPP and PSP for enhanced scale-aware learning. A\nCross-Attention block enables effective multisensor and multispectral feature\nfusion, while the integration of an Efficient Channel Attention Block (ECAB)\nand a Spatial Attention Module adaptively refine feature representations.\nComprehensive experiments on CloudSEN12 and L8Biome demonstrate that MSCloudCAM\ndelivers state-of-the-art segmentation accuracy, surpassing leading baseline\narchitectures while maintaining competitive parameter efficiency and FLOPs.\nThese results underscore the model's effectiveness and practicality, making it\nwell-suited for large-scale Earth observation tasks and real-world\napplications.",
        "url": "http://arxiv.org/abs/2510.10802v1",
        "published_date": "2025-10-12T20:40:22+00:00",
        "updated_date": "2025-10-12T20:40:22+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "F.2.2, I.2.7"
        ],
        "authors": [
            "Md Abdullah Al Mazid",
            "Liangdong Deng",
            "Naphtali Rishe"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "MSCloudCAM proposes a Cross-Attention with Multi-Scale Context Network for multispectral cloud segmentation, achieving state-of-the-art accuracy in satellite imagery analysis.",
        "tldr_zh": "MSCloudCAM提出了一种适用于多光谱云分割的交叉注意力与多尺度上下文网络，实现卫星图像分析的最先进准确性。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.0
    },
    {
        "title": "Structured Spectral Graph Learning for Multi-label Abnormality Classification in 3D Chest CT Scans",
        "summary": "With the growing volume of CT examinations, there is an increasing demand for\nautomated tools such as organ segmentation, abnormality detection, and report\ngeneration to support radiologists in managing their clinical workload.\nMulti-label classification of 3D Chest CT scans remains a critical yet\nchallenging problem due to the complex spatial relationships inherent in\nvolumetric data and the wide variability of abnormalities. Existing methods\nbased on 3D convolutional neural networks struggle to capture long-range\ndependencies, while Vision Transformers often require extensive pre-training on\nlarge-scale, domain-specific datasets to perform competitively. In this work,\nwe propose a 2.5D alternative by introducing a new graph-based framework that\nrepresents 3D CT volumes as structured graphs, where axial slice triplets serve\nas nodes processed through spectral graph convolution, enabling the model to\nreason over inter-slice dependencies while maintaining complexity compatible\nwith clinical deployment. Our method, trained and evaluated on 3 datasets from\nindependent institutions, achieves strong cross-dataset generalization, and\nshows competitive performance compared to state-of-the-art visual encoders. We\nfurther conduct comprehensive ablation studies to evaluate the impact of\nvarious aggregation strategies, edge-weighting schemes, and graph connectivity\npatterns. Additionally, we demonstrate the broader applicability of our\napproach through transfer experiments on automated radiology report generation\nand abdominal CT data.\\\\ This work extends our previous contribution presented\nat the MICCAI 2025 EMERGE Workshop.",
        "url": "http://arxiv.org/abs/2510.10779v1",
        "published_date": "2025-10-12T19:49:51+00:00",
        "updated_date": "2025-10-12T19:49:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Theo Di Piazza",
            "Carole Lazarus",
            "Olivier Nempont",
            "Loic Boussel"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes a graph-based framework for multi-label abnormality classification in 3D Chest CT scans, achieving strong performance and cross-dataset generalization.",
        "tldr_zh": "本文提出了一种基于图的框架，用于多标签异常分类的3D胸部CT扫描，在跨数据集泛化方面表现出色。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "SCOOP'D: Learning Mixed-Liquid-Solid Scooping via Sim2Real Generative Policy",
        "summary": "Scooping items with tools such as spoons and ladles is common in daily life,\nranging from assistive feeding to retrieving items from environmental disaster\nsites. However, developing a general and autonomous robotic scooping policy is\nchallenging since it requires reasoning about complex tool-object interactions.\nFurthermore, scooping often involves manipulating deformable objects, such as\ngranular media or liquids, which is challenging due to their\ninfinite-dimensional configuration spaces and complex dynamics. We propose a\nmethod, SCOOP'D, which uses simulation from OmniGibson (built on NVIDIA\nOmniverse) to collect scooping demonstrations using algorithmic procedures that\nrely on privileged state information. Then, we use generative policies via\ndiffusion to imitate demonstrations from observational input. We directly apply\nthe learned policy in diverse real-world scenarios, testing its performance on\nvarious item quantities, item characteristics, and container types. In\nzero-shot deployment, our method demonstrates promising results across 465\ntrials in diverse scenarios, including objects of different difficulty levels\nthat we categorize as \"Level 1\" and \"Level 2.\" SCOOP'D outperforms all\nbaselines and ablations, suggesting that this is a promising approach to\nacquiring robotic scooping skills. Project page is at\nhttps://scoopdiff.github.io/.",
        "url": "http://arxiv.org/abs/2510.11566v1",
        "published_date": "2025-10-13T16:11:34+00:00",
        "updated_date": "2025-10-13T16:11:34+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Kuanning Wang",
            "Yongchong Gu",
            "Yuqian Fu",
            "Zeyu Shangguan",
            "Sicheng He",
            "Xiangyang Xue",
            "Yanwei Fu",
            "Daniel Seita"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes a method called SCOOP'D for robotic scooping using simulation and generative policies, showing promising results in diverse real-world scenarios.",
        "tldr_zh": "本文提出了一种名为SCOOP'D的方法，利用模拟和生成策略进行机器人舀取，在多样的现实场景中展示出了有希望的结果。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "REACT3D: Recovering Articulations for Interactive Physical 3D Scenes",
        "summary": "Interactive 3D scenes are increasingly vital for embodied intelligence, yet\nexisting datasets remain limited due to the labor-intensive process of\nannotating part segmentation, kinematic types, and motion trajectories. We\npresent REACT3D, a scalable zero-shot framework that converts static 3D scenes\ninto simulation-ready interactive replicas with consistent geometry, enabling\ndirect use in diverse downstream tasks. Our contributions include: (i)\nopenable-object detection and segmentation to extract candidate movable parts\nfrom static scenes, (ii) articulation estimation that infers joint types and\nmotion parameters, (iii) hidden-geometry completion followed by interactive\nobject assembly, and (iv) interactive scene integration in widely supported\nformats to ensure compatibility with standard simulation platforms. We achieve\nstate-of-the-art performance on detection/segmentation and articulation metrics\nacross diverse indoor scenes, demonstrating the effectiveness of our framework\nand providing a practical foundation for scalable interactive scene generation,\nthereby lowering the barrier to large-scale research on articulated scene\nunderstanding. Our project page is\n\\textit{\\hypersetup{urlcolor=black}\\href{https://react3d.github.io/}{react3d.github.io}}.",
        "url": "http://arxiv.org/abs/2510.11340v1",
        "published_date": "2025-10-13T12:37:59+00:00",
        "updated_date": "2025-10-13T12:37:59+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Zhao Huang",
            "Boyang Sun",
            "Alexandros Delitzas",
            "Jiaqi Chen",
            "Marc Pollefeys"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "REACT3D presents a framework for converting static 3D scenes into interactive replicas with consistent geometry, aiding in scalable interactive scene generation for articulated scene understanding.",
        "tldr_zh": "REACT3D提出了一个框架，将静态3D场景转换为具有一致几何结构的交互副本，有助于可伸缩的交互式场景生成以实现关节场景理解。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Investigating Identity Signals in Conversational Facial Dynamics via Disentangled Expression Features",
        "summary": "This work investigates whether individuals can be identified solely through\nthe pure dynamical components of their facial expressions, independent of\nstatic facial appearance. We leverage the FLAME 3D morphable model to achieve\nexplicit disentanglement between facial shape and expression dynamics,\nextracting frame-by-frame parameters from conversational videos while retaining\nonly expression and jaw coefficients. On the CANDOR dataset of 1,429 speakers\nin naturalistic conversations, our Conformer model with supervised contrastive\nlearning achieves 61.14\\%accuracy on 1,429-way classification -- 458 times\nabove chance -- demonstrating that facial dynamics carry strong identity\nsignatures. We introduce a drift-to-noise ratio (DNR) that quantifies the\nreliability of shape expression separation by measuring across-session shape\nchanges relative to within-session variability. DNR strongly negatively\ncorrelates with recognition performance, confirming that unstable shape\nestimation compromises dynamic identification. Our findings reveal\nperson-specific signatures in conversational facial dynamics, with implications\nfor social perception and clinical assessment.",
        "url": "http://arxiv.org/abs/2510.11223v1",
        "published_date": "2025-10-13T10:06:25+00:00",
        "updated_date": "2025-10-13T10:06:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Masoumeh Chapariniya",
            "Pierre Vuillecard",
            "Jean-Marc Odobez",
            "Volker Dellwo",
            "Teodora Vukovic"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper investigates the identification of individuals through facial dynamics in conversational videos, showing that facial expressions carry strong identity signatures.",
        "tldr_zh": "本文研究了通过对话视频中的面部动态来识别个人，表明面部表情具有强烈的身份特征。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Saudi Sign Language Translation Using T5",
        "summary": "This paper explores the application of T5 models for Saudi Sign Language\n(SSL) translation using a novel dataset. The SSL dataset includes three\nchallenging testing protocols, enabling comprehensive evaluation across\ndifferent scenarios. Additionally, it captures unique SSL characteristics, such\nas face coverings, which pose challenges for sign recognition and translation.\nIn our experiments, we investigate the impact of pre-training on American Sign\nLanguage (ASL) data by comparing T5 models pre-trained on the YouTubeASL\ndataset with models trained directly on the SSL dataset. Experimental results\ndemonstrate that pre-training on YouTubeASL significantly improves models'\nperformance (roughly $3\\times$ in BLEU-4), indicating cross-linguistic\ntransferability in sign language models. Our findings highlight the benefits of\nleveraging large-scale ASL data to improve SSL translation and provide insights\ninto the development of more effective sign language translation systems. Our\ncode is publicly available at our GitHub repository.",
        "url": "http://arxiv.org/abs/2510.11183v1",
        "published_date": "2025-10-13T09:18:34+00:00",
        "updated_date": "2025-10-13T09:18:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ali Alhejab",
            "Tomas Zelezny",
            "Lamya Alkanhal",
            "Ivan Gruber",
            "Yazeed Alharbi",
            "Jakub Straka",
            "Vaclav Javorek",
            "Marek Hruz",
            "Badriah Alkalifah",
            "Ahmed Ali"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "This paper explores using T5 models for Saudi Sign Language translation, showing significant performance improvements by pre-training on American Sign Language data.",
        "tldr_zh": "本文探讨了使用T5模型进行沙特手语翻译，在美国手语数据上进行预训练显著提高了性能。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Full segmentation annotations of 3D time-lapse microscopy images of MDA231 cells",
        "summary": "High-quality, publicly available segmentation annotations of image and video\ndatasets are critical for advancing the field of image processing. In\nparticular, annotations of volumetric images of a large number of targets are\ntime-consuming and challenging. In (Melnikova, A., & Matula, P., 2025), we\npresented the first publicly available full 3D time-lapse segmentation\nannotations of migrating cells with complex dynamic shapes. Concretely, three\ndistinct humans annotated two sequences of MDA231 human breast carcinoma cells\n(Fluo-C3DL-MDA231) from the Cell Tracking Challenge (CTC).\n  This paper aims to provide a comprehensive description of the dataset and\naccompanying experiments that were not included in (Melnikova, A., & Matula,\nP., 2025) due to limitations in publication space. Namely, we show that the\ncreated annotations are consistent with the previously published tracking\nmarkers provided by the CTC organizers and the segmentation accuracy measured\nbased on the 2D gold truth of CTC is within the inter-annotator variability\nmargins. We compared the created 3D annotations with automatically created\nsilver truth provided by CTC. We have found the proposed annotations better\nrepresent the complexity of the input images. The presented annotations can be\nused for testing and training cell segmentation, or analyzing 3D shapes of\nhighly dynamic objects.",
        "url": "http://arxiv.org/abs/2510.10797v1",
        "published_date": "2025-10-12T20:31:40+00:00",
        "updated_date": "2025-10-12T20:31:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aleksandra Melnikova",
            "Petr Matula"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper provides full segmentation annotations of 3D time-lapse images of MDA231 cells for cell tracking and segmentation tasks.",
        "tldr_zh": "本文提供了MDA231细胞的3D时间细胞追踪和分割任务的完整分割标注。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Nepali Sign Language Characters Recognition: Dataset Development and Deep Learning Approaches",
        "summary": "Sign languages serve as essential communication systems for individuals with\nhearing and speech impairments. However, digital linguistic dataset resources\nfor underrepresented sign languages, such as Nepali Sign Language (NSL), remain\nscarce. This study introduces the first benchmark dataset for NSL, consisting\nof 36 gesture classes with 1,500 samples per class, designed to capture the\nstructural and visual features of the language. To evaluate recognition\nperformance, we fine-tuned MobileNetV2 and ResNet50 architectures on the\ndataset, achieving classification accuracies of 90.45% and 88.78%,\nrespectively. These findings demonstrate the effectiveness of convolutional\nneural networks in sign recognition tasks, particularly within low-resource\nsettings. To the best of our knowledge, this work represents the first\nsystematic effort to construct a benchmark dataset and assess deep learning\napproaches for NSL recognition, highlighting the potential of transfer learning\nand fine-tuning for advancing research in underexplored sign languages.",
        "url": "http://arxiv.org/abs/2510.11243v1",
        "published_date": "2025-10-13T10:29:08+00:00",
        "updated_date": "2025-10-13T10:29:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Birat Poudel",
            "Satyam Ghimire",
            "Sijan Bhattarai",
            "Saurav Bhandari",
            "Suramya Sharma Dahal"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "This paper introduces a benchmark dataset for Nepali Sign Language (NSL) and assesses deep learning approaches for NSL recognition, achieving high classification accuracies using MobileNetV2 and ResNet50 architectures.",
        "tldr_zh": "本文介绍了尼泊尔手语（NSL）的基准数据集，并评估了深度学习方法对NSL识别的效果，在使用MobileNetV2和ResNet50架构时达到了较高的分类准确性。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.25
    },
    {
        "title": "DKPMV: Dense Keypoints Fusion from Multi-View RGB Frames for 6D Pose Estimation of Textureless Objects",
        "summary": "6D pose estimation of textureless objects is valuable for industrial robotic\napplications, yet remains challenging due to the frequent loss of depth\ninformation. Current multi-view methods either rely on depth data or\ninsufficiently exploit multi-view geometric cues, limiting their performance.\nIn this paper, we propose DKPMV, a pipeline that achieves dense keypoint-level\nfusion using only multi-view RGB images as input. We design a three-stage\nprogressive pose optimization strategy that leverages dense multi-view keypoint\ngeometry information. To enable effective dense keypoint fusion, we enhance the\nkeypoint network with attentional aggregation and symmetry-aware training,\nimproving prediction accuracy and resolving ambiguities on symmetric objects.\nExtensive experiments on the ROBI dataset demonstrate that DKPMV outperforms\nstate-of-the-art multi-view RGB approaches and even surpasses the RGB-D methods\nin the majority of cases. The code will be available soon.",
        "url": "http://arxiv.org/abs/2510.10933v1",
        "published_date": "2025-10-13T02:45:55+00:00",
        "updated_date": "2025-10-13T02:45:55+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Jiahong Chen",
            "Jinghao Wang",
            "Zi Wang",
            "Ziwen Wang",
            "Banglei Guan",
            "Qifeng Yu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a method, DKPMV, for 6D pose estimation of textureless objects using multi-view RGB images, outperforming existing methods on the ROBI dataset.",
        "tldr_zh": "本文提出了一种使用多视角RGB图像的DKPMV方法，用于对无纹理物体进行6D位姿估计，在ROBI数据集上表现优于现有方法。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.25
    },
    {
        "title": "rareboost3d: a synthetic lidar dataset with enhanced rare classes",
        "summary": "Real-world point cloud datasets have made significant contributions to the\ndevelopment of LiDAR-based perception technologies, such as object segmentation\nfor autonomous driving. However, due to the limited number of instances in some\nrare classes, the long-tail problem remains a major challenge in existing\ndatasets. To address this issue, we introduce a novel, synthetic point cloud\ndataset named RareBoost3D, which complements existing real-world datasets by\nproviding significantly more instances for object classes that are rare in\nreal-world datasets. To effectively leverage both synthetic and real-world\ndata, we further propose a cross-domain semantic alignment method named CSC\nloss that aligns feature representations of the same class across different\ndomains. Experimental results demonstrate that this alignment significantly\nenhances the performance of LiDAR point cloud segmentation models over\nreal-world data.",
        "url": "http://arxiv.org/abs/2510.10876v1",
        "published_date": "2025-10-13T01:02:33+00:00",
        "updated_date": "2025-10-13T01:02:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shutong Lin",
            "Zhengkang Xiang",
            "Jianzhong Qi",
            "Kourosh Khoshelham"
        ],
        "ai_categories": [
            "Dataset",
            "GAN"
        ],
        "tldr": "The paper introduces a synthetic LiDAR dataset, RareBoost3D, to address the long-tail problem in real-world datasets for autonomous driving. They also propose a cross-domain semantic alignment method that enhances LiDAR point cloud segmentation models.",
        "tldr_zh": "该论文介绍了一种合成LiDAR数据集RareBoost3D，以解决自动驾驶中现实世界数据集中的长尾问题。他们还提出了一种跨域语义对齐方法，可以提升LiDAR点云分割模型的性能。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.25
    },
    {
        "title": "How many samples to label for an application given a foundation model? Chest X-ray classification study",
        "summary": "Chest X-ray classification is vital yet resource-intensive, typically\ndemanding extensive annotated data for accurate diagnosis. Foundation models\nmitigate this reliance, but how many labeled samples are required remains\nunclear. We systematically evaluate the use of power-law fits to predict the\ntraining size necessary for specific ROC-AUC thresholds. Testing multiple\npathologies and foundation models, we find XrayCLIP and XraySigLIP achieve\nstrong performance with significantly fewer labeled examples than a ResNet-50\nbaseline. Importantly, learning curve slopes from just 50 labeled cases\naccurately forecast final performance plateaus. Our results enable\npractitioners to minimize annotation costs by labeling only the essential\nsamples for targeted performance.",
        "url": "http://arxiv.org/abs/2510.11553v1",
        "published_date": "2025-10-13T15:53:55+00:00",
        "updated_date": "2025-10-13T15:53:55+00:00",
        "categories": [
            "cs.CV",
            "68T07 (Primary) 68T45, 62H30, 62P10 (Secondary)"
        ],
        "authors": [
            "Nikolay Nechaev",
            "Evgenia Przhezdzetskaya",
            "Viktor Gombolevskiy",
            "Dmitry Umerenkov",
            "Dmitry Dylov"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper evaluates the use of power-law fits to determine the training size needed for specific ROC-AUC thresholds in chest X-ray classification. It shows that certain foundation models can achieve strong performance with significantly fewer labeled examples.",
        "tldr_zh": "本文评估了使用幂律拟合来确定胸部X光分类中特定ROC-AUC阈值所需的训练大小。结果表明，某些基础模型可以在少量标记示例的情况下取得良好表现。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "Evaluating the effects of preprocessing, method selection, and hyperparameter tuning on SAR-based flood mapping and water depth estimation",
        "summary": "Flood mapping and water depth estimation from Synthetic Aperture Radar (SAR)\nimagery are crucial for calibrating and validating hydraulic models. This study\nuses SAR imagery to evaluate various preprocessing (especially speckle noise\nreduction), flood mapping, and water depth estimation methods. The impact of\nthe choice of method at different steps and its hyperparameters is studied by\nconsidering an ensemble of preprocessed images, flood maps, and water depth\nfields. The evaluation is conducted for two flood events on the Garonne River\n(France) in 2019 and 2021, using hydrodynamic simulations and in-situ\nobservations as reference data. Results show that the choice of speckle filter\nalters flood extent estimations with variations of several square kilometers.\nFurthermore, the selection and tuning of flood mapping methods also affect\nperformance. While supervised methods outperformed unsupervised ones, tuned\nunsupervised approaches (such as local thresholding or change detection) can\nachieve comparable results. The compounded uncertainty from preprocessing and\nflood mapping steps also introduces high variability in the water depth field\nestimates. This study highlights the importance of considering the entire\nprocessing pipeline, encompassing preprocessing, flood mapping, and water depth\nestimation methods and their associated hyperparameters. Rather than relying on\na single configuration, adopting an ensemble approach and accounting for\nmethodological uncertainty should be privileged. For flood mapping, the method\nchoice has the most influence. For water depth estimation, the most influential\nprocessing step was the flood map input resulting from the flood mapping step\nand the hyperparameters of the methods.",
        "url": "http://arxiv.org/abs/2510.11305v1",
        "published_date": "2025-10-13T11:54:42+00:00",
        "updated_date": "2025-10-13T11:54:42+00:00",
        "categories": [
            "cs.CV",
            "physics.geo-ph"
        ],
        "authors": [
            "Jean-Paul Travert",
            "Cédric Goeury",
            "Sébastien Boyaval",
            "Vito Bacchi",
            "Fabrice Zaoui"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper evaluates the effects of preprocessing, method selection, and hyperparameter tuning on SAR-based flood mapping and water depth estimation.",
        "tldr_zh": "该论文评估了预处理、方法选择和超参数调整对基于SAR的洪水制图和水深估计的影响。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.0
    },
    {
        "title": "When Does Supervised Training Pay Off? The Hidden Economics of Object Detection in the Era of Vision-Language Models",
        "summary": "Object detection systems have traditionally relied on supervised learning\nwith manually annotated bounding boxes, achieving high accuracy at the cost of\nsubstantial annotation investment. The emergence of Vision-Language Models\n(VLMs) offers an alternative paradigm enabling zero-shot detection through\nnatural language queries, eliminating annotation requirements but operating\nwith reduced accuracy. This paper presents the first comprehensive\ncost-effectiveness analysis comparing supervised detection (YOLO) with\nzero-shot VLM inference (Gemini Flash 2.5). Through systematic evaluation on\n1,000 stratified COCO images and 200 diverse product images spanning consumer\nelectronics and rare categories, combined with detailed Total Cost of Ownership\nmodeling, we establish quantitative break-even thresholds governing\narchitecture selection. Our findings reveal that supervised YOLO achieves 91.2%\naccuracy versus 68.5% for zero-shot Gemini on standard categories, representing\na 22.7 percentage point advantage that costs $10,800 in annotation for\n100-category systems. However, this advantage justifies investment only beyond\n55 million inferences, equivalent to 151,000 images daily for one year.\nZero-shot Gemini demonstrates 52.3% accuracy on diverse product categories\n(ranging from highly web-prevalent consumer electronics at 75-85% to rare\nspecialized equipment at 25-40%) where supervised YOLO achieves 0% due to\narchitectural constraints preventing detection of untrained classes. Cost per\nCorrect Detection analysis reveals substantially lower per-detection costs for\nGemini ($0.00050 vs $0.143) at 100,000 inferences despite accuracy deficits. We\ndevelop decision frameworks demonstrating that optimal architecture selection\ndepends critically on deployment volume, category stability, budget\nconstraints, and accuracy requirements rather than purely technical performance\nmetrics.",
        "url": "http://arxiv.org/abs/2510.11302v1",
        "published_date": "2025-10-13T11:48:48+00:00",
        "updated_date": "2025-10-13T11:48:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Samer Al-Hamadani"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper compares the cost-effectiveness of supervised object detection with zero-shot detection using Vision-Language Models, showing thresholds for architecture selection based on annotation costs and deployment volume.",
        "tldr_zh": "本文比较了监督物体检测和零样本检测的成本效益，展示了基于标注成本和部署量的架构选择阈值。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "Comparative Evaluation of Neural Network Architectures for Generalizable Human Spatial Preference Prediction in Unseen Built Environments",
        "summary": "The capacity to predict human spatial preferences within built environments\nis instrumental for developing Cyber-Physical-Social Infrastructure Systems\n(CPSIS). A significant challenge in this domain is the generalizability of\npreference models, particularly their efficacy in predicting preferences within\nenvironmental configurations not encountered during training. While deep\nlearning models have shown promise in learning complex spatial and contextual\ndependencies, it remains unclear which neural network architectures are most\neffective at generalizing to unseen layouts. To address this, we conduct a\ncomparative study of Graph Neural Networks, Convolutional Neural Networks, and\nstandard feedforward Neural Networks using synthetic data generated from a\nsimplified and synthetic pocket park environment. Beginning with this\nillustrative case study, allows for controlled analysis of each model's ability\nto transfer learned preference patterns to unseen spatial scenarios. The models\nare evaluated based on their capacity to predict preferences influenced by\nheterogeneous physical, environmental, and social features. Generalizability\nscore is calculated using the area under the precision-recall curve for the\nseen and unseen layouts. This generalizability score is appropriate for\nimbalanced data, providing insights into the suitability of each neural network\narchitecture for preference-aware human behavior modeling in unseen built\nenvironments.",
        "url": "http://arxiv.org/abs/2510.10954v1",
        "published_date": "2025-10-13T03:04:48+00:00",
        "updated_date": "2025-10-13T03:04:48+00:00",
        "categories": [
            "cs.CE",
            "cs.CV",
            "cs.LG",
            "cs.MA"
        ],
        "authors": [
            "Maral Doctorarastoo",
            "Katherine A. Flanigan",
            "Mario Bergés",
            "Christopher McComb"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper compares different neural network architectures for predicting human spatial preferences in unseen environments, focusing on generalizability.",
        "tldr_zh": "该论文比较了不同的神经网络架构，用于预测未知环境中人类的空间偏好，重点在于泛化能力。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "Restricted Receptive Fields for Face Verification",
        "summary": "Understanding how deep neural networks make decisions is crucial for\nanalyzing their behavior and diagnosing failure cases. In computer vision, a\ncommon approach to improve interpretability is to assign importance to\nindividual pixels using post-hoc methods. Although they are widely used to\nexplain black-box models, their fidelity to the model's actual reasoning is\nuncertain due to the lack of reliable evaluation metrics. This limitation\nmotivates an alternative approach, which is to design models whose decision\nprocesses are inherently interpretable. To this end, we propose a face\nsimilarity metric that breaks down global similarity into contributions from\nrestricted receptive fields. Our method defines the similarity between two face\nimages as the sum of patch-level similarity scores, providing a locally\nadditive explanation without relying on post-hoc analysis. We show that the\nproposed approach achieves competitive verification performance even with\npatches as small as 28x28 within 112x112 face images, and surpasses\nstate-of-the-art methods when using 56x56 patches.",
        "url": "http://arxiv.org/abs/2510.10753v1",
        "published_date": "2025-10-12T18:46:56+00:00",
        "updated_date": "2025-10-12T18:46:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kagan Ozturk",
            "Aman Bhatta",
            "Haiyu Wu",
            "Patrick Flynn",
            "Kevin W. Bowyer"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a face similarity metric that breaks down global similarity into contributions from restricted receptive fields, achieving competitive performance with small patches.",
        "tldr_zh": "本文介绍了一种面部相似性度量，将全局相似性分解为来自受限接受域的贡献，通过小块实现了竞争性能。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]