[
    {
        "title": "Improved Classification of Nitrogen Stress Severity in Plants Under Combined Stress Conditions Using Spatio-Temporal Deep Learning Framework",
        "summary": "Plants in their natural habitats endure an array of interacting stresses,\nboth biotic and abiotic, that rarely occur in isolation. Nutrient\nstress-particularly nitrogen deficiency-becomes even more critical when\ncompounded with drought and weed competition, making it increasingly difficult\nto distinguish and address its effects. Early detection of nitrogen stress is\ntherefore crucial for protecting plant health and implementing effective\nmanagement strategies. This study proposes a novel deep learning framework to\naccurately classify nitrogen stress severity in a combined stress environment.\nOur model uses a unique blend of four imaging modalities-RGB, multispectral,\nand two infrared wavelengths-to capture a wide range of physiological plant\nresponses from canopy images. These images, provided as time-series data,\ndocument plant health across three levels of nitrogen availability (low,\nmedium, and high) under varying water stress and weed pressures. The core of\nour approach is a spatio-temporal deep learning pipeline that merges a\nConvolutional Neural Network (CNN) for extracting spatial features from images\nwith a Long Short-Term Memory (LSTM) network to capture temporal dependencies.\nWe also devised and evaluated a spatial-only CNN pipeline for comparison. Our\nCNN-LSTM pipeline achieved an impressive accuracy of 98%, impressively\nsurpassing the spatial-only model's 80.45% and other previously reported\nmachine learning method's 76%. These results bring actionable insights based on\nthe power of our CNN-LSTM approach in effectively capturing the subtle and\ncomplex interactions between nitrogen deficiency, water stress, and weed\npressure. This robust platform offers a promising tool for the timely and\nproactive identification of nitrogen stress severity, enabling better crop\nmanagement and improved plant health.",
        "url": "http://arxiv.org/abs/2509.06625v1",
        "published_date": "2025-09-08T12:41:45+00:00",
        "updated_date": "2025-09-08T12:41:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Aswini Kumar Patra"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "A deep learning framework is proposed to classify nitrogen stress severity in plants under combined stress conditions, achieving high accuracy by capturing spatial and temporal plant responses.",
        "tldr_zh": "提出了一种深度学习框架，用于在组合应激条件下分类植物的氮胁迫严重性，通过捕捉空间和时间植物响应实现了高准确性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Prototype-Aware Multimodal Alignment for Open-Vocabulary Visual Grounding",
        "summary": "Visual Grounding (VG) aims to utilize given natural language queries to\nlocate specific target objects within images. While current transformer-based\napproaches demonstrate strong localization performance in standard scene (i.e,\nscenarios without any novel objects), they exhibit notable limitations in\nopen-vocabulary scene (i.e, both familiar and novel object categories during\ntesting). These limitations primarily stem from three key factors: (1)\nimperfect alignment between visual and linguistic modalities, (2) insufficient\ncross-modal feature fusion, and (3) ineffective utilization of semantic\nprototype information. To overcome these challenges, we present Prototype-Aware\nMultimodal Learning (PAML), an innovative framework that systematically\naddresses these issues through several key components: First, we leverage ALBEF\nto establish robust cross-modal alignment during initial feature encoding.\nSubsequently, our Visual Discriminative Feature Encoder selectively enhances\nsalient object representations while suppressing irrelevant visual context. The\nframework then incorporates a novel prototype discovering and inheriting\nmechanism that extracts and aggregates multi-neighbor semantic prototypes to\nfacilitate open-vocabulary recognition. These enriched features undergo\ncomprehensive multimodal integration through our Multi-stage Decoder before\nfinal bounding box regression. Extensive experiments across five benchmark\ndatasets validate our approach, showing competitive performance in standard\nscene while achieving state-of-the-art results in open-vocabulary scene. Our\ncode is available at https://github.com/plankXie/PAML.",
        "url": "http://arxiv.org/abs/2509.06291v1",
        "published_date": "2025-09-08T02:27:10+00:00",
        "updated_date": "2025-09-08T02:27:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiangnan Xie",
            "Xiaolong Zheng",
            "Liang Zheng"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "Paper introduces Prototype-Aware Multimodal Learning framework for visual grounding, addressing alignment, feature fusion, and prototype utilization issues, achieving competitive performance in standard scenes and state-of-the-art results in open-vocabulary scenes.",
        "tldr_zh": "本文介绍了一种针对视觉定位的原型感知多模态学习框架，解决了对齐、特征融合和原型利用等问题，在标准场景中表现竞争力，而在开放词汇场景中达到了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data",
        "summary": "Large transformer-based models have made significant progress in\ngeneralizable novel view synthesis (NVS) from sparse input views, generating\nnovel viewpoints without the need for test-time optimization. However, these\nmodels are constrained by the limited diversity of publicly available scene\ndatasets, making most real-world (in-the-wild) scenes out-of-distribution. To\novercome this, we incorporate synthetic training data generated from diffusion\nmodels, which improves generalization across unseen domains. While synthetic\ndata offers scalability, we identify artifacts introduced during data\ngeneration as a key bottleneck affecting reconstruction quality. To address\nthis, we propose a token disentanglement process within the transformer\narchitecture, enhancing feature separation and ensuring more effective\nlearning. This refinement not only improves reconstruction quality over\nstandard transformers but also enables scalable training with synthetic data.\nAs a result, our method outperforms existing models on both in-dataset and\ncross-dataset evaluations, achieving state-of-the-art results across multiple\nbenchmarks while significantly reducing computational costs. Project page:\nhttps://scaling3dnvs.github.io/",
        "url": "http://arxiv.org/abs/2509.06950v1",
        "published_date": "2025-09-08T17:58:06+00:00",
        "updated_date": "2025-09-08T17:58:06+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Nithin Gopalakrishnan Nair",
            "Srinivas Kaza",
            "Xuan Luo",
            "Vishal M. Patel",
            "Stephen Lombardi",
            "Jungyeon Park"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a method for improving novel view synthesis using token disentanglement within transformer models and synthetic data, achieving state-of-the-art results and reducing computational costs.",
        "tldr_zh": "这篇论文介绍了一种通过在转换器模型中使用标记解缠和合成数据来改善新颖视角合成的方法，取得了最先进的结果并降低了计算成本。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesis",
        "summary": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html.",
        "url": "http://arxiv.org/abs/2509.06579v1",
        "published_date": "2025-09-08T11:49:51+00:00",
        "updated_date": "2025-09-08T11:49:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xin Kong",
            "Daniel Watson",
            "Yannick Strümpler",
            "Michael Niemeyer",
            "Federico Tombari"
        ],
        "ai_categories": [
            "Diffusion"
        ],
        "tldr": "The paper introduces CausNVS, an autoregressive multi-view diffusion model for flexible 3D novel view synthesis, achieving strong visual quality across diverse settings.",
        "tldr_zh": "本文介绍了CausNVS，这是一个用于灵活3D新视图合成的自回归多视图扩散模型，在不同设置下实现了强大的视觉质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "TIDE: Achieving Balanced Subject-Driven Image Generation via Target-Instructed Diffusion Enhancement",
        "summary": "Subject-driven image generation (SDIG) aims to manipulate specific subjects\nwithin images while adhering to textual instructions, a task crucial for\nadvancing text-to-image diffusion models. SDIG requires reconciling the tension\nbetween maintaining subject identity and complying with dynamic edit\ninstructions, a challenge inadequately addressed by existing methods. In this\npaper, we introduce the Target-Instructed Diffusion Enhancing (TIDE) framework,\nwhich resolves this tension through target supervision and preference learning\nwithout test-time fine-tuning. TIDE pioneers target-supervised triplet\nalignment, modelling subject adaptation dynamics using a (reference image,\ninstruction, target images) triplet. This approach leverages the Direct Subject\nDiffusion (DSD) objective, training the model with paired \"winning\" (balanced\npreservation-compliance) and \"losing\" (distorted) targets, systematically\ngenerated and evaluated via quantitative metrics. This enables implicit reward\nmodelling for optimal preservation-compliance balance. Experimental results on\nstandard benchmarks demonstrate TIDE's superior performance in generating\nsubject-faithful outputs while maintaining instruction compliance,\noutperforming baseline methods across multiple quantitative metrics. TIDE's\nversatility is further evidenced by its successful application to diverse\ntasks, including structural-conditioned generation, image-to-image generation,\nand text-image interpolation. Our code is available at\nhttps://github.com/KomJay520/TIDE.",
        "url": "http://arxiv.org/abs/2509.06499v1",
        "published_date": "2025-09-08T10:06:37+00:00",
        "updated_date": "2025-09-08T10:06:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jibai Lin",
            "Bo Ma",
            "Yating Yang",
            "Rong Ma",
            "Turghun Osman",
            "Ahtamjan Ahmat",
            "Rui Dong",
            "Lei Wang",
            "Xi Zhou"
        ],
        "ai_categories": [
            "AIGC",
            "Diffusion"
        ],
        "tldr": "The paper introduces the TIDE framework for subject-driven image generation, achieving balanced preservation and compliance with dynamic edit instructions.",
        "tldr_zh": "本文介绍了 TIDE 框架，用于主题驱动图像生成，在保持主题身份的同时符合动态编辑指令。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "Text4Seg++: Advancing Image Segmentation via Generative Language Modeling",
        "summary": "Multimodal Large Language Models (MLLMs) have shown exceptional capabilities\nin vision-language tasks. However, effectively integrating image segmentation\ninto these models remains a significant challenge. In this work, we propose a\nnovel text-as-mask paradigm that casts image segmentation as a text generation\nproblem, eliminating the need for additional decoders and significantly\nsimplifying the segmentation process. Our key innovation is semantic\ndescriptors, a new textual representation of segmentation masks where each\nimage patch is mapped to its corresponding text label. We first introduce\nimage-wise semantic descriptors, a patch-aligned textual representation of\nsegmentation masks that integrates naturally into the language modeling\npipeline. To enhance efficiency, we introduce the Row-wise Run-Length Encoding\n(R-RLE), which compresses redundant text sequences, reducing the length of\nsemantic descriptors by 74% and accelerating inference by $3\\times$, without\ncompromising performance. Building upon this, our initial framework Text4Seg\nachieves strong segmentation performance across a wide range of vision tasks.\nTo further improve granularity and compactness, we propose box-wise semantic\ndescriptors, which localizes regions of interest using bounding boxes and\nrepresents region masks via structured mask tokens called semantic bricks. This\nleads to our refined model, Text4Seg++, which formulates segmentation as a\nnext-brick prediction task, combining precision, scalability, and generative\nefficiency. Comprehensive experiments on natural and remote sensing datasets\nshow that Text4Seg++ consistently outperforms state-of-the-art models across\ndiverse benchmarks without any task-specific fine-tuning, while remaining\ncompatible with existing MLLM backbones. Our work highlights the effectiveness,\nscalability, and generalizability of text-driven image segmentation within the\nMLLM framework.",
        "url": "http://arxiv.org/abs/2509.06321v1",
        "published_date": "2025-09-08T04:07:14+00:00",
        "updated_date": "2025-09-08T04:07:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mengcheng Lan",
            "Chaofeng Chen",
            "Jiaxing Xu",
            "Zongrui Li",
            "Yiping Ke",
            "Xudong Jiang",
            "Yingchen Yu",
            "Yunqing Zhao",
            "Song Bai"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes Text4Seg++, a method that integrates image segmentation into language models by representing segmentation masks as text, achieving strong performance without task-specific fine-tuning.",
        "tldr_zh": "本文提出了Text4Seg ++，一种将图像分割与语言模型相结合的方法，通过将分割掩模表示为文本，实现强大的性能而无需特定任务微调。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Evaluating the Efficiency of Latent Spaces via the Coupling-Matrix",
        "summary": "A central challenge in representation learning is constructing latent\nembeddings that are both expressive and efficient. In practice, deep networks\noften produce redundant latent spaces where multiple coordinates encode\noverlapping information, reducing effective capacity and hindering\ngeneralization. Standard metrics such as accuracy or reconstruction loss\nprovide only indirect evidence of such redundancy and cannot isolate it as a\nfailure mode. We introduce a redundancy index, denoted rho(C), that directly\nquantifies inter-dimensional dependencies by analyzing coupling matrices\nderived from latent representations and comparing their off-diagonal statistics\nagainst a normal distribution via energy distance. The result is a compact,\ninterpretable, and statistically grounded measure of representational quality.\nWe validate rho(C) across discriminative and generative settings on MNIST\nvariants, Fashion-MNIST, CIFAR-10, and CIFAR-100, spanning multiple\narchitectures and hyperparameter optimization strategies. Empirically, low\nrho(C) reliably predicts high classification accuracy or low reconstruction\nerror, while elevated redundancy is associated with performance collapse.\nEstimator reliability grows with latent dimension, yielding natural lower\nbounds for reliable analysis. We further show that Tree-structured Parzen\nEstimators (TPE) preferentially explore low-rho regions, suggesting that rho(C)\ncan guide neural architecture search and serve as a redundancy-aware\nregularization target. By exposing redundancy as a universal bottleneck across\nmodels and tasks, rho(C) offers both a theoretical lens and a practical tool\nfor evaluating and improving the efficiency of learned representations.",
        "url": "http://arxiv.org/abs/2509.06314v1",
        "published_date": "2025-09-08T03:36:47+00:00",
        "updated_date": "2025-09-08T03:36:47+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Mehmet Can Yavuz",
            "Berrin Yanikoglu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a measure called rho(C) to quantify redundancy in latent spaces, guiding neural architecture search and improving representation efficiency.",
        "tldr_zh": "该论文介绍了一种称为rho(C)的度量，用于量化潜空间中的冗余，指导神经架构搜索和提高表征效率。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions",
        "summary": "Executing language-conditioned tasks in dynamic visual environments remains a\ncentral challenge in embodied AI. Existing Vision-Language-Action (VLA) models\npredominantly adopt reactive state-to-action mappings, often leading to\nshort-sighted behaviors and poor robustness in dynamic scenes. In this paper,\nwe introduce F1, a pretrained VLA framework which integrates the visual\nforesight generation into decision-making pipeline. F1 adopts a\nMixture-of-Transformer architecture with dedicated modules for perception,\nforesight generation, and control, thereby bridging understanding, generation,\nand actions. At its core, F1 employs a next-scale prediction mechanism to\nsynthesize goal-conditioned visual foresight as explicit planning targets. By\nforecasting plausible future visual states, F1 reformulates action generation\nas a foresight-guided inverse dynamics problem, enabling actions that\nimplicitly achieve visual goals. To endow F1 with robust and generalizable\ncapabilities, we propose a three-stage training recipe on an extensive dataset\ncomprising over 330k trajectories across 136 diverse tasks. This training\nscheme enhances modular reasoning and equips the model with transferable visual\nforesight, which is critical for complex and dynamic environments. Extensive\nevaluations on real-world tasks and simulation benchmarks demonstrate F1\nconsistently outperforms existing approaches, achieving substantial gains in\nboth task success rate and generalization ability.",
        "url": "http://arxiv.org/abs/2509.06951v1",
        "published_date": "2025-09-08T17:58:30+00:00",
        "updated_date": "2025-09-08T17:58:30+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Qi Lv",
            "Weijie Kong",
            "Hao Li",
            "Jia Zeng",
            "Zherui Qiu",
            "Delin Qu",
            "Haoming Song",
            "Qizhi Chen",
            "Xiang Deng",
            "Jiangmiao Pang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a Vision-Language-Action model called F1 that integrates visual foresight generation into decision-making, achieving better performance in dynamic environments.",
        "tldr_zh": "本文介绍了一种名为F1的Vision-Language-Action模型，将视觉预测生成整合到决策过程中，在动态环境中表现更好。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Zero-shot 3D-Aware Trajectory-Guided image-to-video generation via Test-Time Training",
        "summary": "Trajectory-Guided image-to-video (I2V) generation aims to synthesize videos\nthat adhere to user-specified motion instructions. Existing methods typically\nrely on computationally expensive fine-tuning on scarce annotated datasets.\nAlthough some zero-shot methods attempt to trajectory control in the latent\nspace, they may yield unrealistic motion by neglecting 3D perspective and\ncreating a misalignment between the manipulated latents and the network's noise\npredictions. To address these challenges, we introduce Zo3T, a novel zero-shot\ntest-time-training framework for trajectory-guided generation with three core\ninnovations: First, we incorporate a 3D-Aware Kinematic Projection, leveraging\ninferring scene depth to derive perspective-correct affine transformations for\ntarget regions. Second, we introduce Trajectory-Guided Test-Time LoRA, a\nmechanism that dynamically injects and optimizes ephemeral LoRA adapters into\nthe denoising network alongside the latent state. Driven by a regional feature\nconsistency loss, this co-adaptation effectively enforces motion constraints\nwhile allowing the pre-trained model to locally adapt its internal\nrepresentations to the manipulated latent, thereby ensuring generative fidelity\nand on-manifold adherence. Finally, we develop Guidance Field Rectification,\nwhich refines the denoising evolutionary path by optimizing the conditional\nguidance field through a one-step lookahead strategy, ensuring efficient\ngenerative progression towards the target trajectory. Zo3T significantly\nenhances 3D realism and motion accuracy in trajectory-controlled I2V\ngeneration, demonstrating superior performance over existing training-based and\nzero-shot approaches.",
        "url": "http://arxiv.org/abs/2509.06723v1",
        "published_date": "2025-09-08T14:21:45+00:00",
        "updated_date": "2025-09-08T14:21:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruicheng Zhang",
            "Jun Zhou",
            "Zunnan Xu",
            "Zihao Liu",
            "Jiehui Huang",
            "Mingyang Zhang",
            "Yu Sun",
            "Xiu Li"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "LoRA",
            "AIGC"
        ],
        "tldr": "The paper introduces Zo3T, a novel zero-shot test-time-training framework for trajectory-guided image-to-video generation, improving 3D realism and motion accuracy.",
        "tldr_zh": "本文介绍了一种新颖的零样本测试时间训练框架Zo3T，用于轨迹引导的图像到视频生成，提高了三维真实感和运动精度。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "MM-DINOv2: Adapting Foundation Models for Multi-Modal Medical Image Analysis",
        "summary": "Vision foundation models like DINOv2 demonstrate remarkable potential in\nmedical imaging despite their origin in natural image domains. However, their\ndesign inherently works best for uni-modal image analysis, limiting their\neffectiveness for multi-modal imaging tasks that are common in many medical\nfields, such as neurology and oncology. While supervised models perform well in\nthis setting, they fail to leverage unlabeled datasets and struggle with\nmissing modalities, a frequent challenge in clinical settings. To bridge these\ngaps, we introduce MM-DINOv2, a novel and efficient framework that adapts the\npre-trained vision foundation model DINOv2 for multi-modal medical imaging. Our\napproach incorporates multi-modal patch embeddings, enabling vision foundation\nmodels to effectively process multi-modal imaging data. To address missing\nmodalities, we employ full-modality masking, which encourages the model to\nlearn robust cross-modality relationships. Furthermore, we leverage\nsemi-supervised learning to harness large unlabeled datasets, enhancing both\nthe accuracy and reliability of medical predictions. Applied to glioma subtype\nclassification from multi-sequence brain MRI, our method achieves a Matthews\nCorrelation Coefficient (MCC) of 0.6 on an external test set, surpassing\nstate-of-the-art supervised approaches by +11.1%. Our work establishes a\nscalable and robust solution for multi-modal medical imaging tasks, leveraging\npowerful vision foundation models pre-trained on natural images while\naddressing real-world clinical challenges such as missing data and limited\nannotations.",
        "url": "http://arxiv.org/abs/2509.06617v1",
        "published_date": "2025-09-08T12:34:15+00:00",
        "updated_date": "2025-09-08T12:34:15+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Daniel Scholz",
            "Ayhan Can Erdur",
            "Viktoria Ehm",
            "Anke Meyer-Baese",
            "Jan C. Peeken",
            "Daniel Rueckert",
            "Benedikt Wiestler"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "MM-DINOv2 is a novel framework that adapts a vision foundation model for multi-modal medical imaging, achieving high accuracy on glioma subtype classification from brain MRI.",
        "tldr_zh": "MM-DINOv2是一个新颖的框架，将一种视觉基础模型应用于多模态医学图像， 在脑MRI数据上实现了较高的分类准确度。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Contrastive Anatomy-Contrast Disentanglement: A Domain-General MRI Harmonization Method",
        "summary": "Magnetic resonance imaging (MRI) is an invaluable tool for clinical and\nresearch applications. Yet, variations in scanners and acquisition parameters\ncause inconsistencies in image contrast, hindering data comparability and\nreproducibility across datasets and clinical studies. Existing scanner\nharmonization methods, designed to address this challenge, face limitations,\nsuch as requiring traveling subjects or struggling to generalize to unseen\ndomains. We propose a novel approach using a conditioned diffusion autoencoder\nwith a contrastive loss and domain-agnostic contrast augmentation to harmonize\nMR images across scanners while preserving subject-specific anatomy. Our method\nenables brain MRI synthesis from a single reference image. It outperforms\nbaseline techniques, achieving a +7% PSNR improvement on a traveling subjects\ndataset and +18% improvement on age regression in unseen. Our model provides\nrobust, effective harmonization of brain MRIs to target scanners without\nrequiring fine-tuning. This advancement promises to enhance comparability,\nreproducibility, and generalizability in multi-site and longitudinal clinical\nstudies, ultimately contributing to improved healthcare outcomes.",
        "url": "http://arxiv.org/abs/2509.06592v1",
        "published_date": "2025-09-08T12:03:34+00:00",
        "updated_date": "2025-09-08T12:03:34+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Daniel Scholz",
            "Ayhan Can Erdur",
            "Robbie Holland",
            "Viktoria Ehm",
            "Jan C. Peeken",
            "Benedikt Wiestler",
            "Daniel Rueckert"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel MRI harmonization method using a conditioned diffusion autoencoder to synthesize brain MRIs across scanners while preserving subject-specific anatomy.",
        "tldr_zh": "本文介绍了一种新型MRI协调方法，使用条件扩散自动编码器合成跨扫描仪的脑MRI，同时保留特定主体解剖。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Your Super Resolution Model is not Enough for Tackling Real-World Scenarios",
        "summary": "Despite remarkable progress in Single Image Super-Resolution (SISR),\ntraditional models often struggle to generalize across varying scale factors,\nlimiting their real-world applicability. To address this, we propose a plug-in\nScale-Aware Attention Module (SAAM) designed to retrofit modern fixed-scale SR\nmodels with the ability to perform arbitrary-scale SR. SAAM employs\nlightweight, scale-adaptive feature extraction and upsampling, incorporating\nthe Simple parameter-free Attention Module (SimAM) for efficient guidance and\ngradient variance loss to enhance sharpness in image details. Our method\nintegrates seamlessly into multiple state-of-the-art SR backbones (e.g., SCNet,\nHiT-SR, OverNet), delivering competitive or superior performance across a wide\nrange of integer and non-integer scale factors. Extensive experiments on\nbenchmark datasets demonstrate that our approach enables robust multi-scale\nupscaling with minimal computational overhead, offering a practical solution\nfor real-world scenarios.",
        "url": "http://arxiv.org/abs/2509.06387v1",
        "published_date": "2025-09-08T07:13:58+00:00",
        "updated_date": "2025-09-08T07:13:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dongsik Yoon",
            "Jongeun Kim"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a Scale-Aware Attention Module to enhance Single Image Super-Resolution models' ability to handle various scale factors, improving performance significantly.",
        "tldr_zh": "本文提出了一种尺度感知注意模块，以增强单图像超分辨率模型处理各种比例因子的能力，显著提高性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments",
        "summary": "Generating collision-free motion in dynamic, partially observable\nenvironments is a fundamental challenge for robotic manipulators. Classical\nmotion planners can compute globally optimal trajectories but require full\nenvironment knowledge and are typically too slow for dynamic scenes. Neural\nmotion policies offer a promising alternative by operating in closed-loop\ndirectly on raw sensory inputs but often struggle to generalize in complex or\ndynamic settings. We propose Deep Reactive Policy (DRP), a visuo-motor neural\nmotion policy designed for reactive motion generation in diverse dynamic\nenvironments, operating directly on point cloud sensory input. At its core is\nIMPACT, a transformer-based neural motion policy pretrained on 10 million\ngenerated expert trajectories across diverse simulation scenarios. We further\nimprove IMPACT's static obstacle avoidance through iterative student-teacher\nfinetuning. We additionally enhance the policy's dynamic obstacle avoidance at\ninference time using DCP-RMP, a locally reactive goal-proposal module. We\nevaluate DRP on challenging tasks featuring cluttered scenes, dynamic moving\nobstacles, and goal obstructions. DRP achieves strong generalization,\noutperforming prior classical and neural methods in success rate across both\nsimulated and real-world settings. Video results and code available at\nhttps://deep-reactive-policy.com",
        "url": "http://arxiv.org/abs/2509.06953v1",
        "published_date": "2025-09-08T17:59:35+00:00",
        "updated_date": "2025-09-08T17:59:35+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.SY",
            "eess.SY"
        ],
        "authors": [
            "Jiahui Yang",
            "Jason Jingzhou Liu",
            "Yulong Li",
            "Youssef Khaky",
            "Kenneth Shaw",
            "Deepak Pathak"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a Deep Reactive Policy for generating collision-free motion in dynamic environments using neural networks, achieving strong generalization in challenging tasks.",
        "tldr_zh": "本文提出了一个深度反应策略，利用神经网络在动态环境中生成无碰撞运动，在挑战性任务中具有很强的泛化能力。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Interleaving Reasoning for Better Text-to-Image Generation",
        "summary": "Unified multimodal understanding and generation models recently have achieve\nsignificant improvement in image generation capability, yet a large gap remains\nin instruction following and detail preservation compared to systems that\ntightly couple comprehension with generation such as GPT-4o. Motivated by\nrecent advances in interleaving reasoning, we explore whether such reasoning\ncan further improve Text-to-Image (T2I) generation. We introduce Interleaving\nReasoning Generation (IRG), a framework that alternates between text-based\nthinking and image synthesis: the model first produces a text-based thinking to\nguide an initial image, then reflects on the result to refine fine-grained\ndetails, visual quality, and aesthetics while preserving semantics. To train\nIRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),\nwhich targets two sub-goals: (1) strengthening the initial think-and-generate\nstage to establish core content and base quality, and (2) enabling high-quality\ntextual reflection and faithful implementation of those refinements in a\nsubsequent image. We curate IRGL-300K, a dataset organized into six decomposed\nlearning modes that jointly cover learning text-based thinking, and full\nthinking-image trajectories. Starting from a unified foundation model that\nnatively emits interleaved text-image outputs, our two-stage training first\nbuilds robust thinking and reflection, then efficiently tunes the IRG pipeline\nin the full thinking-image trajectory data. Extensive experiments show SoTA\nperformance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,\nGenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality\nand fine-grained fidelity. The code, model weights and datasets will be\nreleased in: https://github.com/Osilly/Interleaving-Reasoning-Generation .",
        "url": "http://arxiv.org/abs/2509.06945v1",
        "published_date": "2025-09-08T17:56:23+00:00",
        "updated_date": "2025-09-08T17:56:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Wenxuan Huang",
            "Shuang Chen",
            "Zheyong Xie",
            "Shaosheng Cao",
            "Shixiang Tang",
            "Yufan Shen",
            "Qingyu Yin",
            "Wenbo Hu",
            "Xiaoman Wang",
            "Yuntian Tang",
            "Junbo Qiao",
            "Yue Guo",
            "Yao Hu",
            "Zhenfei Yin",
            "Philip Torr",
            "Yu Cheng",
            "Wanli Ouyang",
            "Shaohui Lin"
        ],
        "ai_categories": [
            "Multimodality",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces Interleaving Reasoning Generation (IRG) for Text-to-Image generation, achieving significant improvements in visual quality and fidelity.",
        "tldr_zh": "该论文引入了交错推理生成（IRG）用于文本到图像的生成，实现了视觉质量和保真度的显著改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "LLaDA-VLA: Vision Language Diffusion Action Models",
        "summary": "The rapid progress of auto-regressive vision-language models (VLMs) has\ninspired growing interest in vision-language-action models (VLA) for robotic\nmanipulation. Recently, masked diffusion models, a paradigm distinct from\nautoregressive models, have begun to demonstrate competitive performance in\ntext generation and multimodal applications, leading to the development of a\nseries of diffusion-based VLMs (d-VLMs). However, leveraging such models for\nrobot policy learning remains largely unexplored. In this work, we present\nLLaDA-VLA, the first Vision-Language-Diffusion-Action model built upon\npretrained d-VLMs for robotic manipulation. To effectively adapt d-VLMs to\nrobotic domain, we introduce two key designs: (1) a localized special-token\nclassification strategy that replaces full-vocabulary classification with\nspecial action token classification, reducing adaptation difficulty; (2) a\nhierarchical action-structured decoding strategy that decodes action sequences\nhierarchically considering the dependencies within and across actions.\nExtensive experiments demonstrate that LLaDA-VLA significantly outperforms\nstate-of-the-art VLAs on both simulation and real-world robots.",
        "url": "http://arxiv.org/abs/2509.06932v1",
        "published_date": "2025-09-08T17:45:40+00:00",
        "updated_date": "2025-09-08T17:45:40+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Yuqing Wen",
            "Hebei Li",
            "Kefan Gu",
            "Yucheng Zhao",
            "Tiancai Wang",
            "Xiaoyan Sun"
        ],
        "ai_categories": [
            "Multimodality",
            "LoRA",
            "Diffusion"
        ],
        "tldr": "LLaDA-VLA is a vision-language-action model for robotic manipulation that outperforms existing models through the use of diffusion-based techniques.",
        "tldr_zh": "LLaDA-VLA是一个用于机器人操作的视觉-语言-动作模型，通过扩散技术优于现有模型。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "BIR-Adapter: A Low-Complexity Diffusion Model Adapter for Blind Image Restoration",
        "summary": "This paper introduces BIR-Adapter, a low-complexity blind image restoration\nadapter for diffusion models. The BIR-Adapter enables the utilization of the\nprior of pre-trained large-scale diffusion models on blind image restoration\nwithout training any auxiliary feature extractor. We take advantage of the\nrobustness of pretrained models. We extract features from degraded images via\nthe model itself and extend the self-attention mechanism with these degraded\nfeatures. We introduce a sampling guidance mechanism to reduce hallucinations.\nWe perform experiments on synthetic and real-world degradations and demonstrate\nthat BIR-Adapter achieves competitive or better performance compared to\nstate-of-the-art methods while having significantly lower complexity.\nAdditionally, its adapter-based design enables integration into other diffusion\nmodels, enabling broader applications in image restoration tasks. We showcase\nthis by extending a super-resolution-only model to perform better under\nadditional unknown degradations.",
        "url": "http://arxiv.org/abs/2509.06904v1",
        "published_date": "2025-09-08T17:22:18+00:00",
        "updated_date": "2025-09-08T17:22:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Cem Eteke",
            "Alexander Griessel",
            "Wolfgang Kellerer",
            "Eckehard Steinbach"
        ],
        "ai_categories": [
            "Diffusion"
        ],
        "tldr": "The paper introduces a low-complexity blind image restoration adapter for diffusion models, achieving competitive performance with significantly lower complexity.",
        "tldr_zh": "本文介绍了一种用于扩散模型的低复杂度盲图像恢复适配器，通过模型本身从降解图像中提取特征，具有较低的复杂度但能实现竞争性的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Automated Radiographic Total Sharp Score (ARTSS) in Rheumatoid Arthritis: A Solution to Reduce Inter-Intra Reader Variation and Enhancing Clinical Practice",
        "summary": "Assessing the severity of rheumatoid arthritis (RA) using the Total Sharp/Van\nDer Heijde Score (TSS) is crucial, but manual scoring is often time-consuming\nand subjective. This study introduces an Automated Radiographic Sharp Scoring\n(ARTSS) framework that leverages deep learning to analyze full-hand X-ray\nimages, aiming to reduce inter- and intra-observer variability. The research\nuniquely accommodates patients with joint disappearance and variable-length\nimage sequences. We developed ARTSS using data from 970 patients, structured\ninto four stages: I) Image pre-processing and re-orientation using ResNet50,\nII) Hand segmentation using UNet.3, III) Joint identification using YOLOv7, and\nIV) TSS prediction using models such as VGG16, VGG19, ResNet50, DenseNet201,\nEfficientNetB0, and Vision Transformer (ViT). We evaluated model performance\nwith Intersection over Union (IoU), Mean Average Precision (MAP), mean absolute\nerror (MAE), Root Mean Squared Error (RMSE), and Huber loss. The average TSS\nfrom two radiologists was used as the ground truth. Model training employed\n3-fold cross-validation, with each fold consisting of 452 training and 227\nvalidation samples, and external testing included 291 unseen subjects. Our\njoint identification model achieved 99% accuracy. The best-performing model,\nViT, achieved a notably low Huber loss of 0.87 for TSS prediction. Our results\ndemonstrate the potential of deep learning to automate RA scoring, which can\nsignificantly enhance clinical practice. Our approach addresses the challenge\nof joint disappearance and variable joint numbers, offers timesaving benefits,\nreduces inter- and intra-reader variability, improves radiologist accuracy, and\naids rheumatologists in making more informed decisions.",
        "url": "http://arxiv.org/abs/2509.06854v1",
        "published_date": "2025-09-08T16:21:45+00:00",
        "updated_date": "2025-09-08T16:21:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hajar Moradmand",
            "Lei Ren"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper introduces an Automated Radiographic Sharp Scoring (ARTSS) framework using deep learning to analyze X-ray images for assessing rheumatoid arthritis severity, aiming to reduce inter- and intra-observer variability.",
        "tldr_zh": "该论文引入了一种使用深度学习分析X射线图像的自动化放射性防强化评分（ARTSS）框架，用于评估类风湿性关节炎严重程度，旨在减少观察者之间和内部的差异。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "ToonOut: Fine-tuned Background-Removal for Anime Characters",
        "summary": "While state-of-the-art background removal models excel at realistic imagery,\nthey frequently underperform in specialized domains such as anime-style\ncontent, where complex features like hair and transparency present unique\nchallenges. To address this limitation, we collected and annotated a custom\ndataset of 1,228 high-quality anime images of characters and objects, and\nfine-tuned the open-sourced BiRefNet model on this dataset. This resulted in\nmarked improvements in background removal accuracy for anime-style images,\nincreasing from 95.3% to 99.5% for our newly introduced Pixel Accuracy metric.\nWe are open-sourcing the code, the fine-tuned model weights, as well as the\ndataset at: https://github.com/MatteoKartoon/BiRefNet.",
        "url": "http://arxiv.org/abs/2509.06839v1",
        "published_date": "2025-09-08T16:08:56+00:00",
        "updated_date": "2025-09-08T16:08:56+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Matteo Muratori",
            "Joël Seytre"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces ToonOut, a fine-tuned background removal model specifically designed for anime-style images, achieving significant accuracy improvements.",
        "tldr_zh": "本文介绍了ToonOut，这是一个专门针对动漫风格图像进行优化的背景去除模型，取得了显著的准确性提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Leveraging Generic Foundation Models for Multimodal Surgical Data Analysis",
        "summary": "We investigate how both the adaptation of a generic foundation model via\ntransfer learning and the integration of complementary modalities from the\noperating room (OR) can support surgical data science. To this end, we use\nV-JEPA as the single-modality foundation of a multimodal model for minimally\ninvasive surgery support. We analyze how the model's downstream performance can\nbenefit (a) from finetuning on unlabeled surgical video data and (b) from\nproviding additional time-resolved data streams from the OR in a multimodal\nsetup.\n  In an in-house dataset of liver surgery videos, we analyze the tasks of\npredicting hospital length of stay and postoperative complications. In videos\nof the public HeiCo dataset, we analyze the task of surgical phase recognition.\nAs a baseline, we apply pretrained V-JEPA to all tasks. We then finetune it on\nunlabeled, held-out videos to investigate its change in performance after\ndomain adaptation. Following the idea of modular decision support networks, we\nintegrate additional data streams from the OR by training a separate encoder to\nform a shared representation space with V-JEPA's embeddings.\n  Our experiments show that finetuning on domain-specific data increases model\nperformance. On the in-house data, integrating additional time-resolved data\nlikewise benefits the model. On the HeiCo data, accuracy of the pretrained\nvideo-only, single-modality baseline setup is on par with the top-performing\nsubmissions of the EndoVis2017 challenge, while finetuning on domain-specific\ndata increases accuracy further. Our results thus demonstrate how surgical data\nscience can leverage public, generic foundation models. Likewise, they indicate\nthe potential of domain adaptation and of integrating suitable complementary\ndata streams from the OR. To support further research, we release our code and\nmodel weights at https://github.com/DigitalSurgeryLab-Basel/ML-CDS-2025.",
        "url": "http://arxiv.org/abs/2509.06831v1",
        "published_date": "2025-09-08T16:04:19+00:00",
        "updated_date": "2025-09-08T16:04:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Simon Pezold",
            "Jérôme A. Kurylec",
            "Jan S. Liechti",
            "Beat P. Müller",
            "Joël L. Lavanchy"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper explores using a generic model for analyzing surgical data by fine-tuning it with domain-specific data and integrating additional data streams from the operating room, showing improved performance in predicting outcomes of surgeries.",
        "tldr_zh": "本文探讨了使用通用模型分析手术数据，通过领域特定数据进行微调，并集成手术室的额外数据流，展示了在预测手术结果方面性能的提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Curia: A Multi-Modal Foundation Model for Radiology",
        "summary": "AI-assisted radiological interpretation is based on predominantly narrow,\nsingle-task models. This approach is impractical for covering the vast spectrum\nof imaging modalities, diseases, and radiological findings. Foundation models\n(FMs) hold the promise of broad generalization across modalities and in\nlow-data settings. However, this potential has remained largely unrealized in\nradiology. We introduce Curia, a foundation model trained on the entire\ncross-sectional imaging output of a major hospital over several years, which to\nour knowledge is the largest such corpus of real-world data-encompassing\n150,000 exams (130 TB). On a newly curated 19-task external validation\nbenchmark, Curia accurately identifies organs, detects conditions like brain\nhemorrhages and myocardial infarctions, and predicts outcomes in tumor staging.\nCuria meets or surpasses the performance of radiologists and recent foundation\nmodels, and exhibits clinically significant emergent properties in\ncross-modality, and low-data regimes. To accelerate progress, we release our\nbase model's weights at https://huggingface.co/raidium/curia.",
        "url": "http://arxiv.org/abs/2509.06830v1",
        "published_date": "2025-09-08T16:04:12+00:00",
        "updated_date": "2025-09-08T16:04:12+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Corentin Dancette",
            "Julien Khlaut",
            "Antoine Saporta",
            "Helene Philippe",
            "Elodie Ferreres",
            "Baptiste Callard",
            "Théo Danielou",
            "Léo Alberge",
            "Léo Machado",
            "Daniel Tordjman",
            "Julie Dupuis",
            "Korentin Le Floch",
            "Jean Du Terrail",
            "Mariam Moshiri",
            "Laurent Dercle",
            "Tom Boeken",
            "Jules Gregory",
            "Maxime Ronot",
            "François Legou",
            "Pascal Roux",
            "Marc Sapoval",
            "Pierre Manceron",
            "Paul Hérent"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "Curia is a foundation model for radiology that shows promise in covering a wide range of imaging tasks and performs well in various scenarios, releasing its base model's weights for further research.",
        "tldr_zh": "Curia是一个用于放射学的基础模型，展示了在各种情况下覆盖广泛的成像任务并表现出色的潜力，发布了其基本模型的权重以供进一步研究。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward",
        "summary": "Recent advancements in image customization exhibit a wide range of\napplication prospects due to stronger customization capabilities. However,\nsince we humans are more sensitive to faces, a significant challenge remains in\npreserving consistent identity while avoiding identity confusion with\nmulti-reference images, limiting the identity scalability of customization\nmodels. To address this, we present UMO, a Unified Multi-identity Optimization\nframework, designed to maintain high-fidelity identity preservation and\nalleviate identity confusion with scalability. With \"multi-to-multi matching\"\nparadigm, UMO reformulates multi-identity generation as a global assignment\noptimization problem and unleashes multi-identity consistency for existing\nimage customization methods generally through reinforcement learning on\ndiffusion models. To facilitate the training of UMO, we develop a scalable\ncustomization dataset with multi-reference images, consisting of both\nsynthesised and real parts. Additionally, we propose a new metric to measure\nidentity confusion. Extensive experiments demonstrate that UMO not only\nimproves identity consistency significantly, but also reduces identity\nconfusion on several image customization methods, setting a new\nstate-of-the-art among open-source methods along the dimension of identity\npreserving. Code and model: https://github.com/bytedance/UMO",
        "url": "http://arxiv.org/abs/2509.06818v1",
        "published_date": "2025-09-08T15:54:55+00:00",
        "updated_date": "2025-09-08T15:54:55+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yufeng Cheng",
            "Wenxu Wu",
            "Shaojin Wu",
            "Mengqi Huang",
            "Fei Ding",
            "Qian He"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Diffusion"
        ],
        "tldr": "UMO is a framework that enhances image customization by maintaining consistent identity and reducing identity confusion with multi-identity images through reinforcement learning.",
        "tldr_zh": "UMO是一个框架，通过强化学习来维持一致的身份和减少多身份图像的混淆，从而提升图像定制的效果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MIORe & VAR-MIORe: Benchmarks to Push the Boundaries of Restoration",
        "summary": "We introduce MIORe and VAR-MIORe, two novel multi-task datasets that address\ncritical limitations in current motion restoration benchmarks. Designed with\nhigh-frame-rate (1000 FPS) acquisition and professional-grade optics, our\ndatasets capture a broad spectrum of motion scenarios, which include complex\nego-camera movements, dynamic multi-subject interactions, and depth-dependent\nblur effects. By adaptively averaging frames based on computed optical flow\nmetrics, MIORe generates consistent motion blur, and preserves sharp inputs for\nvideo frame interpolation and optical flow estimation. VAR-MIORe further\nextends by spanning a variable range of motion magnitudes, from minimal to\nextreme, establishing the first benchmark to offer explicit control over motion\namplitude. We provide high-resolution, scalable ground truths that challenge\nexisting algorithms under both controlled and adverse conditions, paving the\nway for next-generation research of various image and video restoration tasks.",
        "url": "http://arxiv.org/abs/2509.06803v1",
        "published_date": "2025-09-08T15:34:31+00:00",
        "updated_date": "2025-09-08T15:34:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "George Ciubotariu",
            "Zhuyun Zhou",
            "Zongwei Wu",
            "Radu Timofte"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "MIORe and VAR-MIORe are novel multi-task datasets for motion restoration, offering high-quality data for challenging motion scenarios.",
        "tldr_zh": "MIORe和VAR-MIORe是针对运动恢复的新型多任务数据集，为具有挑战性的运动场景提供高质量的数据。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SynthDrive: Scalable Real2Sim2Real Sensor Simulation Pipeline for High-Fidelity Asset Generation and Driving Data Synthesis",
        "summary": "In the field of autonomous driving, sensor simulation is essential for\ngenerating rare and diverse scenarios that are difficult to capture in\nreal-world environments. Current solutions fall into two categories: 1)\nCG-based methods, such as CARLA, which lack diversity and struggle to scale to\nthe vast array of rare cases required for robust perception training; and 2)\nlearning-based approaches, such as NeuSim, which are limited to specific object\ncategories (vehicles) and require extensive multi-sensor data, hindering their\napplicability to generic objects. To address these limitations, we propose a\nscalable real2sim2real system that leverages 3D generation to automate asset\nmining, generation, and rare-case data synthesis.",
        "url": "http://arxiv.org/abs/2509.06798v1",
        "published_date": "2025-09-08T15:29:49+00:00",
        "updated_date": "2025-09-08T15:29:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhengqing Chen",
            "Ruohong Mei",
            "Xiaoyang Guo",
            "Qingjie Wang",
            "Yubin Hu",
            "Wei Yin",
            "Weiqiang Ren",
            "Qian Zhang"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces a new system for simulating sensors in autonomous driving scenarios, addressing limitations of existing methods by automating asset generation and rare-case data synthesis.",
        "tldr_zh": "本文介绍了一种新的系统，用于模拟自动驾驶场景中的传感器，通过自动化资产生成和稀有案例数据合成，解决了现有方法的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "AIM 2025 Challenge on High FPS Motion Deblurring: Methods and Results",
        "summary": "This paper presents a comprehensive review of the AIM 2025 High FPS\nNon-Uniform Motion Deblurring Challenge, highlighting the proposed solutions\nand final results. The objective of this challenge is to identify effective\nnetworks capable of producing clearer and visually compelling images in diverse\nand challenging conditions, by learning representative visual cues for complex\naggregations of motion types. A total of 68 participants registered for the\ncompetition, and 9 teams ultimately submitted valid entries. This paper\nthoroughly evaluates the state-of-the-art advances in high-FPS single image\nmotion deblurring, showcasing the significant progress in the field, while\nleveraging samples of the novel dataset, MIORe, that introduces challenging\nexamples of movement patterns.",
        "url": "http://arxiv.org/abs/2509.06793v1",
        "published_date": "2025-09-08T15:22:35+00:00",
        "updated_date": "2025-09-08T15:22:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "George Ciubotariu",
            "Florin-Alexandru Vasluianu",
            "Zhuyun Zhou",
            "Nancy Mehta",
            "Radu Timofte",
            "Ke Wu",
            "Long Sun",
            "Lingshun Kong",
            "Zhongbao Yang",
            "Jinshan Pan",
            "Jiangxin Dong",
            "Jinhui Tang",
            "Hao Chen",
            "Yinghui Fang",
            "Dafeng Zhang",
            "Yongqi Song",
            "Jiangbo Guo",
            "Shuhua Jin",
            "Zeyu Xiao",
            "Rui Zhao",
            "Zhuoyuan Li",
            "Cong Zhang",
            "Yufeng Peng",
            "Xin Lu",
            "Zhijing Sun",
            "Chengjie Ge",
            "Zihao Li",
            "Zishun Liao",
            "Ziang Zhou",
            "Qiyu Kang",
            "Xueyang Fu",
            "Zheng-Jun Zha",
            "Yuqian Zhang",
            "Shuai Liu",
            "Jie Liu",
            "Zhuhao Zhang",
            "Lishen Qu",
            "Zhihao Liu",
            "Shihao Zhou",
            "Yaqi Luo",
            "Juncheng Zhou",
            "Jufeng Yang",
            "Qianfeng Yang",
            "Qiyuan Guan",
            "Xiang Chen",
            "Guiyue Jin",
            "Jiyu Jin"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper reviews the AIM 2025 Challenge on High FPS Non-Uniform Motion Deblurring, showcasing state-of-the-art solutions and results.",
        "tldr_zh": "这篇论文总结了AIM 2025关于高FPS非均匀运动去模糊的挑战，展示了最新的解决方案和结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UrbanTwin: High-Fidelity Synthetic Replicas of Roadside Lidar Datasets",
        "summary": "This article presents UrbanTwin datasets - high-fidelity, realistic replicas\nof three public roadside lidar datasets: LUMPI, V2X-Real-IC, and TUMTraf-I.\nEach UrbanTwin dataset contains 10K annotated frames corresponding to one of\nthe public datasets. Annotations include 3D bounding boxes, instance\nsegmentation labels, and tracking IDs for six object classes, along with\nsemantic segmentation labels for nine classes. These datasets are synthesized\nusing emulated lidar sensors within realistic digital twins, modeled based on\nsurrounding geometry, road alignment at lane level, and the lane topology and\nvehicle movement patterns at intersections of the actual locations\ncorresponding to each real dataset. Due to the precise digital twin modeling,\nthe synthetic datasets are well aligned with their real counterparts, offering\nstrong standalone and augmentative value for training deep learning models on\ntasks such as 3D object detection, tracking, and semantic and instance\nsegmentation. We evaluate the alignment of the synthetic replicas through\nstatistical and structural similarity analysis with real data, and further\ndemonstrate their utility by training 3D object detection models solely on\nsynthetic data and testing them on real, unseen data. The high similarity\nscores and improved detection performance, compared to the models trained on\nreal data, indicate that the UrbanTwin datasets effectively enhance existing\nbenchmark datasets by increasing sample size and scene diversity. In addition,\nthe digital twins can be adapted to test custom scenarios by modifying the\ndesign and dynamics of the simulations. To our knowledge, these are the first\ndigitally synthesized datasets that can replace in-domain real-world datasets\nfor lidar perception tasks. UrbanTwin datasets are publicly available at\nhttps://dataverse.harvard.edu/dataverse/ucf-ut.",
        "url": "http://arxiv.org/abs/2509.06781v1",
        "published_date": "2025-09-08T15:06:02+00:00",
        "updated_date": "2025-09-08T15:06:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Muhammad Shahbaz",
            "Shaurya Agarwal"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper introduces UrbanTwin datasets, high-fidelity synthetic replicas of roadside lidar datasets, useful for training deep learning models in tasks like 3D object detection and tracking.",
        "tldr_zh": "本文介绍了UrbanTwin数据集，这是路边激光雷达数据集的高保真合成副本，可用于训练深度学习模型进行3D物体检测和跟踪。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Raw2Event: Converting Raw Frame Camera into Event Camera",
        "summary": "Event cameras offer unique advantages such as high temporal resolution, low\nlatency, and high dynamic range, making them more and more popular for vision\ntasks under challenging light conditions. However, their high cost, limited\nresolution, and lack of features such as autofocus hinder their broad adoption,\nparticularly for early-stage development and prototyping. In this work, we\npresent Raw2Event, a complete hardware-software system that enables real-time\nevent generation from low-cost raw frame-based cameras. By leveraging direct\naccess to raw Bayer data and bypassing traditional image signal processors\n(ISP), our system is able to utilize the full potential of camera hardware,\ndelivering higher dynamic range, higher resolution, and more faithful output\nthan RGB-based frame-to-event converters.\n  Built upon the DVS-Voltmeter model, Raw2Event features a configurable\nsimulation framework optimized for deployment on embedded platforms. We further\ndesign a data acquisition pipeline that supports synchronized recording of raw,\nRGB, and event streams, facilitating downstream evaluation and dataset\ncreation. Experimental results show that Raw2Event can generate event streams\nclosely resembling those from real event cameras, while benefiting from higher\nresolution and autofocus capabilities. The system also supports user-intuitive\nparameter tuning, enabling flexible adaptation to various application\nrequirements. Finally, we deploy the system on a Raspberry Pi for real-time\noperation, providing a scalable and cost-effective solution for event-based\nvision research and early-stage system development.\n  The codes are available online:\nhttps://anonymous.4open.science/r/raw2event-BFF2/README.md.",
        "url": "http://arxiv.org/abs/2509.06767v1",
        "published_date": "2025-09-08T14:53:01+00:00",
        "updated_date": "2025-09-08T14:53:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zijie Ning",
            "Enmin Lin",
            "Sudarshan R. Iyengar",
            "Patrick Vandewalle"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "Raw2Event system converts raw frame-based cameras into event cameras, offering high resolution, dynamic range, and autofocus capabilities.",
        "tldr_zh": "Raw2Event系统将原始帧相机转换为事件相机，提供高分辨率、动态范围和自动对焦功能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Event Spectroscopy: Event-based Multispectral and Depth Sensing using Structured Light",
        "summary": "Uncrewed aerial vehicles (UAVs) are increasingly deployed in forest\nenvironments for tasks such as environmental monitoring and search and rescue,\nwhich require safe navigation through dense foliage and precise data\ncollection. Traditional sensing approaches, including passive multispectral and\nRGB imaging, suffer from latency, poor depth resolution, and strong dependence\non ambient light - especially under forest canopies. In this work, we present a\nnovel event spectroscopy system that simultaneously enables high-resolution,\nlow-latency depth reconstruction and multispectral imaging using a single\nsensor. Depth is reconstructed using structured light, and by modulating the\nwavelength of the projected structured light, our system captures spectral\ninformation in controlled bands between 650 nm and 850 nm. We demonstrate up to\n$60\\%$ improvement in RMSE over commercial depth sensors and validate the\nspectral accuracy against a reference spectrometer and commercial multispectral\ncameras, demonstrating comparable performance. A portable version limited to\nRGB (3 wavelengths) is used to collect real-world depth and spectral data from\na Masoala Rainforest. We demonstrate the use of this prototype for color image\nreconstruction and material differentiation between leaves and branches using\nspectral and depth data. Our results show that adding depth (available at no\nextra effort with our setup) to material differentiation improves the accuracy\nby over $30\\%$ compared to color-only method. Our system, tested in both lab\nand real-world rainforest environments, shows strong performance in depth\nestimation, RGB reconstruction, and material differentiation - paving the way\nfor lightweight, integrated, and robust UAV perception and data collection in\ncomplex natural environments.",
        "url": "http://arxiv.org/abs/2509.06741v1",
        "published_date": "2025-09-08T14:34:55+00:00",
        "updated_date": "2025-09-08T14:34:55+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Christian Geckeler",
            "Niklas Neugebauer",
            "Manasi Muglikar",
            "Davide Scaramuzza",
            "Stefano Mintchev"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper presents a novel event spectroscopy system for UAVs to navigate through forest environments, enabling high-resolution depth reconstruction and multispectral imaging using a single sensor.",
        "tldr_zh": "本文提出了一种新颖的事件光谱系统，用于 UAV 在森林环境中导航，实现高分辨率深度重建和多光谱成像。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Co-Seg: Mutual Prompt-Guided Collaborative Learning for Tissue and Nuclei Segmentation",
        "summary": "Histopathology image analysis is critical yet challenged by the demand of\nsegmenting tissue regions and nuclei instances for tumor microenvironment and\ncellular morphology analysis. Existing studies focused on tissue semantic\nsegmentation or nuclei instance segmentation separately, but ignored the\ninherent relationship between these two tasks, resulting in insufficient\nhistopathology understanding. To address this issue, we propose a Co-Seg\nframework for collaborative tissue and nuclei segmentation. Specifically, we\nintroduce a novel co-segmentation paradigm, allowing tissue and nuclei\nsegmentation tasks to mutually enhance each other. To this end, we first devise\na region-aware prompt encoder (RP-Encoder) to provide high-quality semantic and\ninstance region prompts as prior constraints. Moreover, we design a mutual\nprompt mask decoder (MP-Decoder) that leverages cross-guidance to strengthen\nthe contextual consistency of both tasks, collaboratively computing semantic\nand instance segmentation masks. Extensive experiments on the PUMA dataset\ndemonstrate that the proposed Co-Seg surpasses state-of-the-arts in the\nsemantic, instance and panoptic segmentation of tumor tissues and nuclei\ninstances. The source code is available at https://github.com/xq141839/Co-Seg.",
        "url": "http://arxiv.org/abs/2509.06740v1",
        "published_date": "2025-09-08T14:34:54+00:00",
        "updated_date": "2025-09-08T14:34:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qing Xu",
            "Wenting Duan",
            "Zhen Chen"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "Proposes a Co-Seg framework for collaborative tissue and nuclei segmentation in histopathology images, achieving state-of-the-art results on tumor tissues and nuclei segmentation.",
        "tldr_zh": "提出了一种用于组织和细胞核共同分割的合作框架 Co-Seg，在肿瘤组织和细胞核分割方面取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Cortex-Synth: Differentiable Topology-Aware 3D Skeleton Synthesis with Hierarchical Graph Attention",
        "summary": "We present Cortex Synth, a novel end-to-end differentiable framework for\njoint 3D skeleton geometry and topology synthesis from single 2D images. Our\narchitecture introduces three key innovations: (1) A hierarchical graph\nattention mechanism with multi-scale skeletal refinement, (2) Differentiable\nspectral topology optimization via Laplacian eigen decomposition, and (3)\nAdversarial geometric consistency training for pose structure alignment. The\nframework integrates four synergistic modules: a pseudo 3D point cloud\ngenerator, an enhanced PointNet encoder, a skeleton coordinate decoder, and a\nnovel Differentiable Graph Construction Network (DGCN). Our experiments\ndemonstrate state-of-the-art results with 18.7 percent improvement in MPJPE and\n27.3 percent in Graph Edit Distance on ShapeNet, while reducing topological\nerrors by 42 percent compared to previous approaches. The model's end-to-end\ndifferentiability enables applications in robotic manipulation, medical\nimaging, and automated character rigging.",
        "url": "http://arxiv.org/abs/2509.06705v1",
        "published_date": "2025-09-08T14:03:13+00:00",
        "updated_date": "2025-09-08T14:03:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mohamed Zayaan S"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "AIGC"
        ],
        "tldr": "Cortex-Synth is a framework for synthesizing 3D skeletons from 2D images, showing improved results in pose estimation and reducing topological errors.",
        "tldr_zh": "Cortex-Synth是一个从2D图像合成3D骨架的框架，展示了在姿势估计方面的改进结果，并减少了拓扑错误。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "STAGE: Segmentation-oriented Industrial Anomaly Synthesis via Graded Diffusion with Explicit Mask Alignment",
        "summary": "Segmentation-oriented Industrial Anomaly Synthesis (SIAS) plays a pivotal\nrole in enhancing the performance of downstream anomaly segmentation, as it\nprovides an effective means of expanding abnormal data. However, existing SIAS\nmethods face several critical limitations: (i) the synthesized anomalies often\nlack intricate texture details and fail to align precisely with the surrounding\nbackground, and (ii) they struggle to generate fine-grained, pixel-level\nanomalies. To address these challenges, we propose Segmentation-oriented\nAnomaly synthesis via Graded diffusion with Explicit mask alignment, termed\nSTAGE. STAGE introduces a novel anomaly inference strategy that incorporates\nclean background information as a prior to guide the denoising distribution,\nenabling the model to more effectively distinguish and highlight abnormal\nforegrounds. Furthermore, it employs a graded diffusion framework with an\nanomaly-only branch to explicitly record local anomalies during both the\nforward and reverse processes, ensuring that subtle anomalies are not\noverlooked. Finally, STAGE incorporates the explicit mask alignment (EMA)\nstrategy to progressively align the synthesized anomalies with the background,\nresulting in context-consistent and structurally coherent generations.\nExtensive experiments on the MVTec and BTAD datasets demonstrate that STAGE\nachieves state-of-the-art performance in SIAS, which in turn enhances\ndownstream anomaly segmentation.",
        "url": "http://arxiv.org/abs/2509.06693v1",
        "published_date": "2025-09-08T13:47:01+00:00",
        "updated_date": "2025-09-08T13:47:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xichen Xu",
            "Yanshu Wang",
            "Jinbao Wang",
            "Qunyi Zhang",
            "Xiaoning Lei",
            "Guoyang Xie",
            "Guannan Jiang",
            "Zhichao Lu"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset",
            "GAN"
        ],
        "tldr": "The paper introduces STAGE, a method for segmentation-oriented anomaly synthesis to improve anomaly segmentation performance by incorporating clean background information and aligning synthesized anomalies with the background.",
        "tldr_zh": "本文介绍了一种名为STAGE的方法，用于分割导向异常合成，通过结合清晰的背景信息和将合成的异常与背景对齐来提高异常分割性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ Bioprinting Monitoring",
        "summary": "Bioprinting is a rapidly advancing field that offers a transformative\napproach to fabricating tissue and organ models through the precise deposition\nof cell-laden bioinks. Ensuring the fidelity and consistency of printed\nstructures in real-time remains a core challenge, particularly under\nconstraints imposed by limited imaging data and resource-constrained embedded\nhardware. Semantic segmentation of the extrusion process, differentiating\nbetween nozzle, extruded bioink, and surrounding background, enables in situ\nmonitoring critical to maintaining print quality and biological viability. In\nthis work, we introduce a lightweight semantic segmentation framework tailored\nfor real-time bioprinting applications. We present a novel, manually annotated\ndataset comprising 787 RGB images captured during the bioprinting process,\nlabeled across three classes: nozzle, bioink, and background. To achieve fast\nand efficient inference suitable for integration with bioprinting systems, we\npropose a BioLite U-Net architecture that leverages depthwise separable\nconvolutions to drastically reduce computational load without compromising\naccuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based\nsegmentation baselines using mean Intersection over Union (mIoU), Dice score,\nand pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess\nreal-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85%\nand a Dice score of 96.17%, while being over 1300x smaller than\nMobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame,\ndemonstrating near real-time capability. Compared to MobileNet baselines,\nBioLite U-Net offers a superior tradeoff between segmentation accuracy,\nefficiency, and deployability, making it highly suitable for intelligent,\nclosed-loop bioprinting systems.",
        "url": "http://arxiv.org/abs/2509.06690v1",
        "published_date": "2025-09-08T13:44:55+00:00",
        "updated_date": "2025-09-08T13:44:55+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.AR",
            "N/A",
            "I.2.9; I.2.10; I.4.6"
        ],
        "authors": [
            "Usman Haider",
            "Lukasz Szemet",
            "Daniel Kelly",
            "Vasileios Sergis",
            "Andrew C. Daly",
            "Karl Mason"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a lightweight semantic segmentation framework, BioLite U-Net, for real-time bioprinting monitoring to maintain print quality and biological viability.",
        "tldr_zh": "本文介绍了一种轻量级的语义分割框架BioLite U-Net，用于实时生物打印监控，以维持打印质量和生物活性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level Guidance in Large Scenes",
        "summary": "VIM-GS is a Gaussian Splatting (GS) framework using monocular images for\nnovel-view synthesis (NVS) in large scenes. GS typically requires accurate\ndepth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limited\ndepth sensing range makes it difficult for GS to work in large scenes.\nMonocular images, however, lack depth to guide the learning and lead to\ninferior NVS results. Although large foundation models (LFMs) for monocular\ndepth estimation are available, they suffer from cross-frame inconsistency,\ninaccuracy for distant scenes, and ambiguity in deceptive texture cues. This\npaper aims to generate dense, accurate depth images from monocular RGB inputs\nfor high-definite GS rendering. The key idea is to leverage the accurate but\nsparse depth from visual-inertial Structure-from-Motion (SfM) to refine the\ndense but coarse depth from LFMs. To bridge the sparse input and dense output,\nwe propose an object-segmented depth propagation algorithm that renders the\ndepth of pixels of structured objects. Then we develop a dynamic depth\nrefinement module to handle the crippled SfM depth of dynamic objects and\nrefine the coarse LFM depth. Experiments using public and customized datasets\ndemonstrate the superior rendering quality of VIM-GS in large scenes.",
        "url": "http://arxiv.org/abs/2509.06685v1",
        "published_date": "2025-09-08T13:41:10+00:00",
        "updated_date": "2025-09-08T13:41:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shengkai Zhang",
            "Yuhe Liu",
            "Guanjun Wu",
            "Jianhua He",
            "Xinggang Wang",
            "Mozi Chen",
            "Kezhong Liu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper presents a framework called VIM-GS for generating accurate depth images from monocular RGB inputs to improve novel-view synthesis in large scenes.",
        "tldr_zh": "本文提出了一种名为VIM-GS的框架，用于从单目RGB输入中生成准确的深度图像，以改善大场景中的新视角合成。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Online Clustering of Seafloor Imagery for Interpretation during Long-Term AUV Operations",
        "summary": "As long-endurance and seafloor-resident AUVs become more capable, there is an\nincreasing need for extended, real-time interpretation of seafloor imagery to\nenable adaptive missions and optimise communication efficiency. Although\noffline image analysis methods are well established, they rely on access to\ncomplete datasets and human-labelled examples to manage the strong influence of\nenvironmental and operational conditions on seafloor image\nappearance-requirements that cannot be met in real-time settings. To address\nthis, we introduce an online clustering framework (OCF) capable of interpreting\nseafloor imagery without supervision, which is designed to operate in real-time\non continuous data streams in a scalable, adaptive, and self-consistent manner.\nThe method enables the efficient review and consolidation of common patterns\nacross the entire data history in constant time by identifying and maintaining\na set of representative samples that capture the evolving feature distribution,\nsupporting dynamic cluster merging and splitting without reprocessing the full\nimage history. We evaluate the framework on three diverse seafloor image\ndatasets, analysing the impact of different representative sampling strategies\non both clustering accuracy and computational cost. The OCF achieves the\nhighest average F1 score of 0.68 across the three datasets among all\ncomparative online clustering approaches, with a standard deviation of 3%\nacross three distinct survey trajectories, demonstrating its superior\nclustering capability and robustness to trajectory variation. In addition, it\nmaintains consistently lower and bounded computational time as the data volume\nincreases. These properties are beneficial for generating survey data summaries\nand supporting informative path planning in long-term, persistent autonomous\nmarine exploration.",
        "url": "http://arxiv.org/abs/2509.06678v1",
        "published_date": "2025-09-08T13:36:27+00:00",
        "updated_date": "2025-09-08T13:36:27+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Cailei Liang",
            "Adrian Bodenmann",
            "Sam Fenton",
            "Blair Thornton"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces an online clustering framework for interpreting seafloor imagery in real-time without supervision, achieving high accuracy and efficiency.",
        "tldr_zh": "本文介绍了一种在线聚类框架，用于在实时无监督条件下解释海底图像，实现了高准确性和效率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Investigating Location-Regularised Self-Supervised Feature Learning for Seafloor Visual Imagery",
        "summary": "High-throughput interpretation of robotically gathered seafloor visual\nimagery can increase the efficiency of marine monitoring and exploration.\nAlthough recent research has suggested that location metadata can enhance\nself-supervised feature learning (SSL), its benefits across different SSL\nstrategies, models and seafloor image datasets are underexplored. This study\nevaluates the impact of location-based regularisation on six state-of-the-art\nSSL frameworks, which include Convolutional Neural Network (CNN) and Vision\nTransformer (ViT) models with varying latent-space dimensionality. Evaluation\nacross three diverse seafloor image datasets finds that location-regularisation\nconsistently improves downstream classification performance over standard SSL,\nwith average F1-score gains of $4.9 \\pm 4.0%$ for CNNs and $6.3 \\pm 8.9%$ for\nViTs, respectively. While CNNs pretrained on generic datasets benefit from\nhigh-dimensional latent representations, dataset-optimised SSL achieves similar\nperformance across the high (512) and low (128) dimensional latent\nrepresentations. Location-regularised SSL improves CNN performance over\npre-trained models by $2.7 \\pm 2.7%$ and $10.1 \\pm 9.4%$ for high and\nlow-dimensional latent representations, respectively. For ViTs,\nhigh-dimensionality benefits both pre-trained and dataset-optimised SSL.\nAlthough location-regularisation improves SSL performance compared to standard\nSSL methods, pre-trained ViTs show strong generalisation, matching the\nbest-performing location-regularised SSL with F1-scores of $0.795 \\pm 0.075$\nand $0.795 \\pm 0.077$, respectively. The findings highlight the value of\nlocation metadata for SSL regularisation, particularly when using\nlow-dimensional latent representations, and demonstrate strong generalisation\nof high-dimensional ViTs for seafloor image analysis.",
        "url": "http://arxiv.org/abs/2509.06660v1",
        "published_date": "2025-09-08T13:19:04+00:00",
        "updated_date": "2025-09-08T13:19:04+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Cailei Liang",
            "Adrian Bodenmann",
            "Emma J Curtis",
            "Samuel Simmons",
            "Kazunori Nagano",
            "Stan Brown",
            "Adam Riese",
            "Blair Thornton"
        ],
        "ai_categories": [
            "LoRA",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper investigates the use of location metadata to improve self-supervised feature learning for seafloor imagery, showing consistent performance gains across different models and datasets.",
        "tldr_zh": "该论文研究了使用位置元数据来改进自监督特征学习以用于海底图像，表明在不同模型和数据集上具有一致的性能提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From Skin to Skeleton: Towards Biomechanically Accurate 3D Digital Humans",
        "summary": "Great progress has been made in estimating 3D human pose and shape from\nimages and video by training neural networks to directly regress the parameters\nof parametric human models like SMPL. However, existing body models have\nsimplified kinematic structures that do not correspond to the true joint\nlocations and articulations in the human skeletal system, limiting their\npotential use in biomechanics. On the other hand, methods for estimating\nbiomechanically accurate skeletal motion typically rely on complex motion\ncapture systems and expensive optimization methods. What is needed is a\nparametric 3D human model with a biomechanically accurate skeletal structure\nthat can be easily posed. To that end, we develop SKEL, which re-rigs the SMPL\nbody model with a biomechanics skeleton. To enable this, we need training data\nof skeletons inside SMPL meshes in diverse poses.\n  We build such a dataset by optimizing biomechanically accurate skeletons\ninside SMPL meshes from AMASS sequences. We then learn a regressor from SMPL\nmesh vertices to the optimized joint locations and bone rotations. Finally, we\nre-parametrize the SMPL mesh with the new kinematic parameters. The resulting\nSKEL model is animatable like SMPL but with fewer, and\nbiomechanically-realistic, degrees of freedom. We show that SKEL has more\nbiomechanically accurate joint locations than SMPL, and the bones fit inside\nthe body surface better than previous methods. By fitting SKEL to SMPL meshes\nwe are able to \"upgrade\" existing human pose and shape datasets to include\nbiomechanical parameters. SKEL provides a new tool to enable biomechanics in\nthe wild, while also providing vision and graphics researchers with a better\nconstrained and more realistic model of human articulation. The model, code,\nand data are available for research at https://skel.is.tue.mpg.de..",
        "url": "http://arxiv.org/abs/2509.06607v1",
        "published_date": "2025-09-08T12:24:27+00:00",
        "updated_date": "2025-09-08T12:24:27+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Marilyn Keller",
            "Keenon Werling",
            "Soyong Shin",
            "Scott Delp",
            "Sergi Pujades",
            "C. Karen Liu",
            "Michael J. Black"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces SKEL, a parametric 3D human model with a biomechanically accurate skeletal structure that can be easily posed, improving on existing body models for biomechanics and human articulation.",
        "tldr_zh": "本文介绍了SKEL，这是一种具有生物力学准确骨骼结构的参数化3D人体模型，可以轻松进行姿势设定，改进了现有的身体模型，适用于生物力学和人类关节学。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Hybrid Swin Attention Networks for Simultaneously Low-Dose PET and CT Denoising",
        "summary": "Low-dose computed tomography (LDCT) and positron emission tomography (PET)\nhave emerged as safer alternatives to conventional imaging modalities by\nsignificantly reducing radiation exposure. However, this reduction often\nresults in increased noise and artifacts, which can compromise diagnostic\naccuracy. Consequently, denoising for LDCT/PET has become a vital area of\nresearch aimed at enhancing image quality while maintaining radiation safety.\nIn this study, we introduce a novel Hybrid Swin Attention Network (HSANet),\nwhich incorporates Efficient Global Attention (EGA) modules and a hybrid\nupsampling module. The EGA modules enhance both spatial and channel-wise\ninteraction, improving the network's capacity to capture relevant features,\nwhile the hybrid upsampling module mitigates the risk of overfitting to noise.\nWe validate the proposed approach using a publicly available LDCT/PET dataset.\nExperimental results demonstrate that HSANet achieves superior denoising\nperformance compared to existing methods, while maintaining a lightweight model\nsize suitable for deployment on GPUs with standard memory configurations. This\nmakes our approach highly practical for real-world clinical applications.",
        "url": "http://arxiv.org/abs/2509.06591v1",
        "published_date": "2025-09-08T12:02:38+00:00",
        "updated_date": "2025-09-08T12:02:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yichao Liu",
            "YueYang Teng"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel Hybrid Swin Attention Network for denoising low-dose PET and CT images, achieving superior performance compared to existing methods.",
        "tldr_zh": "本文介绍了一种新型的混合Swin注意力网络，用于去噪低剂量PET和CT图像，表现优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Detection of trade in products derived from threatened species using machine learning and a smartphone",
        "summary": "Unsustainable trade in wildlife is a major threat to biodiversity and is now\nincreasingly prevalent in digital marketplaces and social media. With the sheer\nvolume of digital content, the need for automated methods to detect wildlife\ntrade listings is growing. These methods are especially needed for the\nautomatic identification of wildlife products, such as ivory. We developed\nmachine learning-based object recognition models that can identify wildlife\nproducts within images and highlight them. The data consists of images of\nelephant, pangolin, and tiger products that were identified as being sold\nillegally or that were confiscated by authorities. Specifically, the wildlife\nproducts included elephant ivory and skins, pangolin scales, and claws (raw and\ncrafted), and tiger skins and bones. We investigated various combinations of\ntraining strategies and two loss functions to identify the best model to use in\nthe automatic detection of these wildlife products. Models were trained for\neach species while also developing a single model to identify products from all\nthree species. The best model showed an overall accuracy of 84.2% with\naccuracies of 71.1%, 90.2% and 93.5% in detecting products derived from\nelephants, pangolins, and tigers, respectively. We further demonstrate that the\nmachine learning model can be made easily available to stakeholders, such as\ngovernment authorities and law enforcement agencies, by developing a\nsmartphone-based application that had an overall accuracy of 91.3%. The\napplication can be used in real time to click images and help identify\npotentially prohibited products of target species. Thus, the proposed method is\nnot only applicable for monitoring trade on the web but can also be used e.g.\nin physical markets for monitoring wildlife trade.",
        "url": "http://arxiv.org/abs/2509.06585v1",
        "published_date": "2025-09-08T11:56:26+00:00",
        "updated_date": "2025-09-08T11:56:26+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Ritwik Kulkarni",
            "WU Hanqin",
            "Enrico Di Minin"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper presents a machine learning-based approach to detect and identify wildlife products derived from threatened species, with a focus on elephant ivory, pangolin scales, and tiger bones, through image analysis and a smartphone application.",
        "tldr_zh": "本文提出了一种基于机器学习的方法，通过图像分析和智能手机应用程序，检测和识别野生动物制品，重点是大象象牙、穿山甲鳞片和老虎骨骼。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Back To The Drawing Board: Rethinking Scene-Level Sketch-Based Image Retrieval",
        "summary": "The goal of Scene-level Sketch-Based Image Retrieval is to retrieve natural\nimages matching the overall semantics and spatial layout of a free-hand sketch.\nUnlike prior work focused on architectural augmentations of retrieval models,\nwe emphasize the inherent ambiguity and noise present in real-world sketches.\nThis insight motivates a training objective that is explicitly designed to be\nrobust to sketch variability. We show that with an appropriate combination of\npre-training, encoder architecture, and loss formulation, it is possible to\nachieve state-of-the-art performance without the introduction of additional\ncomplexity. Extensive experiments on a challenging FS-COCO and widely-used\nSketchyCOCO datasets confirm the effectiveness of our approach and underline\nthe critical role of training design in cross-modal retrieval tasks, as well as\nthe need to improve the evaluation scenarios of scene-level SBIR.",
        "url": "http://arxiv.org/abs/2509.06566v1",
        "published_date": "2025-09-08T11:26:40+00:00",
        "updated_date": "2025-09-08T11:26:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Emil Demić",
            "Luka Čehovin Zajc"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a training objective for Scene-level Sketch-Based Image Retrieval that is robust to sketch variability, achieving state-of-the-art performance without added complexity.",
        "tldr_zh": "本文提出了一个针对场景级草图图像检索的训练目标，它能够在不增加复杂性的情况下实现业内领先的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Impact of Labeling Inaccuracy and Image Noise on Tooth Segmentation in Panoramic Radiographs using Federated, Centralized and Local Learning",
        "summary": "Objectives: Federated learning (FL) may mitigate privacy constraints,\nheterogeneous data quality, and inconsistent labeling in dental diagnostic AI.\nWe compared FL with centralized (CL) and local learning (LL) for tooth\nsegmentation in panoramic radiographs across multiple data corruption\nscenarios. Methods: An Attention U-Net was trained on 2066 radiographs from six\ninstitutions across four settings: baseline (unaltered data); label\nmanipulation (dilated/missing annotations); image-quality manipulation\n(additive Gaussian noise); and exclusion of a faulty client with corrupted\ndata. FL was implemented via the Flower AI framework. Per-client training- and\nvalidation-loss trajectories were monitored for anomaly detection and a set of\nmetrics (Dice, IoU, HD, HD95 and ASSD) was evaluated on a hold-out test set.\nFrom these metrics significance results were reported through Wilcoxon\nsigned-rank test. CL and LL served as comparators. Results: Baseline: FL\nachieved a median Dice of 0.94889 (ASSD: 1.33229), slightly better than CL at\n0.94706 (ASSD: 1.37074) and LL at 0.93557-0.94026 (ASSD: 1.51910-1.69777).\nLabel manipulation: FL maintained the best median Dice score at 0.94884 (ASSD:\n1.46487) versus CL's 0.94183 (ASSD: 1.75738) and LL's 0.93003-0.94026 (ASSD:\n1.51910-2.11462). Image noise: FL led with Dice at 0.94853 (ASSD: 1.31088); CL\nscored 0.94787 (ASSD: 1.36131); LL ranged from 0.93179-0.94026 (ASSD:\n1.51910-1.77350). Faulty-client exclusion: FL reached Dice at 0.94790 (ASSD:\n1.33113) better than CL's 0.94550 (ASSD: 1.39318). Loss-curve monitoring\nreliably flagged the corrupted site. Conclusions: FL matches or exceeds CL and\noutperforms LL across corruption scenarios while preserving privacy. Per-client\nloss trajectories provide an effective anomaly-detection mechanism and support\nFL as a practical, privacy-preserving approach for scalable clinical AI\ndeployment.",
        "url": "http://arxiv.org/abs/2509.06553v1",
        "published_date": "2025-09-08T11:07:47+00:00",
        "updated_date": "2025-09-08T11:07:47+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Johan Andreas Balle Rubak",
            "Khuram Naveed",
            "Sanyam Jain",
            "Lukas Esterle",
            "Alexandros Iosifidis",
            "Ruben Pauwels"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper compares federated learning with centralized and local learning for tooth segmentation in panoramic radiographs under various data corruption scenarios, showing that federated learning outperforms the other methods while preserving privacy.",
        "tldr_zh": "本文比较了用于全景X光牙齿分割的联邦学习与集中式和本地学习，在各种数据损坏场景下，表明联邦学习在保护隐私的同时优于其他方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Predicting Brain Tumor Response to Therapy using a Hybrid Deep Learning and Radiomics Approach",
        "summary": "Accurate evaluation of the response of glioblastoma to therapy is crucial for\nclinical decision-making and patient management. The Response Assessment in\nNeuro-Oncology (RANO) criteria provide a standardized framework to assess\npatients' clinical response, but their application can be complex and subject\nto observer variability. This paper presents an automated method for\nclassifying the intervention response from longitudinal MRI scans, developed to\npredict tumor response during therapy as part of the BraTS 2025 challenge. We\npropose a novel hybrid framework that combines deep learning derived feature\nextraction and an extensive set of radiomics and clinically chosen features.\nOur approach utilizes a fine-tuned ResNet-18 model to extract features from 2D\nregions of interest across four MRI modalities. These deep features are then\nfused with a rich set of more than 4800 radiomic and clinically driven\nfeatures, including 3D radiomics of tumor growth and shrinkage masks,\nvolumetric changes relative to the nadir, and tumor centroid shift. Using the\nfused feature set, a CatBoost classifier achieves a mean ROC AUC of 0.81 and a\nMacro F1 score of 0.50 in the 4-class response prediction task (Complete\nResponse, Partial Response, Stable Disease, Progressive Disease). Our results\nhighlight that synergizing learned image representations with domain-targeted\nradiomic features provides a robust and effective solution for automated\ntreatment response assessment in neuro-oncology.",
        "url": "http://arxiv.org/abs/2509.06511v1",
        "published_date": "2025-09-08T10:15:23+00:00",
        "updated_date": "2025-09-08T10:15:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Daniil Tikhonov",
            "Matheus Scatolin",
            "Mohor Banerjee",
            "Qiankun Ji",
            "Ahmed Jaheen",
            "Mostafa Salem",
            "Abdelrahman Elsayed",
            "Hu Wang",
            "Sarim Hashmi",
            "Mohammad Yaqub"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper presents a hybrid deep learning and radiomics approach to predict brain tumor response to therapy using longitudinal MRI scans, achieving promising results.",
        "tldr_zh": "本文提出了一种混合深度学习和放射组学方法，利用纵向MRI扫描来预测脑肿瘤对治疗的反应，取得了令人满意的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution Remote Sensing Change Detection",
        "summary": "Change detection from high-resolution remote sensing images lies as a\ncornerstone of Earth observation applications, yet its efficacy is often\ncompromised by two critical challenges. First, false alarms are prevalent as\nmodels misinterpret radiometric variations from temporal shifts (e.g.,\nillumination, season) as genuine changes. Second, a non-negligible semantic gap\nbetween deep abstract features and shallow detail-rich features tends to\nobstruct their effective fusion, culminating in poorly delineated boundaries.\nTo step further in addressing these issues, we propose the Frequency-Spatial\nSynergistic Gated Network (FSG-Net), a novel paradigm that aims to\nsystematically disentangle semantic changes from nuisance variations.\nSpecifically, FSG-Net first operates in the frequency domain, where a\nDiscrepancy-Aware Wavelet Interaction Module (DAWIM) adaptively mitigates\npseudo-changes by discerningly processing different frequency components.\nSubsequently, the refined features are enhanced in the spatial domain by a\nSynergistic Temporal-Spatial Attention Module (STSAM), which amplifies the\nsaliency of genuine change regions. To finally bridge the semantic gap, a\nLightweight Gated Fusion Unit (LGFU) leverages high-level semantics to\nselectively gate and integrate crucial details from shallow layers.\nComprehensive experiments on the CDD, GZ-CD, and LEVIR-CD benchmarks validate\nthe superiority of FSG-Net, establishing a new state-of-the-art with F1-scores\nof 94.16%, 89.51%, and 91.27%, respectively. The code will be made available at\nhttps://github.com/zxXie-Air/FSG-Net after a possible publication.",
        "url": "http://arxiv.org/abs/2509.06482v1",
        "published_date": "2025-09-08T09:46:33+00:00",
        "updated_date": "2025-09-08T09:46:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhongxiang Xie",
            "Shuangxi Miao",
            "Yuhan Jiang",
            "Zhewei Zhang",
            "Jing Yao",
            "Xuecao Li",
            "Jianxi Huang",
            "Pedram Ghamisi"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces FSG-Net, a novel network for high-resolution remote sensing change detection that addresses challenges of false alarms and semantic gap.",
        "tldr_zh": "本文介绍了FSG-Net，一种用于高分辨率遥感变化检测的新型网络，解决了虚警和语义差距的挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Does DINOv3 Set a New Medical Vision Standard?",
        "summary": "The advent of large-scale vision foundation models, pre-trained on diverse\nnatural images, has marked a paradigm shift in computer vision. However, how\nthe frontier vision foundation models' efficacies transfer to specialized\ndomains remains such as medical imaging remains an open question. This report\ninvestigates whether DINOv3, a state-of-the-art self-supervised vision\ntransformer (ViT) that features strong capability in dense prediction tasks,\ncan directly serve as a powerful, unified encoder for medical vision tasks\nwithout domain-specific pre-training. To answer this, we benchmark DINOv3\nacross common medical vision tasks, including 2D/3D classification and\nsegmentation on a wide range of medical imaging modalities. We systematically\nanalyze its scalability by varying model sizes and input image resolutions. Our\nfindings reveal that DINOv3 shows impressive performance and establishes a\nformidable new baseline. Remarkably, it can even outperform medical-specific\nfoundation models like BiomedCLIP and CT-Net on several tasks, despite being\ntrained solely on natural images. However, we identify clear limitations: The\nmodel's features degrade in scenarios requiring deep domain specialization,\nsuch as in Whole-Slide Pathological Images (WSIs), Electron Microscopy (EM),\nand Positron Emission Tomography (PET). Furthermore, we observe that DINOv3\ndoes not consistently obey scaling law in the medical domain; performance does\nnot reliably increase with larger models or finer feature resolutions, showing\ndiverse scaling behaviors across tasks. Ultimately, our work establishes DINOv3\nas a strong baseline, whose powerful visual features can serve as a robust\nprior for multiple complex medical tasks. This opens promising future\ndirections, such as leveraging its features to enforce multiview consistency in\n3D reconstruction.",
        "url": "http://arxiv.org/abs/2509.06467v1",
        "published_date": "2025-09-08T09:28:57+00:00",
        "updated_date": "2025-09-08T09:28:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Che Liu",
            "Yinda Chen",
            "Haoyuan Shi",
            "Jinpeng Lu",
            "Bailiang Jian",
            "Jiazhen Pan",
            "Linghan Cai",
            "Jiayi Wang",
            "Yundi Zhang",
            "Jun Li",
            "Cosmin I. Bercea",
            "Cheng Ouyang",
            "Chen Chen",
            "Zhiwei Xiong",
            "Benedikt Wiestler",
            "Christian Wachinger",
            "Daniel Rueckert",
            "Wenjia Bai",
            "Rossella Arcucci"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper investigates the use of a self-supervised vision transformer for medical imaging tasks and finds that it can outperform specialized models in some areas but has limitations in others.",
        "tldr_zh": "本文研究了使用自监督视觉转换器进行医学影像任务，并发现在某些领域可以胜过专门的模型，但在其他领域存在局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning",
        "summary": "Vision-Language Models (VLMs) have demonstrated remarkable success across\ndiverse visual tasks, yet their performance degrades in complex visual\nenvironments. While existing enhancement approaches require additional\ntraining, rely on external segmentation tools, or operate at coarse-grained\nlevels, they overlook the innate ability within VLMs. To bridge this gap, we\ninvestigate VLMs' attention patterns and discover that: (1) visual complexity\nstrongly correlates with attention entropy, negatively impacting reasoning\nperformance; (2) attention progressively refines from global scanning in\nshallow layers to focused convergence in deeper layers, with convergence degree\ndetermined by visual complexity. (3) Theoretically, we prove that the contrast\nof attention maps between general queries and task-specific queries enables the\ndecomposition of visual signal into semantic signals and visual noise\ncomponents. Building on these insights, we propose Contrastive Attention\nRefinement for Visual Enhancement (CARVE), a training-free method that extracts\ntask-relevant visual signals through attention contrasting at the pixel level.\nExtensive experiments demonstrate that CARVE consistently enhances performance,\nachieving up to 75% improvement on open-source models. Our work provides\ncritical insights into the interplay between visual complexity and attention\nmechanisms, offering an efficient pathway for improving visual reasoning with\ncontrasting attention.",
        "url": "http://arxiv.org/abs/2509.06461v1",
        "published_date": "2025-09-08T09:20:04+00:00",
        "updated_date": "2025-09-08T09:20:04+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuyao Ge",
            "Shenghua Liu",
            "Yiwei Wang",
            "Lingrui Mei",
            "Baolong Bi",
            "Xuanshan Zhou",
            "Jiayu Yao",
            "Jiafeng Guo",
            "Xueqi Cheng"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper proposes a method called CARVE that uses contrastive attention to enhance visual reasoning in Vision-Language Models, achieving up to 75% improvement on open-source models.",
        "tldr_zh": "本文提出了一种名为CARVE的方法，利用对比注意力来增强视觉推理在视觉语言模型中的作用，在开源模型上实现了高达75%的性能提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Phantom-Insight: Adaptive Multi-cue Fusion for Video Camouflaged Object Detection with Multimodal LLM",
        "summary": "Video camouflaged object detection (VCOD) is challenging due to dynamic\nenvironments. Existing methods face two main issues: (1) SAM-based methods\nstruggle to separate camouflaged object edges due to model freezing, and (2)\nMLLM-based methods suffer from poor object separability as large language\nmodels merge foreground and background. To address these issues, we propose a\nnovel VCOD method based on SAM and MLLM, called Phantom-Insight. To enhance the\nseparability of object edge details, we represent video sequences with temporal\nand spatial clues and perform feature fusion via LLM to increase information\ndensity. Next, multiple cues are generated through the dynamic foreground\nvisual token scoring module and the prompt network to adaptively guide and\nfine-tune the SAM model, enabling it to adapt to subtle textures. To enhance\nthe separability of objects and background, we propose a decoupled\nforeground-background learning strategy. By generating foreground and\nbackground cues separately and performing decoupled training, the visual token\ncan effectively integrate foreground and background information independently,\nenabling SAM to more accurately segment camouflaged objects in the video.\nExperiments on the MoCA-Mask dataset show that Phantom-Insight achieves\nstate-of-the-art performance across various metrics. Additionally, its ability\nto detect unseen camouflaged objects on the CAD2016 dataset highlights its\nstrong generalization ability.",
        "url": "http://arxiv.org/abs/2509.06422v1",
        "published_date": "2025-09-08T08:17:47+00:00",
        "updated_date": "2025-09-08T08:17:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hua Zhang",
            "Changjiang Luo",
            "Ruoyu Chen"
        ],
        "ai_categories": [
            "Multimodality",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel method, Phantom-Insight, for video camouflaged object detection using a fusion of SAM and MLLM with multiple cues for enhanced object separability.",
        "tldr_zh": "该论文提出了一种新的方法，Phantom-Insight，用于视频伪装物体检测，利用SAM和MLLM的融合，多重线索增强对象分离能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Index-Preserving Lightweight Token Pruning for Efficient Document Understanding in Vision-Language Models",
        "summary": "Recent progress in vision-language models (VLMs) has led to impressive\nresults in document understanding tasks, but their high computational demands\nremain a challenge. To mitigate the compute burdens, we propose a lightweight\ntoken pruning framework that filters out non-informative background regions\nfrom document images prior to VLM processing. A binary patch-level classifier\nremoves non-text areas, and a max-pooling refinement step recovers fragmented\ntext regions to enhance spatial coherence. Experiments on real-world document\ndatasets demonstrate that our approach substantially lowers computational\ncosts, while maintaining comparable accuracy.",
        "url": "http://arxiv.org/abs/2509.06415v1",
        "published_date": "2025-09-08T08:12:26+00:00",
        "updated_date": "2025-09-08T08:12:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Jaemin Son",
            "Sujin Choi",
            "Inyong Yun"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a lightweight token pruning framework for efficient document understanding in vision-language models by filtering out non-informative background regions from document images.",
        "tldr_zh": "本文提出了一种轻量级标记修剪框架，通过过滤文档图像中的非信息背景区域，实现视觉 - 语言模型中对文档的高效理解。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VQualA 2025 Challenge on Image Super-Resolution Generated Content Quality Assessment: Methods and Results",
        "summary": "This paper presents the ISRGC-Q Challenge, built upon the Image\nSuper-Resolution Generated Content Quality Assessment (ISRGen-QA) dataset, and\norganized as part of the Visual Quality Assessment (VQualA) Competition at the\nICCV 2025 Workshops. Unlike existing Super-Resolution Image Quality Assessment\n(SR-IQA) datasets, ISRGen-QA places a greater emphasis on SR images generated\nby the latest generative approaches, including Generative Adversarial Networks\n(GANs) and diffusion models. The primary goal of this challenge is to analyze\nthe unique artifacts introduced by modern super-resolution techniques and to\nevaluate their perceptual quality effectively. A total of 108 participants\nregistered for the challenge, with 4 teams submitting valid solutions and fact\nsheets for the final testing phase. These submissions demonstrated\nstate-of-the-art (SOTA) performance on the ISRGen-QA dataset. The project is\npublicly available at: https://github.com/Lighting-YXLI/ISRGen-QA.",
        "url": "http://arxiv.org/abs/2509.06413v1",
        "published_date": "2025-09-08T08:07:50+00:00",
        "updated_date": "2025-09-08T08:07:50+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Yixiao Li",
            "Xin Li",
            "Chris Wei Zhou",
            "Shuo Xing",
            "Hadi Amirpour",
            "Xiaoshuai Hao",
            "Guanghui Yue",
            "Baoquan Zhao",
            "Weide Liu",
            "Xiaoyuan Yang",
            "Zhengzhong Tu",
            "Xinyu Li",
            "Chuanbiao Song",
            "Chenqi Zhang",
            "Jun Lan",
            "Huijia Zhu",
            "Weiqiang Wang",
            "Xiaoyan Sun",
            "Shishun Tian",
            "Dongyang Yan",
            "Weixia Zhang",
            "Junlin Chen",
            "Wei Sun",
            "Zhihua Wang",
            "Zhuohang Shi",
            "Zhizun Luo",
            "Hang Ouyang",
            "Tianxin Xiao",
            "Fan Yang",
            "Zhaowang Wu",
            "Kaixin Deng"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "This paper introduces the ISRGC-Q Challenge focusing on assessing the quality of super-resolution images generated by modern techniques like GANs, with state-of-the-art performance demonstrated by participants.",
        "tldr_zh": "本文介绍了ISRGC-Q挑战赛，重点评估由现代技术如GAN生成的超分辨率图像质量，挑战赛参与者表现出最新技术的最先进性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Multi-Modal Deep Learning Framework for Colorectal Pathology Diagnosis: Integrating Histological and Colonoscopy Data in a Pilot Study",
        "summary": "Colorectal diseases, including inflammatory conditions and neoplasms, require\nquick, accurate care to be effectively treated. Traditional diagnostic\npipelines require extensive preparation and rely on separate, individual\nevaluations on histological images and colonoscopy footage, introducing\npossible variability and inefficiencies. This pilot study proposes a unified\ndeep learning network that uses convolutional neural networks (CN N s) to\nclassify both histopathological slides and colonoscopy video frames in one\npipeline. The pipeline integrates class-balancing learning, robust\naugmentation, and calibration methods to ensure accurate results. Static colon\nhistology images were taken from the PathMNIST dataset, and the lower\ngastrointestinal (colonoscopy) videos were drawn from the HyperKvasir dataset.\nThe CNN architecture used was ResNet-50. This study demonstrates an\ninterpretable and reproducible diagnostic pipeline that unifies multiple\ndiagnostic modalities to advance and ease the detection of colorectal diseases.",
        "url": "http://arxiv.org/abs/2509.06351v1",
        "published_date": "2025-09-08T05:54:03+00:00",
        "updated_date": "2025-09-08T05:54:03+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Krithik Ramesh",
            "Ritvik Koneru"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper presents a unified deep learning network using CNNs to diagnose colorectal diseases by analyzing both histopathological slides and colonoscopy videos in one pipeline.",
        "tldr_zh": "本文提出了一种统一的深度学习网络，利用卷积神经网络在一个管道中对结直肠疾病进行诊断，分析组织病理学切片和结肠镜视频。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multi View Slot Attention Using Paraphrased Texts For Face Anti-Spoofing",
        "summary": "Recent face anti-spoofing (FAS) methods have shown remarkable cross-domain\nperformance by employing vision-language models like CLIP. However, existing\nCLIP-based FAS models do not fully exploit CLIP's patch embedding tokens,\nfailing to detect critical spoofing clues. Moreover, these models rely on a\nsingle text prompt per class (e.g., 'live' or 'fake'), which limits\ngeneralization. To address these issues, we propose MVP-FAS, a novel framework\nincorporating two key modules: Multi-View Slot attention (MVS) and Multi-Text\nPatch Alignment (MTPA). Both modules utilize multiple paraphrased texts to\ngenerate generalized features and reduce dependence on domain-specific text.\nMVS extracts local detailed spatial features and global context from patch\nembeddings by leveraging diverse texts with multiple perspectives. MTPA aligns\npatches with multiple text representations to improve semantic robustness.\nExtensive experiments demonstrate that MVP-FAS achieves superior generalization\nperformance, outperforming previous state-of-the-art methods on cross-domain\ndatasets. Code: https://github.com/Elune001/MVP-FAS.",
        "url": "http://arxiv.org/abs/2509.06336v1",
        "published_date": "2025-09-08T04:53:46+00:00",
        "updated_date": "2025-09-08T04:53:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CR"
        ],
        "authors": [
            "Jeongmin Yu",
            "Susang Kim",
            "Kisu Lee",
            "Taekyoung Kwon",
            "Won-Yong Shin",
            "Ha Young Kim"
        ],
        "ai_categories": [
            "Multimodality",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces MVP-FAS, a framework using Multi-View Slot attention and Multi-Text Patch Alignment to improve face anti-spoofing performance by leveraging diverse paraphrased texts.",
        "tldr_zh": "本文提出了MVP-FAS，一个利用Multi-View Slot关注和Multi-Text Patch对准来改善人脸反欺诈性能的框架，通过利用多样化的释义文本。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Harnessing Object Grounding for Time-Sensitive Video Understanding",
        "summary": "We propose to improve the time-sensitive video understanding (TSV) capability\nof video large language models (Video-LLMs) with grounded objects (GO). We\nhypothesize that TSV tasks can benefit from GO within frames, which is\nsupported by our preliminary experiments on LITA, a state-of-the-art Video-LLM\nfor reasoning temporal localization. While augmenting prompts with textual\ndescription of these object annotations improves the performance of LITA, it\nalso introduces extra token length and susceptibility to the noise in object\nlevel information. To address this, we propose GO-Tokenizer, a lightweight\nadd-on module for Video-LLMs leveraging off-the-shelf object detectors to\nencode compact object information on the fly. Experimental results demonstrate\nthat pretraining with GO-Tokenizer outperforms the vanilla Video-LLM and its\ncounterpart utilizing textual description of objects in the prompt. The gain\ngeneralizes across different models, datasets and video understanding tasks\nsuch as reasoning temporal localization and dense captioning.",
        "url": "http://arxiv.org/abs/2509.06335v1",
        "published_date": "2025-09-08T04:52:00+00:00",
        "updated_date": "2025-09-08T04:52:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tz-Ying Wu",
            "Sharath Nittur Sridhar",
            "Subarna Tripathi"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes using grounded objects to enhance time-sensitive video understanding through a lightweight module called GO-Tokenizer, outperforming traditional methods in various video understanding tasks.",
        "tldr_zh": "该论文提出使用基于对象的方法通过一个名为GO-Tokenizer的轻量级模块来增强时间敏感的视频理解，优于传统方法在各种视频理解任务中的表现。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multi-Modal Camera-Based Detection of Vulnerable Road Users",
        "summary": "Vulnerable road users (VRUs) such as pedestrians, cyclists, and motorcyclists\nrepresent more than half of global traffic deaths, yet their detection remains\nchallenging in poor lighting, adverse weather, and unbalanced data sets. This\npaper presents a multimodal detection framework that integrates RGB and thermal\ninfrared imaging with a fine-tuned YOLOv8 model. Training leveraged KITTI,\nBDD100K, and Teledyne FLIR datasets, with class re-weighting and light\naugmentations to improve minority-class performance and robustness, experiments\nshow that 640-pixel resolution and partial backbone freezing optimise accuracy\nand efficiency, while class-weighted losses enhance recall for rare VRUs.\nResults highlight that thermal models achieve the highest precision, and\nRGB-to-thermal augmentation boosts recall, demonstrating the potential of\nmultimodal detection to improve VRU safety at intersections.",
        "url": "http://arxiv.org/abs/2509.06333v1",
        "published_date": "2025-09-08T04:39:07+00:00",
        "updated_date": "2025-09-08T04:39:07+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Penelope Brown",
            "Julie Stephany Berrio Perez",
            "Mao Shan",
            "Stewart Worrall"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a multi-modal detection framework using RGB and thermal imaging to improve vulnerable road user detection in challenging conditions.",
        "tldr_zh": "本文介绍了一种使用RGB和热红外成像的多模态检测框架，以改善在挑战性条件下对易受伤路人的检测。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Video-based Generalized Category Discovery via Memory-Guided Consistency-Aware Contrastive Learning",
        "summary": "Generalized Category Discovery (GCD) is an emerging and challenging\nopen-world problem that has garnered increasing attention in recent years. Most\nexisting GCD methods focus on discovering categories in static images. However,\nrelying solely on static visual content is often insufficient to reliably\ndiscover novel categories. To bridge this gap, we extend the GCD problem to the\nvideo domain and introduce a new setting, termed Video-GCD. Thus, effectively\nintegrating multi-perspective information across time is crucial for accurate\nVideo-GCD. To tackle this challenge, we propose a novel Memory-guided\nConsistency-aware Contrastive Learning (MCCL) framework, which explicitly\ncaptures temporal-spatial cues and incorporates them into contrastive learning\nthrough a consistency-guided voting mechanism. MCCL consists of two core\ncomponents: Consistency-Aware Contrastive Learning(CACL) and Memory-Guided\nRepresentation Enhancement (MGRE). CACL exploits multiperspective temporal\nfeatures to estimate consistency scores between unlabeled instances, which are\nthen used to weight the contrastive loss accordingly. MGRE introduces a\ndual-level memory buffer that maintains both feature-level and logit-level\nrepresentations, providing global context to enhance intra-class compactness\nand inter-class separability. This in turn refines the consistency estimation\nin CACL, forming a mutually reinforcing feedback loop between representation\nlearning and consistency modeling. To facilitate a comprehensive evaluation, we\nconstruct a new and challenging Video-GCD benchmark, which includes action\nrecognition and bird classification video datasets. Extensive experiments\ndemonstrate that our method significantly outperforms competitive GCD\napproaches adapted from image-based settings, highlighting the importance of\ntemporal information for discovering novel categories in videos. The code will\nbe publicly available.",
        "url": "http://arxiv.org/abs/2509.06306v1",
        "published_date": "2025-09-08T03:12:57+00:00",
        "updated_date": "2025-09-08T03:12:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhang Jing",
            "Pu Nan",
            "Xie Yu Xiang",
            "Guo Yanming",
            "Lu Qianqi",
            "Zou Shiwei",
            "Yan Jie",
            "Chen Yan"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces a new approach, Memory-guided Consistency-aware Contrastive Learning, to tackle the Generalized Category Discovery problem in videos by integrating temporal-spatial cues for accurate category discovery.",
        "tldr_zh": "本文提出了一种新方法，即记忆引导的一致性感知对比学习，在视频中整合时间-空间线索以实现准确的类别发现。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AI-driven Remote Facial Skin Hydration and TEWL Assessment from Selfie Images: A Systematic Solution",
        "summary": "Skin health and disease resistance are closely linked to the skin barrier\nfunction, which protects against environmental factors and water loss. Two key\nphysiological indicators can quantitatively represent this barrier function:\nskin hydration (SH) and trans-epidermal water loss (TEWL). Measurement of SH\nand TEWL is valuable for the public to monitor skin conditions regularly,\ndiagnose dermatological issues, and personalize their skincare regimens.\nHowever, these measurements are not easily accessible to general users unless\nthey visit a dermatology clinic with specialized instruments. To tackle this\nproblem, we propose a systematic solution to estimate SH and TEWL from selfie\nfacial images remotely with smartphones. Our solution encompasses multiple\nstages, including SH/TEWL data collection, data preprocessing, and formulating\na novel Skin-Prior Adaptive Vision Transformer model for SH/TEWL regression.\nThrough experiments, we identified the annotation imbalance of the SH/TEWL data\nand proposed a symmetric-based contrastive regularization to reduce the model\nbias due to the imbalance effectively. This work is the first study to explore\nskin assessment from selfie facial images without physical measurements. It\nbridges the gap between computer vision and skin care research, enabling\nAI-driven accessible skin analysis for broader real-world applications.",
        "url": "http://arxiv.org/abs/2509.06282v1",
        "published_date": "2025-09-08T02:06:37+00:00",
        "updated_date": "2025-09-08T02:06:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Cecelia Soh",
            "Rizhao Cai",
            "Monalisha Paul",
            "Dennis Sng",
            "Alex Kot"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a system to assess skin hydration and water loss from selfie images using AI, bridging computer vision and skincare research.",
        "tldr_zh": "本文提出了一种使用人工智能从自拍图像评估皮肤水合和水分流失的系统，弥合了计算机视觉与护肤研究之间的差距。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes",
        "summary": "Understanding 3D spatial relationships remains a major limitation of current\nVision-Language Models (VLMs). Prior work has addressed this issue by creating\nspatial question-answering (QA) datasets based on single images or indoor\nvideos. However, real-world embodied AI agents such as robots and self-driving\ncars typically rely on ego-centric, multi-view observations. To this end, we\nintroduce Ego3D-Bench, a new benchmark designed to evaluate the spatial\nreasoning abilities of VLMs using ego-centric, multi-view outdoor data.\nEgo3D-Bench comprises over 8,600 QA pairs, created with significant involvement\nfrom human annotators to ensure quality and diversity. We benchmark 16 SOTA\nVLMs, including GPT-4o, Gemini1.5-Pro, InternVL3, and Qwen2.5-VL. Our results\nreveal a notable performance gap between human level scores and VLM\nperformance, highlighting that current VLMs still fall short of human level\nspatial understanding. To bridge this gap, we propose Ego3D-VLM, a\npost-training framework that enhances 3D spatial reasoning of VLMs. Ego3D-VLM\ngenerates cognitive map based on estimated global 3D coordinates, resulting in\n12% average improvement on multi-choice QA and 56% average improvement on\nabsolute distance estimation. Ego3D-VLM is modular and can be integrated with\nany existing VLM. Together, Ego3D-Bench and Ego3D-VLM offer valuable tools for\nadvancing toward human level spatial understanding in real-world, multi-view\nenvironments.",
        "url": "http://arxiv.org/abs/2509.06266v1",
        "published_date": "2025-09-08T01:08:41+00:00",
        "updated_date": "2025-09-08T01:08:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mohsen Gholami",
            "Ahmad Rezaei",
            "Zhou Weimin",
            "Yong Zhang",
            "Mohammad Akbari"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces Ego3D-Bench, a benchmark to evaluate spatial reasoning in Vision-Language Models using multi-view outdoor data. They propose Ego3D-VLM to enhance 3D spatial reasoning, showing significant improvements.",
        "tldr_zh": "本文介绍了Ego3D-Bench，一个用于评价视觉语言模型在多视角户外数据中的空间推理能力的基准。他们提出了Ego3D-VLM来增强3D空间推理，展示了显著的改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AI-Based Applied Innovation for Fracture Detection in X-rays Using Custom CNN and Transfer Learning Models",
        "summary": "Bone fractures present a major global health challenge, often resulting in\npain, reduced mobility, and productivity loss, particularly in low-resource\nsettings where access to expert radiology services is limited. Conventional\nimaging methods suffer from high costs, radiation exposure, and dependency on\nspecialized interpretation. To address this, we developed an AI-based solution\nfor automated fracture detection from X-ray images using a custom Convolutional\nNeural Network (CNN) and benchmarked it against transfer learning models\nincluding EfficientNetB0, MobileNetV2, and ResNet50. Training was conducted on\nthe publicly available FracAtlas dataset, comprising 4,083 anonymized\nmusculoskeletal radiographs. The custom CNN achieved 95.96% accuracy, 0.94\nprecision, 0.88 recall, and an F1-score of 0.91 on the FracAtlas dataset.\nAlthough transfer learning models (EfficientNetB0, MobileNetV2, ResNet50)\nperformed poorly in this specific setup, these results should be interpreted in\nlight of class imbalance and data set limitations. This work highlights the\npromise of lightweight CNNs for detecting fractures in X-rays and underscores\nthe importance of fair benchmarking, diverse datasets, and external validation\nfor clinical translation",
        "url": "http://arxiv.org/abs/2509.06228v1",
        "published_date": "2025-09-07T22:30:25+00:00",
        "updated_date": "2025-09-07T22:30:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Amna Hassan",
            "Ilsa Afzaal",
            "Nouman Muneeb",
            "Aneeqa Batool",
            "Hamail Noor"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper presents an AI-based solution for automated fracture detection in X-ray images using custom CNN and transfer learning models.",
        "tldr_zh": "本文提出了一种使用自定义CNN和迁移学习模型在X射线图像中自动检测骨折的AI解决方案。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Learning in ImaginationLand: Omnidirectional Policies through 3D Generative Models (OP-Gen)",
        "summary": "Recent 3D generative models, which are capable of generating full object\nshapes from just a few images, now open up new opportunities in robotics. In\nthis work, we show that 3D generative models can be used to augment a dataset\nfrom a single real-world demonstration, after which an omnidirectional policy\ncan be learned within this imagined dataset. We found that this enables a robot\nto perform a task when initialised from states very far from those observed\nduring the demonstration, including starting from the opposite side of the\nobject relative to the real-world demonstration, significantly reducing the\nnumber of demonstrations required for policy learning. Through several\nreal-world experiments across tasks such as grasping objects, opening a drawer,\nand placing trash into a bin, we study these omnidirectional policies by\ninvestigating the effect of various design choices on policy behaviour, and we\nshow superior performance to recent baselines which use alternative methods for\ndata augmentation.",
        "url": "http://arxiv.org/abs/2509.06191v1",
        "published_date": "2025-09-07T20:00:59+00:00",
        "updated_date": "2025-09-07T20:00:59+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yifei Ren",
            "Edward Johns"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper demonstrates using 3D generative models to augment datasets for training omnidirectional policies, showing improved performance in robotic tasks like grasping and manipulation.",
        "tldr_zh": "本文展示了使用3D生成模型来增强训练全方位策略的数据集，在机器人抓取和操作等任务中表现出更好的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual Representation Learning",
        "summary": "Video Scene Graph Generation (VidSGG) aims to represent dynamic visual\ncontent by detecting objects and modeling their temporal interactions as\nstructured graphs. Prior studies typically target either coarse-grained\nbox-level or fine-grained panoptic pixel-level VidSGG, often requiring\ntask-specific architectures and multi-stage training pipelines. In this paper,\nwe present UNO (UNified Object-centric VidSGG), a single-stage, unified\nframework that jointly addresses both tasks within an end-to-end architecture.\nUNO is designed to minimize task-specific modifications and maximize parameter\nsharing, enabling generalization across different levels of visual granularity.\nThe core of UNO is an extended slot attention mechanism that decomposes visual\nfeatures into object and relation slots. To ensure robust temporal modeling, we\nintroduce object temporal consistency learning, which enforces consistent\nobject representations across frames without relying on explicit tracking\nmodules. Additionally, a dynamic triplet prediction module links relation slots\nto corresponding object pairs, capturing evolving interactions over time. We\nevaluate UNO on standard box-level and pixel-level VidSGG benchmarks. Results\ndemonstrate that UNO not only achieves competitive performance across both\ntasks but also offers improved efficiency through a unified, object-centric\ndesign.",
        "url": "http://arxiv.org/abs/2509.06165v1",
        "published_date": "2025-09-07T18:30:41+00:00",
        "updated_date": "2025-09-07T18:30:41+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Huy Le",
            "Nhat Chung",
            "Tung Kieu",
            "Jingkang Yang",
            "Ngan Le"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a unified framework for one-stage video scene graph generation that addresses both coarse-grained and fine-grained tasks with a focus on object-centric visual representation learning.",
        "tldr_zh": "本文提出了一个统一的框架，用于单阶段视频场景图生成，旨在通过以对象为中心的视觉表示学习来解决粗粒度和细粒度任务。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A New Hybrid Model of Generative Adversarial Network and You Only Look Once Algorithm for Automatic License-Plate Recognition",
        "summary": "Automatic License-Plate Recognition (ALPR) plays a pivotal role in\nIntelligent Transportation Systems (ITS) as a fundamental element of Smart\nCities. However, due to its high variability, ALPR faces challenging issues\nmore efficiently addressed by deep learning techniques. In this paper, a\nselective Generative Adversarial Network (GAN) is proposed for deblurring in\nthe preprocessing step, coupled with the state-of-the-art You-Only-Look-Once\n(YOLO)v5 object detection architectures for License-Plate Detection (LPD), and\nthe integrated Character Segmentation (CS) and Character Recognition (CR)\nsteps. The selective preprocessing bypasses unnecessary and sometimes\ncounter-productive input manipulations, while YOLOv5 LPD/CS+CR delivers high\naccuracy and low computing cost. As a result, YOLOv5 achieves a detection time\nof 0.026 seconds for both LP and CR detection stages, facilitating real-time\napplications with exceptionally rapid responsiveness. Moreover, the proposed\nmodel achieves accuracy rates of 95\\% and 97\\% in the LPD and CR detection\nphases, respectively. Furthermore, the inclusion of the Deblur-GAN\npre-processor significantly improves detection accuracy by nearly 40\\%,\nespecially when encountering blurred License Plates (LPs).To train and test the\nlearning components, we generated and publicly released our blur and ALPR\ndatasets (using Iranian license plates as a use-case), which are more\nrepresentative of close-to-real-life ad-hoc situations. The findings\ndemonstrate that employing the state-of-the-art YOLO model results in excellent\noverall precision and detection time, making it well-suited for portable\napplications. Additionally, integrating the Deblur-GAN model as a preliminary\nprocessing step enhances the overall effectiveness of our comprehensive model,\nparticularly when confronted with blurred scenes captured by the camera as\ninput.",
        "url": "http://arxiv.org/abs/2509.06868v1",
        "published_date": "2025-09-08T16:34:54+00:00",
        "updated_date": "2025-09-08T16:34:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Behnoud Shafiezadeh",
            "Amir Mashmool",
            "Farshad Eshghi",
            "Manoochehr Kelarestaghi"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a hybrid model combining GAN and YOLO for license plate recognition, achieving high accuracy and fast detection times.",
        "tldr_zh": "该论文介绍了一种结合GAN和YOLO的混合模型，用于车牌识别，实现了高准确度和快速检测时间。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "MRI-Based Brain Tumor Detection through an Explainable EfficientNetV2 and MLP-Mixer-Attention Architecture",
        "summary": "Brain tumors are serious health problems that require early diagnosis due to\ntheir high mortality rates. Diagnosing tumors by examining Magnetic Resonance\nImaging (MRI) images is a process that requires expertise and is prone to\nerror. Therefore, the need for automated diagnosis systems is increasing day by\nday. In this context, a robust and explainable Deep Learning (DL) model for the\nclassification of brain tumors is proposed. In this study, a publicly available\nFigshare dataset containing 3,064 T1-weighted contrast-enhanced brain MRI\nimages of three tumor types was used. First, the classification performance of\nnine well-known CNN architectures was evaluated to determine the most effective\nbackbone. Among these, EfficientNetV2 demonstrated the best performance and was\nselected as the backbone for further development. Subsequently, an\nattention-based MLP-Mixer architecture was integrated into EfficientNetV2 to\nenhance its classification capability. The performance of the final model was\ncomprehensively compared with basic CNNs and the methods in the literature.\nAdditionally, Grad-CAM visualization was used to interpret and validate the\ndecision-making process of the proposed model. The proposed model's performance\nwas evaluated using the five-fold cross-validation method. The proposed model\ndemonstrated superior performance with 99.50% accuracy, 99.47% precision,\n99.52% recall and 99.49% F1 score. The results obtained show that the model\noutperforms the studies in the literature. Moreover, Grad-CAM visualizations\ndemonstrate that the model effectively focuses on relevant regions of MRI\nimages, thus improving interpretability and clinical reliability. A robust deep\nlearning model for clinical decision support systems has been obtained by\ncombining EfficientNetV2 and attention-based MLP-Mixer, providing high accuracy\nand interpretability in brain tumor classification.",
        "url": "http://arxiv.org/abs/2509.06713v1",
        "published_date": "2025-09-08T14:08:21+00:00",
        "updated_date": "2025-09-08T14:08:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mustafa Yurdakul",
            "Şakir Taşdemir"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "A deep learning model combining EfficientNetV2 and attention-based MLP-Mixer architecture for brain tumor classification achieved high accuracy and interpretability.",
        "tldr_zh": "将EfficientNetV2和基于注意力的MLP-Mixer结构结合的深度学习模型，实现了对脑瘤分类的高准确性和解释性。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "IGAff: Benchmarking Adversarial Iterative and Genetic Affine Algorithms on Deep Neural Networks",
        "summary": "Deep neural networks currently dominate many fields of the artificial\nintelligence landscape, achieving state-of-the-art results on numerous tasks\nwhile remaining hard to understand and exhibiting surprising weaknesses. An\nactive area of research focuses on adversarial attacks, which aim to generate\ninputs that uncover these weaknesses. However, this proves challenging,\nespecially in the black-box scenario where model details are inaccessible. This\npaper explores in detail the impact of such adversarial algorithms on\nResNet-18, DenseNet-121, Swin Transformer V2, and Vision Transformer network\narchitectures. Leveraging the Tiny ImageNet, Caltech-256, and Food-101\ndatasets, we benchmark two novel black-box iterative adversarial algorithms\nbased on affine transformations and genetic algorithms: 1) Affine\nTransformation Attack (ATA), an iterative algorithm maximizing our attack score\nfunction using random affine transformations, and 2) Affine Genetic Attack\n(AGA), a genetic algorithm that involves random noise and affine\ntransformations. We evaluate the performance of the models in the algorithm\nparameter variation, data augmentation, and global and targeted attack\nconfigurations. We also compare our algorithms with two black-box adversarial\nalgorithms, Pixle and Square Attack. Our experiments yield better results on\nthe image classification task than similar methods in the literature, achieving\nan accuracy improvement of up to 8.82%. We provide noteworthy insights into\nsuccessful adversarial defenses and attacks at both global and targeted levels,\nand demonstrate adversarial robustness through algorithm parameter variation.",
        "url": "http://arxiv.org/abs/2509.06459v1",
        "published_date": "2025-09-08T09:12:27+00:00",
        "updated_date": "2025-09-08T09:12:27+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Sebastian-Vasile Echim",
            "Andrei-Alexandru Preda",
            "Dumitru-Clementin Cercel",
            "Florin Pop"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper explores adversarial attacks on deep neural networks using two novel black-box algorithms based on affine transformations and genetic algorithms.",
        "tldr_zh": "本文使用基于仿射变换和遗传算法的两种新型黑盒算法，探讨了对深度神经网络的对抗攻击。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "Perception-oriented Bidirectional Attention Network for Image Super-resolution Quality Assessment",
        "summary": "Many super-resolution (SR) algorithms have been proposed to increase image\nresolution. However, full-reference (FR) image quality assessment (IQA) metrics\nfor comparing and evaluating different SR algorithms are limited. In this work,\nwe propose the Perception-oriented Bidirectional Attention Network (PBAN) for\nimage SR FR-IQA, which is composed of three modules: an image encoder module, a\nperception-oriented bidirectional attention (PBA) module, and a quality\nprediction module. First, we encode the input images for feature\nrepresentations. Inspired by the characteristics of the human visual system, we\nthen construct the perception-oriented PBA module. Specifically, different from\nexisting attention-based SR IQA methods, we conceive a Bidirectional Attention\nto bidirectionally construct visual attention to distortion, which is\nconsistent with the generation and evaluation processes of SR images. To\nfurther guide the quality assessment towards the perception of distorted\ninformation, we propose Grouped Multi-scale Deformable Convolution, enabling\nthe proposed method to adaptively perceive distortion. Moreover, we design\nSub-information Excitation Convolution to direct visual perception to both\nsub-pixel and sub-channel attention. Finally, the quality prediction module is\nexploited to integrate quality-aware features and regress quality scores.\nExtensive experiments demonstrate that our proposed PBAN outperforms\nstate-of-the-art quality assessment methods.",
        "url": "http://arxiv.org/abs/2509.06442v1",
        "published_date": "2025-09-08T08:39:45+00:00",
        "updated_date": "2025-09-08T08:39:45+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Yixiao Li",
            "Xiaoyuan Yang",
            "Guanghui Yue",
            "Jun Fu",
            "Qiuping Jiang",
            "Xu Jia",
            "Paul L. Rosin",
            "Hantao Liu",
            "Wei Zhou"
        ],
        "ai_categories": [
            "GAN",
            "Transformer"
        ],
        "tldr": "The paper introduces PBAN, a new network for assessing image super-resolution quality by focusing on human perception to outperform existing methods.",
        "tldr_zh": "本文介绍了PBAN，一个新的网络，通过关注人类感知来评估图像超分辨率质量，以优于现有方法。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "MRD-LiNet: A Novel Lightweight Hybrid CNN with Gradient-Guided Unlearning for Improved Drought Stress Identification",
        "summary": "Drought stress is a major threat to global crop productivity, making its\nearly and precise detection essential for sustainable agricultural management.\nTraditional approaches, though useful, are often time-consuming and\nlabor-intensive, which has motivated the adoption of deep learning methods. In\nrecent years, Convolutional Neural Network (CNN) and Vision Transformer\narchitectures have been widely explored for drought stress identification;\nhowever, these models generally rely on a large number of trainable parameters,\nrestricting their use in resource-limited and real-time agricultural settings.\nTo address this challenge, we propose a novel lightweight hybrid CNN framework\ninspired by ResNet, DenseNet, and MobileNet architectures. The framework\nachieves a remarkable 15-fold reduction in trainable parameters compared to\nconventional CNN and Vision Transformer models, while maintaining competitive\naccuracy. In addition, we introduce a machine unlearning mechanism based on a\ngradient norm-based influence function, which enables targeted removal of\nspecific training data influence, thereby improving model adaptability. The\nmethod was evaluated on an aerial image dataset of potato fields with\nexpert-annotated healthy and drought-stressed regions. Experimental results\nshow that our framework achieves high accuracy while substantially lowering\ncomputational costs. These findings highlight its potential as a practical,\nscalable, and adaptive solution for drought stress monitoring in precision\nagriculture, particularly under resource-constrained conditions.",
        "url": "http://arxiv.org/abs/2509.06367v1",
        "published_date": "2025-09-08T06:46:35+00:00",
        "updated_date": "2025-09-08T06:46:35+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Aswini Kumar Patra",
            "Lingaraj Sahoo"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a lightweight hybrid CNN framework with a machine unlearning mechanism for improving drought stress identification in agriculture.",
        "tldr_zh": "本文提出了一种轻量级混合CNN框架，配备了机器遗忘机制，用于改善农业中的干旱应激识别。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "O$^3$Afford: One-Shot 3D Object-to-Object Affordance Grounding for Generalizable Robotic Manipulation",
        "summary": "Grounding object affordance is fundamental to robotic manipulation as it\nestablishes the critical link between perception and action among interacting\nobjects. However, prior works predominantly focus on predicting single-object\naffordance, overlooking the fact that most real-world interactions involve\nrelationships between pairs of objects. In this work, we address the challenge\nof object-to-object affordance grounding under limited data contraints.\nInspired by recent advances in few-shot learning with 2D vision foundation\nmodels, we propose a novel one-shot 3D object-to-object affordance learning\napproach for robotic manipulation. Semantic features from vision foundation\nmodels combined with point cloud representation for geometric understanding\nenable our one-shot learning pipeline to generalize effectively to novel\nobjects and categories. We further integrate our 3D affordance representation\nwith large language models (LLMs) for robotics manipulation, significantly\nenhancing LLMs' capability to comprehend and reason about object interactions\nwhen generating task-specific constraint functions. Our experiments on 3D\nobject-to-object affordance grounding and robotic manipulation demonstrate that\nour O$^3$Afford significantly outperforms existing baselines in terms of both\naccuracy and generalization capability.",
        "url": "http://arxiv.org/abs/2509.06233v1",
        "published_date": "2025-09-07T22:45:06+00:00",
        "updated_date": "2025-09-07T22:45:06+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Tongxuan Tian",
            "Xuhui Kang",
            "Yen-Ling Kuo"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel approach for one-shot 3D object-to-object affordance learning for robotic manipulation, outperforming existing baselines in accuracy and generalization.",
        "tldr_zh": "该论文介绍了一种新颖的方法，用于机器人操作中的一次性3D对象到对象能力学习，优于现有基线模型的准确性和泛化能力。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning",
        "summary": "Dark humor in online memes poses unique challenges due to its reliance on\nimplicit, sensitive, and culturally contextual cues. To address the lack of\nresources and methods for detecting dark humor in multimodal content, we\nintroduce a novel dataset of 4,379 Reddit memes annotated for dark humor,\ntarget category (gender, mental health, violence, race, disability, and other),\nand a three-level intensity rating (mild, moderate, severe). Building on this\nresource, we propose a reasoning-augmented framework that first generates\nstructured explanations for each meme using a Large Vision-Language Model\n(VLM). Through a Role-Reversal Self-Loop, VLM adopts the author's perspective\nto iteratively refine its explanations, ensuring completeness and alignment. We\nthen extract textual features from both the OCR transcript and the self-refined\nreasoning via a text encoder, while visual features are obtained using a vision\ntransformer. A Tri-stream Cross-Reasoning Network (TCRNet) fuses these three\nstreams, text, image, and reasoning, via pairwise attention mechanisms,\nproducing a unified representation for classification. Experimental results\ndemonstrate that our approach outperforms strong baselines across three tasks:\ndark humor detection, target identification, and intensity prediction. The\ndataset, annotations, and code are released to facilitate further research in\nmultimodal humor understanding and content moderation. Code and Dataset are\navailable at:\nhttps://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning",
        "url": "http://arxiv.org/abs/2509.06771v1",
        "published_date": "2025-09-08T14:55:16+00:00",
        "updated_date": "2025-09-08T14:55:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sai Kartheek Reddy Kasu",
            "Mohammad Zia Ur Rehman",
            "Shahid Shafi Dar",
            "Rishi Bharat Junghare",
            "Dhanvin Sanjay Namboodiri",
            "Nagendra Kumar"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a novel dataset and framework for detecting dark humor in online memes using multimodal reasoning, outperforming strong baselines in three tasks.",
        "tldr_zh": "本文引入了一个新的数据集和框架，利用多模态推理检测在线表情包中的黑色幽默，在三项任务中表现优于强基线。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Evolving from Unknown to Known: Retentive Angular Representation Learning for Incremental Open Set Recognition",
        "summary": "Existing open set recognition (OSR) methods are typically designed for static\nscenarios, where models aim to classify known classes and identify unknown ones\nwithin fixed scopes. This deviates from the expectation that the model should\nincrementally identify newly emerging unknown classes from continuous data\nstreams and acquire corresponding knowledge. In such evolving scenarios, the\ndiscriminability of OSR decision boundaries is hard to maintain due to\nrestricted access to former training data, causing severe inter-class\nconfusion. To solve this problem, we propose retentive angular representation\nlearning (RARL) for incremental open set recognition (IOSR). In RARL, unknown\nrepresentations are encouraged to align around inactive prototypes within an\nangular space constructed under the equiangular tight frame, thereby mitigating\nexcessive representation drift during knowledge updates. Specifically, we adopt\na virtual-intrinsic interactive (VII) training strategy, which compacts known\nrepresentations by enforcing clear inter-class margins through\nboundary-proximal virtual classes. Furthermore, a stratified rectification\nstrategy is designed to refine decision boundaries, mitigating representation\nbias and feature space distortion caused by imbalances between old/new and\npositive/negative class samples. We conduct thorough evaluations on CIFAR100\nand TinyImageNet datasets and establish a new benchmark for IOSR. Experimental\nresults across various task setups demonstrate that the proposed method\nachieves state-of-the-art performance.",
        "url": "http://arxiv.org/abs/2509.06570v1",
        "published_date": "2025-09-08T11:35:12+00:00",
        "updated_date": "2025-09-08T11:35:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Runqing Yang",
            "Yimin Fu",
            "Changyuan Wu",
            "Zhunga Liu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a method called retentive angular representation learning for incremental open set recognition, achieving state-of-the-art performance on CIFAR100 and TinyImageNet datasets.",
        "tldr_zh": "本文引入了一种称为保留角表示学习的方法，用于增量式开放集识别，在CIFAR100和TinyImageNet数据集上取得了最先进的性能。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "AI-based response assessment and prediction in longitudinal imaging for brain metastases treated with stereotactic radiosurgery",
        "summary": "Brain Metastases (BM) are a large contributor to mortality of patients with\ncancer. They are treated with Stereotactic Radiosurgery (SRS) and monitored\nwith Magnetic Resonance Imaging (MRI) at regular follow-up intervals according\nto treatment guidelines. Analyzing and quantifying this longitudinal imaging\nrepresents an intractable workload for clinicians. As a result, follow-up\nimages are not annotated and merely assessed by observation. Response to\ntreatment in longitudinal imaging is being studied, to better understand growth\ntrajectories and ultimately predict treatment success or toxicity as early as\npossible. In this study, we implement an automated pipeline to curate a large\nlongitudinal dataset of SRS treatment data, resulting in a cohort of 896 BMs in\n177 patients who were monitored for >360 days at approximately two-month\nintervals at Lausanne University Hospital (CHUV). We use a data-driven\nclustering to identify characteristic trajectories. In addition, we predict 12\nmonths lesion-level response using classical as well as graph machine learning\nGraph Machine Learning (GML). Clustering revealed 5 dominant growth\ntrajectories with distinct final response categories. Response prediction\nreaches up to 0.90 AUC (CI95%=0.88-0.92) using only pre-treatment and first\nfollow-up MRI with gradient boosting. Similarly, robust predictive performance\nof up to 0.88 AUC (CI95%=0.86-0.90) was obtained using GML, offering more\nflexibility with a single model for multiple input time-points configurations.\nOur results suggest potential automation and increased precision for the\ncomprehensive assessment and prediction of BM response to SRS in longitudinal\nMRI. The proposed pipeline facilitates scalable data curation for the\ninvestigation of BM growth patterns, and lays the foundation for clinical\ndecision support systems aiming at optimizing personalized care.",
        "url": "http://arxiv.org/abs/2509.06396v1",
        "published_date": "2025-09-08T07:29:45+00:00",
        "updated_date": "2025-09-08T07:29:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lorenz Achim Kuhn",
            "Daniel Abler",
            "Jonas Richiardi",
            "Andreas F. Hottinger",
            "Luis Schiappacasse",
            "Vincent Dunet",
            "Adrien Depeursinge",
            "Vincent Andrearczyk"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces an automated pipeline to analyze longitudinal brain metastases imaging data to predict treatment success using classical and graph machine learning techniques.",
        "tldr_zh": "本文介绍了一个自动化流程，用于分析长期的脑转移成像数据，利用经典和图机器学习技术预测治疗成功率。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "H$_{2}$OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers",
        "summary": "Transformers have been successfully applied in the field of video-based 3D\nhuman pose estimation. However, the high computational costs of these video\npose transformers (VPTs) make them impractical on resource-constrained devices.\nIn this paper, we present a hierarchical plug-and-play pruning-and-recovering\nframework, called Hierarchical Hourglass Tokenizer (H$_{2}$OT), for efficient\ntransformer-based 3D human pose estimation from videos. H$_{2}$OT begins with\nprogressively pruning pose tokens of redundant frames and ends with recovering\nfull-length sequences, resulting in a few pose tokens in the intermediate\ntransformer blocks and thus improving the model efficiency. It works with two\nkey modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module\n(TRM). TPM dynamically selects a few representative tokens to eliminate the\nredundancy of video frames, while TRM restores the detailed spatio-temporal\ninformation based on the selected tokens, thereby expanding the network output\nto the original full-length temporal resolution for fast inference. Our method\nis general-purpose: it can be easily incorporated into common VPT models on\nboth seq2seq and seq2frame pipelines while effectively accommodating different\ntoken pruning and recovery strategies. In addition, our H$_{2}$OT reveals that\nmaintaining the full pose sequence is unnecessary, and a few pose tokens of\nrepresentative frames can achieve both high efficiency and estimation accuracy.\nExtensive experiments on multiple benchmark datasets demonstrate both the\neffectiveness and efficiency of the proposed method. Code and models are\navailable at https://github.com/NationalGAILab/HoT.",
        "url": "http://arxiv.org/abs/2509.06956v1",
        "published_date": "2025-09-08T17:59:59+00:00",
        "updated_date": "2025-09-08T17:59:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Wenhao Li",
            "Mengyuan Liu",
            "Hong Liu",
            "Pichao Wang",
            "Shijian Lu",
            "Nicu Sebe"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel framework, H$_{2}$OT, for efficient transformer-based 3D human pose estimation from videos by selectively pruning and recovering pose tokens, showcasing both effectiveness and efficiency.",
        "tldr_zh": "这篇论文介绍了一种新颖的框架，H$_{2}$OT，通过选择性修剪和恢复姿势标记，实现了视频的高效基于Transformer的3D人体姿势估计，展示了有效性和效率。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "FoMo4Wheat: Toward reliable crop vision foundation models with globally curated data",
        "summary": "Vision-driven field monitoring is central to digital agriculture, yet models\nbuilt on general-domain pretrained backbones often fail to generalize across\ntasks, owing to the interaction of fine, variable canopy structures with\nfluctuating field conditions. We present FoMo4Wheat, one of the first\ncrop-domain vision foundation model pretrained with self-supervision on\nImAg4Wheat, the largest and most diverse wheat image dataset to date (2.5\nmillion high-resolution images collected over a decade at 30 global sites,\nspanning >2,000 genotypes and >500 environmental conditions). This\nwheat-specific pretraining yields representations that are robust for wheat and\ntransferable to other crops and weeds. Across ten in-field vision tasks at\ncanopy and organ levels, FoMo4Wheat models consistently outperform\nstate-of-the-art models pretrained on general-domain dataset. These results\ndemonstrate the value of crop-specific foundation models for reliable in-field\nperception and chart a path toward a universal crop foundation model with\ncross-species and cross-task capabilities. FoMo4Wheat models and the ImAg4Wheat\ndataset are publicly available online: https://github.com/PheniX-Lab/FoMo4Wheat\nand https://huggingface.co/PheniX-Lab/FoMo4Wheat. The demonstration website is:\nhttps://fomo4wheat.phenix-lab.com/.",
        "url": "http://arxiv.org/abs/2509.06907v1",
        "published_date": "2025-09-08T17:23:28+00:00",
        "updated_date": "2025-09-08T17:23:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bing Han",
            "Chen Zhu",
            "Dong Han",
            "Rui Yu",
            "Songliang Cao",
            "Jianhui Wu",
            "Scott Chapman",
            "Zijian Wang",
            "Bangyou Zheng",
            "Wei Guo",
            "Marie Weiss",
            "Benoit de Solan",
            "Andreas Hund",
            "Lukas Roth",
            "Kirchgessner Norbert",
            "Andrea Visioni",
            "Yufeng Ge",
            "Wenjuan Li",
            "Alexis Comar",
            "Dong Jiang",
            "Dejun Han",
            "Fred Baret",
            "Yanfeng Ding",
            "Hao Lu",
            "Shouyang Liu"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC"
        ],
        "tldr": "FoMo4Wheat introduces a crop-specific vision foundation model pretrained on a large wheat image dataset, outperforming general-domain pretrained models on in-field vision tasks.",
        "tldr_zh": "FoMo4Wheat引入了一种专门针对作物的视觉基础模型，预训练于大规模小麦图像数据集，表现优于通用领域预训练模型在田间视觉任务上。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Intraoperative 2D/3D Registration via Spherical Similarity Learning and Inference-Time Differentiable Levenberg-Marquardt Optimization",
        "summary": "Intraoperative 2D/3D registration aligns preoperative 3D volumes with\nreal-time 2D radiographs, enabling accurate localization of instruments and\nimplants. A recent fully differentiable similarity learning framework\napproximates geodesic distances on SE(3), expanding the capture range of\nregistration and mitigating the effects of substantial disturbances, but\nexisting Euclidean approximations distort manifold structure and slow\nconvergence. To address these limitations, we explore similarity learning in\nnon-Euclidean spherical feature spaces to better capture and fit complex\nmanifold structure. We extract feature embeddings using a CNN-Transformer\nencoder, project them into spherical space, and approximate their geodesic\ndistances with Riemannian distances in the bi-invariant SO(4) space. This\nenables a more expressive and geometrically consistent deep similarity metric,\nenhancing the ability to distinguish subtle pose differences. During inference,\nwe replace gradient descent with fully differentiable Levenberg-Marquardt\noptimization to accelerate convergence. Experiments on real and synthetic\ndatasets show superior accuracy in both patient-specific and patient-agnostic\nscenarios.",
        "url": "http://arxiv.org/abs/2509.06890v1",
        "published_date": "2025-09-08T17:10:43+00:00",
        "updated_date": "2025-09-08T17:10:43+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Minheng Chen",
            "Youyong Kong"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel method for improving the accuracy of aligning 3D volumes with real-time 2D radiographs during surgery, using non-Euclidean spherical feature spaces and Levenberg-Marquardt optimization.",
        "tldr_zh": "本文提出了一种新的方法，通过非欧几里德球面特征空间和Levenberg-Marquardt优化，改善手术过程中将3D体积与实时2D射线图像对齐的准确性。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Barlow-Swin: Toward a novel siamese-based segmentation architecture using Swin-Transformers",
        "summary": "Medical image segmentation is a critical task in clinical workflows,\nparticularly for the detection and delineation of pathological regions. While\nconvolutional architectures like U-Net have become standard for such tasks,\ntheir limited receptive field restricts global context modeling. Recent efforts\nintegrating transformers have addressed this, but often result in deep,\ncomputationally expensive models unsuitable for real-time use. In this work, we\npresent a novel end-to-end lightweight architecture designed specifically for\nreal-time binary medical image segmentation. Our model combines a Swin\nTransformer-like encoder with a U-Net-like decoder, connected via skip pathways\nto preserve spatial detail while capturing contextual information. Unlike\nexisting designs such as Swin Transformer or U-Net, our architecture is\nsignificantly shallower and competitively efficient. To improve the encoder's\nability to learn meaningful features without relying on large amounts of\nlabeled data, we first train it using Barlow Twins, a self-supervised learning\nmethod that helps the model focus on important patterns by reducing unnecessary\nrepetition in the learned features. After this pretraining, we fine-tune the\nentire model for our specific task. Experiments on benchmark binary\nsegmentation tasks demonstrate that our model achieves competitive accuracy\nwith substantially reduced parameter count and faster inference, positioning it\nas a practical alternative for deployment in real-time and resource-limited\nclinical environments. The code for our method is available at Github\nrepository: https://github.com/mkianih/Barlow-Swin.",
        "url": "http://arxiv.org/abs/2509.06885v1",
        "published_date": "2025-09-08T17:05:53+00:00",
        "updated_date": "2025-09-08T17:05:53+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Morteza Kiani Haftlang",
            "Mohammadhossein Malmir",
            "Foroutan Parand",
            "Umberto Michelucci",
            "Safouane El Ghazouali"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "Barlow-Swin is a novel siamese-based segmentation architecture that combines a Swin Transformer-like encoder with a U-Net-like decoder for real-time binary medical image segmentation.",
        "tldr_zh": "Barlow-Swin是一种新颖的siamese-based分割架构，将Swin Transformer-like编码器与U-Net-like解码器结合，用于实时二值医学图像分割。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Matching Shapes Under Different Topologies: A Topology-Adaptive Deformation Guided Approach",
        "summary": "Non-rigid 3D mesh matching is a critical step in computer vision and computer\ngraphics pipelines. We tackle matching meshes that contain topological\nartefacts which can break the assumption made by current approaches. While\nFunctional Maps assume the deformation induced by the ground truth\ncorrespondences to be near-isometric, ARAP-like deformation-guided approaches\nassume the latter to be ARAP. Neither assumption holds in certain topological\nconfigurations of the input shapes. We are motivated by real-world scenarios\nsuch as per-frame multi-view reconstructions, often suffering from topological\nartefacts. To this end, we propose a topology-adaptive deformation model\nallowing changes in shape topology to align shape pairs under ARAP and\nbijective association constraints. Using this model, we jointly optimise for a\ntemplate mesh with adequate topology and for its alignment with the shapes to\nbe matched to extract correspondences. We show that, while not relying on any\ndata-driven prior, our approach applies to highly non-isometric shapes and\nshapes with topological artefacts, including noisy per-frame multi-view\nreconstructions, even outperforming methods trained on large datasets in 3D\nalignment quality.",
        "url": "http://arxiv.org/abs/2509.06862v1",
        "published_date": "2025-09-08T16:29:44+00:00",
        "updated_date": "2025-09-08T16:29:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aymen Merrouche",
            "Stefanie Wuhrer",
            "Edmond Boyer"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a topology-adaptive deformation model for matching non-rigid 3D meshes with topological artefacts, outperforming methods trained on large datasets in 3D alignment quality.",
        "tldr_zh": "本文提出了一种拓扑自适应变形模型，用于匹配具有拓扑缺陷的非刚性3D网格，在3D对齐质量上优于基于大型数据集训练的方法。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Video-Based MPAA Rating Prediction: An Attention-Driven Hybrid Architecture Using Contrastive Learning",
        "summary": "The rapid growth of visual content consumption across platforms necessitates\nautomated video classification for age-suitability standards like the MPAA\nrating system (G, PG, PG-13, R). Traditional methods struggle with large\nlabeled data requirements, poor generalization, and inefficient feature\nlearning. To address these challenges, we employ contrastive learning for\nimproved discrimination and adaptability, exploring three frameworks: Instance\nDiscrimination, Contextual Contrastive Learning, and Multi-View Contrastive\nLearning. Our hybrid architecture integrates an LRCN (CNN+LSTM) backbone with a\nBahdanau attention mechanism, achieving state-of-the-art performance in the\nContextual Contrastive Learning framework, with 88% accuracy and an F1 score of\n0.8815. By combining CNNs for spatial features, LSTMs for temporal modeling,\nand attention mechanisms for dynamic frame prioritization, the model excels in\nfine-grained borderline distinctions, such as differentiating PG-13 and R-rated\ncontent. We evaluate the model's performance across various contrastive loss\nfunctions, including NT-Xent, NT-logistic, and Margin Triplet, demonstrating\nthe robustness of our proposed architecture. To ensure practical application,\nthe model is deployed as a web application for real-time MPAA rating\nclassification, offering an efficient solution for automated content compliance\nacross streaming platforms.",
        "url": "http://arxiv.org/abs/2509.06826v1",
        "published_date": "2025-09-08T16:01:02+00:00",
        "updated_date": "2025-09-08T16:01:02+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Dipta Neogi",
            "Nourash Azmine Chowdhury",
            "Muhammad Rafsan Kabir",
            "Mohammad Ashrafuzzaman Khan"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel hybrid architecture using contrastive learning for predicting MPAA ratings of videos with high accuracy, deployed as a real-time web application.",
        "tldr_zh": "本文介绍了一种新颖的混合架构，利用对比学习来预测视频的MPAA评级，准确率高，并部署为实时网络应用。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "P3-SAM: Native 3D Part Segmentation",
        "summary": "Segmenting 3D assets into their constituent parts is crucial for enhancing 3D\nunderstanding, facilitating model reuse, and supporting various applications\nsuch as part generation. However, current methods face limitations such as poor\nrobustness when dealing with complex objects and cannot fully automate the\nprocess. In this paper, we propose a native 3D point-promptable part\nsegmentation model termed P3-SAM, designed to fully automate the segmentation\nof any 3D objects into components. Inspired by SAM, P3-SAM consists of a\nfeature extractor, multiple segmentation heads, and an IoU predictor, enabling\ninteractive segmentation for users. We also propose an algorithm to\nautomatically select and merge masks predicted by our model for part instance\nsegmentation. Our model is trained on a newly built dataset containing nearly\n3.7 million models with reasonable segmentation labels. Comparisons show that\nour method achieves precise segmentation results and strong robustness on any\ncomplex objects, attaining state-of-the-art performance. Our code will be\nreleased soon.",
        "url": "http://arxiv.org/abs/2509.06784v1",
        "published_date": "2025-09-08T15:12:17+00:00",
        "updated_date": "2025-09-08T15:12:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Changfeng Ma",
            "Yang Li",
            "Xinhao Yan",
            "Jiachen Xu",
            "Yunhan Yang",
            "Chunshi Wang",
            "Zibo Zhao",
            "Yanwen Guo",
            "Zhuo Chen",
            "Chunchao Guo"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a model, P3-SAM, for native 3D part segmentation, achieving precise results and strong robustness on complex objects.",
        "tldr_zh": "本文介绍了一种用于原生3D部件分割的模型P3-SAM，在复杂对象上实现了精确的结果和强大的稳健性。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.0
    },
    {
        "title": "Pothole Detection and Recognition based on Transfer Learning",
        "summary": "With the rapid development of computer vision and machine learning, automated\nmethods for pothole detection and recognition based on image and video data\nhave received significant attention. It is of great significance for social\ndevelopment to conduct an in-depth analysis of road images through feature\nextraction, thereby achieving automatic identification of the pothole condition\nin new images. Consequently, this is the main issue addressed in this study.\nBased on preprocessing techniques such as standardization, normalization, and\ndata augmentation applied to the collected raw dataset, we continuously\nimproved the network model based on experimental results. Ultimately, we\nconstructed a deep learning feature extraction network\nResNet50-EfficientNet-RegNet model based on transfer learning. This model\nexhibits high classification accuracy and computational efficiency. In terms of\nmodel evaluation, this study employed a comparative evaluation approach by\ncomparing the performance of the proposed transfer learning model with other\nmodels, including Random Forest, MLP, SVM, and LightGBM. The comparison\nanalysis was conducted based on metrics such as Accuracy, Recall, Precision,\nF1-score, and FPS, to assess the classification performance of the transfer\nlearning model proposed in this paper. The results demonstrate that our model\nexhibits high performance in terms of recognition speed and accuracy,\nsurpassing the performance of other models. Through careful parameter selection\nand model optimization, our transfer learning model achieved a classification\naccuracy of 97.78% (88/90) on the initial set of 90 test samples and 98.89%\n(890/900) on the expanded test set.",
        "url": "http://arxiv.org/abs/2509.06750v1",
        "published_date": "2025-09-08T14:40:16+00:00",
        "updated_date": "2025-09-08T14:40:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mang Hu",
            "Qianqian Xia"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper presents a pothole detection and recognition system using transfer learning with high accuracy and efficiency.",
        "tldr_zh": "本文提出了一种使用迁移学习进行路坑检测和识别的系统，具有高准确性和效率。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Towards In-Air Ultrasonic QR Codes: Deep Learning for Classification of Passive Reflector Constellations",
        "summary": "In environments where visual sensors falter, in-air sonar provides a reliable\nalternative for autonomous systems. While previous research has successfully\nclassified individual acoustic landmarks, this paper takes a step towards\nincreasing information capacity by introducing reflector constellations as\nencoded tags. Our primary contribution is a multi-label Convolutional Neural\nNetwork (CNN) designed to simultaneously identify multiple, closely spaced\nreflectors from a single in-air 3D sonar measurement. Our initial findings on a\nsmall dataset confirm the feasibility of this approach, validating the ability\nto decode these complex acoustic patterns. Secondly, we investigated using\nadaptive beamforming with null-steering to isolate individual reflectors for\nsingle-label classification. Finally, we discuss the experimental results and\nlimitations, offering key insights and future directions for developing\nacoustic landmark systems with significantly increased information entropy and\ntheir accurate and robust detection and classification.",
        "url": "http://arxiv.org/abs/2509.06615v1",
        "published_date": "2025-09-08T12:33:19+00:00",
        "updated_date": "2025-09-08T12:33:19+00:00",
        "categories": [
            "eess.SP",
            "cs.CV"
        ],
        "authors": [
            "Wouter Jansen",
            "Jan Steckel"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper introduces a deep learning approach to classify passive reflector constellations using in-air ultrasonic sensing, increasing information capacity for autonomous systems.",
        "tldr_zh": "该论文介绍了使用空中超声波感知来分类Passive Reflector Constellations的深度学习方法，增加了自主系统的信息容量。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Tackling Device Data Distribution Real-time Shift via Prototype-based Parameter Editing",
        "summary": "The on-device real-time data distribution shift on devices challenges the\ngeneralization of lightweight on-device models. This critical issue is often\noverlooked in current research, which predominantly relies on data-intensive\nand computationally expensive fine-tuning approaches. To tackle this, we\nintroduce Persona, a novel personalized method using a prototype-based,\nbackpropagation-free parameter editing framework to enhance model\ngeneralization without post-deployment retraining. Persona employs a neural\nadapter in the cloud to generate a parameter editing matrix based on real-time\ndevice data. This matrix adeptly adapts on-device models to the prevailing data\ndistributions, efficiently clustering them into prototype models. The\nprototypes are dynamically refined via the parameter editing matrix,\nfacilitating efficient evolution. Furthermore, the integration of cross-layer\nknowledge transfer ensures consistent and context-aware multi-layer parameter\nchanges and prototype assignment. Extensive experiments on vision task and\nrecommendation task on multiple datasets confirm Persona's effectiveness and\ngenerality.",
        "url": "http://arxiv.org/abs/2509.06552v1",
        "published_date": "2025-09-08T11:06:50+00:00",
        "updated_date": "2025-09-08T11:06:50+00:00",
        "categories": [
            "cs.LG",
            "cs.CV",
            "cs.DC",
            "cs.IR"
        ],
        "authors": [
            "Zheqi Lv",
            "Wenqiao Zhang",
            "Kairui Fu",
            "Qi Tian",
            "Shengyu Zhang",
            "Jiajie Su",
            "Jingyuan Chen",
            "Kun Kuang",
            "Fei Wu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces Persona, a method to handle real-time data distribution shift on devices using a prototype-based parameter editing framework without retraining.",
        "tldr_zh": "本文介绍了Persona，一种使用基于原型的参数编辑框架处理设备上实时数据分布转移的方法，无需重新训练。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Signal-Based Malware Classification Using 1D CNNs",
        "summary": "Malware classification is a contemporary and ongoing challenge in\ncyber-security: modern obfuscation techniques are able to evade traditional\nstatic analysis, while dynamic analysis is too resource intensive to be\ndeployed at a large scale. One prominent line of research addresses these\nlimitations by converting malware binaries into 2D images by heuristically\nreshaping them into a 2D grid before resizing using Lanczos resampling. These\nimages can then be classified based on their textural information using\ncomputer vision approaches. While this approach can detect obfuscated malware\nmore effectively than static analysis, the process of converting files into 2D\nimages results in significant information loss due to both quantisation noise,\ncaused by rounding to integer pixel values, and the introduction of 2D\ndependencies which do not exist in the original data. This loss of signal\nlimits the classification performance of the downstream model. This work\naddresses these weaknesses by instead resizing the files into 1D signals which\navoids the need for heuristic reshaping, and additionally these signals do not\nsuffer from quantisation noise due to being stored in a floating-point format.\nIt is shown that existing 2D CNN architectures can be readily adapted to\nclassify these 1D signals for improved performance. Furthermore, a bespoke 1D\nconvolutional neural network, based on the ResNet architecture and\nsqueeze-and-excitation layers, was developed to classify these signals and\nevaluated on the MalNet dataset. It was found to achieve state-of-the-art\nperformance on binary, type, and family level classification with F1 scores of\n0.874, 0.503, and 0.507, respectively, paving the way for future models to\noperate on the proposed signal modality.",
        "url": "http://arxiv.org/abs/2509.06548v1",
        "published_date": "2025-09-08T11:03:48+00:00",
        "updated_date": "2025-09-08T11:03:48+00:00",
        "categories": [
            "cs.CR",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "I.2.6; K.6.5"
        ],
        "authors": [
            "Jack Wilkie",
            "Hanan Hindy",
            "Ivan Andonovic",
            "Christos Tachtatzis",
            "Robert Atkinson"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a method to classify malware using 1D signals instead of 2D images, achieving state-of-the-art performance.",
        "tldr_zh": "本文提出一种使用1D信号而不是2D图像对恶意软件进行分类的方法，实现了最先进的性能。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Benchmarking EfficientTAM on FMO datasets",
        "summary": "Fast and tiny object tracking remains a challenge in computer vision and in\nthis paper we first introduce a JSON metadata file associated with four open\nsource datasets of Fast Moving Objects (FMOs) image sequences. In addition, we\nextend the description of the FMOs datasets with additional ground truth\ninformation in JSON format (called FMOX) with object size information. Finally\nwe use our FMOX file to test a recently proposed foundational model for\ntracking (called EfficientTAM) showing that its performance compares well with\nthe pipelines originally taylored for these FMO datasets. Our comparison of\nthese state-of-the-art techniques on FMOX is provided with Trajectory\nIntersection of Union (TIoU) scores. The code and JSON is shared open source\nallowing FMOX to be accessible and usable for other machine learning pipelines\naiming to process FMO datasets.",
        "url": "http://arxiv.org/abs/2509.06536v1",
        "published_date": "2025-09-08T10:41:26+00:00",
        "updated_date": "2025-09-08T10:41:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Senem Aktas",
            "Charles Markham",
            "John McDonald",
            "Rozenn Dahyot"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a new dataset for Fast Moving Objects (FMOs) and evaluates the performance of a tracking model called EfficientTAM using this dataset.",
        "tldr_zh": "本文介绍了一个针对快速移动物体(FMOs)的新数据集，并使用该数据集评估了名为EfficientTAM的跟踪模型的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "A Statistical 3D Stomach Shape Model for Anatomical Analysis",
        "summary": "Realistic and parameterized 3D models of human anatomy have become invaluable\nin research, diagnostics, and surgical planning. However, the development of\ndetailed models for internal organs, such as the stomach, has been limited by\ndata availability and methodological challenges. In this paper, we propose a\nnovel pipeline for the generation of synthetic 3D stomach models, enabling the\ncreation of anatomically diverse morphologies informed by established studies\non stomach shape variability. Using this pipeline, we construct a dataset of\nsynthetic stomachs. Building on this dataset, we develop a 3D statistical shape\nmodel of the stomach, trained to capture natural anatomical variability in a\nlow-dimensional shape space. The model is further refined using CT meshes\nderived from publicly available datasets through a semi-supervised alignment\nprocess, enhancing its ability to generalize to unseen anatomical variations.\nWe evaluated the model on a held-out test set of real stomach CT scans,\ndemonstrating robust generalization and fit accuracy. We make the statistical\nshape model along with the synthetic dataset publicly available on GitLab:\nhttps://gitlab.com/Erez.Posner/stomach_pytorch to facilitate further research.\nThis work introduces the first statistical 3D shape model of the stomach, with\napplications ranging from surgical simulation and pre-operative planning to\nmedical education and computational modeling. By combining synthetic data\ngeneration, parametric modeling, and real-world validation, our approach\nrepresents a significant advancement in organ modeling and opens new\npossibilities for personalized healthcare solutions.",
        "url": "http://arxiv.org/abs/2509.06464v1",
        "published_date": "2025-09-08T09:23:11+00:00",
        "updated_date": "2025-09-08T09:23:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Erez Posner",
            "Ore Shtalrid",
            "Oded Erell",
            "Daniel Noy",
            "Moshe Bouhnik"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer",
            "Other"
        ],
        "tldr": "The paper introduces a statistical 3D shape model of the stomach, created using synthetic data generation and real-world validation, with applications in surgical simulation, pre-operative planning, medical education, and computational modeling.",
        "tldr_zh": "该论文介绍了一个统计学的三维胃形状模型，使用合成数据生成和实际验证，具有在外科模拟、术前计划、医学教育和计算建模方面的应用。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Cross3DReg: Towards a Large-scale Real-world Cross-source Point Cloud Registration Benchmark",
        "summary": "Cross-source point cloud registration, which aims to align point cloud data\nfrom different sensors, is a fundamental task in 3D vision. However, compared\nto the same-source point cloud registration, cross-source registration faces\ntwo core challenges: the lack of publicly available large-scale real-world\ndatasets for training the deep registration models, and the inherent\ndifferences in point clouds captured by multiple sensors. The diverse patterns\ninduced by the sensors pose great challenges in robust and accurate point cloud\nfeature extraction and matching, which negatively influence the registration\naccuracy. To advance research in this field, we construct Cross3DReg, the\ncurrently largest and real-world multi-modal cross-source point cloud\nregistration dataset, which is collected by a rotating mechanical lidar and a\nhybrid semi-solid-state lidar, respectively. Moreover, we design an\noverlap-based cross-source registration framework, which utilizes unaligned\nimages to predict the overlapping region between source and target point\nclouds, effectively filtering out redundant points in the irrelevant regions\nand significantly mitigating the interference caused by noise in\nnon-overlapping areas. Then, a visual-geometric attention guided matching\nmodule is proposed to enhance the consistency of cross-source point cloud\nfeatures by fusing image and geometric information to establish reliable\ncorrespondences and ultimately achieve accurate and robust registration.\nExtensive experiments show that our method achieves state-of-the-art\nregistration performance. Our framework reduces the relative rotation error\n(RRE) and relative translation error (RTE) by $63.2\\%$ and $40.2\\%$,\nrespectively, and improves the registration recall (RR) by $5.4\\%$, which\nvalidates its effectiveness in achieving accurate cross-source registration.",
        "url": "http://arxiv.org/abs/2509.06456v1",
        "published_date": "2025-09-08T09:01:13+00:00",
        "updated_date": "2025-09-08T09:01:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zongyi Xu",
            "Zhongpeng Lang",
            "Yilong Chen",
            "Shanshan Zhao",
            "Xiaoshui Huang",
            "Yifan Zuo",
            "Yan Zhang",
            "Qianni Zhang",
            "Xinbo Gao"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a new large-scale dataset and a registration framework to align point cloud data from different sensors, achieving state-of-the-art performance.",
        "tldr_zh": "该论文介绍了一个新的大规模数据集和一个用于对齐来自不同传感器的点云数据的注册框架，实现了最先进的性能。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "When Language Model Guides Vision: Grounding DINO for Cattle Muzzle Detection",
        "summary": "Muzzle patterns are among the most effective biometric traits for cattle\nidentification. Fast and accurate detection of the muzzle region as the region\nof interest is critical to automatic visual cattle identification.. Earlier\napproaches relied on manual detection, which is labor-intensive and\ninconsistent. Recently, automated methods using supervised models like YOLO\nhave become popular for muzzle detection. Although effective, these methods\nrequire extensive annotated datasets and tend to be trained data-dependent,\nlimiting their performance on new or unseen cattle. To address these\nlimitations, this study proposes a zero-shot muzzle detection framework based\non Grounding DINO, a vision-language model capable of detecting muzzles without\nany task-specific training or annotated data. This approach leverages natural\nlanguage prompts to guide detection, enabling scalable and flexible muzzle\nlocalization across diverse breeds and environments. Our model achieves a mean\nAverage Precision (mAP)@0.5 of 76.8\\%, demonstrating promising performance\nwithout requiring annotated data. To our knowledge, this is the first research\nto provide a real-world, industry-oriented, and annotation-free solution for\ncattle muzzle detection. The framework offers a practical alternative to\nsupervised methods, promising improved adaptability and ease of deployment in\nlivestock monitoring applications.",
        "url": "http://arxiv.org/abs/2509.06427v1",
        "published_date": "2025-09-08T08:21:34+00:00",
        "updated_date": "2025-09-08T08:21:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rabin Dulal",
            "Lihong Zheng",
            "Muhammad Ashad Kabir"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a zero-shot muzzle detection framework using a vision-language model for cattle identification without the need for annotated data.",
        "tldr_zh": "本文介绍了一种零样本口探测框架，利用视觉语言模型进行牛识别，无需注释数据。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "3DOF+Quantization: 3DGS quantization for large scenes with limited Degrees of Freedom",
        "summary": "3D Gaussian Splatting (3DGS) is a major breakthrough in 3D scene\nreconstruction. With a number of views of a given object or scene, the\nalgorithm trains a model composed of 3D gaussians, which enables the production\nof novel views from arbitrary points of view. This freedom of movement is\nreferred to as 6DoF for 6 degrees of freedom: a view is produced for any\nposition (3 degrees), orientation of camera (3 other degrees). On large scenes,\nthough, the input views are acquired from a limited zone in space, and the\nreconstruction is valuable for novel views from the same zone, even if the\nscene itself is almost unlimited in size. We refer to this particular case as\n3DoF+, meaning that the 3 degrees of freedom of camera position are limited to\nsmall offsets around the central position. Considering the problem of\ncoordinate quantization, the impact of position error on the projection error\nin pixels is studied. It is shown that the projection error is proportional to\nthe squared inverse distance of the point being projected. Consequently, a new\nquantization scheme based on spherical coordinates is proposed. Rate-distortion\nperformance of the proposed method are illustrated on the well-known Garden\nscene.",
        "url": "http://arxiv.org/abs/2509.06400v1",
        "published_date": "2025-09-08T07:44:28+00:00",
        "updated_date": "2025-09-08T07:44:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Matthieu Gendrin",
            "Stéphane Pateux",
            "Théo Ladune"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a new quantization scheme called 3DOF+Quantization for 3D scene reconstruction, addressing the impact of position error on projection error in pixels.",
        "tldr_zh": "本文介绍了一种新的量化方案，称为3DOF+Quantization，用于3D场景重建，解决了位置误差对像素投影误差的影响。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Quantitative Currency Evaluation in Low-Resource Settings through Pattern Analysis to Assist Visually Impaired Users",
        "summary": "Currency recognition systems often overlook usability and authenticity\nassessment, especially in low-resource environments where visually impaired\nusers and offline validation are common. While existing methods focus on\ndenomination classification, they typically ignore physical degradation and\nforgery, limiting their applicability in real-world conditions. This paper\npresents a unified framework for currency evaluation that integrates three\nmodules: denomination classification using lightweight CNN models, damage\nquantification through a novel Unified Currency Damage Index (UCDI), and\ncounterfeit detection using feature-based template matching. The dataset\nconsists of over 82,000 annotated images spanning clean, damaged, and\ncounterfeit notes. Our Custom_CNN model achieves high classification\nperformance with low parameter count. The UCDI metric provides a continuous\nusability score based on binary mask loss, chromatic distortion, and structural\nfeature loss. The counterfeit detection module demonstrates reliable\nidentification of forged notes across varied imaging conditions. The framework\nsupports real-time, on-device inference and addresses key deployment challenges\nin constrained environments. Results show that accurate, interpretable, and\ncompact solutions can support inclusive currency evaluation in practical\nsettings.",
        "url": "http://arxiv.org/abs/2509.06331v1",
        "published_date": "2025-09-08T04:24:31+00:00",
        "updated_date": "2025-09-08T04:24:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Md Sultanul Islam Ovi",
            "Mainul Hossain",
            "Md Badsha Biswas"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer",
            "Other"
        ],
        "tldr": "This paper proposes a framework for evaluating currency in low-resource settings for visually impaired users, integrating denomination classification, damage quantification, and counterfeit detection.",
        "tldr_zh": "本文提出了一个框架，用于评估低资源环境下的货币，以帮助视障用户，包括面额分类、损坏量化和假币检测。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Exploring Light-Weight Object Recognition for Real-Time Document Detection",
        "summary": "Object Recognition and Document Skew Estimation have come a long way in terms\nof performance and efficiency. New models follow one of two directions:\nimproving performance using larger models, and improving efficiency using\nsmaller models. However, real-time document detection and rectification is a\nniche that is largely unexplored by the literature, yet it remains a vital step\nfor automatic information retrieval from visual documents. In this work, we\nstrive towards an efficient document detection pipeline that is satisfactory in\nterms of Optical Character Recognition (OCR) retrieval and faster than other\navailable solutions. We adapt IWPOD-Net, a license plate detection network, and\ntrain it for detection on NBID, a synthetic ID card dataset. We experiment with\ndata augmentation and cross-dataset validation with MIDV (another synthetic ID\nand passport document dataset) to find the optimal scenario for the model.\nOther methods from both the Object Recognition and Skew Estimation\nstate-of-the-art are evaluated for comparison with our approach. We use each\nmethod to detect and rectify the document, which is then read by an OCR system.\nThe OCR output is then evaluated using a novel OCR quality metric based on the\nLevenshtein distance. Since the end goal is to improve automatic information\nretrieval, we use the overall OCR quality as a performance metric. We observe\nthat with a promising model, document rectification does not have to be perfect\nto attain state-of-the-art performance scores. We show that our model is\nsmaller and more efficient than current state-of-the-art solutions while\nretaining a competitive OCR quality metric. All code is available at\nhttps://github.com/BOVIFOCR/iwpod-doc-corners.git",
        "url": "http://arxiv.org/abs/2509.06246v1",
        "published_date": "2025-09-07T23:58:28+00:00",
        "updated_date": "2025-09-07T23:58:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lucas Wojcik",
            "Luiz Coelho",
            "Roger Granada",
            "David Menotti"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer",
            "OCR"
        ],
        "tldr": "The paper explores efficient document detection for automatic information retrieval using a novel model that is smaller and more efficient than current solutions.",
        "tldr_zh": "本文探讨了一种用于自动信息检索的高效文档检测方法，采用了一种比当前解决方案更小更高效的新型模型。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "WS$^2$: Weakly Supervised Segmentation using Before-After Supervision in Waste Sorting",
        "summary": "In industrial quality control, to visually recognize unwanted items within a\nmoving heterogeneous stream, human operators are often still indispensable.\nWaste-sorting stands as a significant example, where operators on multiple\nconveyor belts manually remove unwanted objects to select specific materials.\nTo automate this recognition problem, computer vision systems offer great\npotential in accurately identifying and segmenting unwanted items in such\nsettings. Unfortunately, considering the multitude and the variety of sorting\ntasks, fully supervised approaches are not a viable option to address this\nchallange, as they require extensive labeling efforts. Surprisingly, weakly\nsupervised alternatives that leverage the implicit supervision naturally\nprovided by the operator in his removal action are relatively unexplored. In\nthis paper, we define the concept of Before-After Supervision, illustrating how\nto train a segmentation network by leveraging only the visual differences\nbetween images acquired \\textit{before} and \\textit{after} the operator. To\npromote research in this direction, we introduce WS$^2$ (Weakly Supervised\nsegmentation for Waste-Sorting), the first multiview dataset consisting of more\nthan 11 000 high-resolution video frames captured on top of a conveyor belt,\nincluding \"before\" and \"after\" images. We also present a robust end-to-end\npipeline, used to benchmark several state-of-the-art weakly supervised\nsegmentation methods on WS$^2$.",
        "url": "http://arxiv.org/abs/2509.06485v1",
        "published_date": "2025-09-08T09:48:37+00:00",
        "updated_date": "2025-09-08T09:48:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Andrea Marelli",
            "Alberto Foresti",
            "Leonardo Pesce",
            "Giacomo Boracchi",
            "Mario Grosso"
        ],
        "ai_categories": [
            "Dataset",
            "Segmentation"
        ],
        "tldr": "The paper introduces WS$^2$, a weakly supervised segmentation approach for waste sorting using before-after supervision, along with a new dataset and benchmarking pipeline for evaluation.",
        "tldr_zh": "本文介绍了WS$^2$，一种利用前后监督进行废物分类的弱监督分割方法，以及用于评估的新数据集和基准管道。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Towards scalable organ level 3D plant segmentation: Bridging the data algorithm computing gap",
        "summary": "The precise characterization of plant morphology provides valuable insights\ninto plant environment interactions and genetic evolution. A key technology for\nextracting this information is 3D segmentation, which delineates individual\nplant organs from complex point clouds. Despite significant progress in general\n3D computer vision domains, the adoption of 3D segmentation for plant\nphenotyping remains limited by three major challenges: i) the scarcity of\nlarge-scale annotated datasets, ii) technical difficulties in adapting advanced\ndeep neural networks to plant point clouds, and iii) the lack of standardized\nbenchmarks and evaluation protocols tailored to plant science. This review\nsystematically addresses these barriers by: i) providing an overview of\nexisting 3D plant datasets in the context of general 3D segmentation domains,\nii) systematically summarizing deep learning-based methods for point cloud\nsemantic and instance segmentation, iii) introducing Plant Segmentation Studio\n(PSS), an open-source framework for reproducible benchmarking, and iv)\nconducting extensive quantitative experiments to evaluate representative\nnetworks and sim-to-real learning strategies. Our findings highlight the\nefficacy of sparse convolutional backbones and transformer-based instance\nsegmentation, while also emphasizing the complementary role of modeling-based\nand augmentation-based synthetic data generation for sim-to-real learning in\nreducing annotation demands. In general, this study bridges the gap between\nalgorithmic advances and practical deployment, providing immediate tools for\nresearchers and a roadmap for developing data-efficient and generalizable deep\nlearning solutions in 3D plant phenotyping. Data and code are available at\nhttps://github.com/perrydoremi/PlantSegStudio.",
        "url": "http://arxiv.org/abs/2509.06329v1",
        "published_date": "2025-09-08T04:21:27+00:00",
        "updated_date": "2025-09-08T04:21:27+00:00",
        "categories": [
            "cs.CV",
            "q-bio.QM"
        ],
        "authors": [
            "Ruiming Du",
            "Guangxun Zhai",
            "Tian Qiu",
            "Yu Jiang"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer",
            "AIGC"
        ],
        "tldr": "The paper discusses challenges in 3D plant segmentation and introduces an open-source framework for benchmarking deep learning models, highlighting the efficacy of certain techniques for plant phenotyping.",
        "tldr_zh": "本文讨论了3D植物分割的挑战，并介绍了一个用于评估深度学习模型的开源框架，突出了某些技术对于植物表型研究的有效性。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Evaluating the Impact of Adversarial Attacks on Traffic Sign Classification using the LISA Dataset",
        "summary": "Adversarial attacks pose significant threats to machine learning models by\nintroducing carefully crafted perturbations that cause misclassification. While\nprior work has primarily focused on MNIST and similar datasets, this paper\ninvestigates the vulnerability of traffic sign classifiers using the LISA\nTraffic Sign dataset. We train a convolutional neural network to classify 47\ndifferent traffic signs and evaluate its robustness against Fast Gradient Sign\nMethod (FGSM) and Projected Gradient Descent (PGD) attacks. Our results show a\nsharp decline in classification accuracy as the perturbation magnitude\nincreases, highlighting the models susceptibility to adversarial examples. This\nstudy lays the groundwork for future exploration into defense mechanisms\ntailored for real-world traffic sign recognition systems.",
        "url": "http://arxiv.org/abs/2509.06835v1",
        "published_date": "2025-09-08T16:06:41+00:00",
        "updated_date": "2025-09-08T16:06:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nabeyou Tadessa",
            "Balaji Iyangar",
            "Mashrur Chowdhury"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper investigates the vulnerability of traffic sign classifiers to adversarial attacks using the LISA Traffic Sign dataset and highlights the decline in classification accuracy under perturbations.",
        "tldr_zh": "本文研究了交通标志分类器对于利用 LISA 交通标志数据集进行的对抗攻击的敏感性，并强调了在扰动下分类准确性的下降。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.0
    },
    {
        "title": "Approximating Condorcet Ordering for Vector-valued Mathematical Morphology",
        "summary": "Mathematical morphology provides a nonlinear framework for image and spatial\ndata processing and analysis. Although there have been many successful\napplications of mathematical morphology to vector-valued images, such as color\nand hyperspectral images, there is still no consensus on the most suitable\nvector ordering for constructing morphological operators. This paper addresses\nthis issue by examining a reduced ordering approximating the Condorcet ranking\nderived from a set of vector orderings. Inspired by voting problems, the\nCondorcet ordering ranks elements from most to least voted, with voters\nrepresenting different orderings. In this paper, we develop a machine learning\napproach that learns a reduced ordering that approximates the Condorcet\nordering. Preliminary computational experiments confirm the effectiveness of\nlearning the reduced mapping to define vector-valued morphological operators\nfor color images.",
        "url": "http://arxiv.org/abs/2509.06577v1",
        "published_date": "2025-09-08T11:47:11+00:00",
        "updated_date": "2025-09-08T11:47:11+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.NE"
        ],
        "authors": [
            "Marcos Eduardo Valle",
            "Santiago Velasco-Forero",
            "Joao Batista Florindo",
            "Gustavo Jesus Angulo"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper proposes a machine learning approach to approximate Condorcet ordering for constructing morphological operators for color images.",
        "tldr_zh": "本文提出了一种机器学习方法，用于逼近康多塞序列，以构建用于彩色图像的形态学运算符。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "On the Reproducibility of \"FairCLIP: Harnessing Fairness in Vision-Language Learning''",
        "summary": "We investigated the reproducibility of FairCLIP, proposed by Luo et al.\n(2024), for improving the group fairness of CLIP (Radford et al., 2021) by\nminimizing image-text similarity score disparities across sensitive groups\nusing the Sinkhorn distance. The experimental setup of Luo et al. (2024) was\nreproduced to primarily investigate the research findings for FairCLIP. The\nmodel description by Luo et al. (2024) was found to differ from the original\nimplementation. Therefore, a new implementation, A-FairCLIP, is introduced to\nexamine specific design choices. Furthermore, FairCLIP+ is proposed to extend\nthe FairCLIP objective to include multiple attributes. Additionally, the impact\nof the distance minimization on FairCLIP's fairness and performance was\nexplored. In alignment with the original authors, CLIP was found to be biased\ntowards certain demographics when applied to zero-shot glaucoma classification\nusing medical scans and clinical notes from the Harvard-FairVLMed dataset.\nHowever, the experimental results on two datasets do not support their claim\nthat FairCLIP improves the performance and fairness of CLIP. Although the\nregularization objective reduces Sinkhorn distances, both the official\nimplementation and the aligned implementation, A-FairCLIP, were not found to\nimprove performance nor fairness in zero-shot glaucoma classification.",
        "url": "http://arxiv.org/abs/2509.06535v1",
        "published_date": "2025-09-08T10:41:10+00:00",
        "updated_date": "2025-09-08T10:41:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Hua Chang Bakker",
            "Stan Fris",
            "Angela Madelon Bernardy",
            "Stan Deutekom"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "The paper investigates the reproducibility of FairCLIP, aiming to improve group fairness in vision-language learning. However, the experimental results do not support the claim that FairCLIP enhances performance and fairness.",
        "tldr_zh": "本文研究了FairCLIP的可重复性，旨在提高视觉-语言学习中的组公平性。然而，实验结果并不支持FairCLIP能提升性能和公平性的观点。",
        "relevance_score": 2,
        "novelty_claim_score": 3,
        "clarity_score": 7,
        "potential_impact_score": 4,
        "overall_priority_score": 4
    }
]