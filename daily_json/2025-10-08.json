[
    {
        "title": "Multimodal Feature Prototype Learning for Interpretable and Discriminative Cancer Survival Prediction",
        "summary": "Survival analysis plays a vital role in making clinical decisions. However,\nthe models currently in use are often difficult to interpret, which reduces\ntheir usefulness in clinical settings. Prototype learning presents a potential\nsolution, yet traditional methods focus on local similarities and static\nmatching, neglecting the broader tumor context and lacking strong semantic\nalignment with genomic data. To overcome these issues, we introduce an\ninnovative prototype-based multimodal framework, FeatProto, aimed at enhancing\ncancer survival prediction by addressing significant limitations in current\nprototype learning methodologies within pathology. Our framework establishes a\nunified feature prototype space that integrates both global and local features\nof whole slide images (WSI) with genomic profiles. This integration facilitates\ntraceable and interpretable decision-making processes. Our approach includes\nthree main innovations: (1) A robust phenotype representation that merges\ncritical patches with global context, harmonized with genomic data to minimize\nlocal bias. (2) An Exponential Prototype Update Strategy (EMA ProtoUp) that\nsustains stable cross-modal associations and employs a wandering mechanism to\nadapt prototypes flexibly to tumor heterogeneity. (3) A hierarchical prototype\nmatching scheme designed to capture global centrality, local typicality, and\ncohort-level trends, thereby refining prototype inference. Comprehensive\nevaluations on four publicly available cancer datasets indicate that our method\nsurpasses current leading unimodal and multimodal survival prediction\ntechniques in both accuracy and interoperability, providing a new perspective\non prototype learning for critical medical applications. Our source code is\navailable at https://github.com/JSLiam94/FeatProto.",
        "url": "http://arxiv.org/abs/2510.06113v1",
        "published_date": "2025-10-07T16:49:52+00:00",
        "updated_date": "2025-10-07T16:49:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuo Jiang",
            "Zhuwen Chen",
            "Liaoman Xu",
            "Yanming Zhu",
            "Changmiao Wang",
            "Jiong Zhang",
            "Feiwei Qin",
            "Yifei Chen",
            "Zhu Zhu"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a new multimodal framework for cancer survival prediction that integrates whole slide images and genomic data, outperforming current methods in accuracy and interoperability.",
        "tldr_zh": "本文介绍了一种新的多模态框架，用于癌症生存预测，整合了全切片图像和基因组数据，在准确性和可互操作性方面优于当前方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "$\\bf{D^3}$QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection",
        "summary": "The emergence of visual autoregressive (AR) models has revolutionized image\ngeneration while presenting new challenges for synthetic image detection.\nUnlike previous GAN or diffusion-based methods, AR models generate images\nthrough discrete token prediction, exhibiting both marked improvements in image\nsynthesis quality and unique characteristics in their vector-quantized\nrepresentations. In this paper, we propose to leverage Discrete Distribution\nDiscrepancy-aware Quantization Error (D$^3$QE) for autoregressive-generated\nimage detection that exploits the distinctive patterns and the frequency\ndistribution bias of the codebook existing in real and fake images. We\nintroduce a discrete distribution discrepancy-aware transformer that integrates\ndynamic codebook frequency statistics into its attention mechanism, fusing\nsemantic features and quantization error latent. To evaluate our method, we\nconstruct a comprehensive dataset termed ARForensics covering 7 mainstream\nvisual AR models. Experiments demonstrate superior detection accuracy and\nstrong generalization of D$^3$QE across different AR models, with robustness to\nreal-world perturbations. Code is available at\n\\href{https://github.com/Zhangyr2022/D3QE}{https://github.com/Zhangyr2022/D3QE}.",
        "url": "http://arxiv.org/abs/2510.05891v1",
        "published_date": "2025-10-07T13:02:27+00:00",
        "updated_date": "2025-10-07T13:02:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yanran Zhang",
            "Bingyao Yu",
            "Yu Zheng",
            "Wenzhao Zheng",
            "Yueqi Duan",
            "Lei Chen",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces D$^3$QE, a method for detecting fake images generated by autoregressive models by leveraging the distinctive patterns and frequency distribution bias of codebooks in real and fake images.",
        "tldr_zh": "本文引入了D$^3$QE方法，通过利用真假图像中代码表的独特模式和频率分布偏差来检测由自回归模型生成的伪造图像。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "BioAutoML-NAS: An End-to-End AutoML Framework for Multimodal Insect Classification via Neural Architecture Search on Large-Scale Biodiversity Data",
        "summary": "Insect classification is important for agricultural management and ecological\nresearch, as it directly affects crop health and production. However, this task\nremains challenging due to the complex characteristics of insects, class\nimbalance, and large-scale datasets. To address these issues, we propose\nBioAutoML-NAS, the first BioAutoML model using multimodal data, including\nimages, and metadata, which applies neural architecture search (NAS) for images\nto automatically learn the best operations for each connection within each\ncell. Multiple cells are stacked to form the full network, each extracting\ndetailed image feature representations. A multimodal fusion module combines\nimage embeddings with metadata, allowing the model to use both visual and\ncategorical biological information to classify insects. An alternating bi-level\noptimization training strategy jointly updates network weights and architecture\nparameters, while zero operations remove less important connections, producing\nsparse, efficient, and high-performing architectures. Extensive evaluation on\nthe BIOSCAN-5M dataset demonstrates that BioAutoML-NAS achieves 96.81%\naccuracy, 97.46% precision, 96.81% recall, and a 97.05% F1 score, outperforming\nstate-of-the-art transfer learning, transformer, AutoML, and NAS methods by\napproximately 16%, 10%, and 8% respectively. Further validation on the\nInsects-1M dataset obtains 93.25% accuracy, 93.71% precision, 92.74% recall,\nand a 93.22% F1 score. These results demonstrate that BioAutoML-NAS provides\naccurate, confident insect classification that supports modern sustainable\nfarming.",
        "url": "http://arxiv.org/abs/2510.05888v1",
        "published_date": "2025-10-07T13:00:12+00:00",
        "updated_date": "2025-10-07T13:00:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Arefin Ittesafun Abian",
            "Debopom Sutradhar",
            "Md Rafi Ur Rashid",
            "Reem E. Mohamed",
            "Md Rafiqul Islam",
            "Asif Karim",
            "Kheng Cher Yeo",
            "Sami Azam"
        ],
        "ai_categories": [
            "Multimodality",
            "AIGC",
            "Dataset",
            "Transformer"
        ],
        "tldr": "BioAutoML-NAS is an end-to-end AutoML framework for insect classification using neural architecture search on large-scale biodiversity data, achieving high accuracy and outperforming existing methods.",
        "tldr_zh": "BioAutoML-NAS是一个端到端的AutoML框架，用于使用神经架构搜索进行昆虫分类，取得了较高的准确性，并超越了现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect",
        "summary": "The rapid development of generative models has made it increasingly crucial\nto develop detectors that can reliably detect synthetic images. Although most\nof the work has now focused on cross-generator generalization, we argue that\nthis viewpoint is too limited. Detecting synthetic images involves another\nequally important challenge: generalization across visual domains. To bridge\nthis gap,we present the OmniGen Benchmark. This comprehensive evaluation\ndataset incorporates 12 state-of-the-art generators, providing a more realistic\nway of evaluating detector performance under realistic conditions. In addition,\nwe introduce a new method, FusionDetect, aimed at addressing both vectors of\ngeneralization. FusionDetect draws on the benefits of two frozen foundation\nmodels: CLIP & Dinov2. By deriving features from both complementary models,we\ndevelop a cohesive feature space that naturally adapts to changes in both\nthecontent and design of the generator. Our extensive experiments demonstrate\nthat FusionDetect delivers not only a new state-of-the-art, which is 3.87% more\naccurate than its closest competitor and 6.13% more precise on average on\nestablished benchmarks, but also achieves a 4.48% increase in accuracy on\nOmniGen,along with exceptional robustness to common image perturbations. We\nintroduce not only a top-performing detector, but also a new benchmark and\nframework for furthering universal AI image detection. The code and dataset are\navailable at http://github.com/amir-aman/FusionDetect",
        "url": "http://arxiv.org/abs/2510.05740v1",
        "published_date": "2025-10-07T10:01:32+00:00",
        "updated_date": "2025-10-07T10:01:32+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Amirtaha Amanzadi",
            "Zahra Dehghanian",
            "Hamid Beigy",
            "Hamid R. Rabiee"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a new method, FusionDetect, for detecting synthetic images across different visual domains, achieving state-of-the-art accuracy and robustness.",
        "tldr_zh": "本文介绍了一种新方法FusionDetect，用于在不同视觉领域中检测合成图像，实现了最先进的准确性和稳健性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "DeepAf: One-Shot Spatiospectral Auto-Focus Model for Digital Pathology",
        "summary": "While Whole Slide Imaging (WSI) scanners remain the gold standard for\ndigitizing pathology samples, their high cost limits accessibility in many\nhealthcare settings. Other low-cost solutions also face critical limitations:\nautomated microscopes struggle with consistent focus across varying tissue\nmorphology, traditional auto-focus methods require time-consuming focal stacks,\nand existing deep-learning approaches either need multiple input images or lack\ngeneralization capability across tissue types and staining protocols. We\nintroduce a novel automated microscopic system powered by DeepAf, a novel\nauto-focus framework that uniquely combines spatial and spectral features\nthrough a hybrid architecture for single-shot focus prediction. The proposed\nnetwork automatically regresses the distance to the optimal focal point using\nthe extracted spatiospectral features and adjusts the control parameters for\noptimal image outcomes. Our system transforms conventional microscopes into\nefficient slide scanners, reducing focusing time by 80% compared to stack-based\nmethods while achieving focus accuracy of 0.18 {\\mu}m on the same-lab samples,\nmatching the performance of dual-image methods (0.19 {\\mu}m) with half the\ninput requirements. DeepAf demonstrates robust cross-lab generalization with\nonly 0.72% false focus predictions and 90% of predictions within the depth of\nfield. Through an extensive clinical study of 536 brain tissue samples, our\nsystem achieves 0.90 AUC in cancer classification at 4x magnification, a\nsignificant achievement at lower magnification than typical 20x WSI scans. This\nresults in a comprehensive hardware-software design enabling accessible,\nreal-time digital pathology in resource-constrained settings while maintaining\ndiagnostic accuracy.",
        "url": "http://arxiv.org/abs/2510.05315v1",
        "published_date": "2025-10-06T19:28:08+00:00",
        "updated_date": "2025-10-06T19:28:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yousef Yeganeh",
            "Maximilian Frantzen",
            "Michael Lee",
            "Kun-Hsing Yu",
            "Nassir Navab",
            "Azade Farshad"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces DeepAf, a novel auto-focus framework for microscopic systems in digital pathology that combines spatial and spectral features for efficient slide scanning and cancer classification at lower magnifications.",
        "tldr_zh": "本文介绍了DeepAf，这是一个将空间和光谱特征结合起来的新型自动对焦框架，用于数字病理学中的显微系统，可以在较低放大倍数下进行高效幻灯片扫描和癌症分类。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "Human3R: Everyone Everywhere All at Once",
        "summary": "We present Human3R, a unified, feed-forward framework for online 4D\nhuman-scene reconstruction, in the world frame, from casually captured\nmonocular videos. Unlike previous approaches that rely on multi-stage\npipelines, iterative contact-aware refinement between humans and scenes, and\nheavy dependencies, e.g., human detection, depth estimation, and SLAM\npre-processing, Human3R jointly recovers global multi-person SMPL-X bodies\n(\"everyone\"), dense 3D scene (\"everywhere\"), and camera trajectories in a\nsingle forward pass (\"all-at-once\"). Our method builds upon the 4D online\nreconstruction model CUT3R, and uses parameter-efficient visual prompt tuning,\nto strive to preserve CUT3R's rich spatiotemporal priors, while enabling direct\nreadout of multiple SMPL-X bodies. Human3R is a unified model that eliminates\nheavy dependencies and iterative refinement. After being trained on the\nrelatively small-scale synthetic dataset BEDLAM for just one day on one GPU, it\nachieves superior performance with remarkable efficiency: it reconstructs\nmultiple humans in a one-shot manner, along with 3D scenes, in one stage, at\nreal-time speed (15 FPS) with a low memory footprint (8 GB). Extensive\nexperiments demonstrate that Human3R delivers state-of-the-art or competitive\nperformance across tasks, including global human motion estimation, local human\nmesh recovery, video depth estimation, and camera pose estimation, with a\nsingle unified model. We hope that Human3R will serve as a simple yet strong\nbaseline, be easily extended for downstream applications.Code available in\nhttps://fanegg.github.io/Human3R",
        "url": "http://arxiv.org/abs/2510.06219v1",
        "published_date": "2025-10-07T17:59:52+00:00",
        "updated_date": "2025-10-07T17:59:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yue Chen",
            "Xingyu Chen",
            "Yuxuan Xue",
            "Anpei Chen",
            "Yuliang Xiu",
            "Gerard Pons-Moll"
        ],
        "ai_categories": [
            "Dataset",
            "GAN"
        ],
        "tldr": "Human3R is a unified framework for real-time 4D human-scene reconstruction from monocular videos, achieving state-of-the-art performance across various tasks with efficiency.",
        "tldr_zh": "Human3R是一个统一的框架，可以从单目视频实时进行4D人体-场景重建，在效率方面表现出色，并在各种任务中取得了最先进的表现。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "GLVD: Guided Learned Vertex Descent",
        "summary": "Existing 3D face modeling methods usually depend on 3D Morphable Models,\nwhich inherently constrain the representation capacity to fixed shape priors.\nOptimization-based approaches offer high-quality reconstructions but tend to be\ncomputationally expensive. In this work, we introduce GLVD, a hybrid method for\n3D face reconstruction from few-shot images that extends Learned Vertex Descent\n(LVD) by integrating per-vertex neural field optimization with global\nstructural guidance from dynamically predicted 3D keypoints. By incorporating\nrelative spatial encoding, GLVD iteratively refines mesh vertices without\nrequiring dense 3D supervision. This enables expressive and adaptable geometry\nreconstruction while maintaining computational efficiency. GLVD achieves\nstate-of-the-art performance in single-view settings and remains highly\ncompetitive in multi-view scenarios, all while substantially reducing inference\ntime.",
        "url": "http://arxiv.org/abs/2510.06046v1",
        "published_date": "2025-10-07T15:40:10+00:00",
        "updated_date": "2025-10-07T15:40:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Pol Caselles Rico",
            "Francesc Moreno Noguer"
        ],
        "ai_categories": [
            "GAN",
            "Other"
        ],
        "tldr": "GLVD is a hybrid method for 3D face reconstruction that integrates per-vertex neural field optimization with global structural guidance, achieving state-of-the-art performance with reduced inference time.",
        "tldr_zh": "GLVD是一种用于3D面部重建的混合方法，将节点神经场优化与全局结构引导相结合，实现了性能最先进且推断时间减少的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders",
        "summary": "In this work, we present FoleyGRAM, a novel approach to video-to-audio\ngeneration that emphasizes semantic conditioning through the use of aligned\nmultimodal encoders. Building on prior advancements in video-to-audio\ngeneration, FoleyGRAM leverages the Gramian Representation Alignment Measure\n(GRAM) to align embeddings across video, text, and audio modalities, enabling\nprecise semantic control over the audio generation process. The core of\nFoleyGRAM is a diffusion-based audio synthesis model conditioned on\nGRAM-aligned embeddings and waveform envelopes, ensuring both semantic richness\nand temporal alignment with the corresponding input video. We evaluate\nFoleyGRAM on the Greatest Hits dataset, a standard benchmark for video-to-audio\nmodels. Our experiments demonstrate that aligning multimodal encoders using\nGRAM enhances the system's ability to semantically align generated audio with\nvideo content, advancing the state of the art in video-to-audio synthesis.",
        "url": "http://arxiv.org/abs/2510.05829v1",
        "published_date": "2025-10-07T11:52:00+00:00",
        "updated_date": "2025-10-07T11:52:00+00:00",
        "categories": [
            "cs.SD",
            "cs.CV",
            "cs.LG",
            "cs.MM",
            "eess.AS"
        ],
        "authors": [
            "Riccardo Fosco Gramaccioni",
            "Christian Marinoni",
            "Eleonora Grassucci",
            "Giordano Cicchetti",
            "Aurelio Uncini",
            "Danilo Comminiello"
        ],
        "ai_categories": [
            "Multimodality",
            "Diffusion",
            "GAN",
            "Dataset"
        ],
        "tldr": "FoleyGRAM is a new method for generating audio from videos with precise semantic control by aligning multimodal encoders using GRAM, showcasing improved performance in video-to-audio synthesis.",
        "tldr_zh": "FoleyGRAM是一种新方法，通过使用GRAM来对齐多模态编码器，实现从视频生成音频并实现精准的语义控制，在视频到音频合成中表现出更好的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "StereoSync: Spatially-Aware Stereo Audio Generation from Video",
        "summary": "Although audio generation has been widely studied over recent years,\nvideo-aligned audio generation still remains a relatively unexplored frontier.\nTo address this gap, we introduce StereoSync, a novel and efficient model\ndesigned to generate audio that is both temporally synchronized with a\nreference video and spatially aligned with its visual context. Moreover,\nStereoSync also achieves efficiency by leveraging pretrained foundation models,\nreducing the need for extensive training while maintaining high-quality\nsynthesis. Unlike existing methods that primarily focus on temporal\nsynchronization, StereoSync introduces a significant advancement by\nincorporating spatial awareness into video-aligned audio generation. Indeed,\ngiven an input video, our approach extracts spatial cues from depth maps and\nbounding boxes, using them as cross-attention conditioning in a diffusion-based\naudio generation model. Such an approach allows StereoSync to go beyond simple\nsynchronization, producing stereo audio that dynamically adapts to the spatial\nstructure and movement of a video scene. We evaluate StereoSync on Walking The\nMaps, a curated dataset comprising videos from video games that feature\nanimated characters walking through diverse environments. Experimental results\ndemonstrate the ability of StereoSync to achieve both temporal and spatial\nalignment, advancing the state of the art in video-to-audio generation and\nresulting in a significantly more immersive and realistic audio experience.",
        "url": "http://arxiv.org/abs/2510.05828v1",
        "published_date": "2025-10-07T11:51:58+00:00",
        "updated_date": "2025-10-07T11:51:58+00:00",
        "categories": [
            "cs.SD",
            "cs.CV",
            "cs.LG",
            "cs.MM",
            "eess.AS"
        ],
        "authors": [
            "Christian Marinoni",
            "Riccardo Fosco Gramaccioni",
            "Kazuki Shimada",
            "Takashi Shibuya",
            "Yuki Mitsufuji",
            "Danilo Comminiello"
        ],
        "ai_categories": [
            "GAN",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "StereoSync is a novel model for generating spatially-aware stereo audio that is synchronized with reference videos, going beyond simple temporal synchronization to create a more immersive audio experience.",
        "tldr_zh": "StereoSync 是一个新颖的模型，用于生成与参考视频空间对齐的立体声音频，超越简单的时间同步，创造更沉浸式的音频体验。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Data Factory with Minimal Human Effort Using VLMs",
        "summary": "Generating enough and diverse data through augmentation offers an efficient\nsolution to the time-consuming and labour-intensive process of collecting and\nannotating pixel-wise images. Traditional data augmentation techniques often\nface challenges in manipulating high-level semantic attributes, such as\nmaterials and textures. In contrast, diffusion models offer a robust\nalternative, by effectively utilizing text-to-image or image-to-image\ntransformation. However, existing diffusion-based methods are either\ncomputationally expensive or compromise on performance. To address this issue,\nwe introduce a novel training-free pipeline that integrates pretrained\nControlNet and Vision-Language Models (VLMs) to generate synthetic images\npaired with pixel-level labels. This approach eliminates the need for manual\nannotations and significantly improves downstream tasks. To improve the\nfidelity and diversity, we add a Multi-way Prompt Generator, Mask Generator and\nHigh-quality Image Selection module. Our results on PASCAL-5i and COCO-20i\npresent promising performance and outperform concurrent work for one-shot\nsemantic segmentation.",
        "url": "http://arxiv.org/abs/2510.05722v1",
        "published_date": "2025-10-07T09:43:24+00:00",
        "updated_date": "2025-10-07T09:43:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaojiao Ye",
            "Jiaxing Zhong",
            "Qian Xie",
            "Yuzhou Zhou",
            "Niki Trigoni",
            "Andrew Markham"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a new method using Vision-Language Models to generate synthetic images with pixel-level labels, eliminating the need for manual annotations and improving downstream tasks like semantic segmentation.",
        "tldr_zh": "本文提出一种新的方法，利用视觉语言模型生成带有像素级标签的合成图像，消除了手动注释的需求，并提高了下游任务的性能，如语义分割。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Teleportraits: Training-Free People Insertion into Any Scene",
        "summary": "The task of realistically inserting a human from a reference image into a\nbackground scene is highly challenging, requiring the model to (1) determine\nthe correct location and poses of the person and (2) perform high-quality\npersonalization conditioned on the background. Previous approaches often treat\nthem as separate problems, overlooking their interconnections, and typically\nrely on training to achieve high performance. In this work, we introduce a\nunified training-free pipeline that leverages pre-trained text-to-image\ndiffusion models. We show that diffusion models inherently possess the\nknowledge to place people in complex scenes without requiring task-specific\ntraining. By combining inversion techniques with classifier-free guidance, our\nmethod achieves affordance-aware global editing, seamlessly inserting people\ninto scenes. Furthermore, our proposed mask-guided self-attention mechanism\nensures high-quality personalization, preserving the subject's identity,\nclothing, and body features from just a single reference image. To the best of\nour knowledge, we are the first to perform realistic human insertions into\nscenes in a training-free manner and achieve state-of-the-art results in\ndiverse composite scene images with excellent identity preservation in\nbackgrounds and subjects.",
        "url": "http://arxiv.org/abs/2510.05660v1",
        "published_date": "2025-10-07T08:12:57+00:00",
        "updated_date": "2025-10-07T08:12:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jialu Gao",
            "K J Joseph",
            "Fernando De La Torre"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Diffusion"
        ],
        "tldr": "The paper introduces a training-free method for inserting people into scenes using pre-trained text-to-image diffusion models and achieves high-quality results in identity preservation.",
        "tldr_zh": "这篇论文介绍了一种无需训练的方法，利用预训练的文本到图像扩散模型将人物插入到场景中，并在身份保留方面取得了高质量结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "PointNSP: Autoregressive 3D Point Cloud Generation with Next-Scale Level-of-Detail Prediction",
        "summary": "Autoregressive point cloud generation has long lagged behind diffusion-based\napproaches in quality. The performance gap stems from the fact that\nautoregressive models impose an artificial ordering on inherently unordered\npoint sets, forcing shape generation to proceed as a sequence of local\npredictions. This sequential bias emphasizes short-range continuity but\nundermines the model's capacity to capture long-range dependencies, hindering\nits ability to enforce global structural properties such as symmetry,\nconsistent topology, and large-scale geometric regularities. Inspired by the\nlevel-of-detail (LOD) principle in shape modeling, we propose PointNSP, a\ncoarse-to-fine generative framework that preserves global shape structure at\nlow resolutions and progressively refines fine-grained geometry at higher\nscales through a next-scale prediction paradigm. This multi-scale factorization\naligns the autoregressive objective with the permutation-invariant nature of\npoint sets, enabling rich intra-scale interactions while avoiding brittle fixed\norderings. Experiments on ShapeNet show that PointNSP establishes\nstate-of-the-art (SOTA) generation quality for the first time within the\nautoregressive paradigm. In addition, it surpasses strong diffusion-based\nbaselines in parameter, training, and inference efficiency. Finally, in dense\ngeneration with 8,192 points, PointNSP's advantages become even more\npronounced, underscoring its scalability potential.",
        "url": "http://arxiv.org/abs/2510.05613v1",
        "published_date": "2025-10-07T06:31:02+00:00",
        "updated_date": "2025-10-07T06:31:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Ziqiao Meng",
            "Qichao Wang",
            "Zhiyang Dou",
            "Zixing Song",
            "Zhipeng Zhou",
            "Irwin King",
            "Peilin Zhao"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "LoRA"
        ],
        "tldr": "PointNSP is a generative framework for 3D point cloud generation that excels in capturing global shape structures and shows state-of-the-art performance within the autoregressive paradigm.",
        "tldr_zh": "PointNSP是一个用于生成3D点云的框架，在捕捉全局形状结构方面表现出色，并且在自回归范式中展现了领先的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Improving Chain-of-Thought Efficiency for Autoregressive Image Generation",
        "summary": "Autoregressive multimodal large language models have recently gained\npopularity for image generation, driven by advances in foundation models. To\nenhance alignment and detail, newer approaches employ chain-of-thought (CoT)\nreasoning, expanding user inputs into elaborated prompts prior to image\nsynthesis. However, this strategy can introduce unnecessary redundancy -- a\nphenomenon we call visual overthinking -- which increases computational costs\nand can introduce details that contradict the original prompt. In this work, we\nexplore how to generate more concise CoT sequences for more efficient image\ngeneration. We introduce ShortCoTI, a lightweight optimization framework that\nencourages more concise CoT while preserving output image quality. ShortCoTI\nrewards more concise prompts with an adaptive function that scales according to\nan estimated difficulty for each task. Incorporating this reward into a\nreinforcement learning paradigm reduces prompt reasoning length by 54% while\nmaintaining or slightly improving quality metrics across multiple benchmarks\n(T2I-CompBench, GenEval). Qualitative analysis shows that our method eliminates\nverbose explanations and repetitive refinements, producing reasoning prompts\nthat are both concise and semantically rich. As a result, ShortCoTI improves\ncomputational efficiency without compromising the fidelity or visual appeal of\ngenerated images.",
        "url": "http://arxiv.org/abs/2510.05593v1",
        "published_date": "2025-10-07T05:40:43+00:00",
        "updated_date": "2025-10-07T05:40:43+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Zeqi Gu",
            "Markos Georgopoulos",
            "Xiaoliang Dai",
            "Marjan Ghazvininejad",
            "Chu Wang",
            "Felix Juefei-Xu",
            "Kunpeng Li",
            "Yujun Shi",
            "Zecheng He",
            "Zijian He",
            "Jiawei Zhou",
            "Abe Davis",
            "Jialiang Wang"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer"
        ],
        "tldr": "The paper introduces ShortCoTI, a framework that improves chain-of-thought efficiency for autoregressive image generation by generating more concise prompts while maintaining image quality.",
        "tldr_zh": "本文引入ShortCoTI框架，通过生成更简洁的提示来提高自回归图像生成的思维链效率，同时保持图像质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models",
        "summary": "Recent advances in generative models have sparked exciting new possibilities\nin the field of autonomous vehicles. Specifically, video generation models are\nnow being explored as controllable virtual testing environments.\nSimultaneously, end-to-end (E2E) driving models have emerged as a streamlined\nalternative to conventional modular autonomous driving systems, gaining\npopularity for their simplicity and scalability. However, the application of\nthese techniques to simulation and planning raises important questions. First,\nwhile video generation models can generate increasingly realistic videos, can\nthese videos faithfully adhere to the specified conditions and be realistic\nenough for E2E autonomous planner evaluation? Second, given that data is\ncrucial for understanding and controlling E2E planners, how can we gain deeper\ninsights into their biases and improve their ability to generalize to\nout-of-distribution scenarios? In this work, we bridge the gap between the\ndriving models and generative world models (Drive&Gen) to address these\nquestions. We propose novel statistical measures leveraging E2E drivers to\nevaluate the realism of generated videos. By exploiting the controllability of\nthe video generation model, we conduct targeted experiments to investigate\ndistribution gaps affecting E2E planner performance. Finally, we show that\nsynthetic data produced by the video generation model offers a cost-effective\nalternative to real-world data collection. This synthetic data effectively\nimproves E2E model generalization beyond existing Operational Design Domains,\nfacilitating the expansion of autonomous vehicle services into new operational\ncontexts.",
        "url": "http://arxiv.org/abs/2510.06209v1",
        "published_date": "2025-10-07T17:58:32+00:00",
        "updated_date": "2025-10-07T17:58:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahao Wang",
            "Zhenpei Yang",
            "Yijing Bai",
            "Yingwei Li",
            "Yuliang Zou",
            "Bo Sun",
            "Abhijit Kundu",
            "Jose Lezama",
            "Luna Yue Huang",
            "Zehao Zhu",
            "Jyh-Jing Hwang",
            "Dragomir Anguelov",
            "Mingxing Tan",
            "Chiyu Max Jiang"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces Drive&Gen, a method to evaluate the realism of generated videos for testing autonomous vehicles and improve their generalization to new scenarios.",
        "tldr_zh": "本文介绍了Drive&Gen，一种用于评估生成视频真实性以测试自动驾驶车辆并提高其泛化能力的方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "ShapeGen4D: Towards High Quality 4D Shape Generation from Videos",
        "summary": "Video-conditioned 4D shape generation aims to recover time-varying 3D\ngeometry and view-consistent appearance directly from an input video. In this\nwork, we introduce a native video-to-4D shape generation framework that\nsynthesizes a single dynamic 3D representation end-to-end from the video. Our\nframework introduces three key components based on large-scale pre-trained 3D\nmodels: (i) a temporal attention that conditions generation on all frames while\nproducing a time-indexed dynamic representation; (ii) a time-aware point\nsampling and 4D latent anchoring that promote temporally consistent geometry\nand texture; and (iii) noise sharing across frames to enhance temporal\nstability. Our method accurately captures non-rigid motion, volume changes, and\neven topological transitions without per-frame optimization. Across diverse\nin-the-wild videos, our method improves robustness and perceptual fidelity and\nreduces failure modes compared with the baselines.",
        "url": "http://arxiv.org/abs/2510.06208v1",
        "published_date": "2025-10-07T17:58:11+00:00",
        "updated_date": "2025-10-07T17:58:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiraphon Yenphraphai",
            "Ashkan Mirzaei",
            "Jianqi Chen",
            "Jiaxu Zou",
            "Sergey Tulyakov",
            "Raymond A. Yeh",
            "Peter Wonka",
            "Chaoyang Wang"
        ],
        "ai_categories": [
            "GAN",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces ShapeGen4D, a framework for generating high-quality 4D shapes from videos, capturing non-rigid motion, volume changes, and topological transitions without per-frame optimization.",
        "tldr_zh": "本文介绍了ShapeGen4D，一种从视频中生成高质量4D形状的框架，能够捕捉非刚性运动、体积变化和拓扑转换而无需逐帧优化。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Universal Neural Architecture Space: Covering ConvNets, Transformers and Everything in Between",
        "summary": "We introduce Universal Neural Architecture Space (UniNAS), a generic search\nspace for neural architecture search (NAS) which unifies convolutional\nnetworks, transformers, and their hybrid architectures under a single, flexible\nframework. Our approach enables discovery of novel architectures as well as\nanalyzing existing architectures in a common framework. We also propose a new\nsearch algorithm that allows traversing the proposed search space, and\ndemonstrate that the space contains interesting architectures, which, when\nusing identical training setup, outperform state-of-the-art hand-crafted\narchitectures. Finally, a unified toolkit including a standardized training and\nevaluation protocol is introduced to foster reproducibility and enable fair\ncomparison in NAS research. Overall, this work opens a pathway towards\nsystematically exploring the full spectrum of neural architectures with a\nunified graph-based NAS perspective.",
        "url": "http://arxiv.org/abs/2510.06035v1",
        "published_date": "2025-10-07T15:31:40+00:00",
        "updated_date": "2025-10-07T15:31:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ondřej Týbl",
            "Lukáš Neumann"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper introduces UniNAS, a generic search space for neural architecture search that covers ConvNets, Transformers, and hybrid architectures, enabling the discovery of novel architectures and outperforming state-of-the-art hand-crafted ones.",
        "tldr_zh": "该论文介绍了UniNAS，这是一个通用的神经架构搜索空间，涵盖了ConvNets、Transformers和混合架构，可以发现新的架构，并在相同训练设置下胜过现有先进手工定制的架构。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Flow4Agent: Long-form Video Understanding via Motion Prior from Optical Flow",
        "summary": "Long-form video understanding has always been a challenging problem due to\nthe significant redundancy in both temporal and spatial contents. This\nchallenge is further exacerbated by the limited context length of Multimodal\nLarge Language Models (MLLMs). To address this issue, many previous works have\nattempted to extract key video information, where the \"key\" is typically\nsemantic-aware and heavily dependent on the CLIP model as prior. In this paper,\nwe propose Flow4Agent, a novel framework that pioneeringly incorporates motion\npriors from optical flow to facilitate LLM-based long video understanding.\nFlow4Agent mitigates the redundancy in long videos at both temporal and spatial\nlevels through two core modules: Temporal Granularity Optimization (TGO)\nadaptively refines framelevel hierarchies, which first leverages coarse flow\npriors to group similar visual contents and then applies semantic priors to\nfilter out highly irrelevant scene information. Motion Token Pruning (MTP)\nfurther refines the intra-frame visual representations, pruning high-redundancy\nvideo tokens using fine-grained optical flow information. Extensive experiments\ndemonstrate that our Flow4Agent outperforms existing methods across a wide\nrange of video MLLM benchmarks, especially for hour-level video understanding\ntasks, achieving 64.7% on Video-MME, 71.4% on MLVU and 60.4% on LongVideoBench.",
        "url": "http://arxiv.org/abs/2510.05836v1",
        "published_date": "2025-10-07T12:01:57+00:00",
        "updated_date": "2025-10-07T12:01:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruyang Liu",
            "Shangkun Sun",
            "Haoran Tang",
            "Ge Li",
            "Wei Gao"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "Flow4Agent proposes a novel framework that incorporates motion priors from optical flow to improve long-form video understanding, outperforming existing methods on various benchmarks.",
        "tldr_zh": "Flow4Agent提出了一种新颖的框架，通过光流中的动作先验来提高对长视频的理解，在各种基准测试中表现优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Context Matters: Learning Global Semantics for Visual Reasoning and Comprehension",
        "summary": "Recent advances in language modeling have witnessed the rise of highly\ndesirable emergent capabilities, such as reasoning and in-context learning.\nHowever, vision models have yet to exhibit comparable progress in these areas.\nIn this paper, we argue that this gap could stem from the lack of semantic and\ncontextual guidance in current vision transformer (ViT) training schemes, and\nsuch a gap can be narrowed through the design of a semantic-grounded objective.\nSpecifically, we notice that individual words in natural language are\ninherently semantic, and modeling directly on word tokens naturally learns a\nrealistic distribution. In contrast, ViTs rely on spatial patchification, which\ninevitably lacks semantic information. To bridge this gap, we propose to\ndirectly model \"object\" as the visual equivalence of \"word,\" pushing the model\nto learn the global context and semantics among visual elements. We investigate\nour hypotheses via masked image modeling (MIM), a framework where our approach\ncan be readily tested by applying masks to visual objects rather than random\npatches. Considerable evidence from qualitative and quantitative evaluations\nreveals a key finding: object-level representation alone helps to learn a\nreal-world distribution, whereas pixel-averaging shortcuts are often learned\nwithout it. Moreover, further evaluations with multimodal LLMs (MLLM) on visual\nquestion answering (VQA, GQA, ScienceQA) tasks demonstrate the strong reasoning\nand contextual understanding gained with this simple objective. We hope our\nstudy highlights the effectiveness of object-level encoding and provides a\nplausible direction for developing stronger vision encoders and tokenizers.\nCode and model will be publicly released. Keywords: Semantic Visual Tokenizer,\nVision Reasoning, In-context Learning, Multimodal Reasoning",
        "url": "http://arxiv.org/abs/2510.05674v1",
        "published_date": "2025-10-07T08:33:36+00:00",
        "updated_date": "2025-10-07T08:33:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jike Zhong",
            "Yuxiang Lai",
            "Xiaofeng Yang",
            "Konstantinos Psounis"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality"
        ],
        "tldr": "The paper proposes a semantic-grounded objective to improve visual reasoning and comprehension in vision models by modeling objects as the visual equivalence of words, leading to strong reasoning and contextual understanding in visual question answering tasks.",
        "tldr_zh": "本文提出了一种基于语义的目标，通过将对象建模为单词的视觉等效物，从而提高了视觉模型中的视觉推理和理解，在视觉问答任务中获得了强大的推理和语境理解。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8.25
    },
    {
        "title": "Dropping the D: RGB-D SLAM Without the Depth Sensor",
        "summary": "We present DropD-SLAM, a real-time monocular SLAM system that achieves\nRGB-D-level accuracy without relying on depth sensors. The system replaces\nactive depth input with three pretrained vision modules: a monocular metric\ndepth estimator, a learned keypoint detector, and an instance segmentation\nnetwork. Dynamic objects are suppressed using dilated instance masks, while\nstatic keypoints are assigned predicted depth values and backprojected into 3D\nto form metrically scaled features. These are processed by an unmodified RGB-D\nSLAM back end for tracking and mapping. On the TUM RGB-D benchmark, DropD-SLAM\nattains 7.4 cm mean ATE on static sequences and 1.8 cm on dynamic sequences,\nmatching or surpassing state-of-the-art RGB-D methods while operating at 22 FPS\non a single GPU. These results suggest that modern pretrained vision models can\nreplace active depth sensors as reliable, real-time sources of metric scale,\nmarking a step toward simpler and more cost-effective SLAM systems.",
        "url": "http://arxiv.org/abs/2510.06216v1",
        "published_date": "2025-10-07T17:59:30+00:00",
        "updated_date": "2025-10-07T17:59:30+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Mert Kiray",
            "Alican Karaomer",
            "Benjamin Busam"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "DropD-SLAM is a real-time monocular SLAM system that achieves RGB-D-level accuracy without depth sensors by using pretrained vision modules and achieves promising results on benchmarks.",
        "tldr_zh": "DropD-SLAM是一个实时的单目SLAM系统，可以在不使用深度传感器的情况下实现RGB-D级精度，利用预训练的视觉模块，并在基准测试中取得了良好的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Fine-grained Defocus Blur Control for Generative Image Models",
        "summary": "Current text-to-image diffusion models excel at generating diverse,\nhigh-quality images, yet they struggle to incorporate fine-grained camera\nmetadata such as precise aperture settings. In this work, we introduce a novel\ntext-to-image diffusion framework that leverages camera metadata, or EXIF data,\nwhich is often embedded in image files, with an emphasis on generating\ncontrollable lens blur. Our method mimics the physical image formation process\nby first generating an all-in-focus image, estimating its monocular depth,\npredicting a plausible focus distance with a novel focus distance transformer,\nand then forming a defocused image with an existing differentiable lens blur\nmodel. Gradients flow backwards through this whole process, allowing us to\nlearn without explicit supervision to generate defocus effects based on content\nelements and the provided EXIF data. At inference time, this enables precise\ninteractive user control over defocus effects while preserving scene contents,\nwhich is not achievable with existing diffusion models. Experimental results\ndemonstrate that our model enables superior fine-grained control without\naltering the depicted scene.",
        "url": "http://arxiv.org/abs/2510.06215v1",
        "published_date": "2025-10-07T17:59:15+00:00",
        "updated_date": "2025-10-07T17:59:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ayush Shrivastava",
            "Connelly Barnes",
            "Xuaner Zhang",
            "Lingzhi Zhang",
            "Andrew Owens",
            "Sohrab Amirghodsi",
            "Eli Shechtman"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a text-to-image diffusion framework that leverages camera metadata to generate controllable lens blur effects, allowing precise interactive user control over defocus effects while preserving scene contents.",
        "tldr_zh": "本文介绍了一种利用相机元数据生成可控镜头模糊效果的文本到图像扩散框架，可以在保持场景内容的同时，实现精确的交互式用户控制。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Bimanual 3D Hand Motion and Articulation Forecasting in Everyday Images",
        "summary": "We tackle the problem of forecasting bimanual 3D hand motion & articulation\nfrom a single image in everyday settings. To address the lack of 3D hand\nannotations in diverse settings, we design an annotation pipeline consisting of\na diffusion model to lift 2D hand keypoint sequences to 4D hand motion. For the\nforecasting model, we adopt a diffusion loss to account for the multimodality\nin hand motion distribution. Extensive experiments across 6 datasets show the\nbenefits of training on diverse data with imputed labels (14% improvement) and\neffectiveness of our lifting (42% better) & forecasting (16.4% gain) models,\nover the best baselines, especially in zero-shot generalization to everyday\nimages.",
        "url": "http://arxiv.org/abs/2510.06145v1",
        "published_date": "2025-10-07T17:18:56+00:00",
        "updated_date": "2025-10-07T17:18:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Aditya Prakash",
            "David Forsyth",
            "Saurabh Gupta"
        ],
        "ai_categories": [
            "Diffusion",
            "Multimodality",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper addresses the challenge of predicting bimanual 3D hand motion from single images in real-world scenarios, achieving significant improvements over existing methods, especially in zero-shot generalization to everyday images.",
        "tldr_zh": "本文解决了从单个图像中预测双手3D运动的问题，特别是在零样本泛化到日常图像方面取得了显著的改进。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Deforming Videos to Masks: Flow Matching for Referring Video Segmentation",
        "summary": "Referring Video Object Segmentation (RVOS) requires segmenting specific\nobjects in a video guided by a natural language description. The core challenge\nof RVOS is to anchor abstract linguistic concepts onto a specific set of pixels\nand continuously segment them through the complex dynamics of a video. Faced\nwith this difficulty, prior work has often decomposed the task into a pragmatic\n`locate-then-segment' pipeline. However, this cascaded design creates an\ninformation bottleneck by simplifying semantics into coarse geometric prompts\n(e.g, point), and struggles to maintain temporal consistency as the segmenting\nprocess is often decoupled from the initial language grounding. To overcome\nthese fundamental limitations, we propose FlowRVS, a novel framework that\nreconceptualizes RVOS as a conditional continuous flow problem. This allows us\nto harness the inherent strengths of pretrained T2V models, fine-grained pixel\ncontrol, text-video semantic alignment, and temporal coherence. Instead of\nconventional generating from noise to mask or directly predicting mask, we\nreformulate the task by learning a direct, language-guided deformation from a\nvideo's holistic representation to its target mask. Our one-stage, generative\napproach achieves new state-of-the-art results across all major RVOS\nbenchmarks. Specifically, achieving a $\\mathcal{J}\\&\\mathcal{F}$ of 51.1 in\nMeViS (+1.6 over prior SOTA) and 73.3 in the zero shot Ref-DAVIS17 (+2.7),\ndemonstrating the significant potential of modeling video understanding tasks\nas continuous deformation processes.",
        "url": "http://arxiv.org/abs/2510.06139v1",
        "published_date": "2025-10-07T17:14:10+00:00",
        "updated_date": "2025-10-07T17:14:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zanyi Wang",
            "Dengyang Jiang",
            "Liuzhuozheng Li",
            "Sizhe Dang",
            "Chengzu Li",
            "Harry Yang",
            "Guang Dai",
            "Mengmeng Wang",
            "Jingdong Wang"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "FlowRVS proposes a novel framework for Referring Video Object Segmentation by learning a direct, language-guided deformation from a video's holistic representation to its target mask, achieving state-of-the-art results in RVOS benchmarks.",
        "tldr_zh": "FlowRVS提出了一种新颖的框架，通过从视频的整体表达到目标掩模的语言引导变形学习，实现了RVOS基准测试的最新结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation",
        "summary": "Recent advances in generative medical models are constrained by\nmodality-specific scenarios that hinder the integration of complementary\nevidence from imaging, pathology, and clinical notes. This fragmentation limits\ntheir evolution into foundation models that can learn and reason across the\nfull spectrum of biomedical data. We propose MeDiM, the first medical discrete\ndiffusion model that learns shared distributions across modalities without\nmodality-specific components. MeDiM unifies multiple generative tasks:\ntranslating between images and text, and jointly producing image-report pairs\nacross domains in response to prompts. Built on a discrete diffusion framework,\nMeDiM bridges vision and language representations through a shared\nprobabilistic space. To enable unified and flexible medical generation, we\nemploy a multimodal large language model (MLLM) as the diffusion backbone,\nleveraging its prior knowledge and cross-modal reasoning. Two key designs are\nintroduced: (1) removing the causal attention mask for bidirectional context,\nand (2) injecting continuous timestep embeddings for diffusion awareness.\nExperiments demonstrate high-fidelity medical generation (FID 16.60 on\nMIMIC-CXR and FID 24.19 on PathGen) and accurate report generation (METEOR\n0.2650 and 0.2580). Jointly generated image-report pairs further enhance\ndownstream performance (plus6.43 percent BLEU-1, plus18.57 percent BLEU-2,\nplus31.58 percent BLEU-3, plus4.80 percent METEOR), showing that MeDiM supports\ncoherent and clinically grounded multimodal outputs.",
        "url": "http://arxiv.org/abs/2510.06131v1",
        "published_date": "2025-10-07T17:06:57+00:00",
        "updated_date": "2025-10-07T17:06:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jiawei Mao",
            "Yuhan Wang",
            "Lifeng Chen",
            "Can Zhao",
            "Yucheng Tang",
            "Dong Yang",
            "Liangqiong Qu",
            "Daguang Xu",
            "Yuyin Zhou"
        ],
        "ai_categories": [
            "Multimodality",
            "Diffusion",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces MeDiM, a medical discrete diffusion model that integrates multimodal information for generating images and text in a unified framework, demonstrating high-fidelity medical generation results.",
        "tldr_zh": "本文介绍了MeDiM，一种医学离散扩散模型，它在统一框架中整合多模态信息，用于生成图像和文本，展示出高保真的医学生成结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Data-Efficient Medical Imaging: A Generative and Semi-Supervised Framework",
        "summary": "Deep learning in medical imaging is often limited by scarce and imbalanced\nannotated data. We present SSGNet, a unified framework that combines class\nspecific generative modeling with iterative semisupervised pseudo labeling to\nenhance both classification and segmentation. Rather than functioning as a\nstandalone model, SSGNet augments existing baselines by expanding training data\nwith StyleGAN3 generated images and refining labels through iterative pseudo\nlabeling. Experiments across multiple medical imaging benchmarks demonstrate\nconsistent gains in classification and segmentation performance, while Frechet\nInception Distance analysis confirms the high quality of generated samples.\nThese results highlight SSGNet as a practical strategy to mitigate annotation\nbottlenecks and improve robustness in medical image analysis.",
        "url": "http://arxiv.org/abs/2510.06123v1",
        "published_date": "2025-10-07T17:03:05+00:00",
        "updated_date": "2025-10-07T17:03:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mosong Ma",
            "Tania Stathaki",
            "Michalis Lazarou"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces SSGNet, a framework that combines generative modeling and semi-supervised learning to improve medical image analysis by expanding training data and refining labels.",
        "tldr_zh": "该论文介绍了SSGNet，一个结合了生成建模和半监督学习的框架，通过扩展训练数据和优化标签来改善医学图像分析。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "When Thinking Drifts: Evidential Grounding for Robust Video Reasoning",
        "summary": "Video reasoning, the task of enabling machines to infer from dynamic visual\ncontent through multi-step logic, is crucial for advanced AI. While the\nChain-of-Thought (CoT) mechanism has enhanced reasoning in text-based tasks,\nits application to video understanding remains underexplored. This paper\npresents a systematic analysis revealing that CoT often degrades performance in\nvideo reasoning, generating verbose but misleading internal monologues, and\nleading to hallucinated visual details and overridden correct intuitions - a\nphenomenon we term \"visual thinking drift\". We explain this drift through a\nBayesian lens, positing that CoT traces often diverge from actual visual\nevidence, instead amplifying internal biases or language priors, causing models\nto storytell rather than engage in grounded reasoning. To counteract this, we\nintroduce Visual Evidence Reward (VER), a novel reinforcement learning\nframework that explicitly rewards the generation of reasoning traces that are\nverifiably grounded in visual evidence. Comprehensive evaluation across 10\ndiverse video understanding benchmarks demonstrates that our Video-VER\nconsistently achieves top performance. Our work sheds light on the distinct\nchallenges of video-centric reasoning and encourages the development of AI that\nrobustly grounds its inferences in visual evidence - for large multimodal\nmodels that not only \"think before answering\", but also \"see while thinking\".",
        "url": "http://arxiv.org/abs/2510.06077v1",
        "published_date": "2025-10-07T16:03:33+00:00",
        "updated_date": "2025-10-07T16:03:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mi Luo",
            "Zihui Xue",
            "Alex Dimakis",
            "Kristen Grauman"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Multimodality"
        ],
        "tldr": "The paper discusses the challenges of video reasoning and introduces a new framework, Visual Evidence Reward, to improve reasoning grounded in visual evidence.",
        "tldr_zh": "本文讨论了视频推理的挑战，并引入了一个新框架Visual Evidence Reward，以改进基于视觉证据的推理。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "There is More to Attention: Statistical Filtering Enhances Explanations in Vision Transformers",
        "summary": "Explainable AI (XAI) has become increasingly important with the rise of large\ntransformer models, yet many explanation methods designed for CNNs transfer\npoorly to Vision Transformers (ViTs). Existing ViT explanations often rely on\nattention weights, which tend to yield noisy maps as they capture\ntoken-to-token interactions within each layer.While attribution methods\nincorporating MLP blocks have been proposed, we argue that attention remains a\nvaluable and interpretable signal when properly filtered. We propose a method\nthat combines attention maps with a statistical filtering, initially proposed\nfor CNNs, to remove noisy or uninformative patterns and produce more faithful\nexplanations. We further extend our approach with a class-specific variant that\nyields discriminative explanations. Evaluation against popular state-of-the-art\nmethods demonstrates that our approach produces sharper and more interpretable\nmaps. In addition to perturbation-based faithfulness metrics, we incorporate\nhuman gaze data to assess alignment with human perception, arguing that human\ninterpretability remains essential for XAI. Across multiple datasets, our\napproach consistently outperforms or is comparable to the SOTA methods while\nremaining efficient and human plausible.",
        "url": "http://arxiv.org/abs/2510.06070v1",
        "published_date": "2025-10-07T15:59:04+00:00",
        "updated_date": "2025-10-07T15:59:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Meghna P Ayyar",
            "Jenny Benois-Pineau",
            "Akka Zemmari"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper proposes a method that enhances explanations in Vision Transformers by combining attention maps with statistical filtering, resulting in sharper and more interpretable maps.",
        "tldr_zh": "本文提出一种方法，通过将注意力图与统计滤波相结合，增强了Vision Transformers中的解释，产生更清晰和可解释的图。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Controllable Audio-Visual Viewpoint Generation from 360° Spatial Information",
        "summary": "The generation of sounding videos has seen significant advancements with the\nadvent of diffusion models. However, existing methods often lack the\nfine-grained control needed to generate viewpoint-specific content from larger,\nimmersive 360-degree environments. This limitation restricts the creation of\naudio-visual experiences that are aware of off-camera events. To the best of\nour knowledge, this is the first work to introduce a framework for controllable\naudio-visual generation, addressing this unexplored gap. Specifically, we\npropose a diffusion model by introducing a set of powerful conditioning signals\nderived from the full 360-degree space: a panoramic saliency map to identify\nregions of interest, a bounding-box-aware signed distance map to define the\ntarget viewpoint, and a descriptive caption of the entire scene. By integrating\nthese controls, our model generates spatially-aware viewpoint videos and audios\nthat are coherently influenced by the broader, unseen environmental context,\nintroducing a strong controllability that is essential for realistic and\nimmersive audio-visual generation. We show audiovisual examples proving the\neffectiveness of our framework.",
        "url": "http://arxiv.org/abs/2510.06060v1",
        "published_date": "2025-10-07T15:53:31+00:00",
        "updated_date": "2025-10-07T15:53:31+00:00",
        "categories": [
            "cs.MM",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Christian Marinoni",
            "Riccardo Fosco Gramaccioni",
            "Eleonora Grassucci",
            "Danilo Comminiello"
        ],
        "ai_categories": [
            "GAN",
            "Multimodality",
            "Transformer"
        ],
        "tldr": "The paper introduces a framework for controllable audio-visual viewpoint generation from 360° spatial information, addressing the lack of fine-grained control in existing methods.",
        "tldr_zh": "本文提出了一个从360°空间信息中生成可控音频-视觉视角的框架，解决了现有方法中缺乏细粒度控制的问题。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Continual Learning for Image Captioning through Improved Image-Text Alignment",
        "summary": "Generating accurate and coherent image captions in a continual learning\nsetting remains a major challenge due to catastrophic forgetting and the\ndifficulty of aligning evolving visual concepts with language over time. In\nthis work, we propose a novel multi-loss framework for continual image\ncaptioning that integrates semantic guidance through prompt-based continual\nlearning and contrastive alignment. Built upon a pretrained ViT-GPT-2 backbone,\nour approach combines standard cross-entropy loss with three additional\ncomponents: (1) a prompt-based cosine similarity loss that aligns image\nembeddings with synthetically constructed prompts encoding objects, attributes,\nand actions; (2) a CLIP-style loss that promotes alignment between image\nembeddings and target caption embedding; and (3) a language-guided contrastive\nloss that employs a triplet loss to enhance class-level discriminability\nbetween tasks. Notably, our approach introduces no additional overhead at\ninference time and requires no prompts during caption generation. We find that\nthis approach mitigates catastrophic forgetting, while achieving better\nsemantic caption alignment compared to state-of-the-art methods. The code can\nbe found via the following link https://github.com/\nGepardius/Taetz_Bordelius_Continual_ImageCaptioning.",
        "url": "http://arxiv.org/abs/2510.06009v1",
        "published_date": "2025-10-07T15:08:26+00:00",
        "updated_date": "2025-10-07T15:08:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bertram Taetz",
            "Gal Bordelius"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a multi-loss framework for continual image captioning that integrates semantic guidance and contrastive alignment, achieving better semantic caption alignment compared to existing methods.",
        "tldr_zh": "本文提出了一个多损失框架，用于持续图像字幕生成，通过集成语义指导和对比对齐，实现了与现有方法相比更好的语义字幕对齐。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Detection and Measurement of Hailstones with Multimodal Large Language Models",
        "summary": "This study examines the use of social media and news images to detect and\nmeasure hailstones, utilizing pre-trained multimodal large language models. The\ndataset for this study comprises 474 crowdsourced images of hailstones from\ndocumented hail events in Austria, which occurred between January 2022 and\nSeptember 2024. These hailstones have maximum diameters ranging from 2 to 11cm.\nWe estimate the hail diameters and compare four different models utilizing\none-stage and two-stage prompting strategies. The latter utilizes additional\nsize cues from reference objects, such as human hands, within the image. Our\nresults show that pretrained models already have the potential to measure\nhailstone diameters from images with an average mean absolute error of 1.12cm\nfor the best model. In comparison to a single-stage prompt, two-stage prompting\nimproves the reliability of most models. Our study suggests that these\noff-the-shelf models, even without fine-tuning, can complement traditional hail\nsensors by extracting meaningful and spatially dense information from social\nmedia imagery, enabling faster and more detailed assessments of severe weather\nevents. The automated real-time image harvesting from social media and other\nsources remains an open task, but it will make our approach directly applicable\nto future hail events.",
        "url": "http://arxiv.org/abs/2510.06008v1",
        "published_date": "2025-10-07T15:07:29+00:00",
        "updated_date": "2025-10-07T15:07:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "68T07, 68T45, 86A10",
            "I.4; I.2"
        ],
        "authors": [
            "Moritz Alker",
            "David C. Schedl",
            "Andreas Stöckl"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper investigates using pre-trained models to measure hailstones from images on social media and news, showing promising results for faster and more detailed assessments of severe weather events.",
        "tldr_zh": "本文研究使用预先训练的模型从社交媒体和新闻图片中测量冰雹，显示出有希望在更快速更详细评估严重天气事件方面的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Diffusion-Based Image Editing for Breaking Robust Watermarks",
        "summary": "Robust invisible watermarking aims to embed hidden information into images\nsuch that the watermark can survive various image manipulations. However, the\nrise of powerful diffusion-based image generation and editing techniques poses\na new threat to these watermarking schemes. In this paper, we present a\ntheoretical study and method demonstrating that diffusion models can\neffectively break robust image watermarks that were designed to resist\nconventional perturbations. We show that a diffusion-driven ``image\nregeneration'' process can erase embedded watermarks while preserving\nperceptual image content. We further introduce a novel guided diffusion attack\nthat explicitly targets the watermark signal during generation, significantly\ndegrading watermark detectability. Theoretically, we prove that as an image\nundergoes sufficient diffusion-based transformation, the mutual information\nbetween the watermarked image and the embedded watermark payload vanishes,\nresulting in decoding failure. Experimentally, we evaluate our approach on\nmultiple state-of-the-art watermarking schemes (including the deep\nlearning-based methods StegaStamp, TrustMark, and VINE) and demonstrate\nnear-zero watermark recovery rates after attack, while maintaining high visual\nfidelity of the regenerated images. Our findings highlight a fundamental\nvulnerability in current robust watermarking techniques against generative\nmodel-based attacks, underscoring the need for new watermarking strategies in\nthe era of generative AI.",
        "url": "http://arxiv.org/abs/2510.05978v1",
        "published_date": "2025-10-07T14:34:42+00:00",
        "updated_date": "2025-10-07T14:34:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yunyi Ni",
            "Finn Carter",
            "Ze Niu",
            "Emily Davis",
            "Bo Zhang"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN"
        ],
        "tldr": "The paper presents a method using diffusion models to break robust image watermarks, highlighting vulnerabilities in current watermarking techniques against generative AI attacks.",
        "tldr_zh": "本文介绍了一种使用扩散模型破解鲁棒图像水印的方法，突出了当前水印技术在生成AI攻击下的脆弱性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Dynamic Mode Decomposition Approach to Morphological Component Analysis",
        "summary": "This paper introduces a novel methodology of adapting the representation of\nvideos based on the dynamics of their scene content variation. In particular,\nwe demonstrate how the clustering of dynamic mode decomposition eigenvalues can\nbe leveraged to learn an adaptive video representation for separating\nstructurally distinct morphologies of a video. We extend the morphological\ncomponent analysis (MCA) algorithm, which uses multiple predefined incoherent\ndictionaries and a sparsity prior to separate distinct sources in signals, by\nintroducing our novel eigenspace clustering technique to obtain data-driven MCA\ndictionaries, which we call dynamic morphological component analysis (DMCA).\nAfter deriving our novel algorithm, we offer a motivational example of DMCA\napplied to a still image, then demonstrate DMCA's effectiveness in denoising\napplications on videos from the Adobe 240fps dataset. Afterwards, we provide an\nexample of DMCA enhancing the signal-to-noise ratio of a faint target summed\nwith a sea state, and conclude the paper by applying DMCA to separate a bicycle\nfrom wind clutter in inverse synthetic aperture radar images.",
        "url": "http://arxiv.org/abs/2510.05977v1",
        "published_date": "2025-10-07T14:33:46+00:00",
        "updated_date": "2025-10-07T14:33:46+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Owen T. Huber",
            "Raghu G. Raj",
            "Tianyu Chen",
            "Zacharie I. Idriss"
        ],
        "ai_categories": [
            "Other"
        ],
        "tldr": "The paper introduces a new method, Dynamic Morphological Component Analysis (DMCA), for adapting video representations based on scene content dynamics, with applications in denoising and signal separation.",
        "tldr_zh": "该论文介绍了一种新的方法，即动态形态成分分析（DMCA），用于根据场景内容动态调整视频表示，具有去噪和信号分离应用。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis",
        "summary": "Low-light image enhancement (LLIE) is vital for safety-critical applications\nsuch as surveillance, autonomous navigation, and medical imaging, where\nvisibility degradation can impair downstream task performance. Recently,\ndiffusion models have emerged as a promising generative paradigm for LLIE due\nto their capacity to model complex image distributions via iterative denoising.\nThis survey provides an up-to-date critical analysis of diffusion models for\nLLIE, distinctively featuring an in-depth comparative performance evaluation\nagainst Generative Adversarial Network and Transformer-based state-of-the-art\nmethods, a thorough examination of practical deployment challenges, and a\nforward-looking perspective on the role of emerging paradigms like foundation\nmodels. We propose a multi-perspective taxonomy encompassing six categories:\nIntrinsic Decomposition, Spectral & Latent, Accelerated, Guided, Multimodal,\nand Autonomous; that map enhancement methods across physical priors,\nconditioning schemes, and computational efficiency. Our taxonomy is grounded in\na hybrid view of both the model mechanism and the conditioning signals. We\nevaluate qualitative failure modes, benchmark inconsistencies, and trade-offs\nbetween interpretability, generalization, and inference efficiency. We also\ndiscuss real-world deployment constraints (e.g., memory, energy use) and\nethical considerations. This survey aims to guide the next generation of\ndiffusion-based LLIE research by highlighting trends and surfacing open\nresearch questions, including novel conditioning, real-time adaptation, and the\npotential of foundation models.",
        "url": "http://arxiv.org/abs/2510.05976v1",
        "published_date": "2025-10-07T14:30:36+00:00",
        "updated_date": "2025-10-07T14:30:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Eashan Adhikarla",
            "Yixin Liu",
            "Brian D. Davison"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN",
            "Transformer",
            "Multimodality"
        ],
        "tldr": "This paper surveys diffusion models for low-light image enhancement, comparing them with other state-of-the-art methods and proposing a multi-perspective taxonomy for categorizing enhancement methods.",
        "tldr_zh": "本文调查了用于低光图像增强的扩散模型，并将它们与其他最新方法进行比较，并提出了一个多角度的分类法来分类增强方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Shaken or Stirred? An Analysis of MetaFormer's Token Mixing for Medical Imaging",
        "summary": "The generalization of the Transformer architecture via MetaFormer has\nreshaped our understanding of its success in computer vision. By replacing\nself-attention with simpler token mixers, MetaFormer provides strong baselines\nfor vision tasks. However, while extensively studied on natural image datasets,\nits use in medical imaging remains scarce, and existing works rarely compare\ndifferent token mixers, potentially overlooking more suitable designs choices.\nIn this work, we present the first comprehensive study of token mixers for\nmedical imaging. We systematically analyze pooling-, convolution-, and\nattention-based token mixers within the MetaFormer architecture on image\nclassification (global prediction task) and semantic segmentation (dense\nprediction task). Our evaluation spans eight datasets covering diverse\nmodalities and common challenges in the medical domain. Given the prevalence of\npretraining from natural images to mitigate medical data scarcity, we also\nexamine transferring pretrained weights to new token mixers. Our results show\nthat, for classification, low-complexity token mixers (e.g. grouped convolution\nor pooling) are sufficient, aligning with findings on natural images.\nPretrained weights remain useful despite the domain gap introduced by the new\ntoken mixer. For segmentation, we find that the local inductive bias of\nconvolutional token mixers is essential. Grouped convolutions emerge as the\npreferred choice, as they reduce runtime and parameter count compared to\nstandard convolutions, while the MetaFormer's channel-MLPs already provide the\nnecessary cross-channel interactions. Our code is available on GitHub.",
        "url": "http://arxiv.org/abs/2510.05971v1",
        "published_date": "2025-10-07T14:28:04+00:00",
        "updated_date": "2025-10-07T14:28:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ron Keuth",
            "Paul Kaftan",
            "Mattias P. Heinrich"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper analyzes different token mixers in the MetaFormer architecture for medical imaging tasks, finding low-complexity token mixers work well for classification and convolutional token mixers are essential for segmentation.",
        "tldr_zh": "本文分析了MetaFormer架构中的不同token混合器在医学图像任务中的表现，发现低复杂度的token混合器在分类任务中效果良好，而卷积token混合器对分割任务至关重要。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Gaussian Embeddings: How JEPAs Secretly Learn Your Data Density",
        "summary": "Joint Embedding Predictive Architectures (JEPAs) learn representations able\nto solve numerous downstream tasks out-of-the-box. JEPAs combine two\nobjectives: (i) a latent-space prediction term, i.e., the representation of a\nslightly perturbed sample must be predictable from the original sample's\nrepresentation, and (ii) an anti-collapse term, i.e., not all samples should\nhave the same representation. While (ii) is often considered as an obvious\nremedy to representation collapse, we uncover that JEPAs' anti-collapse term\ndoes much more--it provably estimates the data density. In short, any\nsuccessfully trained JEPA can be used to get sample probabilities, e.g., for\ndata curation, outlier detection, or simply for density estimation. Our\ntheoretical finding is agnostic of the dataset and architecture used--in any\ncase one can compute the learned probabilities of sample $x$ efficiently and in\nclosed-form using the model's Jacobian matrix at $x$. Our findings are\nempirically validated across datasets (synthetic, controlled, and Imagenet) and\nacross different Self Supervised Learning methods falling under the JEPA family\n(I-JEPA and DINOv2) and on multimodal models, such as MetaCLIP. We denote the\nmethod extracting the JEPA learned density as {\\bf JEPA-SCORE}.",
        "url": "http://arxiv.org/abs/2510.05949v1",
        "published_date": "2025-10-07T14:06:30+00:00",
        "updated_date": "2025-10-07T14:06:30+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "stat.ML"
        ],
        "authors": [
            "Randall Balestriero",
            "Nicolas Ballas",
            "Mike Rabbat",
            "Yann LeCun"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "Joint Embedding Predictive Architectures (JEPAs) can estimate data density, enabling tasks like data curation and outlier detection.",
        "tldr_zh": "联合嵌入式预测架构（JEPAs）可以估计数据密度，从而实现数据整理和异常检测等任务。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Warm-basis Method for Bridging Learning and Iteration: a Case Study in Fluorescence Molecular Tomography",
        "summary": "Fluorescence Molecular Tomography (FMT) is a widely used non-invasive optical\nimaging technology in biomedical research. It usually faces significant\naccuracy challenges in depth reconstruction, and conventional iterative methods\nstruggle with poor $z$-resolution even with advanced regularization. Supervised\nlearning approaches can improve recovery accuracy but rely on large,\nhigh-quality paired training dataset that is often impractical to acquire in\npractice. This naturally raises the question of how learning-based approaches\ncan be effectively combined with iterative schemes to yield more accurate and\nstable algorithms. In this work, we present a novel warm-basis iterative\nprojection method (WB-IPM) and establish its theoretical underpinnings. The\nmethod is able to achieve significantly more accurate reconstructions than the\nlearning-based and iterative-based methods. In addition, it allows a weaker\nloss function depending solely on the directional component of the difference\nbetween ground truth and neural network output, thereby substantially reducing\nthe training effort. These features are justified by our error analysis as well\nas simulated and real-data experiments.",
        "url": "http://arxiv.org/abs/2510.05926v1",
        "published_date": "2025-10-07T13:38:37+00:00",
        "updated_date": "2025-10-07T13:38:37+00:00",
        "categories": [
            "math.NA",
            "cs.CV",
            "cs.NA"
        ],
        "authors": [
            "Ruchi Guo",
            "Jiahua Jiang",
            "Bangti Jin",
            "Wuwei Ren",
            "Jianru Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a warm-basis iterative projection method for improving accuracy in depth reconstruction in Fluorescence Molecular Tomography by combining supervised learning and iterative schemes.",
        "tldr_zh": "本文介绍了一种温基迭代投影方法，通过结合监督学习和迭代方案来改善荧光分子断层扫描中深度重建的准确性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Efficient Universal Models for Medical Image Segmentation via Weakly Supervised In-Context Learning",
        "summary": "Universal models for medical image segmentation, such as interactive and\nin-context learning (ICL) models, offer strong generalization but require\nextensive annotations. Interactive models need repeated user prompts for each\nimage, while ICL relies on dense, pixel-level labels. To address this, we\npropose Weakly Supervised In-Context Learning (WS-ICL), a new ICL paradigm that\nleverages weak prompts (e.g., bounding boxes or points) instead of dense labels\nfor context. This approach significantly reduces annotation effort by\neliminating the need for fine-grained masks and repeated user prompting for all\nimages. We evaluated the proposed WS-ICL model on three held-out benchmarks.\nExperimental results demonstrate that WS-ICL achieves performance comparable to\nregular ICL models at a significantly lower annotation cost. In addition,\nWS-ICL is highly competitive even under the interactive paradigm. These\nfindings establish WS-ICL as a promising step toward more efficient and unified\nuniversal models for medical image segmentation. Our code and model are\npublicly available at https://github.com/jiesihu/Weak-ICL.",
        "url": "http://arxiv.org/abs/2510.05899v1",
        "published_date": "2025-10-07T13:07:27+00:00",
        "updated_date": "2025-10-07T13:07:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiesi Hu",
            "Yanwu Yang",
            "Zhiyu Ye",
            "Jinyan Zhou",
            "Jianfeng Cao",
            "Hanyang Peng",
            "Ting Ma"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces Weakly Supervised In-Context Learning (WS-ICL) for medical image segmentation, reducing annotation effort by using weak prompts instead of dense labels, showing comparable performance to regular models at a lower cost.",
        "tldr_zh": "该论文介绍了用于医学图像分割的弱监督上下文学习（WS-ICL），通过使用弱提示而不是密集标签来减少注释工作量，显示出与常规模型相媲美的性能，成本更低。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "acia-workflows: Automated Single-cell Imaging Analysis for Scalable and Deep Learning-based Live-cell Imaging Analysis Workflows",
        "summary": "Live-cell imaging (LCI) technology enables the detailed spatio-temporal\ncharacterization of living cells at the single-cell level, which is critical\nfor advancing research in the life sciences, from biomedical applications to\nbioprocessing. High-throughput setups with tens to hundreds of parallel cell\ncultivations offer the potential for robust and reproducible insights. However,\nthese insights are obscured by the large amount of LCI data recorded per\nexperiment. Recent advances in state-of-the-art deep learning methods for cell\nsegmentation and tracking now enable the automated analysis of such large data\nvolumes, offering unprecedented opportunities to systematically study\nsingle-cell dynamics. The next key challenge lies in integrating these powerful\ntools into accessible, flexible, and user-friendly workflows that support\nroutine application in biological research. In this work, we present\nacia-workflows, a platform that combines three key components: (1) the\nAutomated live-Cell Imaging Analysis (acia) Python library, which supports the\nmodular design of image analysis pipelines offering eight deep learning\nsegmentation and tracking approaches; (2) workflows that assemble the image\nanalysis pipeline, its software dependencies, documentation, and visualizations\ninto a single Jupyter Notebook, leading to accessible, reproducible and\nscalable analysis workflows; and (3) a collection of application workflows\nshowcasing the analysis and customization capabilities in real-world\napplications. Specifically, we present three workflows to investigate various\ntypes of microfluidic LCI experiments ranging from growth rate comparisons to\nprecise, minute-resolution quantitative analyses of individual dynamic cells\nresponses to changing oxygen conditions. Our collection of more than ten\napplication workflows is open source and publicly available at\nhttps://github.com/JuBiotech/acia-workflows.",
        "url": "http://arxiv.org/abs/2510.05886v1",
        "published_date": "2025-10-07T12:58:11+00:00",
        "updated_date": "2025-10-07T12:58:11+00:00",
        "categories": [
            "cs.CV",
            "q-bio.QM"
        ],
        "authors": [
            "Johannes Seiffarth",
            "Keitaro Kasahara",
            "Michelle Bund",
            "Benita Lückel",
            "Richard D. Paul",
            "Mathias Pesch",
            "Lennart Witting",
            "Michael Bott",
            "Dietrich Kohlheyer",
            "Katharina Nöh"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper presents acia-workflows, a platform for automated single-cell imaging analysis using deep learning approaches, supporting reproducible and scalable workflows for biological research applications.",
        "tldr_zh": "该论文介绍了acia-workflows，这是一个利用深度学习方法进行自动单细胞成像分析的平台，支持生物研究应用的可重现和可扩展工作流。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "The Safety Challenge of World Models for Embodied AI Agents: A Review",
        "summary": "The rapid progress in embodied artificial intelligence has highlighted the\nnecessity for more advanced and integrated models that can perceive, interpret,\nand predict environmental dynamics. In this context, World Models (WMs) have\nbeen introduced to provide embodied agents with the abilities to anticipate\nfuture environmental states and fill in knowledge gaps, thereby enhancing\nagents' ability to plan and execute actions. However, when dealing with\nembodied agents it is fundamental to ensure that predictions are safe for both\nthe agent and the environment. In this article, we conduct a comprehensive\nliterature review of World Models in the domains of autonomous driving and\nrobotics, with a specific focus on the safety implications of scene and control\ngeneration tasks. Our review is complemented by an empirical analysis, wherein\nwe collect and examine predictions from state-of-the-art models, identify and\ncategorize common faults (herein referred to as pathologies), and provide a\nquantitative evaluation of the results.",
        "url": "http://arxiv.org/abs/2510.05865v1",
        "published_date": "2025-10-07T12:35:09+00:00",
        "updated_date": "2025-10-07T12:35:09+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Lorenzo Baraldi",
            "Zifan Zeng",
            "Chongzhe Zhang",
            "Aradhana Nayak",
            "Hongbo Zhu",
            "Feng Liu",
            "Qunli Zhang",
            "Peng Wang",
            "Shiming Liu",
            "Zheng Hu",
            "Angelo Cangelosi",
            "Lorenzo Baraldi"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer"
        ],
        "tldr": "The paper reviews the safety implications of World Models for embodied AI agents in domains like autonomous driving and robotics, emphasizing the importance of ensuring predictions are safe for the agent and environment.",
        "tldr_zh": "本文回顾了World Models在自主驾驶和机器人领域中对具身人工智能代理的安全影响，强调了确保预测对代理和环境都是安全的重要性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Robust and Realible Multimodal Fake News Detection with Incomplete Modality",
        "summary": "Multimodal fake news detection (MFND) has become an urgent task with the\nemergence of huge multimodal fake content on social media platforms. Previous\nstudies mainly focus on complex feature extraction and fusion to learn\ndiscriminative information from multimodal content. However, in real-world\napplications, multimedia news may naturally lose some information during\ndissemination, resulting in modality incompleteness, which is detrimental to\nthe generalization and robustness of existing models. To this end, we propose a\nnovel generic and robust multimodal fusion strategy, termed Multi-expert\nModality-incomplete Learning Network (MMLNet), which is simple yet effective.\nIt consists of three key steps: (1) Multi-Expert Collaborative Reasoning to\ncompensate for missing modalities by dynamically leveraging complementary\ninformation through multiple experts. (2) Incomplete Modality Adapters\ncompensates for the missing information by leveraging the new feature\ndistribution. (3) Modality Missing Learning leveraging an label-aware adaptive\nweighting strategy to learn a robust representation with contrastive learning.\nWe evaluate MMLNet on three real-world benchmarks across two languages,\ndemonstrating superior performance compared to state-of-the-art methods while\nmaintaining relative simplicity. By ensuring the accuracy of fake news\ndetection in incomplete modality scenarios caused by information propagation,\nMMLNet effectively curbs the spread of malicious misinformation. Code is\npublicly available at https://github.com/zhyhome/MMLNet.",
        "url": "http://arxiv.org/abs/2510.05839v1",
        "published_date": "2025-10-07T12:03:17+00:00",
        "updated_date": "2025-10-07T12:03:17+00:00",
        "categories": [
            "cs.MM",
            "cs.CV"
        ],
        "authors": [
            "Hengyang Zhou",
            "Yiwei Wei",
            "Jian Yang",
            "Zhenyu Zhang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes a novel method, MMLNet, for detecting fake news in multimodal content with missing information, outperforming existing methods.",
        "tldr_zh": "本文提出了一种新的方法MMLNet，用于检测含有缺失信息的多模态内容中的虚假新闻，表现优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Deformable Image Registration for Self-supervised Cardiac Phase Detection in Multi-View Multi-Disease Cardiac Magnetic Resonance Images",
        "summary": "Cardiovascular magnetic resonance (CMR) is the gold standard for assessing\ncardiac function, but individual cardiac cycles complicate automatic temporal\ncomparison or sub-phase analysis. Accurate cardiac keyframe detection can\neliminate this problem. However, automatic methods solely derive end-systole\n(ES) and end-diastole (ED) frames from left ventricular volume curves, which do\nnot provide a deeper insight into myocardial motion. We propose a\nself-supervised deep learning method detecting five keyframes in short-axis\n(SAX) and four-chamber long-axis (4CH) cine CMR. Initially, dense deformable\nregistration fields are derived from the images and used to compute a 1D motion\ndescriptor, which provides valuable insights into global cardiac contraction\nand relaxation patterns. From these characteristic curves, keyframes are\ndetermined using a simple set of rules. The method was independently evaluated\nfor both views using three public, multicentre, multidisease datasets. M&Ms-2\n(n=360) dataset was used for training and evaluation, and M&Ms (n=345) and ACDC\n(n=100) datasets for repeatability control. Furthermore, generalisability to\npatients with rare congenital heart defects was tested using the German\nCompetence Network (GCN) dataset. Our self-supervised approach achieved\nimproved detection accuracy by 30% - 51% for SAX and 11% - 47% for 4CH in ED\nand ES, as measured by cyclic frame difference (cFD), compared with the\nvolume-based approach. We can detect ED and ES, as well as three additional\nkeyframes throughout the cardiac cycle with a mean cFD below 1.31 frames for\nSAX and 1.73 for LAX. Our approach enables temporally aligned inter- and\nintra-patient analysis of cardiac dynamics, irrespective of cycle or phase\nlengths. GitHub repository:\nhttps://github.com/Cardio-AI/cmr-multi-view-phase-detection.git",
        "url": "http://arxiv.org/abs/2510.05819v1",
        "published_date": "2025-10-07T11:40:17+00:00",
        "updated_date": "2025-10-07T11:40:17+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sven Koehler",
            "Sarah Kaye Mueller",
            "Jonathan Kiekenap",
            "Gerald Greil",
            "Tarique Hussain",
            "Samir Sarikouch",
            "Florian André",
            "Norbert Frey",
            "Sandy Engelhardt"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a self-supervised deep learning method for detecting keyframes in cardiac magnetic resonance images, achieving improved accuracy compared to traditional methods.",
        "tldr_zh": "本文提出了一种自监督深度学习方法，用于在心脏磁共振图像中检测关键帧，其准确性优于传统方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Rasterized Steered Mixture of Experts for Efficient 2D Image Regression",
        "summary": "The Steered Mixture of Experts regression framework has demonstrated strong\nperformance in image reconstruction, compression, denoising, and\nsuper-resolution. However, its high computational cost limits practical\napplications. This work introduces a rasterization-based optimization strategy\nthat combines the efficiency of rasterized Gaussian kernel rendering with the\nedge-aware gating mechanism of the Steered Mixture of Experts. The proposed\nmethod is designed to accelerate two-dimensional image regression while\nmaintaining the model's inherent sparsity and reconstruction quality. By\nreplacing global iterative optimization with a rasterized formulation, the\nmethod achieves significantly faster parameter updates and more\nmemory-efficient model representations. In addition, the proposed framework\nsupports applications such as native super-resolution and image denoising,\nwhich are not directly achievable with standard rasterized Gaussian kernel\napproaches. The combination of fast rasterized optimization with the edge-aware\nstructure of the Steered Mixture of Experts provides a new balance between\ncomputational efficiency and reconstruction fidelity for two-dimensional image\nprocessing tasks.",
        "url": "http://arxiv.org/abs/2510.05814v1",
        "published_date": "2025-10-07T11:32:44+00:00",
        "updated_date": "2025-10-07T11:32:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yi-Hsin Li",
            "Thomas Sikora",
            "Sebastian Knorr",
            "Mårten Sjöström"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer"
        ],
        "tldr": "This paper introduces a rasterization-based optimization strategy to accelerate two-dimensional image regression using a Steered Mixture of Experts framework.",
        "tldr_zh": "本文提出了一种基于栅格化的优化策略，利用专家混合框架加快了二维图像回归的速度。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Novel Technique for Robust Training of Deep Networks With Multisource Weak Labeled Remote Sensing Data",
        "summary": "Deep learning has gained broad interest in remote sensing image scene\nclassification thanks to the effectiveness of deep neural networks in\nextracting the semantics from complex data. However, deep networks require\nlarge amounts of training samples to obtain good generalization capabilities\nand are sensitive to errors in the training labels. This is a problem in remote\nsensing since highly reliable labels can be obtained at high costs and in\nlimited amount. However, many sources of less reliable labeled data are\navailable, e.g., obsolete digital maps. In order to train deep networks with\nlarger datasets, we propose both the combination of single or multiple weak\nsources of labeled data with a small but reliable dataset to generate\nmultisource labeled datasets and a novel training strategy where the\nreliability of each source is taken in consideration. This is done by\nexploiting the transition matrices describing the statistics of the errors of\neach source. The transition matrices are embedded into the labels and used\nduring the training process to weigh each label according to the related\nsource. The proposed method acts as a weighting scheme at gradient level, where\neach instance contributes with different weights to the optimization of\ndifferent classes. The effectiveness of the proposed method is validated by\nexperiments on different datasets. The results proved the robustness and\ncapability of leveraging on unreliable source of labels of the proposed method.",
        "url": "http://arxiv.org/abs/2510.05760v1",
        "published_date": "2025-10-07T10:25:43+00:00",
        "updated_date": "2025-10-07T10:25:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Gianmarco Perantoni",
            "Lorenzo Bruzzone"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper proposes a method to train deep neural networks with multiple weak sources of labeled data in remote sensing, improving robustness and generalizability.",
        "tldr_zh": "本文提出了一种方法，用于在遥感中利用多个弱标记数据源训练深度神经网络，提高鲁棒性和泛化能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "OneVision: An End-to-End Generative Framework for Multi-view E-commerce Vision Search",
        "summary": "Traditional vision search, similar to search and recommendation systems,\nfollows the multi-stage cascading architecture (MCA) paradigm to balance\nefficiency and conversion. Specifically, the query image undergoes feature\nextraction, recall, pre-ranking, and ranking stages, ultimately presenting the\nuser with semantically similar products that meet their preferences. This\nmulti-view representation discrepancy of the same object in the query and the\noptimization objective collide across these stages, making it difficult to\nachieve Pareto optimality in both user experience and conversion. In this\npaper, an end-to-end generative framework, OneVision, is proposed to address\nthese problems. OneVision builds on VRQ, a vision-aligned residual quantization\nencoding, which can align the vastly different representations of an object\nacross multiple viewpoints while preserving the distinctive features of each\nproduct as much as possible. Then a multi-stage semantic alignment scheme is\nadopted to maintain strong visual similarity priors while effectively\nincorporating user-specific information for personalized preference generation.\nIn offline evaluations, OneVision performs on par with online MCA, while\nimproving inference efficiency by 21% through dynamic pruning. In A/B tests, it\nachieves significant online improvements: +2.15% item CTR, +2.27% CVR, and\n+3.12% order volume. These results demonstrate that a semantic ID centric,\ngenerative architecture can unify retrieval and personalization while\nsimplifying the serving pathway.",
        "url": "http://arxiv.org/abs/2510.05759v1",
        "published_date": "2025-10-07T10:25:21+00:00",
        "updated_date": "2025-10-07T10:25:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zexin Zheng",
            "Huangyu Dai",
            "Lingtao Mao",
            "Xinyu Sun",
            "Zihan Liang",
            "Ben Chen",
            "Yuqing Ding",
            "Chenyi Lei",
            "Wenwu Ou",
            "Han Li",
            "Kun Gai"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "OneVision proposes an end-to-end generative framework for multi-view e-commerce vision search, addressing the challenges of representation discrepancy and optimization objectives in traditional search systems.",
        "tldr_zh": "OneVision提出了一种端到端的生成框架，用于多视角电子商务视觉搜索，解决了传统搜索系统中的表示不一致和优化目标的挑战。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AgeBooth: Controllable Facial Aging and Rejuvenation via Diffusion Models",
        "summary": "Recent diffusion model research focuses on generating identity-consistent\nimages from a reference photo, but they struggle to accurately control age\nwhile preserving identity, and fine-tuning such models often requires costly\npaired images across ages. In this paper, we propose AgeBooth, a novel\nage-specific finetuning approach that can effectively enhance the age control\ncapability of adapterbased identity personalization models without the need for\nexpensive age-varied datasets. To reduce dependence on a large amount of\nage-labeled data, we exploit the linear nature of aging by introducing\nage-conditioned prompt blending and an age-specific LoRA fusion strategy that\nleverages SVDMix, a matrix fusion technique. These techniques enable\nhigh-quality generation of intermediate-age portraits. Our AgeBooth produces\nrealistic and identity-consistent face images across different ages from a\nsingle reference image. Experiments show that AgeBooth achieves superior age\ncontrol and visual quality compared to previous state-of-the-art editing-based\nmethods.",
        "url": "http://arxiv.org/abs/2510.05715v1",
        "published_date": "2025-10-07T09:25:09+00:00",
        "updated_date": "2025-10-07T09:25:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shihao Zhu",
            "Bohan Cao",
            "Ziheng Ouyang",
            "Zhen Li",
            "Peng-Tao Jiang",
            "Qibin Hou"
        ],
        "ai_categories": [
            "Diffusion",
            "LoRA"
        ],
        "tldr": "AgeBooth proposes a novel age-specific finetuning approach to enhance age control capability in generating realistic and identity-consistent face images across different ages from a single reference image.",
        "tldr_zh": "AgeBooth提出了一种新颖的年龄特定微调方法，可以从单个参考图像中生成跨不同年龄的逼真和与身份一致的面部图像。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DeLTa: Demonstration and Language-Guided Novel Transparent Object Manipulation",
        "summary": "Despite the prevalence of transparent object interactions in human everyday\nlife, transparent robotic manipulation research remains limited to\nshort-horizon tasks and basic grasping capabilities.Although some methods have\npartially addressed these issues, most of them have limitations in\ngeneralizability to novel objects and are insufficient for precise long-horizon\nrobot manipulation. To address this limitation, we propose DeLTa (Demonstration\nand Language-Guided Novel Transparent Object Manipulation), a novel framework\nthat integrates depth estimation, 6D pose estimation, and vision-language\nplanning for precise long-horizon manipulation of transparent objects guided by\nnatural task instructions. A key advantage of our method is its\nsingle-demonstration approach, which generalizes 6D trajectories to novel\ntransparent objects without requiring category-level priors or additional\ntraining. Additionally, we present a task planner that refines the\nVLM-generated plan to account for the constraints of a single-arm, eye-in-hand\nrobot for long-horizon object manipulation tasks. Through comprehensive\nevaluation, we demonstrate that our method significantly outperforms existing\ntransparent object manipulation approaches, particularly in long-horizon\nscenarios requiring precise manipulation capabilities. Project page:\nhttps://sites.google.com/view/DeLTa25/",
        "url": "http://arxiv.org/abs/2510.05662v1",
        "published_date": "2025-10-07T08:18:29+00:00",
        "updated_date": "2025-10-07T08:18:29+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Taeyeop Lee",
            "Gyuree Kang",
            "Bowen Wen",
            "Youngho Kim",
            "Seunghyeok Back",
            "In So Kweon",
            "David Hyunchul Shim",
            "Kuk-Jin Yoon"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "DeLTa proposes a new framework for precise long-horizon manipulation of transparent objects guided by natural language instructions, outperforming existing methods.",
        "tldr_zh": "DeLTa提出了一个新框架，通过自然语言指令精确长时程操纵透明物体，比现有方法表现更优秀。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "When and How to Cut Classical Concerts? A Multimodal Automated Video Editing Approach",
        "summary": "Automated video editing remains an underexplored task in the computer vision\nand multimedia domains, especially when contrasted with the growing interest in\nvideo generation and scene understanding. In this work, we address the specific\nchallenge of editing multicamera recordings of classical music concerts by\ndecomposing the problem into two key sub-tasks: when to cut and how to cut.\nBuilding on recent literature, we propose a novel multimodal architecture for\nthe temporal segmentation task (when to cut), which integrates log-mel\nspectrograms from the audio signals, plus an optional image embedding, and\nscalar temporal features through a lightweight convolutional-transformer\npipeline. For the spatial selection task (how to cut), we improve the\nliterature by updating from old backbones, e.g. ResNet, with a CLIP-based\nencoder and constraining distractor selection to segments from the same\nconcert. Our dataset was constructed following a pseudo-labeling approach, in\nwhich raw video data was automatically clustered into coherent shot segments.\nWe show that our models outperformed previous baselines in detecting cut points\nand provide competitive visual shot selection, advancing the state of the art\nin multimodal automated video editing.",
        "url": "http://arxiv.org/abs/2510.05661v1",
        "published_date": "2025-10-07T08:18:27+00:00",
        "updated_date": "2025-10-07T08:18:27+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Daniel Gonzálbez-Biosca",
            "Josep Cabacas-Maso",
            "Carles Ventura",
            "Ismael Benito-Altamirano"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper presents a novel approach for automated video editing of classical music concerts using multimodal architecture for temporal segmentation and CLIP-based encoder for spatial selection.",
        "tldr_zh": "本文提出了一种新颖的方法，利用多模态架构进行古典音乐会的自动视频编辑。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SD-MVSum: Script-Driven Multimodal Video Summarization Method and Datasets",
        "summary": "In this work, we extend a recent method for script-driven video\nsummarization, originally considering just the visual content of the video, to\ntake into account the relevance of the user-provided script also with the\nvideo's spoken content. In the proposed method, SD-MVSum, the dependence\nbetween each considered pair of data modalities, i.e., script-video and\nscript-transcript, is modeled using a new weighted cross-modal attention\nmechanism. This explicitly exploits the semantic similarity between the paired\nmodalities in order to promote the parts of the full-length video with the\nhighest relevance to the user-provided script. Furthermore, we extend two\nlarge-scale datasets for video summarization (S-VideoXum, MrHiSum), to make\nthem suitable for training and evaluation of script-driven multimodal video\nsummarization methods. Experimental comparisons document the competitiveness of\nour SD-MVSum method against other SOTA approaches for script-driven and generic\nvideo summarization. Our new method and extended datasets are available at:\nhttps://github.com/IDT-ITI/SD-MVSum.",
        "url": "http://arxiv.org/abs/2510.05652v1",
        "published_date": "2025-10-07T08:03:56+00:00",
        "updated_date": "2025-10-07T08:03:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Manolis Mylonas",
            "Charalampia Zerva",
            "Evlampios Apostolidis",
            "Vasileios Mezaris"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper presents a method for script-driven multimodal video summarization, considering both visual content and spoken script in videos, with a novel cross-modal attention mechanism. It also extends two datasets for training and evaluating the method.",
        "tldr_zh": "该论文提出了一种脚本驱动的多模态视频摘要方法，考虑了视频中的视觉内容和讲话脚本，使用了新颖的跨模态注意力机制。同时，它还扩展了两个数据集以用于该方法的训练和评估。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "NEO: No-Optimization Test-Time Adaptation through Latent Re-Centering",
        "summary": "Test-Time Adaptation (TTA) methods are often computationally expensive,\nrequire a large amount of data for effective adaptation, or are brittle to\nhyperparameters. Based on a theoretical foundation of the geometry of the\nlatent space, we are able to significantly improve the alignment between source\nand distribution-shifted samples by re-centering target data embeddings at the\norigin. This insight motivates NEO -- a hyperparameter-free fully TTA method,\nthat adds no significant compute compared to vanilla inference. NEO is able to\nimprove the classification accuracy of ViT-Base on ImageNet-C from 55.6% to\n59.2% after adapting on just one batch of 64 samples. When adapting on 512\nsamples NEO beats all 7 TTA methods we compare against on ImageNet-C,\nImageNet-R and ImageNet-S and beats 6/7 on CIFAR-10-C, while using the least\namount of compute. NEO performs well on model calibration metrics and\nadditionally is able to adapt from 1 class to improve accuracy on 999 other\nclasses in ImageNet-C. On Raspberry Pi and Jetson Orin Nano devices, NEO\nreduces inference time by 63% and memory usage by 9% compared to baselines. Our\nresults based on 3 ViT architectures and 4 datasets show that NEO can be used\nefficiently and effectively for TTA.",
        "url": "http://arxiv.org/abs/2510.05635v1",
        "published_date": "2025-10-07T07:35:55+00:00",
        "updated_date": "2025-10-07T07:35:55+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Alexander Murphy",
            "Michal Danilowski",
            "Soumyajit Chatterjee",
            "Abhirup Ghosh"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "NEO is a hyperparameter-free fully Test-Time Adaptation method that significantly improves classification accuracy and model calibration metrics on various datasets while using minimal compute resources.",
        "tldr_zh": "NEO是一种无超参数的完全测试时适应方法，显著提高了各种数据集上的分类准确度和模型校准指标，同时使用最少的计算资源.",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Beyond Spectral Peaks: Interpreting the Cues Behind Synthetic Image Detection",
        "summary": "Over the years, the forensics community has proposed several deep\nlearning-based detectors to mitigate the risks of generative AI. Recently,\nfrequency-domain artifacts (particularly periodic peaks in the magnitude\nspectrum), have received significant attention, as they have been often\nconsidered a strong indicator of synthetic image generation. However,\nstate-of-the-art detectors are typically used as black-boxes, and it still\nremains unclear whether they truly rely on these peaks. This limits their\ninterpretability and trust. In this work, we conduct a systematic study to\naddress this question. We propose a strategy to remove spectral peaks from\nimages and analyze the impact of this operation on several detectors. In\naddition, we introduce a simple linear detector that relies exclusively on\nfrequency peaks, providing a fully interpretable baseline free from the\nconfounding influence of deep learning. Our findings reveal that most detectors\nare not fundamentally dependent on spectral peaks, challenging a widespread\nassumption in the field and paving the way for more transparent and reliable\nforensic tools.",
        "url": "http://arxiv.org/abs/2510.05633v1",
        "published_date": "2025-10-07T07:33:47+00:00",
        "updated_date": "2025-10-07T07:33:47+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CR"
        ],
        "authors": [
            "Sara Mandelli",
            "Diego Vila-Portela",
            "David Vázquez-Padín",
            "Paolo Bestagini",
            "Fernando Pérez-González"
        ],
        "ai_categories": [
            "GAN",
            "AIGC"
        ],
        "tldr": "The paper investigates whether deep learning detectors truly rely on spectral peaks in frequency-domain artifacts to detect synthetic images, challenging a widespread assumption in the field.",
        "tldr_zh": "本文研究了深度学习检测器是否真正依赖频域伪影中的频率峰值来检测合成图像，挑战了该领域的普遍假设。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Efficient Conditional Generation on Scale-based Visual Autoregressive Models",
        "summary": "Recent advances in autoregressive (AR) models have demonstrated their\npotential to rival diffusion models in image synthesis. However, for complex\nspatially-conditioned generation, current AR approaches rely on fine-tuning the\npre-trained model, leading to significant training costs. In this paper, we\npropose the Efficient Control Model (ECM), a plug-and-play framework featuring\na lightweight control module that introduces control signals via a distributed\narchitecture. This architecture consists of context-aware attention layers that\nrefine conditional features using real-time generated tokens, and a shared\ngated feed-forward network (FFN) designed to maximize the utilization of its\nlimited capacity and ensure coherent control feature learning. Furthermore,\nrecognizing the critical role of early-stage generation in determining semantic\nstructure, we introduce an early-centric sampling strategy that prioritizes\nlearning early control sequences. This approach reduces computational cost by\nlowering the number of training tokens per iteration, while a complementary\ntemperature scheduling during inference compensates for the resulting\ninsufficient training of late-stage tokens. Extensive experiments on\nscale-based AR models validate that our method achieves high-fidelity and\ndiverse control over image generation, surpassing existing baselines while\nsignificantly improving both training and inference efficiency.",
        "url": "http://arxiv.org/abs/2510.05610v1",
        "published_date": "2025-10-07T06:27:03+00:00",
        "updated_date": "2025-10-07T06:27:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaqi Liu",
            "Tao Huang",
            "Chang Xu"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper proposes a new Efficient Control Model for conditional image generation, optimizing training and inference efficiency while achieving high-fidelity results.",
        "tldr_zh": "本文提出了一种新的有效控制模型，用于条件图像生成，优化训练和推断效率同时实现高保真度结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval",
        "summary": "Existing Visual Language Models (VLMs) suffer structural limitations where a\nfew low contribution tokens may excessively capture global semantics,\ndominating the information aggregation process and suppressing the\ndiscriminative features in text-driven image retrieval tasks. To address this,\nwe introduce \\textbf{CalibCLIP}, a training-free method designed to calibrate\nthe suppressive effect of dominant tokens. Specifically, in the visual space,\nwe propose the Contrastive Visual Enhancer (CVE), which decouples visual\nfeatures into target and low information regions. Subsequently, it identifies\ndominant tokens and dynamically suppresses their representations.In the textual\nspace, we introduce the Discriminative Concept Calibrator (DCC), which aims to\ndifferentiate between general and discriminative concepts within the text\nquery. By mitigating the challenges posed by generic concepts and improving the\nrepresentations of discriminative concepts, DCC strengthens the differentiation\namong similar samples. Finally, extensive experiments demonstrate consistent\nimprovements across seven benchmarks spanning three image retrieval tasks,\nunderscoring the effectiveness of CalibCLIP. Code is available at:\nhttps://github.com/kangbin98/CalibCLIP",
        "url": "http://arxiv.org/abs/2510.05586v1",
        "published_date": "2025-10-07T05:16:29+00:00",
        "updated_date": "2025-10-07T05:16:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bin Kang",
            "Bin Chen",
            "Junjie Wang",
            "Yulin Li",
            "Junzhi Zhao",
            "Zhuotao Tian"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces CalibCLIP, a method to address limitations in existing Visual Language Models for text-driven image retrieval tasks by calibrating the suppressive effect of dominant tokens.",
        "tldr_zh": "本文介绍了CalibCLIP，一种能够校准主导token抑制效果的方法，以解决现有视觉语言模型在文本驱动图像检索任务中的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video",
        "summary": "Digitizing the physical world into accurate simulation-ready virtual\nenvironments offers significant opportunities in a variety of fields such as\naugmented and virtual reality, gaming, and robotics. However, current 3D\nreconstruction and scene-understanding methods commonly fall short in one or\nmore critical aspects, such as geometry completeness, object interactivity,\nphysical plausibility, photorealistic rendering, or realistic physical\nproperties for reliable dynamic simulation. To address these limitations, we\nintroduce HoloScene, a novel interactive 3D reconstruction framework that\nsimultaneously achieves these requirements. HoloScene leverages a comprehensive\ninteractive scene-graph representation, encoding object geometry, appearance,\nand physical properties alongside hierarchical and inter-object relationships.\nReconstruction is formulated as an energy-based optimization problem,\nintegrating observational data, physical constraints, and generative priors\ninto a unified, coherent objective. Optimization is efficiently performed via a\nhybrid approach combining sampling-based exploration with gradient-based\nrefinement. The resulting digital twins exhibit complete and precise geometry,\nphysical stability, and realistic rendering from novel viewpoints. Evaluations\nconducted on multiple benchmark datasets demonstrate superior performance,\nwhile practical use-cases in interactive gaming and real-time digital-twin\nmanipulation illustrate HoloScene's broad applicability and effectiveness.\nProject page: https://xiahongchi.github.io/HoloScene.",
        "url": "http://arxiv.org/abs/2510.05560v1",
        "published_date": "2025-10-07T04:12:18+00:00",
        "updated_date": "2025-10-07T04:12:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hongchi Xia",
            "Chih-Hao Lin",
            "Hao-Yu Hsu",
            "Quentin Leboutet",
            "Katelyn Gao",
            "Michael Paulitsch",
            "Benjamin Ummenhofer",
            "Shenlong Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "HoloScene is a novel interactive 3D reconstruction framework that addresses limitations in current methods by achieving complete geometry, physical stability, realistic rendering, and interactive capabilities.",
        "tldr_zh": "HoloScene是一种新颖的交互式3D重建框架，通过实现完整的几何形状、物理稳定性、逼真渲染和交互功能，解决了当前方法的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics",
        "summary": "Object recognition and motion understanding are key components of perception\nthat complement each other. While self-supervised learning methods have shown\npromise in their ability to learn from unlabeled data, they have primarily\nfocused on obtaining rich representations for either recognition or motion\nrather than both in tandem. On the other hand, latent dynamics modeling has\nbeen used in decision making to learn latent representations of observations\nand their transformations over time for control and planning tasks. In this\nwork, we present Midway Network, a new self-supervised learning architecture\nthat is the first to learn strong visual representations for both object\nrecognition and motion understanding solely from natural videos, by extending\nlatent dynamics modeling to this domain. Midway Network leverages a midway\ntop-down path to infer motion latents between video frames, as well as a dense\nforward prediction objective and hierarchical structure to tackle the complex,\nmulti-object scenes of natural videos. We demonstrate that after pretraining on\ntwo large-scale natural video datasets, Midway Network achieves strong\nperformance on both semantic segmentation and optical flow tasks relative to\nprior self-supervised learning methods. We also show that Midway Network's\nlearned dynamics can capture high-level correspondence via a novel analysis\nmethod based on forward feature perturbation.",
        "url": "http://arxiv.org/abs/2510.05558v1",
        "published_date": "2025-10-07T04:07:44+00:00",
        "updated_date": "2025-10-07T04:07:44+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Christopher Hoang",
            "Mengye Ren"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "Midway Network is a self-supervised learning architecture that learns strong visual representations for object recognition and motion understanding from natural videos by extending latent dynamics modeling.",
        "tldr_zh": "Midway Network 是一种自监督学习体系结构，通过扩展潜在动力学建模，从自然视频中学习强大的物体识别和运动理解的视觉表示。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "nnSAM2: nnUNet-Enhanced One-Prompt SAM2 for Few-shot Multi-Modality Segmentation and Composition Analysis of Lumbar Paraspinal Muscles",
        "summary": "Purpose: To develop and validate No-New SAM2 (nnsam2) for few-shot\nsegmentation of lumbar paraspinal muscles using only a single annotated slice\nper dataset, and to assess its statistical comparability with expert\nmeasurements across multi-sequence MRI and multi-protocol CT.\n  Methods: We retrospectively analyzed 1,219 scans (19,439 slices) from 762\nparticipants across six datasets. Six slices (one per dataset) served as\nlabeled examples, while the remaining 19,433 slices were used for testing. In\nthis minimal-supervision setting, nnsam2 used single-slice SAM2 prompts to\ngenerate pseudo-labels, which were pooled across datasets and refined through\nthree sequential, independent nnU-Net models. Segmentation performance was\nevaluated using the Dice similarity coefficient (DSC), and automated\nmeasurements-including muscle volume, fat ratio, and CT attenuation-were\nassessed with two one-sided tests (TOST) and intraclass correlation\ncoefficients (ICC).\n  Results: nnsam2 outperformed vanilla SAM2, its medical variants,\nTotalSegmentator, and the leading few-shot method, achieving DSCs of 0.94-0.96\non MR images and 0.92-0.93 on CT. Automated and expert measurements were\nstatistically equivalent for muscle volume (MRI/CT), CT attenuation, and Dixon\nfat ratio (TOST, P < 0.05), with consistently high ICCs (0.86-1.00).\n  Conclusion: We developed nnsam2, a state-of-the-art few-shot framework for\nmulti-modality LPM segmentation, producing muscle volume (MRI/CT), attenuation\n(CT), and fat ratio (Dixon MRI) measurements that were statistically comparable\nto expert references. Validated across multimodal, multicenter, and\nmultinational cohorts, and released with open code and data, nnsam2\ndemonstrated high annotation efficiency, robust generalizability, and\nreproducibility.",
        "url": "http://arxiv.org/abs/2510.05555v1",
        "published_date": "2025-10-07T03:53:47+00:00",
        "updated_date": "2025-10-07T03:53:47+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Zhongyi Zhang",
            "Julie A. Hides",
            "Enrico De Martino",
            "Abdul Joseph Fofanah",
            "Gervase Tuxworth"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces nnSAM2, a few-shot framework for multi-modality segmentation of lumbar paraspinal muscles, which outperforms existing methods and shows statistical comparability with expert measurements.",
        "tldr_zh": "本文介绍了nnSAM2，一种多模态分割腰椎旁肌的少样本学习框架，优于现有方法，并显示了与专家测量的统计可比性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Seeing the Big Picture: Evaluating Multimodal LLMs' Ability to Interpret and Grade Handwritten Student Work",
        "summary": "Recent advances in multimodal large language models (MLLMs) raise the\nquestion of their potential for grading, analyzing, and offering feedback on\nhandwritten student classwork. This capability would be particularly beneficial\nin elementary and middle-school mathematics education, where most work remains\nhandwritten, because seeing students' full working of a problem provides\nvaluable insights into their learning processes, but is extremely\ntime-consuming to grade. We present two experiments investigating MLLM\nperformance on handwritten student mathematics classwork. Experiment A examines\n288 handwritten responses from Ghanaian middle school students solving\narithmetic problems with objective answers. In this context, models achieved\nnear-human accuracy (95%, k = 0.90) but exhibited occasional errors that human\neducators would be unlikely to make. Experiment B evaluates 150 mathematical\nillustrations from American elementary students, where the drawings are the\nanswer to the question. These tasks lack single objective answers and require\nsophisticated visual interpretation as well as pedagogical judgment in order to\nanalyze and evaluate them. We attempted to separate MLLMs' visual capabilities\nfrom their pedagogical abilities by first asking them to grade the student\nillustrations directly, and then by augmenting the image with a detailed human\ndescription of the illustration. We found that when the models had to analyze\nthe student illustrations directly, they struggled, achieving only k = 0.20\nwith ground truth scores, but when given human descriptions, their agreement\nlevels improved dramatically to k = 0.47, which was in line with human-to-human\nagreement levels. This gap suggests MLLMs can \"see\" and interpret arithmetic\nwork relatively well, but still struggle to \"see\" student mathematical\nillustrations.",
        "url": "http://arxiv.org/abs/2510.05538v1",
        "published_date": "2025-10-07T02:59:18+00:00",
        "updated_date": "2025-10-07T02:59:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Owen Henkel",
            "Bill Roberts",
            "Doug Jaffe",
            "Laurence Holt"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper explores the ability of multimodal large language models to interpret and grade handwritten student work in mathematics, showing promising results but also highlighting areas where improvement is needed.",
        "tldr_zh": "本文探讨了多模式大型语言模型在解释和评分手写学生数学作业方面的能力，结果表现出有希望的成果，但也强调了需要改进的领域。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Teamwork: Collaborative Diffusion with Low-rank Coordination and Adaptation",
        "summary": "Large pretrained diffusion models can provide strong priors beneficial for\nmany graphics applications. However, generative applications such as neural\nrendering and inverse methods such as SVBRDF estimation and intrinsic image\ndecomposition require additional input or output channels. Current solutions\nfor channel expansion are often application specific and these solutions can be\ndifficult to adapt to different diffusion models or new tasks. This paper\nintroduces Teamwork: a flexible and efficient unified solution for jointly\nincreasing the number of input and output channels as well as adapting a\npretrained diffusion model to new tasks. Teamwork achieves channel expansion\nwithout altering the pretrained diffusion model architecture by coordinating\nand adapting multiple instances of the base diffusion model (\\ie, teammates).\nWe employ a novel variation of Low Rank-Adaptation (LoRA) to jointly address\nboth adaptation and coordination between the different teammates. Furthermore\nTeamwork supports dynamic (de)activation of teammates. We demonstrate the\nflexibility and efficiency of Teamwork on a variety of generative and inverse\ngraphics tasks such as inpainting, single image SVBRDF estimation, intrinsic\ndecomposition, neural shading, and intrinsic image synthesis.",
        "url": "http://arxiv.org/abs/2510.05532v1",
        "published_date": "2025-10-07T02:44:57+00:00",
        "updated_date": "2025-10-07T02:44:57+00:00",
        "categories": [
            "cs.CV",
            "cs.GR",
            "cs.LG"
        ],
        "authors": [
            "Sam Sartor",
            "Pieter Peers"
        ],
        "ai_categories": [
            "LoRA",
            "Diffusion",
            "GAN",
            "Dataset"
        ],
        "tldr": "Teamwork introduces a flexible and efficient solution for increasing input and output channels of pretrained diffusion models for graphics applications without altering the model architecture.",
        "tldr_zh": "Teamwork引入了一种灵活高效的解决方案，可以增加预训练扩散模型的输入和输出通道，面向图形应用，而无需改变模型架构。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Be Tangential to Manifold: Discovering Riemannian Metric for Diffusion Models",
        "summary": "Diffusion models are powerful deep generative models (DGMs) that generate\nhigh-fidelity, diverse content. However, unlike classical DGMs, they lack an\nexplicit, tractable low-dimensional latent space that parameterizes the data\nmanifold. This absence limits manifold-aware analysis and operations, such as\ninterpolation and editing. Existing interpolation methods for diffusion models\ntypically follow paths through high-density regions, which are not necessarily\naligned with the data manifold and can yield perceptually unnatural\ntransitions. To exploit the data manifold learned by diffusion models, we\npropose a novel Riemannian metric on the noise space, inspired by recent\nfindings that the Jacobian of the score function captures the tangent spaces to\nthe local data manifold. This metric encourages geodesics in the noise space to\nstay within or run parallel to the learned data manifold. Experiments on image\ninterpolation show that our metric produces perceptually more natural and\nfaithful transitions than existing density-based and naive baselines.",
        "url": "http://arxiv.org/abs/2510.05509v1",
        "published_date": "2025-10-07T01:54:47+00:00",
        "updated_date": "2025-10-07T01:54:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shinnosuke Saito",
            "Takashi Matsubara"
        ],
        "ai_categories": [
            "Diffusion"
        ],
        "tldr": "The paper proposes a novel Riemannian metric for diffusion models to improve image interpolation, resulting in more natural transitions.",
        "tldr_zh": "该论文提出了一种新颖的黎曼度量，用于改进图像插值，实现更加自然的过渡。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ArchitectHead: Continuous Level of Detail Control for 3D Gaussian Head Avatars",
        "summary": "3D Gaussian Splatting (3DGS) has enabled photorealistic and real-time\nrendering of 3D head avatars. Existing 3DGS-based avatars typically rely on\ntens of thousands of 3D Gaussian points (Gaussians), with the number of\nGaussians fixed after training. However, many practical applications require\nadjustable levels of detail (LOD) to balance rendering efficiency and visual\nquality. In this work, we propose \"ArchitectHead\", the first framework for\ncreating 3D Gaussian head avatars that support continuous control over LOD. Our\nkey idea is to parameterize the Gaussians in a 2D UV feature space and propose\na UV feature field composed of multi-level learnable feature maps to encode\ntheir latent features. A lightweight neural network-based decoder then\ntransforms these latent features into 3D Gaussian attributes for rendering.\nArchitectHead controls the number of Gaussians by dynamically resampling\nfeature maps from the UV feature field at the desired resolutions. This method\nenables efficient and continuous control of LOD without retraining.\nExperimental results show that ArchitectHead achieves state-of-the-art (SOTA)\nquality in self and cross-identity reenactment tasks at the highest LOD, while\nmaintaining near SOTA performance at lower LODs. At the lowest LOD, our method\nuses only 6.2\\% of the Gaussians while the quality degrades moderately (L1 Loss\n+7.9\\%, PSNR --0.97\\%, SSIM --0.6\\%, LPIPS Loss +24.1\\%), and the rendering\nspeed nearly doubles.",
        "url": "http://arxiv.org/abs/2510.05488v1",
        "published_date": "2025-10-07T01:08:28+00:00",
        "updated_date": "2025-10-07T01:08:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Peizhi Yan",
            "Rabab Ward",
            "Qiang Tang",
            "Shan Du"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper introduces ArchitectHead, a framework for creating 3D Gaussian head avatars with adjustable levels of detail, enabling efficient and continuous control without retraining.",
        "tldr_zh": "本文介绍了ArchitectHead，一种用于创建支持可调级别细节的3D高斯头像的框架，实现了高效连续控制而无需重新训练。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Personalizing Retrieval using Joint Embeddings or \"the Return of Fluffy\"",
        "summary": "The goal of this paper is to be able to retrieve images using a compound\nquery that combines object instance information from an image, with a natural\ntext description of what that object is doing or where it is. For example, to\nretrieve an image of \"Fluffy the unicorn (specified by an image) on someone's\nhead\". To achieve this we design a mapping network that can \"translate\" from a\nlocal image embedding (of the object instance) to a text token, such that the\ncombination of the token and a natural language query is suitable for CLIP\nstyle text encoding, and image retrieval. Generating a text token in this\nmanner involves a simple training procedure, that only needs to be performed\nonce for each object instance. We show that our approach of using a trainable\nmapping network, termed pi-map, together with frozen CLIP text and image\nencoders, improves the state of the art on two benchmarks designed to assess\npersonalized retrieval.",
        "url": "http://arxiv.org/abs/2510.05411v1",
        "published_date": "2025-10-06T22:08:30+00:00",
        "updated_date": "2025-10-06T22:08:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bruno Korbar",
            "Andrew Zisserman"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a method to personalize image retrieval using joint embeddings, improving state-of-the-art performance on specific benchmarks.",
        "tldr_zh": "本文介绍了一种使用联合嵌入个性化图片检索的方法，提高了特定基准测试的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "See the past: Time-Reversed Scene Reconstruction from Thermal Traces Using Visual Language Models",
        "summary": "Recovering the past from present observations is an intriguing challenge with\npotential applications in forensics and scene analysis. Thermal imaging,\noperating in the infrared range, provides access to otherwise invisible\ninformation. Since humans are typically warmer (37 C -98.6 F) than their\nsurroundings, interactions such as sitting, touching, or leaning leave residual\nheat traces. These fading imprints serve as passive temporal codes, allowing\nfor the inference of recent events that exceed the capabilities of RGB cameras.\nThis work proposes a time-reversed reconstruction framework that uses paired\nRGB and thermal images to recover scene states from a few seconds earlier. The\nproposed approach couples Visual-Language Models (VLMs) with a constrained\ndiffusion process, where one VLM generates scene descriptions and another\nguides image reconstruction, ensuring semantic and structural consistency. The\nmethod is evaluated in three controlled scenarios, demonstrating the\nfeasibility of reconstructing plausible past frames up to 120 seconds earlier,\nproviding a first step toward time-reversed imaging from thermal traces.",
        "url": "http://arxiv.org/abs/2510.05408v1",
        "published_date": "2025-10-06T21:57:26+00:00",
        "updated_date": "2025-10-06T21:57:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kebin Contreras",
            "Luis Toscano-Palomino",
            "Mauro Dalla Mura",
            "Jorge Bacca"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion"
        ],
        "tldr": "The paper proposes a framework using paired RGB and thermal images to reconstruct scene states from a few seconds ago, leveraging Visual-Language Models.",
        "tldr_zh": "本文提出了一种框架，利用配对的RGB和热像图像从几秒钟前重建场景状态，利用视觉语言模型。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation",
        "summary": "Training-free acceleration has emerged as an advanced research area in video\ngeneration based on diffusion models. The redundancy of latents in diffusion\nmodel inference provides a natural entry point for acceleration. In this paper,\nwe decompose the inference process into the encoding, denoising, and decoding\nstages, and observe that cache-based acceleration methods often lead to\nsubstantial memory surges in the latter two stages. To address this problem, we\nanalyze the characteristics of inference across different stages and propose\nstage-specific strategies for reducing memory consumption: 1) Asynchronous\nCache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same\ntime, we ensure that the time overhead introduced by these three strategies\nremains lower than the acceleration gains themselves. Compared with the\nbaseline, our approach achieves faster inference speed and lower memory usage,\nwhile maintaining quality degradation within an acceptable range. The Code is\navailable at https://github.com/NKUShaw/LightCache .",
        "url": "http://arxiv.org/abs/2510.05367v1",
        "published_date": "2025-10-06T20:54:44+00:00",
        "updated_date": "2025-10-06T20:54:44+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yang Xiao",
            "Gen Li",
            "Kaiyuan Deng",
            "Yushu Wu",
            "Zheng Zhan",
            "Yanzhi Wang",
            "Xiaolong Ma",
            "Bo Hui"
        ],
        "ai_categories": [
            "GAN",
            "Transformer"
        ],
        "tldr": "The paper proposes a method called LightCache for memory-efficient and training-free acceleration in video generation based on diffusion models. It achieves faster inference speed and lower memory usage while maintaining acceptable quality.",
        "tldr_zh": "该论文提出了一种名为LightCache的方法，用于在基于扩散模型的视频生成中实现高效节省内存且无需训练的加速。它实现了更快的推理速度和更低的内存使用量，同时保持了可接受的质量。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Mitigating Diffusion Model Hallucinations with Dynamic Guidance",
        "summary": "Diffusion models, despite their impressive demos, often produce hallucinatory\nsamples with structural inconsistencies that lie outside of the support of the\ntrue data distribution. Such hallucinations can be attributed to excessive\nsmoothing between modes of the data distribution. However, semantic\ninterpolations are often desirable and can lead to generation diversity, thus\nwe believe a more nuanced solution is required. In this work, we introduce\nDynamic Guidance, which tackles this issue. Dynamic Guidance mitigates\nhallucinations by selectively sharpening the score function only along the\npre-determined directions known to cause artifacts, while preserving valid\nsemantic variations. To our knowledge, this is the first approach that\naddresses hallucinations at generation time rather than through post-hoc\nfiltering. Dynamic Guidance substantially reduces hallucinations on both\ncontrolled and natural image datasets, significantly outperforming baselines.",
        "url": "http://arxiv.org/abs/2510.05356v1",
        "published_date": "2025-10-06T20:31:13+00:00",
        "updated_date": "2025-10-06T20:31:13+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Kostas Triaridis",
            "Alexandros Graikos",
            "Aggelina Chatziagapi",
            "Grigorios G. Chrysos",
            "Dimitris Samaras"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN"
        ],
        "tldr": "The paper introduces Dynamic Guidance to reduce hallucinations in diffusion models by selectively sharpening the score function along known artifact-causing directions, leading to improved image generation results.",
        "tldr_zh": "该论文引入了动态引导来减少扩散模型中的幻觉，通过选择性地加强分数函数沿着已知的引起异常的方向，从而改善图像生成结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Beyond Monolithic Rewards: A Hybrid and Multi-Aspect Reward Optimization for MLLM Alignment",
        "summary": "Aligning multimodal large language models (MLLMs) with human preferences\noften relies on single-signal, model-based reward methods. Such monolithic\nrewards often lack confidence calibration across domain-specific tasks, fail to\ncapture diverse aspects of human preferences, and require extensive data\nannotation and reward model training. In this work, we propose a hybrid reward\nmodeling framework that integrates complementary reward paradigms: (i)\nmodel-based rewards, where a learned reward model predicts scalar or vector\nscores from synthetic and human feedback, and (ii) rule-based rewards, where\ndomain-specific heuristics provide explicit correctness signals with\nconfidence. Beyond accuracy, we further incorporate multi-aspect rewards to\nenforce instruction adherence and introduce a generalized length-penalty reward\nto stabilize training and improve performance. The proposed framework provides\na flexible and effective approach to aligning MLLMs through reinforcement\nlearning policy optimization. Our experiments show consistent improvements\nacross different multimodal benchmarks when applying hybrid and multi-aspect\nreward modeling. Our best performing model in the 3B family achieves an overall\naverage improvement of ~9.5% across general and math reasoning tasks. Focusing\nspecifically on mathematical benchmarks, the model achieves a significant\naverage improvement of ~16%, highlighting its effectiveness in mathematical\nreasoning and problem solving.",
        "url": "http://arxiv.org/abs/2510.05283v1",
        "published_date": "2025-10-06T18:53:23+00:00",
        "updated_date": "2025-10-06T18:53:23+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Radha Gulhane",
            "Sathish Reddy Indurthi"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a hybrid reward modeling framework to align multimodal large language models with human preferences, showing consistent improvements in various benchmarks.",
        "tldr_zh": "本文提出了一种混合奖励建模框架，以使多模态大语言模型与人类偏好对齐，在各种基准测试中显示出一致的改进。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Medical Vision Language Models as Policies for Robotic Surgery",
        "summary": "Vision-based Proximal Policy Optimization (PPO) struggles with visual\nobservation-based robotic laparoscopic surgical tasks due to the\nhigh-dimensional nature of visual input, the sparsity of rewards in surgical\nenvironments, and the difficulty of extracting task-relevant features from raw\nvisual data. We introduce a simple approach integrating MedFlamingo, a medical\ndomain-specific Vision-Language Model, with PPO. Our method is evaluated on\nfive diverse laparoscopic surgery task environments in LapGym, using only\nendoscopic visual observations. MedFlamingo PPO outperforms and converges\nfaster compared to both standard vision-based PPO and OpenFlamingo PPO\nbaselines, achieving task success rates exceeding 70% across all environments,\nwith improvements ranging from 66.67% to 1114.29% compared to baseline. By\nprocessing task observations and instructions once per episode to generate\nhigh-level planning tokens, our method efficiently combines medical expertise\nwith real-time visual feedback. Our results highlight the value of specialized\nmedical knowledge in robotic surgical planning and decision-making.",
        "url": "http://arxiv.org/abs/2510.06064v1",
        "published_date": "2025-10-07T15:54:34+00:00",
        "updated_date": "2025-10-07T15:54:34+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Akshay Muppidi",
            "Martin Radfar"
        ],
        "ai_categories": [
            "Transformer",
            "Medical",
            "Dataset"
        ],
        "tldr": "The paper introduces a method that combines a medical domain-specific Vision-Language Model with Proximal Policy Optimization (PPO) for robotic surgery tasks, achieving high success rates and faster convergence compared to baseline methods.",
        "tldr_zh": "该论文介绍了一种方法，将医学领域特定的视觉-语言模型与近端策略优化（PPO）相结合，用于机器人手术任务，在成功率和收敛速度上优于基线方法。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7.75
    },
    {
        "title": "Compact Multi-level-prior Tensor Representation for Hyperspectral Image Super-resolution",
        "summary": "Fusing a hyperspectral image with a multispectral image acquired over the\nsame scene, \\textit{i.e.}, hyperspectral image super-resolution, has become a\npopular computational way to access the latent high-spatial-spectral-resolution\nimage. To date, a variety of fusion methods have been proposed, among which the\ntensor-based ones have testified that multiple priors, such as multidimensional\nlow-rankness and spatial total variation at multiple levels, effectively drive\nthe fusion process. However, existing tensor-based models can only effectively\nleverage one or two priors at one or two levels, since simultaneously\nincorporating multi-level priors inevitably increases model complexity. This\nintroduces challenges in both balancing the weights of different priors and\noptimizing multi-block structures. Concerning this, we present a novel\nhyperspectral super-resolution model compactly characterizing these multi-level\npriors of hyperspectral images within the tensor framework. Firstly, the\nproposed model decouples the spectral low-rankness and spatial priors by\ncasting the latent high-spatial-spectral-resolution image into spectral\nsubspace and spatial maps via block term decomposition. Secondly, these spatial\nmaps are stacked as the spatial tensor encoding the high-order spatial\nlow-rankness and smoothness priors, which are co-modeled via the proposed\nnon-convex mode-shuffled tensor correlated total variation. Finally, we draw\ninspiration from the linearized alternating direction method of multipliers to\ndesign an efficient algorithm to optimize the resulting model, theoretically\nproving its Karush-Kuhn-Tucker convergence under mild conditions. Experiments\non multiple datasets demonstrate the effectiveness of the proposed algorithm.\nThe code implementation will be available from https://github.com/WongYinJ.",
        "url": "http://arxiv.org/abs/2510.06098v1",
        "published_date": "2025-10-07T16:26:34+00:00",
        "updated_date": "2025-10-07T16:26:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yinjian Wang",
            "Wei Li",
            "Yuanyuan Gui",
            "Gemine Vivone"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel model for hyperspectral image super-resolution using tensor representation to effectively leverage multi-level priors and optimize the fusion process.",
        "tldr_zh": "本文引入了一种新颖的模型，使用张量表示进行高光谱图像超分辨率，有效利用多级先验和优化融合过程。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via Tree-based Group Relative Policy Optimization",
        "summary": "Understanding hour-long videos with multi-modal large language models\n(MM-LLMs) enriches the landscape of human-centered AI applications. However,\nfor end-to-end video understanding with LLMs, uniformly sampling video frames\nresults in LLMs being overwhelmed by a vast amount of irrelevant information as\nvideo length increases. Existing hierarchical key frame extraction methods\nimprove the accuracy of video understanding but still face two critical\nchallenges. 1) How can the interference of extensive redundant information in\nlong videos be mitigated? 2) How can a model dynamically adapt to complex\nhierarchical structures while accurately identifying key frames? To address\nthese issues, we propose VideoMiner, which iteratively segments, captions, and\nclusters long videos, forming a hierarchical tree structure. The proposed\nVideoMiner progresses from long videos to events to frames while preserving\ntemporal coherence, effectively addressing the first challenge. To precisely\nlocate key frames, we introduce T-GRPO, a tree-based group relative policy\noptimization in reinforcement learning method that guides the exploration of\nthe VideoMiner. The proposed T-GRPO is specifically designed for tree\nstructures, integrating spatiotemporal information at the event level while\nbeing guided by the question, thus solving the second challenge. We achieve\nsuperior performance in all long-video understanding tasks and uncover several\ninteresting insights. Our proposed T-GRPO surprisingly incentivizes the model\nto spontaneously generate a reasoning chain. Additionally, the designed tree\ngrowth auxin dynamically adjusts the expansion depth, obtaining accuracy and\nefficiency gains. The code is publicly available at\nhttps://github.com/caoxinye/VideoMiner.",
        "url": "http://arxiv.org/abs/2510.06040v1",
        "published_date": "2025-10-07T15:34:46+00:00",
        "updated_date": "2025-10-07T15:34:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xinye Cao",
            "Hongcan Guo",
            "Jiawen Qian",
            "Guoshun Nan",
            "Chao Wang",
            "Yuqi Pan",
            "Tianhao Hou",
            "Xiaojuan Wang",
            "Yutong Gao"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "VideoMiner proposes a method to iteratively ground key frames of hour-long videos via tree-based group relative policy optimization, achieving superior performance in video understanding tasks.",
        "tldr_zh": "VideoMiner提出了一种通过基于树结构的组相对策略优化来迭代地确定长视频的关键帧的方法，在视频理解任务中表现出优异性能。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario",
        "summary": "Reproducing cognitive development, group interaction, and long-term evolution\nin virtual classrooms remains a core challenge for educational AI, as real\nclassrooms integrate open-ended cognition, dynamic social interaction,\naffective factors, and multi-session development rarely captured together.\nExisting approaches mostly focus on short-term or single-agent settings,\nlimiting systematic study of classroom complexity and cross-task reuse. We\npresent EduVerse, the first user-defined multi-agent simulation space that\nsupports environment, agent, and session customization. A distinctive\nhuman-in-the-loop interface further allows real users to join the space. Built\non a layered CIE (Cognition-Interaction-Evolution) architecture, EduVerse\nensures individual consistency, authentic interaction, and longitudinal\nadaptation in cognition, emotion, and behavior-reproducing realistic classroom\ndynamics with seamless human-agent integration. We validate EduVerse in\nmiddle-school Chinese classes across three text genres, environments, and\nmultiple sessions. Results show: (1) Instructional alignment: simulated IRF\nrates (0.28-0.64) closely match real classrooms (0.37-0.49), indicating\npedagogical realism; (2) Group interaction and role differentiation: network\ndensity (0.27-0.40) with about one-third of peer links realized, while\nhuman-agent tasks indicate a balance between individual variability and\ninstructional stability; (3) Cross-session evolution: the positive transition\nrate R+ increase by 11.7% on average, capturing longitudinal shifts in\nbehavior, emotion, and cognition and revealing structured learning\ntrajectories. Overall, EduVerse balances realism, reproducibility, and\ninterpretability, providing a scalable platform for educational AI. The system\nwill be open-sourced to foster cross-disciplinary research.",
        "url": "http://arxiv.org/abs/2510.05650v1",
        "published_date": "2025-10-07T07:58:32+00:00",
        "updated_date": "2025-10-07T07:58:32+00:00",
        "categories": [
            "cs.CV",
            "cs.CY"
        ],
        "authors": [
            "Yiping Ma",
            "Shiyu Hu",
            "Buyuan Zhu",
            "Yipei Wang",
            "Yaxuan Kang",
            "Shiqing Liu",
            "Kang Hao Cheong"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces EduVerse, a user-defined multi-agent simulation space for education scenarios, aiming to reproduce realistic classroom dynamics and support cross-disciplinary research in educational AI.",
        "tldr_zh": "该论文介绍了EduVerse，这是一个用户定义的多智能体仿真空间，旨在重新现实现实教室动态，并支持教育人工智能的跨学科研究。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Overlap-aware segmentation for topological reconstruction of obscured objects",
        "summary": "The separation of overlapping objects presents a significant challenge in\nscientific imaging. While deep learning segmentation-regression algorithms can\npredict pixel-wise intensities, they typically treat all regions equally rather\nthan prioritizing overlap regions where attribution is most ambiguous. Recent\nadvances in instance segmentation show that weighting regions of pixel overlap\nin training can improve segmentation boundary predictions in regions of\noverlap, but this idea has not yet been extended to segmentation regression. We\naddress this with Overlap-Aware Segmentation of ImageS (OASIS): a new\nsegmentation-regression framework with a weighted loss function designed to\nprioritize regions of object-overlap during training, enabling extraction of\npixel intensities and topological features from heavily obscured objects. We\ndemonstrate OASIS in the context of the MIGDAL experiment, which aims to\ndirectly image the Migdal effect--a rare process where electron emission is\ninduced by nuclear scattering--in a low-pressure optical time projection\nchamber. This setting poses an extreme test case, as the target for\nreconstruction is a faint electron recoil track which is often heavily-buried\nwithin the orders-of-magnitude brighter nuclear recoil track. Compared to\nunweighted training, OASIS improves median intensity reconstruction errors from\n-32% to -14% for low-energy electron tracks (4-5 keV) and improves topological\nintersection-over-union scores from 0.828 to 0.855. These performance gains\ndemonstrate OASIS's ability to recover obscured signals in overlap-dominated\nregions. The framework provides a generalizable methodology for scientific\nimaging where pixels represent physical quantities and overlap obscures\nfeatures of interest. All code is openly available to facilitate cross-domain\nadoption.",
        "url": "http://arxiv.org/abs/2510.06194v1",
        "published_date": "2025-10-07T17:52:01+00:00",
        "updated_date": "2025-10-07T17:52:01+00:00",
        "categories": [
            "hep-ex",
            "astro-ph.IM",
            "cs.CV"
        ],
        "authors": [
            "J. Schueler",
            "H. M. Araújo",
            "S. N. Balashov",
            "J. E. Borg",
            "C. Brew",
            "F. M. Brunbauer",
            "C. Cazzaniga",
            "A. Cottle",
            "D. Edgeman",
            "C. D. Frost",
            "F. Garcia",
            "D. Hunt",
            "M. Kastriotou",
            "P. Knights",
            "H. Kraus",
            "A. Lindote",
            "M. Lisowska",
            "D. Loomba",
            "E. Lopez Asamar",
            "P. A. Majewski",
            "T. Marley",
            "C. McCabe",
            "L. Millins",
            "R. Nandakumar",
            "T. Neep",
            "F. Neves",
            "K. Nikolopoulos",
            "E. Oliveri",
            "A. Roy",
            "T. J. Sumner",
            "E. Tilly",
            "W. Thompson",
            "M. A. Vogiatzi"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces Overlap-Aware Segmentation of ImageS (OASIS) to address the challenge of separating overlapping objects in scientific imaging. It focuses on improving segmentation boundary predictions in regions of overlap for heavily obscured objects.",
        "tldr_zh": "本文介绍了Overlap-Aware Segmentation of ImageS (OASIS)，以应对科学成像中分离重叠对象的挑战。它专注于改善重叠区域的分割边界预测，用于提取被遮挡对象的像素强度和拓扑特征。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Smartphone-based iris recognition through high-quality visible-spectrum iris image capture.V2",
        "summary": "Smartphone-based iris recognition in the visible spectrum (VIS) remains\ndifficult due to illumination variability, pigmentation differences, and the\nabsence of standardized capture controls. This work presents a compact\nend-to-end pipeline that enforces ISO/IEC 29794-6 quality compliance at\nacquisition and demonstrates that accurate VIS iris recognition is feasible on\ncommodity devices. Using a custom Android application performing real-time\nframing, sharpness evaluation, and feedback, we introduce the CUVIRIS dataset\nof 752 compliant images from 47 subjects. A lightweight MobileNetV3-based\nmulti-task segmentation network (LightIrisNet) is developed for efficient\non-device processing, and a transformer matcher (IrisFormer) is adapted to the\nVIS domain. Under a standardized protocol and comparative benchmarking against\nprior CNN baselines, OSIRIS attains a TAR of 97.9% at FAR=0.01 (EER=0.76%),\nwhile IrisFormer, trained only on UBIRIS.v2, achieves an EER of 0.057% on\nCUVIRIS. The acquisition app, trained models, and a public subset of the\ndataset are released to support reproducibility. These results confirm that\nstandardized capture and VIS-adapted lightweight models enable accurate and\npractical iris recognition on smartphones.",
        "url": "http://arxiv.org/abs/2510.06170v1",
        "published_date": "2025-10-07T17:33:41+00:00",
        "updated_date": "2025-10-07T17:33:41+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Naveenkumar G Venkataswamy",
            "Yu Liu",
            "Soumyabrata Dey",
            "Stephanie Schuckers",
            "Masudul H Imtiaz"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper presents a smartphone-based iris recognition system using visible-spectrum images, achieving high accuracy through standardized capture controls and lightweight models.",
        "tldr_zh": "该论文提出了一种使用可见光谱图像的基于智能手机的虹膜识别系统，通过标准化的捕获控制和轻量级模型实现了高准确性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Reasoning under Vision: Understanding Visual-Spatial Cognition in Vision-Language Models for CAPTCHA",
        "summary": "CAPTCHA, originally designed to distinguish humans from robots, has evolved\ninto a real-world benchmark for assessing the spatial reasoning capabilities of\nvision-language models. In this work, we first show that step-by-step reasoning\nis crucial for vision-language models (VLMs) to solve CAPTCHAs, which represent\nhigh-difficulty spatial reasoning tasks, and that current commercial\nvision-language models still struggle with such reasoning. In particular, we\nobserve that most commercial VLMs (e.g., Gemini, Claude, GPT, etc.) fail to\neffectively solve CAPTCHAs and thus achieve low accuracy (around 21.9 percent).\nHowever, our findings indicate that requiring the model to perform step-by-step\nreasoning before generating the final coordinates can significantly enhance its\nsolving accuracy, underscoring the severity of the gap. To systematically study\nthis issue, we introduce CAPTCHA-X, the first real-world CAPTCHA benchmark with\nreasoning, covering seven categories of CAPTCHAs (such as Gobang, hCaptcha,\netc.) with step-by-step action solutions and grounding annotations. We further\ndefine five reasoning-oriented metrics that enable a comprehensive evaluation\nof models reasoning capabilities. To validate the effectiveness of reasoning,\nwe also propose a general agentic VLM-based framework that incorporates the\nmodels inherent reasoning abilities. Our method achieves state-of-the-art\nperformance across five high-difficulty CAPTCHA types, with an average solving\naccuracy of 83.9 percent, substantially surpassing existing baselines. These\nresults reveal the limitations of current models and highlight the importance\nof reasoning in advancing visual-spatial challenges in the future.",
        "url": "http://arxiv.org/abs/2510.06067v1",
        "published_date": "2025-10-07T15:56:21+00:00",
        "updated_date": "2025-10-07T15:56:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Python Song",
            "Luke Tenyi Chang",
            "Yun-Yun Tsai",
            "Penghui Li",
            "Junfeng Yang"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper explores the importance of step-by-step reasoning in vision-language models to solve CAPTCHAs, introducing a new benchmark and achieving significant performance gains.",
        "tldr_zh": "本文探讨了分步推理在视觉语言模型中解决CAPTCHA的重要性，引入了一个新的基准并取得了显著的性能提升。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Leveraging Vision Transformers for Enhanced Classification of Emotions using ECG Signals",
        "summary": "Biomedical signals provide insights into various conditions affecting the\nhuman body. Beyond diagnostic capabilities, these signals offer a deeper\nunderstanding of how specific organs respond to an individual's emotions and\nfeelings. For instance, ECG data can reveal changes in heart rate variability\nlinked to emotional arousal, stress levels, and autonomic nervous system\nactivity. This data offers a window into the physiological basis of our\nemotional states. Recent advancements in the field diverge from conventional\napproaches by leveraging the power of advanced transformer architectures, which\nsurpass traditional machine learning and deep learning methods. We begin by\nassessing the effectiveness of the Vision Transformer (ViT), a forefront model\nin image classification, for identifying emotions in imaged ECGs. Following\nthis, we present and evaluate an improved version of ViT, integrating both CNN\nand SE blocks, aiming to bolster performance on imaged ECGs associated with\nemotion detection. Our method unfolds in two critical phases: first, we apply\nadvanced preprocessing techniques for signal purification and converting\nsignals into interpretable images using continuous wavelet transform and power\nspectral density analysis; second, we unveil a performance-boosted vision\ntransformer architecture, cleverly enhanced with convolutional neural network\ncomponents, to adeptly tackle the challenges of emotion recognition. Our\nmethodology's robustness and innovation were thoroughly tested using ECG data\nfrom the YAAD and DREAMER datasets, leading to remarkable outcomes. For the\nYAAD dataset, our approach outperformed existing state-of-the-art methods in\nclassifying seven unique emotional states, as well as in valence and arousal\nclassification. Similarly, in the DREAMER dataset, our method excelled in\ndistinguishing between valence, arousal and dominance, surpassing current\nleading techniques.",
        "url": "http://arxiv.org/abs/2510.05826v1",
        "published_date": "2025-10-07T11:49:57+00:00",
        "updated_date": "2025-10-07T11:49:57+00:00",
        "categories": [
            "eess.SP",
            "cs.CV"
        ],
        "authors": [
            "Pubudu L. Indrasiri",
            "Bipasha Kashyap",
            "Pubudu N. Pathirana"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper explores using Vision Transformers with ECG signals to classify emotions, achieving high performance on datasets.",
        "tldr_zh": "本文探讨了使用视觉Transformer结合ECG信号进行情绪分类，在数据集上取得了较高的性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "ALISE: Annotation-Free LiDAR Instance Segmentation for Autonomous Driving",
        "summary": "The manual annotation of outdoor LiDAR point clouds for instance segmentation\nis extremely costly and time-consuming. Current methods attempt to reduce this\nburden but still rely on some form of human labeling. To completely eliminate\nthis dependency, we introduce ALISE, a novel framework that performs LiDAR\ninstance segmentation without any annotations. The central challenge is to\ngenerate high-quality pseudo-labels in a fully unsupervised manner. Our\napproach starts by employing Vision Foundation Models (VFMs), guided by text\nand images, to produce initial pseudo-labels. We then refine these labels\nthrough a dedicated spatio-temporal voting module, which combines 2D and 3D\nsemantics for both offline and online optimization. To achieve superior feature\nlearning, we further introduce two forms of semantic supervision: a set of 2D\nprior-based losses that inject visual knowledge into the 3D network, and a\nnovel prototype-based contrastive loss that builds a discriminative feature\nspace by exploiting 3D semantic consistency. This comprehensive design results\nin significant performance gains, establishing a new state-of-the-art for\nunsupervised 3D instance segmentation. Remarkably, our approach even\noutperforms MWSIS, a method that operates with supervision from ground-truth\n(GT) 2D bounding boxes by a margin of 2.53% in mAP (50.95% vs. 48.42%).",
        "url": "http://arxiv.org/abs/2510.05752v1",
        "published_date": "2025-10-07T10:15:18+00:00",
        "updated_date": "2025-10-07T10:15:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yongxuan Lyu",
            "Guangfeng Jiang",
            "Hongsi Liu",
            "Jun Liu"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "ALISE is a novel framework for LiDAR instance segmentation without human annotations, achieving superior performance in unsupervised 3D instance segmentation.",
        "tldr_zh": "ALISE是一个新颖的框架，用于LiDAR实例分割，无需人工标注，在无监督3D实例分割中表现出色。",
        "relevance_score": 4,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Neighborhood-Adaptive Generalized Linear Graph Embedding with Latent Pattern Mining",
        "summary": "Graph embedding has been widely applied in areas such as network analysis,\nsocial network mining, recommendation systems, and bioinformatics. However,\ncurrent graph construction methods often require the prior definition of\nneighborhood size, limiting the effective revelation of potential structural\ncorrelations in the data. Additionally, graph embedding methods using linear\nprojection heavily rely on a singular pattern mining approach, resulting in\nrelative weaknesses in adapting to different scenarios. To address these\nchallenges, we propose a novel model, Neighborhood-Adaptive Generalized Linear\nGraph Embedding (NGLGE), grounded in latent pattern mining. This model\nintroduces an adaptive graph learning method tailored to the neighborhood,\neffectively revealing intrinsic data correlations. Simultaneously, leveraging a\nreconstructed low-rank representation and imposing $\\ell_{2,0}$ norm constraint\non the projection matrix allows for flexible exploration of additional pattern\ninformation. Besides, an efficient iterative solving algorithm is derived for\nthe proposed model. Comparative evaluations on datasets from diverse scenarios\ndemonstrate the superior performance of our model compared to state-of-the-art\nmethods.",
        "url": "http://arxiv.org/abs/2510.05719v1",
        "published_date": "2025-10-07T09:37:29+00:00",
        "updated_date": "2025-10-07T09:37:29+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "S. Peng",
            "L. Hu",
            "W. Zhang",
            "B. Jie",
            "Y. Luo"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel graph embedding model, NGLGE, that adapts to the neighborhood and incorporates latent pattern mining for improved performance compared to existing methods.",
        "tldr_zh": "该论文引入了一种新颖的图嵌入模型，NGLGE，它适应邻域并融入潜在模式挖掘，相较于现有方法表现更好。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Ocular-Induced Abnormal Head Posture: Diagnosis and Missing Data Imputation",
        "summary": "Ocular-induced abnormal head posture (AHP) is a compensatory mechanism that\narises from ocular misalignment conditions, such as strabismus, enabling\npatients to reduce diplopia and preserve binocular vision. Early diagnosis\nminimizes morbidity and secondary complications such as facial asymmetry;\nhowever, current clinical assessments remain largely subjective and are further\ncomplicated by incomplete medical records. This study addresses both challenges\nthrough two complementary deep learning frameworks. First, AHP-CADNet is a\nmulti-level attention fusion framework for automated diagnosis that integrates\nocular landmarks, head pose features, and structured clinical attributes to\ngenerate interpretable predictions. Second, a curriculum learning-based\nimputation framework is designed to mitigate missing data by progressively\nleveraging structured variables and unstructured clinical notes to enhance\ndiagnostic robustness under realistic data conditions. Evaluation on the\nPoseGaze-AHP dataset demonstrates robust diagnostic performance. AHP-CADNet\nachieves 96.9-99.0 percent accuracy across classification tasks and low\nprediction errors for continuous variables, with MAE ranging from 0.103 to\n0.199 and R2 exceeding 0.93. The imputation framework maintains high accuracy\nacross all clinical variables (93.46-99.78 percent with PubMedBERT), with\nclinical dependency modeling yielding significant improvements (p < 0.001).\nThese findings confirm the effectiveness of both frameworks for automated\ndiagnosis and recovery from missing data in clinical settings.",
        "url": "http://arxiv.org/abs/2510.05649v1",
        "published_date": "2025-10-07T07:51:59+00:00",
        "updated_date": "2025-10-07T07:51:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Saja Al-Dabet",
            "Sherzod Turaev",
            "Nazar Zaki",
            "Arif O. Khan",
            "Luai Eldweik"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper proposes deep learning frameworks for automated diagnosis of ocular-induced abnormal head posture and missing data imputation in clinical settings.",
        "tldr_zh": "本文提出了用于临床环境中自动诊断眼部诱导的异常头部姿势和缺失数据插补的深度学习框架。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Combined Hyperbolic and Euclidean Soft Triple Loss Beyond the Single Space Deep Metric Learning",
        "summary": "Deep metric learning (DML) aims to learn a neural network mapping data to an\nembedding space, which can represent semantic similarity between data points.\nHyperbolic space is attractive for DML since it can represent richer\nstructures, such as tree structures. DML in hyperbolic space is based on\npair-based loss or unsupervised regularization loss. On the other hand,\nsupervised proxy-based losses in hyperbolic space have not been reported yet\ndue to some issues in applying proxy-based losses in a hyperbolic space.\nHowever, proxy-based losses are attractive for large-scale datasets since they\nhave less training complexity. To address these, this paper proposes the\nCombined Hyperbolic and Euclidean Soft Triple (CHEST) loss. CHEST loss is\ncomposed of the proxy-based losses in hyperbolic and Euclidean spaces and the\nregularization loss based on hyperbolic hierarchical clustering. We find that\nthe combination of hyperbolic and Euclidean spaces improves DML accuracy and\nlearning stability for both spaces. Finally, we evaluate the CHEST loss on four\nbenchmark datasets, achieving a new state-of-the-art performance.",
        "url": "http://arxiv.org/abs/2510.05643v1",
        "published_date": "2025-10-07T07:42:49+00:00",
        "updated_date": "2025-10-07T07:42:49+00:00",
        "categories": [
            "cs.CV",
            "68T10,",
            "I.2.10; I.4.7"
        ],
        "authors": [
            "Shozo Saeki",
            "Minoru Kawahara",
            "Hirohisa Aman"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces the Combined Hyperbolic and Euclidean Soft Triple (CHEST) loss for deep metric learning, showing improved accuracy and stability across hyperbolic and Euclidean spaces.",
        "tldr_zh": "本文介绍了用于深度度量学习的Combined Hyperbolic and Euclidean Soft Triple (CHEST)损失，展示了在双曲和欧几里得空间中表现出的提升的准确性和稳定性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "HOI-R1: Exploring the Potential of Multimodal Large Language Models for Human-Object Interaction Detection",
        "summary": "Recent Human-object interaction detection (HOID) methods highly require prior\nknowledge from VLMs to enhance the interaction recognition capabilities. The\ntraining strategies and model architectures for connecting the knowledge from\nVLMs to the HOI instance representations from the object detector are\nchallenging, and the whole framework is complex for further development or\napplication. On the other hand, the inherent reasoning abilities of MLLMs on\nhuman-object interaction detection are under-explored. Inspired by the recent\nsuccess of training MLLMs with reinforcement learning (RL) methods, we propose\nHOI-R1 and first explore the potential of the language model on the HOID task\nwithout any additional detection modules. We introduce an HOI reasoning process\nand HOID reward functions to solve the HOID task by pure text. The results on\nthe HICO-DET dataset show that HOI-R1 achieves 2x the accuracy of the baseline\nwith great generalization ability. The source code is available at\nhttps://github.com/cjw2021/HOI-R1.",
        "url": "http://arxiv.org/abs/2510.05609v1",
        "published_date": "2025-10-07T06:16:02+00:00",
        "updated_date": "2025-10-07T06:16:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Junwen Chen",
            "Peilin Xiong",
            "Keiji Yanai"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes HOI-R1, a method that explores the use of large language models for human-object interaction detection without additional detection modules, achieving 2x accuracy of the baseline on the HICO-DET dataset.",
        "tldr_zh": "本文提出了HOI-R1方法，探索了在人-物交互检测中使用大型语言模型的可能性，无需额外的检测模块，在HICO-DET数据集上实现了基线准确率的2倍。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Human Action Recognition from Point Clouds over Time",
        "summary": "Recent research into human action recognition (HAR) has focused predominantly\non skeletal action recognition and video-based methods. With the increasing\navailability of consumer-grade depth sensors and Lidar instruments, there is a\ngrowing opportunity to leverage dense 3D data for action recognition, to\ndevelop a third way. This paper presents a novel approach for recognizing\nactions from 3D videos by introducing a pipeline that segments human point\nclouds from the background of a scene, tracks individuals over time, and\nperforms body part segmentation. The method supports point clouds from both\ndepth sensors and monocular depth estimation. At the core of the proposed HAR\nframework is a novel backbone for 3D action recognition, which combines\npoint-based techniques with sparse convolutional networks applied to\nvoxel-mapped point cloud sequences. Experiments incorporate auxiliary point\nfeatures including surface normals, color, infrared intensity, and body part\nparsing labels, to enhance recognition accuracy. Evaluation on the NTU RGB- D\n120 dataset demonstrates that the method is competitive with existing skeletal\naction recognition algorithms. Moreover, combining both sensor-based and\nestimated depth inputs in an ensemble setup, this approach achieves 89.3%\naccuracy when different human subjects are considered for training and testing,\noutperforming previous point cloud action recognition methods.",
        "url": "http://arxiv.org/abs/2510.05506v1",
        "published_date": "2025-10-07T01:51:27+00:00",
        "updated_date": "2025-10-07T01:51:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "James Dickens"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "This paper presents a novel approach for human action recognition from 3D point clouds using depth sensors and Lidar data, achieving competitive results with existing methods.",
        "tldr_zh": "本文提出了一种新的方法，使用深度传感器和激光雷达数据从3D点云中识别人类动作，取得了与现有方法竞争力的结果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 7.25
    },
    {
        "title": "RegMix: Adversarial Mutual and Generalization Regularization for Enhancing DNN Robustness",
        "summary": "Adversarial training is the most effective defense against adversarial\nattacks. The effectiveness of the adversarial attacks has been on the design of\nits loss function and regularization term. The most widely used loss function\nin adversarial training is cross-entropy and mean squared error (MSE) as its\nregularization objective. However, MSE enforces overly uniform optimization\nbetween two output distributions during training, which limits its robustness\nin adversarial training scenarios. To address this issue, we revisit the idea\nof mutual learning (originally designed for knowledge distillation) and propose\ntwo novel regularization strategies tailored for adversarial training: (i)\nweighted adversarial mutual regularization and (ii) adversarial generalization\nregularization. In the former, we formulate a decomposed adversarial mutual\nKullback-Leibler divergence (KL-divergence) loss, which allows flexible control\nover the optimization process by assigning unequal weights to the main and\nauxiliary objectives. In the latter, we introduce an additional clean target\ndistribution into the adversarial training objective, improving generalization\nand enhancing model robustness. Extensive experiments demonstrate that our\nproposed methods significantly improve adversarial robustness compared to\nexisting regularization-based approaches.",
        "url": "http://arxiv.org/abs/2510.05317v1",
        "published_date": "2025-10-06T19:30:08+00:00",
        "updated_date": "2025-10-06T19:30:08+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Zhenyu Liu",
            "Varun Ojha"
        ],
        "ai_categories": [
            "GAN",
            "AIGC",
            "Transformer"
        ],
        "tldr": "The paper proposes novel regularization strategies for enhancing DNN robustness in adversarial training scenarios.",
        "tldr_zh": "本文提出了新的正则化策略，以增强在对抗训练场景下的DNN鲁棒性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "SkinMap: Weighted Full-Body Skin Segmentation for Robust Remote Photoplethysmography",
        "summary": "Remote photoplethysmography (rPPG) is an innovative method for monitoring\nheart rate and vital signs by using a simple camera to record a person, as long\nas any part of their skin is visible. This low-cost, contactless approach helps\nin remote patient monitoring, emotion analysis, smart vehicle utilization, and\nmore. Over the years, various techniques have been proposed to improve the\naccuracy of this technology, especially given its sensitivity to lighting and\nmovement. In the unsupervised pipeline, it is necessary to first select skin\nregions from the video to extract the rPPG signal from the skin color changes.\nWe introduce a novel skin segmentation technique that prioritizes skin regions\nto enhance the quality of the extracted signal. It can detect areas of skin all\nover the body, making it more resistant to movement, while removing areas such\nas the mouth, eyes, and hair that may cause interference. Our model is\nevaluated on publicly available datasets, and we also present a new dataset,\ncalled SYNC-rPPG, to better represent real-world conditions. The results\nindicate that our model demonstrates a prior ability to capture heartbeats in\nchallenging conditions, such as talking and head rotation, and maintain the\nmean absolute error (MAE) between predicted and actual heart rates, while other\nmethods fail to do so. In addition, we demonstrate high accuracy in detecting a\ndiverse range of skin tones, making this technique a promising option for\nreal-world applications.",
        "url": "http://arxiv.org/abs/2510.05296v1",
        "published_date": "2025-10-06T19:05:55+00:00",
        "updated_date": "2025-10-06T19:05:55+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Zahra Maleki",
            "Amirhossein Akbari",
            "Amirhossein Binesh",
            "Babak Khalaj"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a new skin segmentation technique for remote monitoring using cameras, improving heart rate detection in challenging conditions.",
        "tldr_zh": "该论文介绍了一种新的皮肤分割技术，可通过摄像头进行远程监测，改善了在复杂条件下的心率检测。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Attention-Enhanced Prototypical Learning for Few-Shot Infrastructure Defect Segmentation",
        "summary": "Few-shot semantic segmentation is vital for deep learning-based\ninfrastructure inspection applications, where labeled training examples are\nscarce and expensive. Although existing deep learning frameworks perform well,\nthe need for extensive labeled datasets and the inability to learn new defect\ncategories with little data are problematic. We present our Enhanced Feature\nPyramid Network (E-FPN) framework for few-shot semantic segmentation of culvert\nand sewer defect categories using a prototypical learning framework. Our\napproach has three main contributions: (1) adaptive E-FPN encoder using\nInceptionSepConv blocks and depth-wise separable convolutions for efficient\nmulti-scale feature extraction; (2) prototypical learning with masked average\npooling for powerful prototype generation from small support examples; and (3)\nattention-based feature representation through global self-attention, local\nself-attention and cross-attention. Comprehensive experimentation on\nchallenging infrastructure inspection datasets illustrates that the method\nachieves excellent few-shot performance, with the best configuration being\n8-way 5-shot training configuration at 82.55% F1-score and 72.26% mIoU in 2-way\nclassification testing. The self-attention method had the most significant\nperformance improvements, providing 2.57% F1-score and 2.9% mIoU gain over\nbaselines. Our framework addresses the critical need to rapidly respond to new\ndefect types in infrastructure inspection systems with limited new training\ndata that lead to more efficient and economical maintenance plans for critical\ninfrastructure systems.",
        "url": "http://arxiv.org/abs/2510.05266v1",
        "published_date": "2025-10-06T18:33:31+00:00",
        "updated_date": "2025-10-06T18:33:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Christina Thrainer",
            "Md Meftahul Ferdaus",
            "Mahdi Abdelguerfi",
            "Christian Guetl",
            "Steven Sloan",
            "Kendall N. Niles",
            "Ken Pathak"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces an Enhanced Feature Pyramid Network (E-FPN) framework for few-shot infrastructure defect segmentation using prototypical learning with attention mechanisms, achieving good performance with limited training data.",
        "tldr_zh": "本文引入了增强特征金字塔网络（E-FPN）框架，采用原型学习和注意机制进行少样本基础设施缺陷分割，利用有限的训练数据取得了良好的性能。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark",
        "summary": "Most existing benchmarks for egocentric vision understanding focus primarily\non daytime scenarios, overlooking the low-light conditions that are inevitable\nin real-world applications. To investigate this gap, we present EgoNight, the\nfirst comprehensive benchmark for nighttime egocentric vision, with visual\nquestion answering (VQA) as the core task. A key feature of EgoNight is the\nintroduction of day-night aligned videos, which enhance night annotation\nquality using the daytime data and reveal clear performance gaps between\nlighting conditions. To achieve this, we collect both synthetic videos rendered\nby Blender and real-world recordings, ensuring that scenes and actions are\nvisually and temporally aligned. Leveraging these paired videos, we construct\nEgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and\nrefinement through extensive human verification. Each QA pair is double-checked\nby annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs\nacross 90 videos, spanning 12 diverse QA types, with more than 300 hours of\nhuman work. Evaluations of state-of-the-art multimodal large language models\n(MLLMs) reveal substantial performance drops when transferring from day to\nnight, underscoring the challenges of reasoning under low-light conditions.\nBeyond VQA, EgoNight also introduces two auxiliary tasks, day-night\ncorrespondence retrieval and egocentric depth estimation at night, that further\nexplore the boundaries of existing models. We believe EgoNight-VQA provides a\nstrong foundation for advancing application-driven egocentric vision research\nand for developing models that generalize across illumination domains. All the\ndata and code will be made available upon acceptance.",
        "url": "http://arxiv.org/abs/2510.06218v1",
        "published_date": "2025-10-07T17:59:47+00:00",
        "updated_date": "2025-10-07T17:59:47+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Deheng Zhang",
            "Yuqian Fu",
            "Runyi Yang",
            "Yang Miao",
            "Tianwen Qian",
            "Xu Zheng",
            "Guolei Sun",
            "Ajad Chhatkuli",
            "Xuanjing Huang",
            "Yu-Gang Jiang",
            "Luc Van Gool",
            "Danda Pani Paudel"
        ],
        "ai_categories": [
            "Dataset",
            "Multimodality"
        ],
        "tldr": "EgoNight introduces a benchmark for nighttime egocentric vision understanding, emphasizing on visual question answering. It reveals performance gaps between day and night scenarios and introduces new tasks to explore model boundaries.",
        "tldr_zh": "EgoNight引入了一个针对夜间自中心视觉理解的基准，强调视觉问答。它揭示了日夜场景之间的性能差距，并引入了新任务来探索模型边界。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Emergent AI Surveillance: Overlearned Person Re-Identification and Its Mitigation in Law Enforcement Context",
        "summary": "Generic instance search models can dramatically reduce the manual effort\nrequired to analyze vast surveillance footage during criminal investigations by\nretrieving specific objects of interest to law enforcement. However, our\nresearch reveals an unintended emergent capability: through overlearning, these\nmodels can single out specific individuals even when trained on datasets\nwithout human subjects. This capability raises concerns regarding\nidentification and profiling of individuals based on their personal data, while\nthere is currently no clear standard on how de-identification can be achieved.\nWe evaluate two technical safeguards to curtail a model's person\nre-identification capacity: index exclusion and confusion loss. Our experiments\ndemonstrate that combining these approaches can reduce person re-identification\naccuracy to below 2% while maintaining 82% of retrieval performance for\nnon-person objects. However, we identify critical vulnerabilities in these\nmitigations, including potential circumvention using partial person images.\nThese findings highlight urgent regulatory questions at the intersection of AI\ngovernance and data protection: How should we classify and regulate systems\nwith emergent identification capabilities? And what technical standards should\nbe required to prevent identification capabilities from developing in seemingly\nbenign applications?",
        "url": "http://arxiv.org/abs/2510.06026v1",
        "published_date": "2025-10-07T15:23:16+00:00",
        "updated_date": "2025-10-07T15:23:16+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CY",
            "cs.LG"
        ],
        "authors": [
            "An Thi Nguyen",
            "Radina Stoykova",
            "Eric Arazo"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper discusses how AI surveillance models can unintentionally identify specific individuals, raising concerns about privacy and profiling. It proposes technical safeguards to reduce person re-identification accuracy.",
        "tldr_zh": "本文讨论了AI监控模型如何无意中识别特定个人，引发了关于隐私和个人画像的担忧。它提出了减少人员重识准确性的技术保障。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Mysteries of the Deep: Role of Intermediate Representations in Out of Distribution Detection",
        "summary": "Out-of-distribution (OOD) detection is essential for reliably deploying\nmachine learning models in the wild. Yet, most methods treat large pre-trained\nmodels as monolithic encoders and rely solely on their final-layer\nrepresentations for detection. We challenge this wisdom. We reveal the\n\\textit{intermediate layers} of pre-trained models, shaped by residual\nconnections that subtly transform input projections, \\textit{can} encode\n\\textit{surprisingly rich and diverse signals} for detecting distributional\nshifts. Importantly, to exploit latent representation diversity across layers,\nwe introduce an entropy-based criterion to \\textit{automatically} identify\nlayers offering the most complementary information in a training-free setting\n-- \\textit{without access to OOD data}. We show that selectively incorporating\nthese intermediate representations can increase the accuracy of OOD detection\nby up to \\textbf{$10\\%$} in far-OOD and over \\textbf{$7\\%$} in near-OOD\nbenchmarks compared to state-of-the-art training-free methods across various\nmodel architectures and training objectives. Our findings reveal a new avenue\nfor OOD detection research and uncover the impact of various training\nobjectives and model architectures on confidence-based OOD detection methods.",
        "url": "http://arxiv.org/abs/2510.05782v1",
        "published_date": "2025-10-07T10:55:47+00:00",
        "updated_date": "2025-10-07T10:55:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "I. M. De la Jara",
            "C. Rodriguez-Opazo",
            "D. Teney",
            "D. Ranasinghe",
            "E. Abbasnejad"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper challenges the common practice of using final-layer representations in out-of-distribution detection, showcasing the potential of intermediate representations to improve detection accuracy.",
        "tldr_zh": "本文挑战了使用最终层表示在超出分布检测中的常见做法，展示了中间表示的潜力，以提高检测准确性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI",
        "summary": "Large language models leverage internet-scale text data, yet embodied AI\nremains constrained by the prohibitive costs of physical trajectory collection.\nDesktop environments -- particularly gaming -- offer a compelling alternative:\nthey provide rich sensorimotor interactions at scale while maintaining the\nstructured observation-action coupling essential for embodied learning. We\npresent D2E (Desktop to Embodied AI), a framework that demonstrates desktop\ninteractions can serve as an effective pretraining substrate for robotics\nembodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT\nfor Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a\ncomplete pipeline from scalable desktop data collection to verified transfer in\nembodied domains. Our framework comprises three components: (1) the OWA Toolkit\nthat unifies diverse desktop interactions into a standardized format with 152x\ncompression, (2) the Generalist-IDM that achieves strong zero-shot\ngeneralization across unseen games through timestamp-based event prediction,\nenabling internet-scale pseudo-labeling, and (3) VAPT that transfers\ndesktop-pretrained representations to physical manipulation and navigation.\nUsing 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of\npseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO\nmanipulation and 83.3% on CANVAS navigation benchmarks. This validates that\nsensorimotor primitives in digital interactions exhibit sufficient invariance\nto transfer meaningfully to physical embodied tasks, establishing desktop\npretraining as a practical paradigm for robotics. We will make all our work\npublic, including the OWA toolkit, datasets of human-collected and\npseudo-labeled, and VAPT-trained models available at\nhttps://worv-ai.github.io/d2e/",
        "url": "http://arxiv.org/abs/2510.05684v1",
        "published_date": "2025-10-07T08:40:33+00:00",
        "updated_date": "2025-10-07T08:40:33+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Suwhan Choi",
            "Jaeyoon Jung",
            "Haebin Seong",
            "Minchan Kim",
            "Minyeong Kim",
            "Yongjun Cho",
            "Yoonshik Kim",
            "Yubeen Park",
            "Youngjae Yu",
            "Yunsung Lee"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper presents D2E framework that uses desktop interactions for pretraining in robotics tasks, achieving high success rates in manipulation and navigation benchmarks.",
        "tldr_zh": "该论文提出了D2E框架，利用桌面交互进行机器人任务的预训练，在操纵和导航基准测试中取得了高成功率。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "A Hierarchical Geometry-guided Transformer for Histological Subtyping of Primary Liver Cancer",
        "summary": "Primary liver malignancies are widely recognized as the most heterogeneous\nand prognostically diverse cancers of the digestive system. Among these,\nhepatocellular carcinoma (HCC) and intrahepatic cholangiocarcinoma (ICC) emerge\nas the two principal histological subtypes, demonstrating significantly greater\ncomplexity in tissue morphology and cellular architecture than other common\ntumors. The intricate representation of features in Whole Slide Images (WSIs)\nencompasses abundant crucial information for liver cancer histological\nsubtyping, regarding hierarchical pyramid structure, tumor microenvironment\n(TME), and geometric representation. However, recent approaches have not\nadequately exploited these indispensable effective descriptors, resulting in a\nlimited understanding of histological representation and suboptimal subtyping\nperformance. To mitigate these limitations, ARGUS is proposed to advance\nhistological subtyping in liver cancer by capturing the macro-meso-micro\nhierarchical information within the TME. Specifically, we first construct a\nmicro-geometry feature to represent fine-grained cell-level pattern via a\ngeometric structure across nuclei, thereby providing a more refined and precise\nperspective for delineating pathological images. Then, a Hierarchical\nField-of-Views (FoVs) Alignment module is designed to model macro- and\nmeso-level hierarchical interactions inherent in WSIs. Finally, the augmented\nmicro-geometry and FoVs features are fused into a joint representation via\npresent Geometry Prior Guided Fusion strategy for modeling holistic phenotype\ninteractions. Extensive experiments on public and private cohorts demonstrate\nthat our ARGUS achieves state-of-the-art (SOTA) performance in histological\nsubtyping of liver cancer, which provide an effective diagnostic tool for\nprimary liver malignancies in clinical practice.",
        "url": "http://arxiv.org/abs/2510.05657v1",
        "published_date": "2025-10-07T08:10:18+00:00",
        "updated_date": "2025-10-07T08:10:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Anwen Lu",
            "Mingxin Liu",
            "Yiping Jiao",
            "Hongyi Gong",
            "Geyang Xu",
            "Jun Chen",
            "Jun Xu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "ARGUS is a Hierarchical Geometry-guided Transformer proposed for histological subtyping of primary liver cancer, achieving state-of-the-art performance in this area.",
        "tldr_zh": "ARGUS是一种用于原发性肝癌组织学分型的层次几何引导变压器，取得了该领域的最新成果。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "InstaGeo: Compute-Efficient Geospatial Machine Learning from Data to Deployment",
        "summary": "Open-access multispectral imagery from missions like Landsat 8-9 and\nSentinel-2 has fueled the development of geospatial foundation models (GFMs)\nfor humanitarian and environmental applications. Yet, their deployment remains\nlimited by (i) the absence of automated geospatial data pipelines and (ii) the\nlarge size of fine-tuned models. Existing GFMs lack workflows for processing\nraw satellite imagery, and downstream adaptations often retain the full\ncomplexity of the original encoder.\n  We present InstaGeo, an open-source, end-to-end framework that addresses\nthese challenges by integrating: (1) automated data curation to transform raw\nimagery into model-ready datasets; (2) task-specific model distillation to\nderive compact, compute-efficient models; and (3) seamless deployment as\ninteractive web-map applications. Using InstaGeo, we reproduced datasets from\nthree published studies and trained models with marginal mIoU differences of\n-0.73 pp for flood mapping, -0.20 pp for crop segmentation, and +1.79 pp for\ndesert locust prediction. The distilled models are up to 8x smaller than\nstandard fine-tuned counterparts, reducing FLOPs and CO2 emissions with minimal\naccuracy loss.\n  Leveraging InstaGeo's streamlined data pipeline, we also curated a larger\ncrop segmentation dataset, achieving a state-of-the-art mIoU of 60.65%, a 12 pp\nimprovement over prior baselines. Moreover, InstaGeo enables users to progress\nfrom raw data to model deployment within a single working day.\n  By unifying data preparation, model compression, and deployment, InstaGeo\ntransforms research-grade GFMs into practical, low-carbon tools for real-time,\nlarge-scale Earth observation. This approach shifts geospatial AI toward data\nquality and application-driven innovation. Source code, datasets, and model\ncheckpoints are available at:\nhttps://github.com/instadeepai/InstaGeo-E2E-Geospatial-ML.git",
        "url": "http://arxiv.org/abs/2510.05617v1",
        "published_date": "2025-10-07T06:57:15+00:00",
        "updated_date": "2025-10-07T06:57:15+00:00",
        "categories": [
            "cs.CV",
            "cs.CY",
            "cs.LG"
        ],
        "authors": [
            "Ibrahim Salihu Yusuf",
            "Iffanice Houndayi",
            "Rym Oualha",
            "Mohamed Aziz Cherif",
            "Kobby Panford-Quainoo",
            "Arnu Pretorius"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "InstaGeo is an end-to-end framework for geospatial machine learning that automates data curation, model distillation, and deployment, leading to compact and efficient models with minimal accuracy loss.",
        "tldr_zh": "InstaGeo是一个端到端的地理空间机器学习框架，自动化数据整理、模型提炼和部署，生成紧凑高效的模型，减少准确性损失。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates",
        "summary": "Dataset condensation (DC) enables the creation of compact, privacy-preserving\nsynthetic datasets that can match the utility of real patient records,\nsupporting democratised access to highly regulated clinical data for developing\ndownstream clinical models. State-of-the-art DC methods supervise synthetic\ndata by aligning the training dynamics of models trained on real and those\ntrained on synthetic data, typically using full stochastic gradient descent\n(SGD) trajectories as alignment targets; however, these trajectories are often\nnoisy, high-curvature, and storage-intensive, leading to unstable gradients,\nslow convergence, and substantial memory overhead. We address these limitations\nby replacing full SGD trajectories with smooth, low-loss parametric surrogates,\nspecifically quadratic B\\'ezier curves that connect the initial and final model\nstates from real training trajectories. These mode-connected paths provide\nnoise-free, low-curvature supervision signals that stabilise gradients,\naccelerate convergence, and eliminate the need for dense trajectory storage. We\ntheoretically justify B\\'ezier-mode connections as effective surrogates for SGD\npaths and empirically show that the proposed method outperforms\nstate-of-the-art condensation approaches across five clinical datasets,\nyielding condensed datasets that enable clinically effective model development.",
        "url": "http://arxiv.org/abs/2510.05805v1",
        "published_date": "2025-10-07T11:22:27+00:00",
        "updated_date": "2025-10-07T11:22:27+00:00",
        "categories": [
            "cs.LG",
            "cs.CV",
            "cs.DB"
        ],
        "authors": [
            "Pafue Christy Nganjimi",
            "Andrew Soltan",
            "Danielle Belgrave",
            "Lei Clifton",
            "David A. Clifton",
            "Anshul Thakur"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "This paper presents a method to improve dataset condensation for clinical data by using parametric surrogates to replace noisy trajectories, resulting in better model development.",
        "tldr_zh": "本文提出一种方法，通过使用参数替代轨迹来改进临床数据的数据集压缩，从而实现更好的模型开发。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Development and Validation of a Low-Cost Imaging System for Seedling Germination Kinetics through Time-Cumulative Analysis",
        "summary": "The study investigates the effects of R. solani inoculation on the\ngermination and early development of Lactuca sativa L. seeds using a low-cost,\nimage-based monitoring system. Multiple cameras were deployed to continuously\ncapture images of the germination process in both infected and control groups.\nThe objective was to assess the impact of the pathogen by analyzing germination\ndynamics and growth over time. To achieve this, a novel image analysis pipeline\nwas developed. The algorithm integrates both morphological and spatial features\nto identify and quantify individual seedlings, even under complex conditions\nwhere traditional image analyses fails. A key innovation of the method lies in\nits temporal integration: each analysis step considers not only the current\nstatus but also their developmental across prior time points. This approach\nenables robust discrimination of individual seedlings, especially when\noverlapping leaves significantly hinder object separation. The method\ndemonstrated high accuracy in seedling counting and vigor assessment, even in\nchallenging scenarios characterized by dense and intertwined growth. Results\nconfirm that R. solani infection significantly reduces germination rates and\nearly seedling vigor. The study also validates the feasibility of combining\nlow-cost imaging hardware with advanced computational tools to obtain\nphenotyping data in a non-destructive and scalable manner. The temporal\nintegration enabled accurate quantification of germinated seeds and precise\ndetermination of seedling emergence timing. This approach proved particularly\neffective in later stages of the experiment, where conventional segmentation\ntechniques failed due to overlapping or intertwined seedlings, making accurate\ncounting. The method achieved a coefficient of determination of 0.98 and a root\nmean square error (RMSE) of 1.12, demonstrating its robustness and reliability.",
        "url": "http://arxiv.org/abs/2510.05668v1",
        "published_date": "2025-10-07T08:26:11+00:00",
        "updated_date": "2025-10-07T08:26:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "M. Torrente",
            "A. Follador",
            "A. Calcante",
            "P. Casati",
            "R. Oberti"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper presents a low-cost imaging system for monitoring seedling germination dynamics and growth under R. solani infection. A novel image analysis pipeline integrates morphological and spatial features for accurate seedling quantification.",
        "tldr_zh": "本文介绍了一种用于监测种子萌发动态和生长的低成本成像系统，以及针对 R. solani 感染的影响。一种新的图像分析管道集成了形态和空间特征，用于准确量化幼苗。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "From Neural Activity to Computation: Biological Reservoirs for Pattern Recognition in Digit Classification",
        "summary": "In this paper, we present a biologically grounded approach to reservoir\ncomputing (RC), in which a network of cultured biological neurons serves as the\nreservoir substrate. This system, referred to as biological reservoir computing\n(BRC), replaces artificial recurrent units with the spontaneous and evoked\nactivity of living neurons. A multi-electrode array (MEA) enables simultaneous\nstimulation and readout across multiple sites: inputs are delivered through a\nsubset of electrodes, while the remaining ones capture the resulting neural\nresponses, mapping input patterns into a high-dimensional biological feature\nspace. We evaluate the system through a case study on digit classification\nusing a custom dataset. Input images are encoded and delivered to the\nbiological reservoir via electrical stimulation, and the corresponding neural\nactivity is used to train a simple linear classifier. To contextualize the\nperformance of the biological system, we also include a comparison with a\nstandard artificial reservoir trained on the same task. The results indicate\nthat the biological reservoir can effectively support classification,\nhighlighting its potential as a viable and interpretable computational\nsubstrate. We believe this work contributes to the broader effort of\nintegrating biological principles into machine learning and aligns with the\ngoals of human-inspired vision by exploring how living neural systems can\ninform the design of efficient and biologically plausible models.",
        "url": "http://arxiv.org/abs/2510.05637v1",
        "published_date": "2025-10-07T07:36:36+00:00",
        "updated_date": "2025-10-07T07:36:36+00:00",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Ludovico Iannello",
            "Luca Ciampi",
            "Fabrizio Tonelli",
            "Gabriele Lagani",
            "Lucio Maria Calcagnile",
            "Federico Cremisi",
            "Angelo Di Garbo",
            "Giuseppe Amato"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces biological reservoir computing using cultured neurons for digit classification, showing promising results compared to artificial reservoirs.",
        "tldr_zh": "该论文介绍了使用培养神经元进行数字分类的生物储留计算，相比于人工储留计算展现出了有前景的结果。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "TFM Dataset: A Novel Multi-task Dataset and Integrated Pipeline for Automated Tear Film Break-Up Segmentation",
        "summary": "Tear film break-up (TFBU) analysis is critical for diagnosing dry eye\nsyndrome, but automated TFBU segmentation remains challenging due to the lack\nof annotated datasets and integrated solutions. This paper introduces the Tear\nFilm Multi-task (TFM) Dataset, the first comprehensive dataset for multi-task\ntear film analysis, comprising 15 high-resolution videos (totaling 6,247\nframes) annotated with three vision tasks: frame-level classification ('clear',\n'closed', 'broken', 'blur'), Placido Ring detection, and pixel-wise TFBU area\nsegmentation. Leveraging this dataset, we first propose TF-Net, a novel and\nefficient baseline segmentation model. TF-Net incorporates a MobileOne-mini\nbackbone with re-parameterization techniques and an enhanced feature pyramid\nnetwork to achieve a favorable balance between accuracy and computational\nefficiency for real-time clinical applications. We further establish benchmark\nperformance on the TFM segmentation subset by comparing TF-Net against several\nstate-of-the-art medical image segmentation models. Furthermore, we design\nTF-Collab, a novel integrated real-time pipeline that synergistically leverages\nmodels trained on all three tasks of the TFM dataset. By sequentially\norchestrating frame classification for BUT determination, pupil region\nlocalization for input standardization, and TFBU segmentation, TF-Collab fully\nautomates the analysis. Experimental results demonstrate the effectiveness of\nthe proposed TF-Net and TF-Collab, providing a foundation for future research\nin ocular surface diagnostics. Our code and the TFM datasets are available at\nhttps://github.com/glory-wan/TF-Net",
        "url": "http://arxiv.org/abs/2510.05615v1",
        "published_date": "2025-10-07T06:45:38+00:00",
        "updated_date": "2025-10-07T06:45:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guangrong Wan",
            "Jun liu",
            "Tang tang",
            "Lianghao Shi",
            "Wenjun Luo",
            "TingTing Xu"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "This paper introduces a novel multi-task dataset and segmentation model for tear film break-up analysis, facilitating automated diagnosis of dry eye syndrome.",
        "tldr_zh": "该论文介绍了一个新颖的多任务数据集和分割模型，用于研究泪膜破裂分析，促进干眼综合征的自动诊断。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "A public cardiac CT dataset featuring the left atrial appendage",
        "summary": "Despite the success of advanced segmentation frameworks such as\nTotalSegmentator (TS), accurate segmentations of the left atrial appendage\n(LAA), coronary arteries (CAs), and pulmonary veins (PVs) remain a significant\nchallenge in medical imaging. In this work, we present the first open-source,\nanatomically coherent dataset of curated, high-resolution segmentations for\nthese structures, supplemented with whole-heart labels produced by TS on the\npublicly available ImageCAS dataset consisting of 1000 cardiac computed\ntomography angiography (CCTA) scans. One purpose of the data set is to foster\nnovel approaches to the analysis of LAA morphology.\n  LAA segmentations on ImageCAS were generated using a state-of-the-art\nsegmentation framework developed specifically for high resolution LAA\nsegmentation. We trained the network on a large private dataset with manual\nannotations provided by medical readers guided by a trained cardiologist and\ntransferred the model to ImageCAS data. CA labels were improved from the\noriginal ImageCAS annotations, while PV segmentations were refined from TS\noutputs. In addition, we provide a list of scans from ImageCAS that contains\ncommon data flaws such as step artefacts, LAAs extending beyond the scanner's\nfield of view, and other types of data defects.",
        "url": "http://arxiv.org/abs/2510.06090v1",
        "published_date": "2025-10-07T16:16:59+00:00",
        "updated_date": "2025-10-07T16:16:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Bjoern Hansen",
            "Jonas Pedersen",
            "Klaus F. Kofoed",
            "Oscar Camara",
            "Rasmus R. Paulsen",
            "Kristine Soerensen"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper presents a public dataset of segmented cardiac CT images, including the left atrial appendage, coronary arteries, and pulmonary veins, to aid in the analysis of LAA morphology.",
        "tldr_zh": "本文提出了一个包含左心房附属物、冠状动脉和肺静脉的心脏CT图像分割公共数据集，以帮助分析LAA形态。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "Kaputt: A Large-Scale Dataset for Visual Defect Detection",
        "summary": "We present a novel large-scale dataset for defect detection in a logistics\nsetting. Recent work on industrial anomaly detection has primarily focused on\nmanufacturing scenarios with highly controlled poses and a limited number of\nobject categories. Existing benchmarks like MVTec-AD [6] and VisA [33] have\nreached saturation, with state-of-the-art methods achieving up to 99.9% AUROC\nscores. In contrast to manufacturing, anomaly detection in retail logistics\nfaces new challenges, particularly in the diversity and variability of object\npose and appearance. Leading anomaly detection methods fall short when applied\nto this new setting. To bridge this gap, we introduce a new benchmark that\novercomes the current limitations of existing datasets. With over 230,000\nimages (and more than 29,000 defective instances), it is 40 times larger than\nMVTec-AD and contains more than 48,000 distinct objects. To validate the\ndifficulty of the problem, we conduct an extensive evaluation of multiple\nstate-of-the-art anomaly detection methods, demonstrating that they do not\nsurpass 56.96% AUROC on our dataset. Further qualitative analysis confirms that\nexisting methods struggle to leverage normal samples under heavy pose and\nappearance variation. With our large-scale dataset, we set a new benchmark and\nencourage future research towards solving this challenging problem in retail\nlogistics anomaly detection. The dataset is available for download under\nhttps://www.kaputt-dataset.com.",
        "url": "http://arxiv.org/abs/2510.05903v1",
        "published_date": "2025-10-07T13:13:18+00:00",
        "updated_date": "2025-10-07T13:13:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Sebastian Höfer",
            "Dorian Henning",
            "Artemij Amiranashvili",
            "Douglas Morrison",
            "Mariliza Tzes",
            "Ingmar Posner",
            "Marc Matvienko",
            "Alessandro Rennola",
            "Anton Milan"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper introduces a large-scale dataset for defect detection in retail logistics, highlighting the challenges posed by diversity and variability of object pose and appearance.",
        "tldr_zh": "该论文介绍了一个用于零售物流中缺陷检测的大规模数据集，突出了物体姿势和外观的多样性和变化所带来的挑战。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.0
    },
    {
        "title": "Fine-Tuned CNN-Based Approach for Multi-Class Mango Leaf Disease Detection",
        "summary": "Mango is an important fruit crop in South Asia, but its cultivation is\nfrequently hampered by leaf diseases that greatly impact yield and quality.\nThis research examines the performance of five pre-trained convolutional neural\nnetworks, DenseNet201, InceptionV3, ResNet152V2, SeResNet152, and Xception, for\nmulti-class identification of mango leaf diseases across eight classes using a\ntransfer learning strategy with fine-tuning. The models were assessed through\nstandard evaluation metrics, such as accuracy, precision, recall, F1-score, and\nconfusion matrices. Among the architectures tested, DenseNet201 delivered the\nbest results, achieving 99.33% accuracy with consistently strong metrics for\nindividual classes, particularly excelling in identifying Cutting Weevil and\nBacterial Canker. Moreover, ResNet152V2 and SeResNet152 provided strong\noutcomes, whereas InceptionV3 and Xception exhibited lower performance in\nvisually similar categories like Sooty Mould and Powdery Mildew. The training\nand validation plots demonstrated stable convergence for the highest-performing\nmodels. The capability of fine-tuned transfer learning models, for precise and\ndependable multi-class mango leaf disease detection in intelligent agricultural\napplications.",
        "url": "http://arxiv.org/abs/2510.05326v1",
        "published_date": "2025-10-06T19:47:26+00:00",
        "updated_date": "2025-10-06T19:47:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jalal Ahmmed",
            "Faruk Ahmed",
            "Rashedul Hasan Shohan",
            "Md. Mahabub Rana",
            "Mahdi Hasan"
        ],
        "ai_categories": [
            "Transfer Learning",
            "Dataset"
        ],
        "tldr": "The paper explores using CNNs for multi-class identification of mango leaf diseases, achieving high accuracy with DenseNet201 as the best-performing model.",
        "tldr_zh": "本文探讨了使用CNN进行多类别芒果叶病识别，其中DenseNet201为表现最佳的模型，取得了较高的准确率。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]