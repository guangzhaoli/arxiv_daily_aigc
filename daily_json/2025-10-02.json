[
    {
        "title": "BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration",
        "summary": "Diffusion Transformer has shown remarkable abilities in generating\nhigh-fidelity videos, delivering visually coherent frames and rich details over\nextended durations. However, existing video generation models still fall short\nin subject-consistent video generation due to an inherent difficulty in parsing\nprompts that specify complex spatial relationships, temporal logic, and\ninteractions among multiple subjects. To address this issue, we propose\nBindWeave, a unified framework that handles a broad range of subject-to-video\nscenarios from single-subject cases to complex multi-subject scenes with\nheterogeneous entities. To bind complex prompt semantics to concrete visual\nsubjects, we introduce an MLLM-DiT framework in which a pretrained multimodal\nlarge language model performs deep cross-modal reasoning to ground entities and\ndisentangle roles, attributes, and interactions, yielding subject-aware hidden\nstates that condition the diffusion transformer for high-fidelity\nsubject-consistent video generation. Experiments on the OpenS2V benchmark\ndemonstrate that our method achieves superior performance across subject\nconsistency, naturalness, and text relevance in generated videos, outperforming\nexisting open-source and commercial models.",
        "url": "http://arxiv.org/abs/2510.00438v1",
        "published_date": "2025-10-01T02:41:11+00:00",
        "updated_date": "2025-10-01T02:41:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhaoyang Li",
            "Dongjun Qian",
            "Kai Su",
            "Qishuai Diao",
            "Xiangyang Xia",
            "Chang Liu",
            "Wenfei Yang",
            "Tianzhu Zhang",
            "Zehuan Yuan"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper introduces BindWeave, a framework for subject-consistent video generation using a pretrained multimodal large language model and diffusion transformer, outperforming existing models on subject consistency, naturalness, and text relevance.",
        "tldr_zh": "该论文介绍了BindWeave，一种用于主体一致视频生成的框架，使用预训练的多模态大型语言模型和扩散变换器，在主体一致性、自然性和文本相关性方面优于现有模型。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Audio Driven Real-Time Facial Animation for Social Telepresence",
        "summary": "We present an audio-driven real-time system for animating photorealistic 3D\nfacial avatars with minimal latency, designed for social interactions in\nvirtual reality for anyone. Central to our approach is an encoder model that\ntransforms audio signals into latent facial expression sequences in real time,\nwhich are then decoded as photorealistic 3D facial avatars. Leveraging the\ngenerative capabilities of diffusion models, we capture the rich spectrum of\nfacial expressions necessary for natural communication while achieving\nreal-time performance (<15ms GPU time). Our novel architecture minimizes\nlatency through two key innovations: an online transformer that eliminates\ndependency on future inputs and a distillation pipeline that accelerates\niterative denoising into a single step. We further address critical design\nchallenges in live scenarios for processing continuous audio signals\nframe-by-frame while maintaining consistent animation quality. The versatility\nof our framework extends to multimodal applications, including semantic\nmodalities such as emotion conditions and multimodal sensors with head-mounted\neye cameras on VR headsets. Experimental results demonstrate significant\nimprovements in facial animation accuracy over existing offline\nstate-of-the-art baselines, achieving 100 to 1000 times faster inference speed.\nWe validate our approach through live VR demonstrations and across various\nscenarios such as multilingual speeches.",
        "url": "http://arxiv.org/abs/2510.01176v1",
        "published_date": "2025-10-01T17:57:05+00:00",
        "updated_date": "2025-10-01T17:57:05+00:00",
        "categories": [
            "cs.GR",
            "cs.CV",
            "cs.LG",
            "cs.SD"
        ],
        "authors": [
            "Jiye Lee",
            "Chenghui Li",
            "Linh Tran",
            "Shih-En Wei",
            "Jason Saragih",
            "Alexander Richard",
            "Hanbyul Joo",
            "Shaojie Bai"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper presents a real-time system for creating photorealistic 3D facial avatars using audio input, achieving high accuracy and speed for social interactions in virtual reality.",
        "tldr_zh": "本文介绍了一种使用音频输入创建逼真3D面部头像的实时系统，为虚拟现实中的社交交互提供高精度和速度。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "ImageDoctor: Diagnosing Text-to-Image Generation via Grounded Image Reasoning",
        "summary": "The rapid advancement of text-to-image (T2I) models has increased the need\nfor reliable human preference modeling, a demand further amplified by recent\nprogress in reinforcement learning for preference alignment. However, existing\napproaches typically quantify the quality of a generated image using a single\nscalar, limiting their ability to provide comprehensive and interpretable\nfeedback on image quality. To address this, we introduce ImageDoctor, a unified\nmulti-aspect T2I model evaluation framework that assesses image quality across\nfour complementary dimensions: plausibility, semantic alignment, aesthetics,\nand overall quality. ImageDoctor also provides pixel-level flaw indicators in\nthe form of heatmaps, which highlight misaligned or implausible regions, and\ncan be used as a dense reward for T2I model preference alignment. Inspired by\nthe diagnostic process, we improve the detail sensitivity and reasoning\ncapability of ImageDoctor by introducing a \"look-think-predict\" paradigm, where\nthe model first localizes potential flaws, then generates reasoning, and\nfinally concludes the evaluation with quantitative scores. Built on top of a\nvision-language model and trained through a combination of supervised\nfine-tuning and reinforcement learning, ImageDoctor demonstrates strong\nalignment with human preference across multiple datasets, establishing its\neffectiveness as an evaluation metric. Furthermore, when used as a reward model\nfor preference tuning, ImageDoctor significantly improves generation quality --\nachieving an improvement of 10% over scalar-based reward models.",
        "url": "http://arxiv.org/abs/2510.01010v1",
        "published_date": "2025-10-01T15:15:55+00:00",
        "updated_date": "2025-10-01T15:15:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuxiang Guo",
            "Jiang Liu",
            "Ze Wang",
            "Hao Chen",
            "Ximeng Sun",
            "Yang Zhao",
            "Jialian Wu",
            "Xiaodong Yu",
            "Zicheng Liu",
            "Emad Barsoum"
        ],
        "ai_categories": [
            "GAN",
            "LoRA",
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "ImageDoctor introduces a comprehensive evaluation framework for assessing image quality in text-to-image generation, with pixel-level flaw indicators and a 'look-think-predict' paradigm. It also improves generation quality significantly when used as a reward model.",
        "tldr_zh": "ImageDoctor引入了一个全面的评估框架，用于评估文本到图像生成中的图像质量，具有像素级缺陷指示和“看-思考-预测”范式。当作为奖励模型时，它也显著提高了生成质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "TextCAM: Explaining Class Activation Map with Text",
        "summary": "Deep neural networks (DNNs) have achieved remarkable success across domains\nbut remain difficult to interpret, limiting their trustworthiness in\nhigh-stakes applications. This paper focuses on deep vision models, for which a\ndominant line of explainability methods are Class Activation Mapping (CAM) and\nits variants working by highlighting spatial regions that drive predictions. We\nfigure out that CAM provides little semantic insight into what attributes\nunderlie these activations. To address this limitation, we propose TextCAM, a\nnovel explanation framework that enriches CAM with natural languages. TextCAM\ncombines the precise spatial localization of CAM with the semantic alignment of\nvision-language models (VLMs). Specifically, we derive channel-level semantic\nrepresentations using CLIP embeddings and linear discriminant analysis, and\naggregate them with CAM weights to produce textual descriptions of salient\nvisual evidence. This yields explanations that jointly specify where the model\nattends and what visual attributes likely support its decision. We further\nextend TextCAM to generate feature channels into semantically coherent groups,\nenabling more fine-grained visual-textual explanations. Experiments on\nImageNet, CLEVR, and CUB demonstrate that TextCAM produces faithful and\ninterpretable rationales that improve human understanding, detect spurious\ncorrelations, and preserve model fidelity.",
        "url": "http://arxiv.org/abs/2510.01004v1",
        "published_date": "2025-10-01T15:11:14+00:00",
        "updated_date": "2025-10-01T15:11:14+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Qiming Zhao",
            "Xingjian Li",
            "Xiaoyu Cao",
            "Xiaolong Wu",
            "Min Xu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces TextCAM, a novel framework that combines visual explanations with natural language to enhance interpretability of deep vision models.",
        "tldr_zh": "本文介绍了TextCAM，一种将视觉解释与自然语言相结合的新框架，以增强深度视觉模型的可解释性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "MetaLogic: Robustness Evaluation of Text-to-Image Models via Logically Equivalent Prompts",
        "summary": "Recent advances in text-to-image (T2I) models, especially diffusion-based\narchitectures, have significantly improved the visual quality of generated\nimages. However, these models continue to struggle with a critical limitation:\nmaintaining semantic consistency when input prompts undergo minor linguistic\nvariations. Despite being logically equivalent, such prompt pairs often yield\nmisaligned or semantically inconsistent images, exposing a lack of robustness\nin reasoning and generalisation. To address this, we propose MetaLogic, a novel\nevaluation framework that detects T2I misalignment without relying on ground\ntruth images. MetaLogic leverages metamorphic testing, generating image pairs\nfrom prompts that differ grammatically but are semantically identical. By\ndirectly comparing these image pairs, the framework identifies inconsistencies\nthat signal failures in preserving the intended meaning, effectively diagnosing\nrobustness issues in the model's logic understanding. Unlike existing\nevaluation methods that compare a generated image to a single prompt, MetaLogic\nevaluates semantic equivalence between paired images, offering a scalable,\nground-truth-free approach to identifying alignment failures. It categorises\nthese alignment errors (e.g., entity omission, duplication, positional\nmisalignment) and surfaces counterexamples that can be used for model debugging\nand refinement. We evaluate MetaLogic across multiple state-of-the-art T2I\nmodels and reveal consistent robustness failures across a range of logical\nconstructs. We find that even the SOTA text-to-image models like Flux.dev and\nDALLE-3 demonstrate a 59 percent and 71 percent misalignment rate,\nrespectively. Our results show that MetaLogic is not only efficient and\nscalable, but also effective in uncovering fine-grained logical inconsistencies\nthat are overlooked by existing evaluation metrics.",
        "url": "http://arxiv.org/abs/2510.00796v1",
        "published_date": "2025-10-01T11:51:13+00:00",
        "updated_date": "2025-10-01T11:51:13+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yifan Shen",
            "Yangyang Shu",
            "Hye-young Paik",
            "Yulei Sui"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes MetaLogic, a novel evaluation framework for assessing the robustness of text-to-image models by detecting misalignment without ground truth images, revealing logical inconsistencies in state-of-the-art models.",
        "tldr_zh": "该论文提出了MetaLogic，一种新颖的评估框架，通过检测无需基准图像的T2I模型的不一致性来揭示最先进模型中的逻辑不一致性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "OTTER: Open-Tagging via Text-Image Representation for Multi-modal Understanding",
        "summary": "We introduce OTTER, a unified open-set multi-label tagging framework that\nharmonizes the stability of a curated, predefined category set with the\nadaptability of user-driven open tags. OTTER is built upon a large-scale,\nhierarchically organized multi-modal dataset, collected from diverse online\nrepositories and annotated through a hybrid pipeline combining automated\nvision-language labeling with human refinement. By leveraging a multi-head\nattention architecture, OTTER jointly aligns visual and textual representations\nwith both fixed and open-set label embeddings, enabling dynamic and\nsemantically consistent tagging. OTTER consistently outperforms competitive\nbaselines on two benchmark datasets: it achieves an overall F1 score of 0.81 on\nOtter and 0.75 on Favorite, surpassing the next-best results by margins of 0.10\nand 0.02, respectively. OTTER attains near-perfect performance on open-set\nlabels, with F1 of 0.99 on Otter and 0.97 on Favorite, while maintaining\ncompetitive accuracy on predefined labels. These results demonstrate OTTER's\neffectiveness in bridging closed-set consistency with open-vocabulary\nflexibility for multi-modal tagging applications.",
        "url": "http://arxiv.org/abs/2510.00652v1",
        "published_date": "2025-10-01T08:31:19+00:00",
        "updated_date": "2025-10-01T08:31:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jieer Ouyang",
            "Xiaoneng Xiang",
            "Zheng Wang",
            "Yangkai Ding"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "OTTER introduces a unified multi-label tagging framework that combines predefined category sets with user-driven open tags for multi-modal understanding. It outperforms competitive baselines on benchmark datasets and achieves near-perfect performance on open-set labels.",
        "tldr_zh": "OTTER引入了一个统一的多标签标记框架，将预定义的类别集与用户驱动的开放标签结合起来，用于多模态理解。在基准数据集上优于竞争基线，并在开放集标签上实现几乎完美的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Arbitrary Generative Video Interpolation",
        "summary": "Video frame interpolation (VFI), which generates intermediate frames from\ngiven start and end frames, has become a fundamental function in video\ngeneration applications. However, existing generative VFI methods are\nconstrained to synthesize a fixed number of intermediate frames, lacking the\nflexibility to adjust generated frame rates or total sequence duration. In this\nwork, we present ArbInterp, a novel generative VFI framework that enables\nefficient interpolation at any timestamp and of any length. Specifically, to\nsupport interpolation at any timestamp, we propose the Timestamp-aware Rotary\nPosition Embedding (TaRoPE), which modulates positions in temporal RoPE to\nalign generated frames with target normalized timestamps. This design enables\nfine-grained control over frame timestamps, addressing the inflexibility of\nfixed-position paradigms in prior work. For any-length interpolation, we\ndecompose long-sequence generation into segment-wise frame synthesis. We\nfurther design a novel appearance-motion decoupled conditioning strategy: it\nleverages prior segment endpoints to enforce appearance consistency and\ntemporal semantics to maintain motion coherence, ensuring seamless\nspatiotemporal transitions across segments. Experimentally, we build\ncomprehensive benchmarks for multi-scale frame interpolation (2x to 32x) to\nassess generalizability across arbitrary interpolation factors. Results show\nthat ArbInterp outperforms prior methods across all scenarios with higher\nfidelity and more seamless spatiotemporal continuity. Project website:\nhttps://mcg-nju.github.io/ArbInterp-Web/.",
        "url": "http://arxiv.org/abs/2510.00578v1",
        "published_date": "2025-10-01T06:57:10+00:00",
        "updated_date": "2025-10-01T06:57:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guozhen Zhang",
            "Haiguang Wang",
            "Chunyu Wang",
            "Yuan Zhou",
            "Qinglin Lu",
            "Limin Wang"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "ArbInterp is a novel generative video interpolation framework that allows for interpolation at any timestamp and of any length, outperforming prior methods in terms of fidelity and spatiotemporal continuity.",
        "tldr_zh": "ArbInterp 是一种新颖的视频插值框架，允许在任意时间戳和任意长度上进行插值，在保真度和时空连续性方面优于先前方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory",
        "summary": "Humans possess a remarkable ability to mentally explore and replay 3D\nenvironments they have previously experienced. Inspired by this mental process,\nwe present EvoWorld: a world model that bridges panoramic video generation with\nevolving 3D memory to enable spatially consistent long-horizon exploration.\nGiven a single panoramic image as input, EvoWorld first generates future video\nframes by leveraging a video generator with fine-grained view control, then\nevolves the scene's 3D reconstruction using a feedforward plug-and-play\ntransformer, and finally synthesizes futures by conditioning on geometric\nreprojections from this evolving explicit 3D memory. Unlike prior\nstate-of-the-arts that synthesize videos only, our key insight lies in\nexploiting this evolving 3D reconstruction as explicit spatial guidance for the\nvideo generation process, projecting the reconstructed geometry onto target\nviewpoints to provide rich spatial cues that significantly enhance both visual\nrealism and geometric consistency. To evaluate long-range exploration\ncapabilities, we introduce the first comprehensive benchmark spanning synthetic\noutdoor environments, Habitat indoor scenes, and challenging real-world\nscenarios, with particular emphasis on loop-closure detection and spatial\ncoherence over extended trajectories. Extensive experiments demonstrate that\nour evolving 3D memory substantially improves visual fidelity and maintains\nspatial scene coherence compared to existing approaches, representing a\nsignificant advance toward long-horizon spatially consistent world modeling.",
        "url": "http://arxiv.org/abs/2510.01183v1",
        "published_date": "2025-10-01T17:59:38+00:00",
        "updated_date": "2025-10-01T17:59:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahao Wang",
            "Luoxin Ye",
            "TaiMing Lu",
            "Junfei Xiao",
            "Jiahan Zhang",
            "Yuxiang Guo",
            "Xijun Liu",
            "Rama Chellappa",
            "Cheng Peng",
            "Alan Yuille",
            "Jieneng Chen"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "EvoWorld is a world model that combines panoramic video generation with evolving 3D memory to improve visual realism and spatial coherence for long-horizon exploration.",
        "tldr_zh": "EvoWorld是一个世界模型，结合全景视频生成和进化的3D记忆，以提高视觉逼真度和空间一致性，用于长程探索。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Strategic Fusion of Vision Language Models: Shapley-Credited Context-Aware Dawid-Skene for Multi-Label Tasks in Autonomous Driving",
        "summary": "Large vision-language models (VLMs) are increasingly used in\nautonomous-vehicle (AV) stacks, but hallucination limits their reliability in\nsafety-critical pipelines. We present Shapley-credited Context-Aware\nDawid-Skene with Agreement, a game-theoretic fusion method for multi-label\nunderstanding of ego-view dashcam video. It learns per-model, per-label,\ncontext-conditioned reliabilities from labelled history and, at inference,\nconverts each model's report into an agreement-guardrailed log-likelihood ratio\nthat is combined with a contextual prior and a public reputation state updated\nvia Shapley-based team credit. The result is calibrated, thresholdable\nposteriors that (i) amplify agreement among reliable models, (ii) preserve\nuniquely correct single-model signals, and (iii) adapt to drift. To specialise\ngeneral VLMs, we curate 1,000 real-world dashcam clips with structured\nannotations (scene description, manoeuvre recommendation, rationale) via an\nautomatic pipeline that fuses HDD ground truth, vehicle kinematics, and YOLOv11\n+ BoT-SORT tracking, guided by a three-step chain-of-thought prompt; three\nheterogeneous VLMs are then fine-tuned with LoRA. We evaluate with Hamming\ndistance, Micro-Macro-F1, and average per-video latency. Empirically, the\nproposed method achieves a 23% reduction in Hamming distance, 55% improvement\nin Macro-F1, and 47% improvement in Micro-F1 when comparing with the best\nsingle model, supporting VLM fusion as a calibrated, interpretable, and robust\ndecision-support component for AV pipelines.",
        "url": "http://arxiv.org/abs/2510.01126v1",
        "published_date": "2025-10-01T17:14:11+00:00",
        "updated_date": "2025-10-01T17:14:11+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Yuxiang Feng",
            "Keyang Zhang",
            "Hassane Ouchouid",
            "Ashwil Kaniamparambil",
            "Ioannis Souflas",
            "Panagiotis Angeloudis"
        ],
        "ai_categories": [
            "Multimodality",
            "LoRA",
            "Dataset"
        ],
        "tldr": "The paper introduces a method to fuse vision language models for multi-label understanding in autonomous driving, improving reliability and accuracy.",
        "tldr_zh": "本文介绍了一种融合视觉语言模型的方法，用于自动驾驶中的多标签理解，提高了可靠性和准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Training-free Uncertainty Guidance for Complex Visual Tasks with MLLMs",
        "summary": "Multimodal Large Language Models (MLLMs) often struggle with fine-grained\nperception, such as identifying small objects in high-resolution images or\nfinding key moments in long videos. Existing works typically rely on\ncomplicated, task-specific fine-tuning, which limits their generalizability and\nincreases model complexity. In this work, we propose an effective,\ntraining-free framework that uses an MLLM's intrinsic uncertainty as a\nproactive guidance signal. Our core insight is that a model's output entropy\ndecreases when presented with relevant visual information. We introduce a\nunified mechanism that scores candidate visual inputs by response uncertainty,\nenabling the model to autonomously focus on the most salient data. We apply\nthis simple principle to three complex visual tasks: Visual Search, Long Video\nUnderstanding, and Temporal Grounding, allowing off-the-shelf MLLMs to achieve\nperformance competitive with specialized, fine-tuned methods. Our work\nvalidates that harnessing intrinsic uncertainty is a powerful, general strategy\nfor enhancing fine-grained multimodal performance.",
        "url": "http://arxiv.org/abs/2510.00705v1",
        "published_date": "2025-10-01T09:20:51+00:00",
        "updated_date": "2025-10-01T09:20:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sanghwan Kim",
            "Rui Xiao",
            "Stephan Alaniz",
            "Yongqin Xian",
            "Zeynep Akata"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer"
        ],
        "tldr": "The paper introduces a training-free framework using Multimodal Large Language Models' uncertainty to guide complex visual tasks effectively.",
        "tldr_zh": "本文提出了一种无需训练的框架，利用多模态大语言模型的不确定性来有效地指导复杂的视觉任务。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "A Geometric Unification of Generative AI with Manifold-Probabilistic Projection Models",
        "summary": "The foundational premise of generative AI for images is the assumption that\nimages are inherently low-dimensional objects embedded within a\nhigh-dimensional space. Additionally, it is often implicitly assumed that\nthematic image datasets form smooth or piecewise smooth manifolds. Common\napproaches overlook the geometric structure and focus solely on probabilistic\nmethods, approximating the probability distribution through universal\napproximation techniques such as the kernel method. In some generative models,\nthe low dimensional nature of the data manifest itself by the introduction of a\nlower dimensional latent space. Yet, the probability distribution in the latent\nor the manifold coordinate space is considered uninteresting and is predefined\nor considered uniform. This study unifies the geometric and probabilistic\nperspectives by providing a geometric framework and a kernel-based\nprobabilistic method simultaneously. The resulting framework demystifies\ndiffusion models by interpreting them as a projection mechanism onto the\nmanifold of ``good images''. This interpretation leads to the construction of a\nnew deterministic model, the Manifold-Probabilistic Projection Model (MPPM),\nwhich operates in both the representation (pixel) space and the latent space.\nWe demonstrate that the Latent MPPM (LMPPM) outperforms the Latent Diffusion\nModel (LDM) across various datasets, achieving superior results in terms of\nimage restoration and generation.",
        "url": "http://arxiv.org/abs/2510.00666v1",
        "published_date": "2025-10-01T08:50:30+00:00",
        "updated_date": "2025-10-01T08:50:30+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Leah Bar",
            "Liron Mor Yosef",
            "Shai Zucker",
            "Neta Shoham",
            "Inbar Seroussi",
            "Nir Sochen"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a unified geometric and probabilistic framework for generative AI, leading to a new model that outperforms existing models in image restoration and generation.",
        "tldr_zh": "本文介绍了一种统一的几何和概率框架用于生成人工智能，导致一个新模型在图像恢复和生成方面优于现有模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Assessing Foundation Models for Mold Colony Detection with Limited Training Data",
        "summary": "The process of quantifying mold colonies on Petri dish samples is of critical\nimportance for the assessment of indoor air quality, as high colony counts can\nindicate potential health risks and deficiencies in ventilation systems.\nConventionally the automation of such a labor-intensive process, as well as\nother tasks in microbiology, relies on the manual annotation of large datasets\nand the subsequent extensive training of models like YoloV9. To demonstrate\nthat exhaustive annotation is not a prerequisite anymore when tackling a new\nvision task, we compile a representative dataset of 5000 Petri dish images\nannotated with bounding boxes, simulating both a traditional data collection\napproach as well as few-shot and low-shot scenarios with well curated subsets\nwith instance level masks. We benchmark three vision foundation models against\ntraditional baselines on task specific metrics, reflecting realistic real-world\nrequirements. Notably, MaskDINO attains near-parity with an extensively trained\nYoloV9 model while finetuned only on 150 images, retaining competitive\nperformance with as few as 25 images, still being reliable on $\\approx$ 70% of\nthe samples. Our results show that data-efficient foundation models can match\ntraditional approaches with only a fraction of the required data, enabling\nearlier development and faster iterative improvement of automated\nmicrobiological systems with a superior upper-bound performance than\ntraditional models would achieve.",
        "url": "http://arxiv.org/abs/2510.00561v1",
        "published_date": "2025-10-01T06:25:45+00:00",
        "updated_date": "2025-10-01T06:25:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Henrik Pichler",
            "Janis Keuper",
            "Matthew Copping"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper demonstrates that data-efficient vision models can achieve competitive performance with traditional models using a fraction of the required data, specifically for mold colony detection on Petri dish samples.",
        "tldr_zh": "该论文展示了数据效率的视觉模型可以使用少量数据达到与传统模型竞争性能，特别适用于蒙蒂萨皿样本上的霉菌群检测。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "IMAGEdit: Let Any Subject Transform",
        "summary": "In this paper, we present IMAGEdit, a training-free framework for any number\nof video subject editing that manipulates the appearances of multiple\ndesignated subjects while preserving non-target regions, without finetuning or\nretraining. We achieve this by providing robust multimodal conditioning and\nprecise mask sequences through a prompt-guided multimodal alignment module and\na prior-based mask retargeting module. We first leverage large models'\nunderstanding and generation capabilities to produce multimodal information and\nmask motion sequences for multiple subjects across various types. Then, the\nobtained prior mask sequences are fed into a pretrained mask-driven video\ngeneration model to synthesize the edited video. With strong generalization\ncapability, IMAGEdit remedies insufficient prompt-side multimodal conditioning\nand overcomes mask boundary entanglement in videos with any number of subjects,\nthereby significantly expanding the applicability of video editing. More\nimportantly, IMAGEdit is compatible with any mask-driven video generation\nmodel, significantly improving overall performance. Extensive experiments on\nour newly constructed multi-subject benchmark MSVBench verify that IMAGEdit\nconsistently surpasses state-of-the-art methods. Code, models, and datasets are\npublicly available at https://github.com/XWH-A/IMAGEdit.",
        "url": "http://arxiv.org/abs/2510.01186v1",
        "published_date": "2025-10-01T17:59:56+00:00",
        "updated_date": "2025-10-01T17:59:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fei Shen",
            "Weihao Xu",
            "Rui Yan",
            "Dong Zhang",
            "Xiangbo Shu",
            "Jinhui Tang"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "IMAGEdit is a training-free framework for video subject editing that manipulates appearances of multiple subjects while preserving non-target regions, achieving this through robust multimodal conditioning and precise mask sequences.",
        "tldr_zh": "IMAGEdit是一个无需训练的框架，用于视频主题编辑，通过强大的多模式条件和精确的蒙板序列，改变多个主题的外观同时保留非目标区域。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Code2Video: A Code-centric Paradigm for Educational Video Generation",
        "summary": "While recent generative models advance pixel-space video synthesis, they\nremain limited in producing professional educational videos, which demand\ndisciplinary knowledge, precise visual structures, and coherent transitions,\nlimiting their applicability in educational scenarios. Intuitively, such\nrequirements are better addressed through the manipulation of a renderable\nenvironment, which can be explicitly controlled via logical commands (e.g.,\ncode). In this work, we propose Code2Video, a code-centric agent framework for\ngenerating educational videos via executable Python code. The framework\ncomprises three collaborative agents: (i) Planner, which structures lecture\ncontent into temporally coherent flows and prepares corresponding visual\nassets; (ii) Coder, which converts structured instructions into executable\nPython codes while incorporating scope-guided auto-fix to enhance efficiency;\nand (iii) Critic, which leverages vision-language models (VLM) with visual\nanchor prompts to refine spatial layout and ensure clarity. To support\nsystematic evaluation, we build MMMC, a benchmark of professionally produced,\ndiscipline-specific educational videos. We evaluate MMMC across diverse\ndimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and\nparticularly, TeachQuiz, a novel end-to-end metric that quantifies how well a\nVLM, after unlearning, can recover knowledge by watching the generated videos.\nOur results demonstrate the potential of Code2Video as a scalable,\ninterpretable, and controllable approach, achieving 40% improvement over direct\ncode generation and producing videos comparable to human-crafted tutorials. The\ncode and datasets are available at https://github.com/showlab/Code2Video.",
        "url": "http://arxiv.org/abs/2510.01174v1",
        "published_date": "2025-10-01T17:56:48+00:00",
        "updated_date": "2025-10-01T17:56:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.HC",
            "cs.MM"
        ],
        "authors": [
            "Yanzhe Chen",
            "Kevin Qinghong Lin",
            "Mike Zheng Shou"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer",
            "Other"
        ],
        "tldr": "Code2Video is a code-centric framework for generating educational videos using executable Python code, achieving improved results compared to direct code generation.",
        "tldr_zh": "Code2Video是一个以代码为中心的框架，使用可执行的Python代码生成教育视频，与直接代码生成相比取得了改进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EditTrack: Detecting and Attributing AI-assisted Image Editing",
        "summary": "In this work, we formulate and study the problem of image-editing detection\nand attribution: given a base image and a suspicious image, detection seeks to\ndetermine whether the suspicious image was derived from the base image using an\nAI editing model, while attribution further identifies the specific editing\nmodel responsible. Existing methods for detecting and attributing AI-generated\nimages are insufficient for this problem, as they focus on determining whether\nan image was AI-generated/edited rather than whether it was edited from a\nparticular base image. To bridge this gap, we propose EditTrack, the first\nframework for this image-editing detection and attribution problem. Building on\nfour key observations about the editing process, EditTrack introduces a novel\nre-editing strategy and leverages carefully designed similarity metrics to\ndetermine whether a suspicious image originates from a base image and, if so,\nby which model. We evaluate EditTrack on five state-of-the-art editing models\nacross six datasets, demonstrating that it consistently achieves accurate\ndetection and attribution, significantly outperforming five baselines.",
        "url": "http://arxiv.org/abs/2510.01173v1",
        "published_date": "2025-10-01T17:56:35+00:00",
        "updated_date": "2025-10-01T17:56:35+00:00",
        "categories": [
            "cs.CR",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Zhengyuan Jiang",
            "Yuyang Zhang",
            "Moyang Guo",
            "Neil Zhenqiang Gong"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces EditTrack, a framework for detecting and attributing AI-assisted image editing, achieving accurate detection and attribution of editing models.",
        "tldr_zh": "该论文介绍了EditTrack，一个用于检测和归因AI辅助图像编辑的框架，实现了对编辑模型的准确检测和归因。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ReSWD: ReSTIR'd, not shaken. Combining Reservoir Sampling and Sliced Wasserstein Distance for Variance Reduction",
        "summary": "Distribution matching is central to many vision and graphics tasks, where the\nwidely used Wasserstein distance is too costly to compute for high dimensional\ndistributions. The Sliced Wasserstein Distance (SWD) offers a scalable\nalternative, yet its Monte Carlo estimator suffers from high variance,\nresulting in noisy gradients and slow convergence. We introduce Reservoir SWD\n(ReSWD), which integrates Weighted Reservoir Sampling into SWD to adaptively\nretain informative projection directions in optimization steps, resulting in\nstable gradients while remaining unbiased. Experiments on synthetic benchmarks\nand real-world tasks such as color correction and diffusion guidance show that\nReSWD consistently outperforms standard SWD and other variance reduction\nbaselines. Project page: https://reservoirswd.github.io/",
        "url": "http://arxiv.org/abs/2510.01061v1",
        "published_date": "2025-10-01T16:01:17+00:00",
        "updated_date": "2025-10-01T16:01:17+00:00",
        "categories": [
            "cs.GR",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Mark Boss",
            "Andreas Engelhardt",
            "Simon Donné",
            "Varun Jampani"
        ],
        "ai_categories": [
            "Other",
            "Dataset"
        ],
        "tldr": "The paper introduces Reservoir SWD (ReSWD) which combines Reservoir Sampling and Sliced Wasserstein Distance for variance reduction in distribution matching tasks, outperforming standard methods in various benchmarks and real-world tasks.",
        "tldr_zh": "本文介绍了Reservoir SWD（ReSWD），它将Reservoir Sampling和Sliced Wasserstein Distance结合起来，用于减小分布匹配任务中的方差，比标准方法在各种基准测试和实际任务中表现更好。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "KeySG: Hierarchical Keyframe-Based 3D Scene Graphs",
        "summary": "In recent years, 3D scene graphs have emerged as a powerful world\nrepresentation, offering both geometric accuracy and semantic richness.\nCombining 3D scene graphs with large language models enables robots to reason,\nplan, and navigate in complex human-centered environments. However, current\napproaches for constructing 3D scene graphs are semantically limited to a\npredefined set of relationships, and their serialization in large environments\ncan easily exceed an LLM's context window. We introduce KeySG, a framework that\nrepresents 3D scenes as a hierarchical graph consisting of floors, rooms,\nobjects, and functional elements, where nodes are augmented with multi-modal\ninformation extracted from keyframes selected to optimize geometric and visual\ncoverage. The keyframes allow us to efficiently leverage VLM to extract scene\ninformation, alleviating the need to explicitly model relationship edges\nbetween objects, enabling more general, task-agnostic reasoning and planning.\nOur approach can process complex and ambiguous queries while mitigating the\nscalability issues associated with large scene graphs by utilizing a\nhierarchical retrieval-augmented generation (RAG) pipeline to extract relevant\ncontext from the graph. Evaluated across four distinct benchmarks -- including\n3D object segmentation and complex query retrieval -- KeySG outperforms prior\napproaches on most metrics, demonstrating its superior semantic richness and\nefficiency.",
        "url": "http://arxiv.org/abs/2510.01049v1",
        "published_date": "2025-10-01T15:53:27+00:00",
        "updated_date": "2025-10-01T15:53:27+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Abdelrhman Werby",
            "Dennis Rotondi",
            "Fabio Scaparro",
            "Kai O. Arras"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "KeySG proposes a hierarchical keyframe-based 3D scene graph framework that improves reasoning and planning in complex environments, showcasing superior performance in various benchmarks.",
        "tldr_zh": "KeySG提出了一种基于关键帧的层次化3D场景图框架，改善了复杂环境中的推理和规划能力，在各种基准测试中展现出卓越表现。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Authentic Discrete Diffusion Model",
        "summary": "We propose an Authentic Discrete Diffusion (ADD) framework that fundamentally\nredefines prior pseudo-discrete approaches by preserving core diffusion\ncharacteristics directly in the one-hot space through a suite of coordinated\nmechanisms. Unlike conventional \"pseudo\" discrete diffusion (PDD) methods, ADD\nreformulates the diffusion input by directly using float-encoded one-hot class\ndata, without relying on diffusing in the continuous latent spaces or masking\npolicies. At its core, a timestep-conditioned cross-entropy loss is introduced\nbetween the diffusion model's outputs and the original one-hot labels. This\nsynergistic design establishes a bridge between discriminative and generative\nlearning. Our experiments demonstrate that ADD not only achieves superior\nperformance on classification tasks compared to the baseline, but also exhibits\nexcellent text generation capabilities on Image captioning. Extensive ablations\nvalidate the measurable gains of each component.",
        "url": "http://arxiv.org/abs/2510.01047v1",
        "published_date": "2025-10-01T15:51:10+00:00",
        "updated_date": "2025-10-01T15:51:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Xiao Li",
            "Jiaqi Zhang",
            "Shuxiang Zhang",
            "Tianshui Chen",
            "Liang Lin",
            "Guangrun Wang"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN"
        ],
        "tldr": "The paper introduces an Authentic Discrete Diffusion framework that enhances performance on classification tasks and text generation compared to conventional methods by directly using float-encoded one-hot class data.",
        "tldr_zh": "本文介绍了一种名为Authentic Discrete Diffusion (ADD)的框架，通过直接使用浮点编码的独热类数据，在分类任务和文本生成方面相对于传统方法展现出更好的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Secure and reversible face anonymization with diffusion models",
        "summary": "Face images processed by computer vision algorithms contain sensitive\npersonal information that malicious actors can capture without consent. These\nprivacy and security risks highlight the need for effective face anonymization\nmethods. Current methods struggle to propose a good trade-off between a secure\nscheme with high-quality image generation and reversibility for later person\nauthentication. Diffusion-based approaches produce high-quality anonymized\nimages but lack the secret key mechanism to ensure that only authorized parties\ncan reverse the process. In this paper, we introduce, to our knowledge, the\nfirst secure, high-quality reversible anonymization method based on a diffusion\nmodel. We propose to combine the secret key with the latent faces\nrepresentation of the diffusion model. To preserve identity-irrelevant\nfeatures, generation is constrained by a facial mask, maintaining high-quality\nimages. By using a deterministic forward and backward diffusion process, our\napproach enforces that the original face can be recovered with the correct\nsecret key. We also show that the proposed method produces anonymized faces\nthat are less visually similar to the original faces, compared to other\nprevious work.",
        "url": "http://arxiv.org/abs/2510.01031v1",
        "published_date": "2025-10-01T15:37:20+00:00",
        "updated_date": "2025-10-01T15:37:20+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Pol Labarbarie",
            "Vincent Itier",
            "William Puech"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN"
        ],
        "tldr": "The paper introduces a secure and reversible face anonymization method based on diffusion models, combining a secret key with the latent faces representation.",
        "tldr_zh": "该论文介绍了一种基于扩散模型的安全可逆的人脸匿名化方法，将秘钥与潜在面部表示结合。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "POVQA: Preference-Optimized Video Question Answering with Rationales for Data Efficiency",
        "summary": "Video Question Answering (VQA) with Large Vision Language Models (LVLMs) has\ngained significant traction in research ever since the Flamingo was introduced\nby Deepmind. Recent advancements in large context/long video question answering\nhave allowed VQA tasks to have context window of 1500+ frames. However, this\nonly leads to 50 seconds of video footage without losing any significant\ninformation. We introduce POVQA, a data-efficient pipeline that compresses each\nsecond of video into a single temporally pooled image (via motion blur and\nweighted averaging variants) and then align LVLMs with lightweight supervision.\nConcretely, we build 1 fps input sources using Blend Blur with Last Frame,\nWeighted Average, Exponential and Ramp pooling and fine-tune QWEN-2.5-VL 7B\nwith supervised two turn target including reasoning and final answer. We apply\nSupervised Fine Tuning (SFT) and Direct Preference Optimization (DPO) on our\nnovel dataset ReasonVQA consisting of 12 movies with 239 human annotated\nquestion-answer with reasoning prompts. On our ReasonVQA dataset, this method\ndramatically improves performance over pooled baselines: F1 score improves from\n0.212 to 0.543, BLEU-4 from 0.031 to 0.291, and ROUGE-L from 0.196 to 0.528.\nRationale quality also significantly increases. Cross-evaluation of SFT + DPO\non various pooling functions show that the gains persist regardless of the\npooling scheme used at train or test time, indicating strong robustness on\nsummarization of temporal evidence. Similar observations were made on zero-shot\nin TVQA.",
        "url": "http://arxiv.org/abs/2510.01009v1",
        "published_date": "2025-10-01T15:15:36+00:00",
        "updated_date": "2025-10-01T15:15:36+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Ashim Dahal",
            "Ankit Ghimire",
            "Saydul Akbar Murad",
            "Nick Rahimi"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a data-efficient pipeline, POVQA, for video question answering by compressing video into temporally pooled images and fine-tuning LVLMs with supervised learning.",
        "tldr_zh": "本文介绍了一种数据高效的视频问答流水线，通过将视频压缩为时间池化图像，并通过监督学习微调LVLMs。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SoftCFG: Uncertainty-guided Stable Guidance for Visual Autoregressive Model",
        "summary": "Autoregressive (AR) models have emerged as powerful tools for image\ngeneration by modeling images as sequences of discrete tokens. While\nClassifier-Free Guidance (CFG) has been adopted to improve conditional\ngeneration, its application in AR models faces two key issues: guidance\ndiminishing, where the conditional-unconditional gap quickly vanishes as\ndecoding progresses, and over-guidance, where strong conditions distort visual\ncoherence. To address these challenges, we propose SoftCFG, an\nuncertainty-guided inference method that distributes adaptive perturbations\nacross all tokens in the sequence. The key idea behind SoftCFG is to let each\ngenerated token contribute certainty-weighted guidance, ensuring that the\nsignal persists across steps while resolving conflicts between text guidance\nand visual context. To further stabilize long-sequence generation, we introduce\nStep Normalization, which bounds cumulative perturbations of SoftCFG. Our\nmethod is training-free, model-agnostic, and seamlessly integrates with\nexisting AR pipelines. Experiments show that SoftCFG significantly improves\nimage quality over standard CFG and achieves state-of-the-art FID on ImageNet\n256*256 among autoregressive models.",
        "url": "http://arxiv.org/abs/2510.00996v2",
        "published_date": "2025-10-01T15:04:00+00:00",
        "updated_date": "2025-10-02T09:32:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dongli Xu",
            "Aleksei Tiulpin",
            "Matthew B. Blaschko"
        ],
        "ai_categories": [
            "GAN",
            "Transformer"
        ],
        "tldr": "SoftCFG is an uncertainty-guided inference method for visual autoregressive models that improves image quality and achieves state-of-the-art results on ImageNet 256*256.",
        "tldr_zh": "SoftCFG是一种面向视觉自回归模型的不确定性引导推断方法，可提高图像质量，在ImageNet 256*256上取得了最新成果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Visual Self-Refinement for Autoregressive Models",
        "summary": "Autoregressive models excel in sequential modeling and have proven to be\neffective for vision-language data. However, the spatial nature of visual\nsignals conflicts with the sequential dependencies of next-token prediction,\nleading to suboptimal results. This work proposes a plug-and-play refinement\nmodule to enhance the complex spatial correspondence modeling within the\ngenerated visual sequence. This module operates as a post-pretraining step to\njointly refine all generated tokens of autoregressive model, enhancing\nvision-language modeling under a shared sequential prediction framework. By\nleveraging global context and relationship across the tokens, our method\nmitigates the error accumulation issue within the sequential generation.\nExperiments demonstrate that the proposed method improves the generation\nquality, enhancing the model's ability to produce semantically consistent\nresults.",
        "url": "http://arxiv.org/abs/2510.00993v1",
        "published_date": "2025-10-01T15:03:32+00:00",
        "updated_date": "2025-10-01T15:03:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiamian Wang",
            "Ziqi Zhou",
            "Chaithanya Kumar Mummadi",
            "Sohail Dianat",
            "Majid Rabbani",
            "Raghuveer Rao",
            "Chen Qiu",
            "Zhiqiang Tao"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper proposes a method to refine the spatial modeling of visual sequences generated by autoregressive models, improving the quality of vision-language modeling.",
        "tldr_zh": "本文提出了一种方法，用于优化自回归模型生成的视觉序列的空间建模，从而提高视觉-语言建模的质量。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "JEPA-T: Joint-Embedding Predictive Architecture with Text Fusion for Image Generation",
        "summary": "Modern Text-to-Image (T2I) generation increasingly relies on token-centric\narchitectures that are trained with self-supervision, yet effectively fusing\ntext with visual tokens remains a challenge. We propose \\textbf{JEPA-T}, a\nunified multimodal framework that encodes images and captions into discrete\nvisual and textual tokens, processed by a joint-embedding predictive\nTransformer. To enhance fusion, we incorporate cross-attention after the\nfeature predictor for conditional denoising while maintaining a task-agnostic\nbackbone. Additionally, raw texts embeddings are injected prior to the flow\nmatching loss to improve alignment during training. During inference, the same\nnetwork performs both class-conditional and free-text image generation by\niteratively denoising visual tokens conditioned on text. Evaluations on\nImageNet-1K demonstrate that JEPA-T achieves strong data efficiency,\nopen-vocabulary generalization, and consistently outperforms non-fusion and\nlate-fusion baselines. Our approach shows that late architectural fusion\ncombined with objective-level alignment offers an effective balance between\nconditioning strength and backbone generality in token-based T2I.The code is\nnow available: https://github.com/justin-herry/JEPA-T.git",
        "url": "http://arxiv.org/abs/2510.00974v1",
        "published_date": "2025-10-01T14:51:10+00:00",
        "updated_date": "2025-10-01T14:51:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Siheng Wan",
            "Zhengtao Yao",
            "Zhengdao Li",
            "Junhao Dong",
            "Yanshu Li",
            "Yikai Li",
            "Linshan Li",
            "Haoyan Xu",
            "Yijiang Li",
            "Zhikang Dong",
            "Huacan Wang",
            "Jifeng Shen"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "JEPA-T proposes a joint-embedding predictive architecture for text-to-image generation, achieving strong data efficiency and outperforming baselines.",
        "tldr_zh": "JEPA-T 提出了一种联合嵌入预测架构，用于文本到图像的生成，在数据效率方面表现强大，优于基线模型。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "InfVSR: Breaking Length Limits of Generic Video Super-Resolution",
        "summary": "Real-world videos often extend over thousands of frames. Existing video\nsuper-resolution (VSR) approaches, however, face two persistent challenges when\nprocessing long sequences: (1) inefficiency due to the heavy cost of multi-step\ndenoising for full-length sequences; and (2) poor scalability hindered by\ntemporal decomposition that causes artifacts and discontinuities. To break\nthese limits, we propose InfVSR, which novelly reformulates VSR as an\nautoregressive-one-step-diffusion paradigm. This enables streaming inference\nwhile fully leveraging pre-trained video diffusion priors. First, we adapt the\npre-trained DiT into a causal structure, maintaining both local and global\ncoherence via rolling KV-cache and joint visual guidance. Second, we distill\nthe diffusion process into a single step efficiently, with patch-wise pixel\nsupervision and cross-chunk distribution matching. Together, these designs\nenable efficient and scalable VSR for unbounded-length videos. To fill the gap\nin long-form video evaluation, we build a new benchmark tailored for extended\nsequences and further introduce semantic-level metrics to comprehensively\nassess temporal consistency. Our method pushes the frontier of long-form VSR,\nachieves state-of-the-art quality with enhanced semantic consistency, and\ndelivers up to 58x speed-up over existing methods such as MGLD-VSR. Code will\nbe available at https://github.com/Kai-Liu001/InfVSR.",
        "url": "http://arxiv.org/abs/2510.00948v1",
        "published_date": "2025-10-01T14:21:45+00:00",
        "updated_date": "2025-10-01T14:21:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziqing Zhang",
            "Kai Liu",
            "Zheng Chen",
            "Xi Li",
            "Yucong Chen",
            "Bingnan Duan",
            "Linghe Kong",
            "Yulun Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper introduces InfVSR, a novel approach for video super-resolution that breaks length limits and enhances semantic consistency, achieving state-of-the-art quality with significant speed-up.",
        "tldr_zh": "本文介绍了InfVSR，一种突破长度限制并提高语义一致性的视频超分辨率方法，实现了最先进的质量和显著的加速。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8.0
    },
    {
        "title": "Equivariant Splitting: Self-supervised learning from incomplete data",
        "summary": "Self-supervised learning for inverse problems allows to train a\nreconstruction network from noise and/or incomplete data alone. These methods\nhave the potential of enabling learning-based solutions when obtaining\nground-truth references for training is expensive or even impossible. In this\npaper, we propose a new self-supervised learning strategy devised for the\nchallenging setting where measurements are observed via a single incomplete\nobservation model. We introduce a new definition of equivariance in the context\nof reconstruction networks, and show that the combination of self-supervised\nsplitting losses and equivariant reconstruction networks results in the same\nminimizer in expectation as the one of a supervised loss. Through a series of\nexperiments on image inpainting, accelerated magnetic resonance imaging, and\ncompressive sensing, we demonstrate that the proposed loss achieves\nstate-of-the-art performance in settings with highly rank-deficient forward\nmodels.",
        "url": "http://arxiv.org/abs/2510.00929v2",
        "published_date": "2025-10-01T14:08:17+00:00",
        "updated_date": "2025-10-02T15:27:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Victor Sechaud",
            "Jérémy Scanvic",
            "Quentin Barthélemy",
            "Patrice Abry",
            "Julián Tachella"
        ],
        "ai_categories": [
            "GAN",
            "Transformer"
        ],
        "tldr": "The paper introduces a self-supervised learning strategy for reconstructing data from incomplete observations, achieving state-of-the-art performance in image inpainting, MRI acceleration, and compressive sensing.",
        "tldr_zh": "本文介绍了一种自监督学习策略，用于从不完整观测中重建数据，在图像修复、磁共振加速和压缩感知方面取得了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PAL-Net: A Point-Wise CNN with Patch-Attention for 3D Facial Landmark Localization",
        "summary": "Manual annotation of anatomical landmarks on 3D facial scans is a\ntime-consuming and expertise-dependent task, yet it remains critical for\nclinical assessments, morphometric analysis, and craniofacial research. While\nseveral deep learning methods have been proposed for facial landmark\nlocalization, most focus on pseudo-landmarks or require complex input\nrepresentations, limiting their clinical applicability. This study presents a\nfully automated deep learning pipeline (PAL-Net) for localizing 50 anatomical\nlandmarks on stereo-photogrammetry facial models. The method combines coarse\nalignment, region-of-interest filtering, and an initial approximation of\nlandmarks with a patch-based pointwise CNN enhanced by attention mechanisms.\nTrained and evaluated on 214 annotated scans from healthy adults, PAL-Net\nachieved a mean localization error of 3.686 mm and preserves relevant\nanatomical distances with a 2.822 mm average error, comparable to\nintra-observer variability. To assess generalization, the model was further\nevaluated on 700 subjects from the FaceScape dataset, achieving a point-wise\nerror of 0.41\\,mm and a distance-wise error of 0.38\\,mm. Compared to existing\nmethods, PAL-Net offers a favorable trade-off between accuracy and\ncomputational cost. While performance degrades in regions with poor mesh\nquality (e.g., ears, hairline), the method demonstrates consistent accuracy\nacross most anatomical regions. PAL-Net generalizes effectively across datasets\nand facial regions, outperforming existing methods in both point-wise and\nstructural evaluations. It provides a lightweight, scalable solution for\nhigh-throughput 3D anthropometric analysis, with potential to support clinical\nworkflows and reduce reliance on manual annotation. Source code can be found at\nhttps://github.com/Ali5hadman/PAL-Net-A-Point-Wise-CNN-with-Patch-Attention",
        "url": "http://arxiv.org/abs/2510.00910v1",
        "published_date": "2025-10-01T13:52:35+00:00",
        "updated_date": "2025-10-01T13:52:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ali Shadman Yazdi",
            "Annalisa Cappella",
            "Benedetta Baldini",
            "Riccardo Solazzo",
            "Gianluca Tartaglia",
            "Chiarella Sforza",
            "Giuseppe Baselli"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces PAL-Net, a deep learning model for 3D facial landmark localization using patch-based CNN with attention mechanisms, achieving high accuracy and generalization across datasets.",
        "tldr_zh": "本文介绍了PAL-Net，一种使用基于CNN的补丁和注意机制的深度学习模型，用于3D面部标志定位，在数据集上取得了很高的准确度和泛化能力。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MorphGen: Controllable and Morphologically Plausible Generative Cell-Imaging",
        "summary": "Simulating in silico cellular responses to interventions is a promising\ndirection to accelerate high-content image-based assays, critical for advancing\ndrug discovery and gene editing. To support this, we introduce MorphGen, a\nstate-of-the-art diffusion-based generative model for fluorescent microscopy\nthat enables controllable generation across multiple cell types and\nperturbations. To capture biologically meaningful patterns consistent with\nknown cellular morphologies, MorphGen is trained with an alignment loss to\nmatch its representations to the phenotypic embeddings of OpenPhenom, a\nstate-of-the-art biological foundation model. Unlike prior approaches that\ncompress multichannel stains into RGB images -- thus sacrificing\norganelle-specific detail -- MorphGen generates the complete set of fluorescent\nchannels jointly, preserving per-organelle structures and enabling a\nfine-grained morphological analysis that is essential for biological\ninterpretation. We demonstrate biological consistency with real images via\nCellProfiler features, and MorphGen attains an FID score over $35\\%$ lower than\nthe prior state-of-the-art MorphoDiff, which only generates RGB images for a\nsingle cell type. Code is available at https://github.com/czi-ai/MorphGen.",
        "url": "http://arxiv.org/abs/2510.01298v1",
        "published_date": "2025-10-01T13:34:29+00:00",
        "updated_date": "2025-10-01T13:34:29+00:00",
        "categories": [
            "q-bio.QM",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Berker Demirel",
            "Marco Fumero",
            "Theofanis Karaletsos",
            "Francesco Locatello"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "MorphGen is a generative model for simulating cellular responses to interventions in high-content image-based assays, preserving organelle-specific detail and enabling fine-grained morphological analysis.",
        "tldr_zh": "MorphGen 是一个生成模型，用于模拟细胞对干预的响应，在高内容图像分析中保留细胞器特定细节，实现精细的形态分析。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Gather-Scatter Mamba: Accelerating Propagation with Efficient State Space Model",
        "summary": "State Space Models (SSMs)-most notably RNNs-have historically played a\ncentral role in sequential modeling. Although attention mechanisms such as\nTransformers have since dominated due to their ability to model global context,\ntheir quadratic complexity and limited scalability make them less suited for\nlong sequences. Video super-resolution (VSR) methods have traditionally relied\non recurrent architectures to propagate features across frames. However, such\napproaches suffer from well-known issues including vanishing gradients, lack of\nparallelism, and slow inference speed. Recent advances in selective SSMs like\nMamba offer a compelling alternative: by enabling input-dependent state\ntransitions with linear-time complexity, Mamba mitigates these issues while\nmaintaining strong long-range modeling capabilities. Despite this potential,\nMamba alone struggles to capture fine-grained spatial dependencies due to its\ncausal nature and lack of explicit context aggregation. To address this, we\npropose a hybrid architecture that combines shifted window self-attention for\nspatial context aggregation with Mamba-based selective scanning for efficient\ntemporal propagation. Furthermore, we introduce Gather-Scatter Mamba (GSM), an\nalignment-aware mechanism that warps features toward a center anchor frame\nwithin the temporal window before Mamba propagation and scatters them back\nafterward, effectively reducing occlusion artifacts and ensuring effective\nredistribution of aggregated information across all frames. The official\nimplementation is provided at: https://github.com/Ko-Lani/GSMamba.",
        "url": "http://arxiv.org/abs/2510.00862v1",
        "published_date": "2025-10-01T13:11:13+00:00",
        "updated_date": "2025-10-01T13:11:13+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hyun-kyu Ko",
            "Youbin Kim",
            "Jihyeon Park",
            "Dongheok Park",
            "Gyeongjin Kang",
            "Wonjun Cho",
            "Hyung Yi",
            "Eunbyung Park"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces Gather-Scatter Mamba (GSM), a hybrid architecture combining self-attention for spatial context aggregation with selective scanning for efficient temporal propagation in video super-resolution.",
        "tldr_zh": "本文介绍了Gather-Scatter Mamba（GSM），一种混合架构，结合了自注意力机制进行空间上下文聚合和选择性扫描进行高效时间传播，用于视频超分辨率。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Can World Models Benefit VLMs for World Dynamics?",
        "summary": "Trained on internet-scale video data, generative world models are\nincreasingly recognized as powerful world simulators that can generate\nconsistent and plausible dynamics over structure, motion, and physics. This\nraises a natural question: with the advent of strong video foundational models,\nmight they supplant conventional vision encoder paradigms for general-purpose\nmultimodal understanding? While recent studies have begun to explore the\npotential of world models on common vision tasks, these explorations typically\nlack a systematic investigation of generic, multimodal tasks. In this work, we\nstrive to investigate the capabilities when world model priors are transferred\ninto Vision-Language Models: we re-purpose a video diffusion model as a\ngenerative encoder to perform a single denoising step and treat the resulting\nlatents as a set of visual embedding. We empirically investigate this class of\nmodels, which we refer to as World-Language Models (WorldLMs), and we find that\ngenerative encoders can capture latents useful for downstream understanding\nthat show distinctions from conventional encoders. Naming our best-performing\nvariant Dynamic Vision Aligner (DyVA), we further discover that this method\nsignificantly enhances spatial reasoning abilities and enables single-image\nmodels to perform multi-frame reasoning. Through the curation of a suite of\nvisual reasoning tasks, we find DyVA to surpass both open-source and\nproprietary baselines, achieving state-of-the-art or comparable performance. We\nattribute these gains to WorldLM's inherited motion-consistency internalization\nfrom video pre-training. Finally, we systematically explore extensive model\ndesigns to highlight promising directions for future work. We hope our study\ncan pave the way for a new family of VLMs that leverage priors from world\nmodels and are on a promising path towards generalist vision learners.",
        "url": "http://arxiv.org/abs/2510.00855v1",
        "published_date": "2025-10-01T13:07:05+00:00",
        "updated_date": "2025-10-01T13:07:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Kevin Zhang",
            "Kuangzhi Ge",
            "Xiaowei Chi",
            "Renrui Zhang",
            "Shaojun Shi",
            "Zhen Dong",
            "Sirui Han",
            "Shanghang Zhang"
        ],
        "ai_categories": [
            "Manymodal"
        ],
        "tldr": "The paper explores the use of world models in Vision-Language Models to enhance visual reasoning abilities and achieve state-of-the-art performance on visual reasoning tasks.",
        "tldr_zh": "本文探讨了在视觉-语言模型中使用世界模型以增强视觉推理能力，并在视觉推理任务中取得了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "NSARM: Next-Scale Autoregressive Modeling for Robust Real-World Image Super-Resolution",
        "summary": "Most recent real-world image super-resolution (Real-ISR) methods employ\npre-trained text-to-image (T2I) diffusion models to synthesize the high-quality\nimage either from random Gaussian noise, which yields realistic results but is\nslow due to iterative denoising, or directly from the input low-quality image,\nwhich is efficient but at the price of lower output quality. These approaches\ntrain ControlNet or LoRA modules while keeping the pre-trained model fixed,\nwhich often introduces over-enhanced artifacts and hallucinations, suffering\nfrom the robustness to inputs of varying degradations. Recent visual\nautoregressive (AR) models, such as pre-trained Infinity, can provide strong\nT2I generation capabilities while offering superior efficiency by using the\nbitwise next-scale prediction strategy. Building upon next-scale prediction, we\nintroduce a robust Real-ISR framework, namely Next-Scale Autoregressive\nModeling (NSARM). Specifically, we train NSARM in two stages: a transformation\nnetwork is first trained to map the input low-quality image to preliminary\nscales, followed by an end-to-end full-model fine-tuning. Such a comprehensive\nfine-tuning enhances the robustness of NSARM in Real-ISR tasks without\ncompromising its generative capability. Extensive quantitative and qualitative\nevaluations demonstrate that as a pure AR model, NSARM achieves superior visual\nresults over existing Real-ISR methods while maintaining a fast inference\nspeed. Most importantly, it demonstrates much higher robustness to the quality\nof input images, showing stronger generalization performance. Project page:\nhttps://github.com/Xiangtaokong/NSARM",
        "url": "http://arxiv.org/abs/2510.00820v1",
        "published_date": "2025-10-01T12:29:58+00:00",
        "updated_date": "2025-10-01T12:29:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiangtao Kong",
            "Rongyuan Wu",
            "Shuaizheng Liu",
            "Lingchen Sun",
            "Lei Zhang"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Dataset"
        ],
        "tldr": "NSARM introduces a robust Real-ISR framework for image super-resolution through autoregressive modeling, demonstrating superior visual results and robustness to input quality.",
        "tldr_zh": "NSARM通过自回归建模引入了一个强大的Real-ISR框架，实现了出色的视觉结果和对输入质量的稳健性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PhraseStereo: The First Open-Vocabulary Stereo Image Segmentation Dataset",
        "summary": "Understanding how natural language phrases correspond to specific regions in\nimages is a key challenge in multimodal semantic segmentation. Recent advances\nin phrase grounding are largely limited to single-view images, neglecting the\nrich geometric cues available in stereo vision. For this, we introduce\nPhraseStereo, the first novel dataset that brings phrase-region segmentation to\nstereo image pairs. PhraseStereo builds upon the PhraseCut dataset by\nleveraging GenStereo to generate accurate right-view images from existing\nsingle-view data, enabling the extension of phrase grounding into the stereo\ndomain. This new setting introduces unique challenges and opportunities for\nmultimodal learning, particularly in leveraging depth cues for more precise and\ncontext-aware grounding. By providing stereo image pairs with aligned\nsegmentation masks and phrase annotations, PhraseStereo lays the foundation for\nfuture research at the intersection of language, vision, and 3D perception,\nencouraging the development of models that can reason jointly over semantics\nand geometry. The PhraseStereo dataset will be released online upon acceptance\nof this work.",
        "url": "http://arxiv.org/abs/2510.00818v1",
        "published_date": "2025-10-01T12:29:24+00:00",
        "updated_date": "2025-10-01T12:29:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Thomas Campagnolo",
            "Ezio Malis",
            "Philippe Martinet",
            "Gaetan Bahl"
        ],
        "ai_categories": [
            "Dataset",
            "Multimodality"
        ],
        "tldr": "The paper introduces PhraseStereo, a novel dataset for stereo image segmentation, combining language phrases and image regions, enabling multimodal learning.",
        "tldr_zh": "本文介绍了PhraseStereo，这是一个专为立体图像分割而设计的新型数据集，将语言短语和图像区域相结合，实现多模态学习。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From Seeing to Predicting: A Vision-Language Framework for Trajectory Forecasting and Controlled Video Generation",
        "summary": "Current video generation models produce physically inconsistent motion that\nviolates real-world dynamics. We propose TrajVLM-Gen, a two-stage framework for\nphysics-aware image-to-video generation. First, we employ a Vision Language\nModel to predict coarse-grained motion trajectories that maintain consistency\nwith real-world physics. Second, these trajectories guide video generation\nthrough attention-based mechanisms for fine-grained motion refinement. We build\na trajectory prediction dataset based on video tracking data with realistic\nmotion patterns. Experiments on UCF-101 and MSR-VTT demonstrate that\nTrajVLM-Gen outperforms existing methods, achieving competitive FVD scores of\n545 on UCF-101 and 539 on MSR-VTT.",
        "url": "http://arxiv.org/abs/2510.00806v1",
        "published_date": "2025-10-01T12:11:36+00:00",
        "updated_date": "2025-10-01T12:11:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fan Yang",
            "Zhiyang Chen",
            "Yousong Zhu",
            "Xin Li",
            "Jinqiao Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a framework, TrajVLM-Gen, for physics-aware image-to-video generation using a Vision Language Model for trajectory prediction and attention-based mechanisms for motion refinement, outperforming existing methods on two datasets.",
        "tldr_zh": "本文提出了一种框架TrajVLM-Gen，用于基于视觉语言模型进行物理感知的图像到视频生成，通过轨迹预测和基于注意力机制的运动细化，优于现有方法在两个数据集上的表现。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Uncertainty-Aware Concept Bottleneck Models with Enhanced Interpretability",
        "summary": "In the context of image classification, Concept Bottleneck Models (CBMs)\nfirst embed images into a set of human-understandable concepts, followed by an\nintrinsically interpretable classifier that predicts labels based on these\nintermediate representations. While CBMs offer a semantically meaningful and\ninterpretable classification pipeline, they often sacrifice predictive\nperformance compared to end-to-end convolutional neural networks. Moreover, the\npropagation of uncertainty from concept predictions to final label decisions\nremains underexplored. In this paper, we propose a novel uncertainty-aware and\ninterpretable classifier for the second stage of CBMs. Our method learns a set\nof binary class-level concept prototypes and uses the distances between\npredicted concept vectors and each class prototype as both a classification\nscore and a measure of uncertainty. These prototypes also serve as\ninterpretable classification rules, indicating which concepts should be present\nin an image to justify a specific class prediction. The proposed framework\nenhances both interpretability and robustness by enabling conformal prediction\nfor uncertain or outlier inputs based on their deviation from the learned\nbinary class-level concept prototypes.",
        "url": "http://arxiv.org/abs/2510.00773v1",
        "published_date": "2025-10-01T11:11:18+00:00",
        "updated_date": "2025-10-01T11:11:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Haifei Zhang",
            "Patrick Barry",
            "Eduardo Brandao"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer"
        ],
        "tldr": "The paper proposes a novel uncertainty-aware and interpretable classifier for Concept Bottleneck Models in image classification. It enhances interpretability and robustness by incorporating uncertainty measures and binary class-level concept prototypes.",
        "tldr_zh": "本文提出了一种新颖的关于概念瓶颈模型的不确定性感知和可解释的分类器，用于图像分类。通过包含不确定性度量和二进制类级概念原型，增强了解释性和鲁棒性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ZQBA: Zero Query Black-box Adversarial Attack",
        "summary": "Current black-box adversarial attacks either require multiple queries or\ndiffusion models to produce adversarial samples that can impair the target\nmodel performance. However, these methods require training a surrogate loss or\ndiffusion models to produce adversarial samples, which limits their\napplicability in real-world settings. Thus, we propose a Zero Query Black-box\nAdversarial (ZQBA) attack that exploits the representations of Deep Neural\nNetworks (DNNs) to fool other networks. Instead of requiring thousands of\nqueries to produce deceiving adversarial samples, we use the feature maps\nobtained from a DNN and add them to clean images to impair the classification\nof a target model. The results suggest that ZQBA can transfer the adversarial\nsamples to different models and across various datasets, namely CIFAR and Tiny\nImageNet. The experiments also show that ZQBA is more effective than\nstate-of-the-art black-box attacks with a single query, while maintaining the\nimperceptibility of perturbations, evaluated both quantitatively (SSIM) and\nqualitatively, emphasizing the vulnerabilities of employing DNNs in real-world\ncontexts. All the source code is available at\nhttps://github.com/Joana-Cabral/ZQBA.",
        "url": "http://arxiv.org/abs/2510.00769v1",
        "published_date": "2025-10-01T11:00:53+00:00",
        "updated_date": "2025-10-01T11:00:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Joana C. Costa",
            "Tiago Roxo",
            "Hugo Proença",
            "Pedro R. M. Inácio"
        ],
        "ai_categories": [
            "GAN",
            "Transformer"
        ],
        "tldr": "The paper introduces a Zero Query Black-box Adversarial Attack (ZQBA) that can fool deep neural networks with minimal queries by using feature maps to generate imperceptible perturbations on images.",
        "tldr_zh": "本文介绍了一种零查询黑盒对抗攻击 (ZQBA)，通过使用特征图生成图像的不可察觉扰动，可以用最少的查询数愚弄深度神经网络。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multi-Objective Task-Aware Predictor for Image-Text Alignment",
        "summary": "Evaluating image-text alignment while reflecting human preferences across\nmultiple aspects is a significant issue for the development of reliable\nvision-language applications. It becomes especially crucial in real-world\nscenarios where multiple valid descriptions exist depending on contexts or user\nneeds. However, research progress is hindered by the lack of comprehensive\nbenchmarks and existing evaluation predictors lacking at least one of these key\nproperties: (1) Alignment with human judgments, (2) Long-sequence processing,\n(3) Inference efficiency, and (4) Applicability to multi-objective scoring. To\naddress these challenges, we propose a plug-and-play architecture to build a\nrobust predictor, MULTI-TAP (Multi-Objective Task-Aware Predictor), capable of\nboth multi and single-objective scoring. MULTI-TAP can produce a single overall\nscore, utilizing a reward head built on top of a large vision-language model\n(LVLMs). We show that MULTI-TAP is robust in terms of application to different\nLVLM architectures, achieving significantly higher performance than existing\nmetrics and even on par with the GPT-4o-based predictor, G-VEval, with a\nsmaller size (7-8B). By training a lightweight ridge regression layer on the\nfrozen hidden states of a pre-trained LVLM, MULTI-TAP can produce fine-grained\nscores for multiple human-interpretable objectives. MULTI-TAP performs better\nthan VisionREWARD, a high-performing multi-objective reward model, in both\nperformance and efficiency on multi-objective benchmarks and our newly released\ntext-image-to-text dataset, EYE4ALL. Our new dataset, consisting of\nchosen/rejected human preferences (EYE4ALLPref) and human-annotated\nfine-grained scores across seven dimensions (EYE4ALLMulti), can serve as a\nfoundation for developing more accessible AI systems by capturing the\nunderlying preferences of users, including blind and low-vision (BLV)\nindividuals.",
        "url": "http://arxiv.org/abs/2510.00766v1",
        "published_date": "2025-10-01T10:55:33+00:00",
        "updated_date": "2025-10-01T10:55:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Eunki Kim",
            "Na Min An",
            "James Thorne",
            "Hyunjung Shim"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "GAN"
        ],
        "tldr": "The paper proposes a multi-objective task-aware predictor for image-text alignment, addressing challenges in evaluating alignment with human judgments, long-sequence processing, inference efficiency, and applicability to multi-objective scoring.",
        "tldr_zh": "本文提出了一种多目标任务感知的图文对齐预测器，解决了评估图文对齐时面临的挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Extreme Blind Image Restoration via Prompt-Conditioned Information Bottleneck",
        "summary": "Blind Image Restoration (BIR) methods have achieved remarkable success but\nfalter when faced with Extreme Blind Image Restoration (EBIR), where inputs\nsuffer from severe, compounded degradations beyond their training scope.\nDirectly learning a mapping from extremely low-quality (ELQ) to high-quality\n(HQ) images is challenging due to the massive domain gap, often leading to\nunnatural artifacts and loss of detail. To address this, we propose a novel\nframework that decomposes the intractable ELQ-to-HQ restoration process. We\nfirst learn a projector that maps an ELQ image onto an intermediate,\nless-degraded LQ manifold. This intermediate image is then restored to HQ using\na frozen, off-the-shelf BIR model. Our approach is grounded in information\ntheory; we provide a novel perspective of image restoration as an Information\nBottleneck problem and derive a theoretically-driven objective to train our\nprojector. This loss function effectively stabilizes training by balancing a\nlow-quality reconstruction term with a high-quality prior-matching term. Our\nframework enables Look Forward Once (LFO) for inference-time prompt refinement,\nand supports plug-and-play strengthening of existing image restoration models\nwithout need for finetuning. Extensive experiments under severe degradation\nregimes provide a thorough analysis of the effectiveness of our work.",
        "url": "http://arxiv.org/abs/2510.00728v1",
        "published_date": "2025-10-01T10:13:27+00:00",
        "updated_date": "2025-10-01T10:13:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Hongeun Kim",
            "Bryan Sangwoo Kim",
            "Jong Chul Ye"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel framework for Extreme Blind Image Restoration by decomposing the restoration process into multiple stages, using an Information Bottleneck approach.",
        "tldr_zh": "该论文提出了一种新颖的框架，通过信息瓶颈方法将极端盲图像恢复的过程分解成多个阶段。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Deep learning motion correction of quantitative stress perfusion cardiovascular magnetic resonance",
        "summary": "Background: Quantitative stress perfusion cardiovascular magnetic resonance\n(CMR) is a powerful tool for assessing myocardial ischemia. Motion correction\nis essential for accurate pixel-wise mapping but traditional registration-based\nmethods are slow and sensitive to acquisition variability, limiting robustness\nand scalability.\n  Methods: We developed an unsupervised deep learning-based motion correction\npipeline that replaces iterative registration with efficient one-shot\nestimation. The method corrects motion in three steps and uses robust principal\ncomponent analysis to reduce contrast-related effects. It aligns the perfusion\nseries and auxiliary images (arterial input function and proton\ndensity-weighted series). Models were trained and validated on multivendor data\nfrom 201 patients, with 38 held out for testing. Performance was assessed via\ntemporal alignment and quantitative perfusion values, compared to a previously\npublished registration-based method.\n  Results: The deep learning approach significantly improved temporal\nsmoothness of time-intensity curves (p<0.001). Myocardial alignment (Dice =\n0.92 (0.04) and 0.91 (0.05)) was comparable to the baseline and superior to\nbefore registration (Dice = 0.80 (0.09), p<0.001). Perfusion maps showed\nreduced motion, with lower standard deviation in the myocardium (0.52 (0.39)\nml/min/g) compared to baseline (0.55 (0.44) ml/min/g). Processing time was\nreduced 15-fold.\n  Conclusion: This deep learning pipeline enables fast, robust motion\ncorrection for stress perfusion CMR, improving accuracy across dynamic and\nauxiliary images. Trained on multivendor data, it generalizes across sequences\nand may facilitate broader clinical adoption of quantitative perfusion imaging.",
        "url": "http://arxiv.org/abs/2510.00723v1",
        "published_date": "2025-10-01T09:59:48+00:00",
        "updated_date": "2025-10-01T09:59:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Noortje I. P. Schueler",
            "Nathan C. K. Wong",
            "Richard J. Crawley",
            "Josien P. W. Pluim",
            "Amedeo Chiribiri",
            "Cian M. Scannell"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "This paper presents a deep learning-based motion correction method for quantitative stress perfusion cardiovascular magnetic resonance imaging, improving accuracy and reducing processing time significantly.",
        "tldr_zh": "本文提出了一种基于深度学习的运动校正方法，用于定量应激灌注心血管磁共振成像，显著提高准确性并大幅减少处理时间。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From 2D to 3D, Deep Learning-based Shape Reconstruction in Magnetic Resonance Imaging: A Review",
        "summary": "Deep learning-based 3-dimensional (3D) shape reconstruction from\n2-dimensional (2D) magnetic resonance imaging (MRI) has become increasingly\nimportant in medical disease diagnosis, treatment planning, and computational\nmodeling. This review surveys the methodological landscape of 3D MRI\nreconstruction, focusing on 4 primary approaches: point cloud, mesh-based,\nshape-aware, and volumetric models. For each category, we analyze the current\nstate-of-the-art techniques, their methodological foundation, limitations, and\napplications across anatomical structures. We provide an extensive overview\nranging from cardiac to neurological to lung imaging. We also focus on the\nclinical applicability of models to diseased anatomy, and the influence of\ntheir training and testing data. We examine publicly available datasets,\ncomputational demands, and evaluation metrics. Finally, we highlight the\nemerging research directions including multimodal integration and\ncross-modality frameworks. This review aims to provide researchers with a\nstructured overview of current 3D reconstruction methodologies to identify\nopportunities for advancing deep learning towards more robust, generalizable,\nand clinically impactful solutions.",
        "url": "http://arxiv.org/abs/2510.01296v1",
        "published_date": "2025-10-01T09:57:29+00:00",
        "updated_date": "2025-10-01T09:57:29+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Emma McMillian",
            "Abhirup Banerjee",
            "Alfonso Bueno-Orovio"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Multimodality"
        ],
        "tldr": "The paper reviews deep learning-based 3D shape reconstruction from 2D MRI scans, focusing on different approaches and their clinical applicability.",
        "tldr_zh": "该论文回顾了基于深度学习的3D形状重建从2D MRI扫描，重点关注不同方法及其临床适用性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Graph Integrated Multimodal Concept Bottleneck Model",
        "summary": "With growing demand for interpretability in deep learning, especially in high\nstakes domains, Concept Bottleneck Models (CBMs) address this by inserting\nhuman understandable concepts into the prediction pipeline, but they are\ngenerally single modal and ignore structured concept relationships. To overcome\nthese limitations, we present MoE-SGT, a reasoning driven framework that\naugments CBMs with a structure injecting Graph Transformer and a Mixture of\nExperts (MoE) module. We construct answer-concept and answer-question graphs\nfor multimodal inputs to explicitly model the structured relationships among\nconcepts. Subsequently, we integrate Graph Transformer to capture multi level\ndependencies, addressing the limitations of traditional Concept Bottleneck\nModels in modeling concept interactions. However, it still encounters\nbottlenecks in adapting to complex concept patterns. Therefore, we replace the\nfeed forward layers with a Mixture of Experts (MoE) module, enabling the model\nto have greater capacity in learning diverse concept relationships while\ndynamically allocating reasoning tasks to different sub experts, thereby\nsignificantly enhancing the model's adaptability to complex concept reasoning.\nMoE-SGT achieves higher accuracy than other concept bottleneck networks on\nmultiple datasets by modeling structured relationships among concepts and\nutilizing a dynamic expert selection mechanism.",
        "url": "http://arxiv.org/abs/2510.00701v1",
        "published_date": "2025-10-01T09:18:38+00:00",
        "updated_date": "2025-10-01T09:18:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiakai Lin",
            "Jinchang Zhang",
            "Guoyu Lu"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Transformer"
        ],
        "tldr": "The paper introduces MoE-SGT, a framework that enhances Concept Bottleneck Models with a Graph Transformer and Mixture of Experts module to model structured relationships among concepts in multimodal inputs, improving adaptability to complex concept reasoning.",
        "tldr_zh": "该论文介绍了MoE-SGT，这是一个通过图转换器和专家组合模块来增强概念瓶颈模型，从而在多模态输入中建模概念之间的结构关系，提高对复杂概念推理的适应性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ProtoMask: Segmentation-Guided Prototype Learning",
        "summary": "XAI gained considerable importance in recent years. Methods based on\nprototypical case-based reasoning have shown a promising improvement in\nexplainability. However, these methods typically rely on additional post-hoc\nsaliency techniques to explain the semantics of learned prototypes. Multiple\ncritiques have been raised about the reliability and quality of such\ntechniques. For this reason, we study the use of prominent image segmentation\nfoundation models to improve the truthfulness of the mapping between embedding\nand input space. We aim to restrict the computation area of the saliency map to\na predefined semantic image patch to reduce the uncertainty of such\nvisualizations. To perceive the information of an entire image, we use the\nbounding box from each generated segmentation mask to crop the image. Each mask\nresults in an individual input in our novel model architecture named ProtoMask.\nWe conduct experiments on three popular fine-grained classification datasets\nwith a wide set of metrics, providing a detailed overview on explainability\ncharacteristics. The comparison with other popular models demonstrates\ncompetitive performance and unique explainability features of our model.\nhttps://github.com/uos-sis/quanproto",
        "url": "http://arxiv.org/abs/2510.00683v1",
        "published_date": "2025-10-01T09:07:24+00:00",
        "updated_date": "2025-10-01T09:07:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Steffen Meinert",
            "Philipp Schlinge",
            "Nils Strodthoff",
            "Martin Atzmueller"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces ProtoMask, a model that uses image segmentation to improve the explainability of prototypical case-based reasoning in XAI, demonstrating competitive performance and unique explainability features.",
        "tldr_zh": "本文介绍了ProtoMask模型，它利用图像分割来改善XAI中原型案例推理的可解释性，展示了竞争性能和独特的可解释性特征。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multi-Domain Brain Vessel Segmentation Through Feature Disentanglement",
        "summary": "The intricate morphology of brain vessels poses significant challenges for\nautomatic segmentation models, which usually focus on a single imaging\nmodality. However, accurately treating brain-related conditions requires a\ncomprehensive understanding of the cerebrovascular tree, regardless of the\nspecific acquisition procedure. Our framework effectively segments brain\narteries and veins in various datasets through image-to-image translation while\navoiding domain-specific model design and data harmonization between the source\nand the target domain. This is accomplished by employing disentanglement\ntechniques to independently manipulate different image properties, allowing\nthem to move from one domain to another in a label-preserving manner.\nSpecifically, we focus on manipulating vessel appearances during adaptation\nwhile preserving spatial information, such as shapes and locations, which are\ncrucial for correct segmentation. Our evaluation effectively bridges large and\nvaried domain gaps across medical centers, image modalities, and vessel types.\nAdditionally, we conduct ablation studies on the optimal number of required\nannotations and other architectural choices. The results highlight our\nframework's robustness and versatility, demonstrating the potential of domain\nadaptation methodologies to perform cerebrovascular image segmentation in\nmultiple scenarios accurately. Our code is available at\nhttps://github.com/i-vesseg/MultiVesSeg.",
        "url": "http://arxiv.org/abs/2510.00665v2",
        "published_date": "2025-10-01T08:48:11+00:00",
        "updated_date": "2025-10-02T07:45:24+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Francesco Galati",
            "Daniele Falcetta",
            "Rosa Cortese",
            "Ferran Prados",
            "Ninon Burgos",
            "Maria A. Zuluaga"
        ],
        "ai_categories": [
            "Dataset",
            "GAN",
            "Other"
        ],
        "tldr": "The paper presents a framework for segmenting brain vessels across different datasets using disentanglement techniques, allowing for accurate segmentation without domain-specific design.",
        "tldr_zh": "本文提出了一种框架，通过解耦技术在不同数据集中对脑血管进行分割，实现了准确的分割而无需特定领域的设计。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Batch-CAM: Introduction to better reasoning in convolutional deep learning models",
        "summary": "Understanding the inner workings of deep learning models is crucial for\nadvancing artificial intelligence, particularly in high-stakes fields such as\nhealthcare, where accurate explanations are as vital as precision. This paper\nintroduces Batch-CAM, a novel training paradigm that fuses a batch\nimplementation of the Grad-CAM algorithm with a prototypical reconstruction\nloss. This combination guides the model to focus on salient image features,\nthereby enhancing its performance across classification tasks. Our results\ndemonstrate that Batch-CAM achieves a simultaneous improvement in accuracy and\nimage reconstruction quality while reducing training and inference times. By\nensuring models learn from evidence-relevant information,this approach makes a\nrelevant contribution to building more transparent, explainable, and\ntrustworthy AI systems.",
        "url": "http://arxiv.org/abs/2510.00664v1",
        "published_date": "2025-10-01T08:47:00+00:00",
        "updated_date": "2025-10-01T08:47:00+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "68",
            "I.2; I.4"
        ],
        "authors": [
            "Giacomo Ignesti",
            "Davide Moroni",
            "Massimo Martinelli"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces Batch-CAM, a training paradigm that enhances deep learning models' performance in image classification tasks by focusing on salient image features, improving accuracy, image reconstruction quality, and reducing training and inference times.",
        "tldr_zh": "本文介绍了Batch-CAM，一种训练方法，通过专注于显著的图像特征，提高了深度学习模型在图像分类任务中的性能，改善了准确性、图像重建质量，并减少了训练和推断时间。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unsupervised Unfolded rPCA (U2-rPCA): Deep Interpretable Clutter Filtering for Ultrasound Microvascular Imaging",
        "summary": "High-sensitivity clutter filtering is a fundamental step in ultrasound\nmicrovascular imaging. Singular value decomposition (SVD) and robust principal\ncomponent analysis (rPCA) are the main clutter filtering strategies. However,\nboth strategies are limited in feature modeling and tissue-blood flow\nseparation for high-quality microvascular imaging. Recently, deep\nlearning-based clutter filtering has shown potential in more thoroughly\nseparating tissue and blood flow signals. However, the existing supervised\nfilters face the challenges of interpretability and lack of in-vitro and\nin-vivo ground truths. While the interpretability issue can be addressed by\nalgorithm deep unfolding, the training ground truth remains unsolved. To this\nend, this paper proposes an unsupervised unfolded rPCA (U2-rPCA) method that\npreserves mathematical interpretability and is insusceptible to learning\nlabels. Specifically, U2-rPCA is unfolded from an iteratively reweighted least\nsquares (IRLS) rPCA baseline with intrinsic low-rank and sparse regularization.\nA sparse-enhancement unit is added to the network to strengthen its capability\nto capture the sparse micro-flow signals. U2-rPCA is like an adaptive filter\nthat is trained with part of the image sequence and then used for the following\nframes. Experimental validations on a in-silico dataset and public in-vivo\ndatasets demonstrated the outperformance of U2-rPCA when compared with the\nSVD-based method, the rPCA baseline, and another deep learning-based filter.\nParticularly, the proposed method improved the contrastto-noise ratio (CNR) of\nthe power Doppler image by 2 dB to 10 dB when compared with other methods.\nFurthermore, the effectiveness of the building modules of U2-rPCA was validated\nthrough ablation studies.",
        "url": "http://arxiv.org/abs/2510.00660v1",
        "published_date": "2025-10-01T08:39:58+00:00",
        "updated_date": "2025-10-01T08:39:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Huaying Li",
            "Liansheng Wang",
            "Yinran Chen"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes an unsupervised deep learning method, U2-rPCA, for clutter filtering in ultrasound microvascular imaging, outperforming existing methods in improving image quality.",
        "tldr_zh": "本文提出了一种无监督深度学习方法，U2-rPCA，用于超声微血管成像中的杂波滤除，优于现有方法在提高图像质量方面的表现。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Align Your Tangent: Training Better Consistency Models via Manifold-Aligned Tangents",
        "summary": "With diffusion and flow matching models achieving state-of-the-art generating\nperformance, the interest of the community now turned to reducing the inference\ntime without sacrificing sample quality. Consistency Models (CMs), which are\ntrained to be consistent on diffusion or probability flow ordinary differential\nequation (PF-ODE) trajectories, enable one or two-step flow or diffusion\nsampling. However, CMs typically require prolonged training with large batch\nsizes to obtain competitive sample quality. In this paper, we examine the\ntraining dynamics of CMs near convergence and discover that CM tangents -- CM\noutput update directions -- are quite oscillatory, in the sense that they move\nparallel to the data manifold, not towards the manifold. To mitigate\noscillatory tangents, we propose a new loss function, called the manifold\nfeature distance (MFD), which provides manifold-aligned tangents that point\ntoward the data manifold. Consequently, our method -- dubbed Align Your Tangent\n(AYT) -- can accelerate CM training by orders of magnitude and even out-perform\nthe learned perceptual image patch similarity metric (LPIPS). Furthermore, we\nfind that our loss enables training with extremely small batch sizes without\ncompromising sample quality. Code: https://github.com/1202kbs/AYT",
        "url": "http://arxiv.org/abs/2510.00658v1",
        "published_date": "2025-10-01T08:35:18+00:00",
        "updated_date": "2025-10-01T08:35:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Beomsu Kim",
            "Byunghee Cha",
            "Jong Chul Ye"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a new method called Align Your Tangent (AYT) to accelerate the training of Consistency Models (CMs) for image generation by aligning the tangents towards the data manifold, ultimately outperforming existing methods like LPIPS.",
        "tldr_zh": "本文提出了一种名为Align Your Tangent (AYT)的新方法，通过将切线对准数据流形来加速一致性模型（CMs）的训练，最终优于现有方法如LPIPS。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FIN: Fast Inference Network for Map Segmentation",
        "summary": "Multi-sensor fusion in autonomous vehicles is becoming more common to offer a\nmore robust alternative for several perception tasks. This need arises from the\nunique contribution of each sensor in collecting data: camera-radar fusion\noffers a cost-effective solution by combining rich semantic information from\ncameras with accurate distance measurements from radar, without incurring\nexcessive financial costs or overwhelming data processing requirements. Map\nsegmentation is a critical task for enabling effective vehicle behaviour in its\nenvironment, yet it continues to face significant challenges in achieving high\naccuracy and meeting real-time performance requirements. Therefore, this work\npresents a novel and efficient map segmentation architecture, using cameras and\nradars, in the \\acrfull{bev} space. Our model introduces a real-time map\nsegmentation architecture considering aspects such as high accuracy, per-class\nbalancing, and inference time. To accomplish this, we use an advanced loss set\ntogether with a new lightweight head to improve the perception results. Our\nresults show that, with these modifications, our approach achieves results\ncomparable to large models, reaching 53.5 mIoU, while also setting a new\nbenchmark for inference time, improving it by 260\\% over the strongest baseline\nmodels.",
        "url": "http://arxiv.org/abs/2510.00651v1",
        "published_date": "2025-10-01T08:29:59+00:00",
        "updated_date": "2025-10-01T08:29:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruan Bispo",
            "Tim Brophy",
            "Reenu Mohandas",
            "Anthony Scanlan",
            "Ciarán Eising"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "LoRA",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel and efficient map segmentation architecture using cameras and radars in the BEV space, achieving high accuracy and real-time performance improvements.",
        "tldr_zh": "本文介绍了一种新颖高效的地图分割架构，利用摄像头和雷达在BEV空间中，实现了高准确性和实时性能提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Erased, But Not Forgotten: Erased Rectified Flow Transformers Still Remain Unsafe Under Concept Attack",
        "summary": "Recent advances in text-to-image (T2I) diffusion models have enabled\nimpressive generative capabilities, but they also raise significant safety\nconcerns due to the potential to produce harmful or undesirable content. While\nconcept erasure has been explored as a mitigation strategy, most existing\napproaches and corresponding attack evaluations are tailored to Stable\nDiffusion (SD) and exhibit limited effectiveness when transferred to\nnext-generation rectified flow transformers such as Flux. In this work, we\npresent ReFlux, the first concept attack method specifically designed to assess\nthe robustness of concept erasure in the latest rectified flow-based T2I\nframework. Our approach is motivated by the observation that existing concept\nerasure techniques, when applied to Flux, fundamentally rely on a phenomenon\nknown as attention localization. Building on this insight, we propose a simple\nyet effective attack strategy that specifically targets this property. At its\ncore, a reverse-attention optimization strategy is introduced to effectively\nreactivate suppressed signals while stabilizing attention. This is further\nreinforced by a velocity-guided dynamic that enhances the robustness of concept\nreactivation by steering the flow matching process, and a\nconsistency-preserving objective that maintains the global layout and preserves\nunrelated content. Extensive experiments consistently demonstrate the\neffectiveness and efficiency of the proposed attack method, establishing a\nreliable benchmark for evaluating the robustness of concept erasure strategies\nin rectified flow transformers.",
        "url": "http://arxiv.org/abs/2510.00635v1",
        "published_date": "2025-10-01T08:12:07+00:00",
        "updated_date": "2025-10-01T08:12:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nanxiang Jiang",
            "Zhaoxin Fan",
            "Enhan Kang",
            "Daiheng Gao",
            "Yun Zhou",
            "Yanxia Chang",
            "Zheng Zhu",
            "Yeying Jin",
            "Wenjun Wu"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Diffusion"
        ],
        "tldr": "The paper introduces a new concept attack method to assess the robustness of concept erasure in text-to-image models, specifically targeting rectified flow transformers like Flux.",
        "tldr_zh": "该论文介绍了一种新的概念攻击方法，用于评估文本到图像模型中概念删除的稳健性，特别针对Flux等纠正流变压器。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LAKAN: Landmark-assisted Adaptive Kolmogorov-Arnold Network for Face Forgery Detection",
        "summary": "The rapid development of deepfake generation techniques necessitates robust\nface forgery detection algorithms. While methods based on Convolutional Neural\nNetworks (CNNs) and Transformers are effective, there is still room for\nimprovement in modeling the highly complex and non-linear nature of forgery\nartifacts. To address this issue, we propose a novel detection method based on\nthe Kolmogorov-Arnold Network (KAN). By replacing fixed activation functions\nwith learnable splines, our KAN-based approach is better suited to this\nchallenge. Furthermore, to guide the network's focus towards critical facial\nareas, we introduce a Landmark-assisted Adaptive Kolmogorov-Arnold Network\n(LAKAN) module. This module uses facial landmarks as a structural prior to\ndynamically generate the internal parameters of the KAN, creating an\ninstance-specific signal that steers a general-purpose image encoder towards\nthe most informative facial regions with artifacts. This core innovation\ncreates a powerful combination between geometric priors and the network's\nlearning process. Extensive experiments on multiple public datasets show that\nour proposed method achieves superior performance.",
        "url": "http://arxiv.org/abs/2510.00634v1",
        "published_date": "2025-10-01T08:10:38+00:00",
        "updated_date": "2025-10-01T08:10:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiayao Jiang",
            "Siran Peng",
            "Bin Liu",
            "Qi Chu",
            "Nenghai Yu"
        ],
        "ai_categories": [
            "GAN",
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces LAKAN, a novel face forgery detection method that combines a Kolmogorov-Arnold Network with facial landmarks for improved performance.",
        "tldr_zh": "本文介绍了一种新的面部伪造检测方法LAKAN，它将科尔莫戈洛夫-阿诺德网络与面部标志结合起来，以提高性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Virtual Fashion Photo-Shoots: Building a Large-Scale Garment-Lookbook Dataset",
        "summary": "Fashion image generation has so far focused on narrow tasks such as virtual\ntry-on, where garments appear in clean studio environments. In contrast,\neditorial fashion presents garments through dynamic poses, diverse locations,\nand carefully crafted visual narratives. We introduce the task of virtual\nfashion photo-shoot, which seeks to capture this richness by transforming\nstandardized garment images into contextually grounded editorial imagery. To\nenable this new direction, we construct the first large-scale dataset of\ngarment-lookbook pairs, bridging the gap between e-commerce and fashion media.\nBecause such pairs are not readily available, we design an automated retrieval\npipeline that aligns garments across domains, combining visual-language\nreasoning with object-level localization. We construct a dataset with three\ngarment-lookbook pair accuracy levels: high quality (10,000 pairs), medium\nquality (50,000 pairs), and low quality (300,000 pairs). This dataset offers a\nfoundation for models that move beyond catalog-style generation and toward\nfashion imagery that reflects creativity, atmosphere, and storytelling.",
        "url": "http://arxiv.org/abs/2510.00633v1",
        "published_date": "2025-10-01T08:05:05+00:00",
        "updated_date": "2025-10-01T08:05:05+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yannick Hauri",
            "Luca A. Lanzendörfer",
            "Till Aczel"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper introduces the task of virtual fashion photo-shoots to generate editorial fashion imagery from standardized garments. They create a large-scale dataset for garment-lookbook pairs to bridge the gap between e-commerce and fashion media.",
        "tldr_zh": "本文介绍了虚拟时尚摄影任务，旨在从标准服装生成时尚编辑图像。他们创建了一个大规模的服装-目录对数据集，以弥合电子商务和时尚媒体之间的鸿沟。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UCD: Unconditional Discriminator Promotes Nash Equilibrium in GANs",
        "summary": "Adversarial training turns out to be the key to one-step generation,\nespecially for Generative Adversarial Network (GAN) and diffusion model\ndistillation. Yet in practice, GAN training hardly converges properly and\nstruggles in mode collapse. In this work, we quantitatively analyze the extent\nof Nash equilibrium in GAN training, and conclude that redundant shortcuts by\ninputting condition in $D$ disables meaningful knowledge extraction. We thereby\npropose to employ an unconditional discriminator (UCD), in which $D$ is\nenforced to extract more comprehensive and robust features with no condition\ninjection. In this way, $D$ is able to leverage better knowledge to supervise\n$G$, which promotes Nash equilibrium in GAN literature. Theoretical guarantee\non compatibility with vanilla GAN theory indicates that UCD can be implemented\nin a plug-in manner. Extensive experiments confirm the significant performance\nimprovements with high efficiency. For instance, we achieved \\textbf{1.47 FID}\non the ImageNet-64 dataset, surpassing StyleGAN-XL and several state-of-the-art\none-step diffusion models. The code will be made publicly available.",
        "url": "http://arxiv.org/abs/2510.00624v1",
        "published_date": "2025-10-01T07:58:33+00:00",
        "updated_date": "2025-10-01T07:58:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mengfei Xia",
            "Nan Xue",
            "Jiapeng Zhu",
            "Yujun Shen"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes using an unconditional discriminator in GAN training to promote Nash equilibrium and improve performance.",
        "tldr_zh": "本文提出在GAN训练中使用无条件鉴别器以促进纳什均衡和提高性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Robust Context-Aware Object Recognition",
        "summary": "In visual recognition, both the object of interest (referred to as\nforeground, FG, for simplicity) and its surrounding context (background, BG)\nplay an important role. However, standard supervised learning often leads to\nunintended over-reliance on the BG, known as shortcut learning of spurious\ncorrelations, limiting model robustness in real-world deployment settings. In\nthe literature, the problem is mainly addressed by suppressing the BG,\nsacrificing context information for improved generalization.\n  We propose RCOR -- Robust Context-Aware Object Recognition -- the first\napproach that jointly achieves robustness and context-awareness without\ncompromising either. RCOR treats localization as an integral part of\nrecognition to decouple object-centric and context-aware modelling, followed by\na robust, non-parametric fusion. It improves the performance of both supervised\nmodels and VLM on datasets with both in-domain and out-of-domain BG, even\nwithout fine-tuning. The results confirm that localization before recognition\nis now possible even in complex scenes as in ImageNet-1k.",
        "url": "http://arxiv.org/abs/2510.00618v1",
        "published_date": "2025-10-01T07:45:38+00:00",
        "updated_date": "2025-10-01T07:45:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Klara Janouskova",
            "Cristian Gavrus",
            "Jiri Matas"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes an approach, RCOR, for robust object recognition that maintains both robustness and context-awareness without sacrificing either.",
        "tldr_zh": "该论文提出了一种名为RCOR的方法，用于强大的目标识别，保持了鲁棒性和上下文感知而不会牺牲任何一个。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Hybrid Training for Vision-Language-Action Models",
        "summary": "Using Large Language Models to produce intermediate thoughts, a.k.a.\nChain-of-thought (CoT), before providing an answer has been a successful recipe\nfor solving complex language tasks. In robotics, similar embodied CoT\nstrategies, generating thoughts before actions, have also been shown to lead to\nimproved performance when using Vision-Language-Action models (VLAs). As these\ntechniques increase the length of the model's generated outputs to include the\nthoughts, the inference time is negatively affected. Delaying an agent's\nactions in real-world executions, as in robotic manipulation settings, strongly\naffects the usability of a method, as tasks require long sequences of actions.\nHowever, is the generation of long chains-of-thought a strong prerequisite for\nachieving performance improvements? In this work, we explore the idea of Hybrid\nTraining (HyT), a framework that enables VLAs to learn from thoughts and\nbenefit from the associated performance gains, while enabling the possibility\nto leave out CoT generation during inference. Furthermore, by learning to\nconditionally predict a diverse set of outputs, HyT supports flexibility at\ninference time, enabling the model to either predict actions directly, generate\nthoughts or follow instructions. We evaluate the proposed method in a series of\nsimulated benchmarks and real-world experiments.",
        "url": "http://arxiv.org/abs/2510.00600v1",
        "published_date": "2025-10-01T07:27:15+00:00",
        "updated_date": "2025-10-01T07:27:15+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Pietro Mazzaglia",
            "Cansu Sancaktar",
            "Markus Peschl",
            "Daniel Dijkman"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality"
        ],
        "tldr": "The paper introduces a Hybrid Training framework for Vision-Language-Action models, allowing them to learn from intermediate thoughts while still being able to skip generating them during inference, enabling flexibility.",
        "tldr_zh": "本文介绍了一种用于视觉-语言-动作模型的混合训练框架，允许模型在学习中间思考的同时，在推理过程中跳过生成这些思考，实现灵活性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multi-level Dynamic Style Transfer for NeRFs",
        "summary": "As the application of neural radiance fields (NeRFs) in various 3D vision\ntasks continues to expand, numerous NeRF-based style transfer techniques have\nbeen developed. However, existing methods typically integrate style statistics\ninto the original NeRF pipeline, often leading to suboptimal results in both\ncontent preservation and artistic stylization. In this paper, we present\nmulti-level dynamic style transfer for NeRFs (MDS-NeRF), a novel approach that\nreengineers the NeRF pipeline specifically for stylization and incorporates an\ninnovative dynamic style injection module. Particularly, we propose a\nmulti-level feature adaptor that helps generate a multi-level feature grid\nrepresentation from the content radiance field, effectively capturing the\nmulti-scale spatial structure of the scene. In addition, we present a dynamic\nstyle injection module that learns to extract relevant style features and\nadaptively integrates them into the content patterns. The stylized multi-level\nfeatures are then transformed into the final stylized view through our proposed\nmulti-level cascade decoder. Furthermore, we extend our 3D style transfer\nmethod to support omni-view style transfer using 3D style references. Extensive\nexperiments demonstrate that MDS-NeRF achieves outstanding performance for 3D\nstyle transfer, preserving multi-scale spatial structures while effectively\ntransferring stylistic characteristics.",
        "url": "http://arxiv.org/abs/2510.00592v1",
        "published_date": "2025-10-01T07:19:27+00:00",
        "updated_date": "2025-10-01T07:19:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zesheng Li",
            "Shuaibo Li",
            "Wei Ma",
            "Jianwei Guo",
            "Hongbin Zha"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel approach, MDS-NeRF, for dynamic style transfer in neural radiance fields, achieving outstanding performance for 3D style transfer.",
        "tldr_zh": "本文介绍了一种新的方法，MDS-NeRF，用于神经辐射场中的动态样式转移，在3D样式转移方面表现出色。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "U-DFA: A Unified DINOv2-Unet with Dual Fusion Attention for Multi-Dataset Medical Segmentation",
        "summary": "Accurate medical image segmentation plays a crucial role in overall diagnosis\nand is one of the most essential tasks in the diagnostic pipeline. CNN-based\nmodels, despite their extensive use, suffer from a local receptive field and\nfail to capture the global context. A common approach that combines CNNs with\ntransformers attempts to bridge this gap but fails to effectively fuse the\nlocal and global features. With the recent emergence of VLMs and foundation\nmodels, they have been adapted for downstream medical imaging tasks; however,\nthey suffer from an inherent domain gap and high computational cost. To this\nend, we propose U-DFA, a unified DINOv2-Unet encoder-decoder architecture that\nintegrates a novel Local-Global Fusion Adapter (LGFA) to enhance segmentation\nperformance. LGFA modules inject spatial features from a CNN-based Spatial\nPattern Adapter (SPA) module into frozen DINOv2 blocks at multiple stages,\nenabling effective fusion of high-level semantic and spatial features. Our\nmethod achieves state-of-the-art performance on the Synapse and ACDC datasets\nwith only 33\\% of the trainable model parameters. These results demonstrate\nthat U-DFA is a robust and scalable framework for medical image segmentation\nacross multiple modalities.",
        "url": "http://arxiv.org/abs/2510.00585v1",
        "published_date": "2025-10-01T07:06:49+00:00",
        "updated_date": "2025-10-01T07:06:49+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Zulkaif Sajjad",
            "Furqan Shaukat",
            "Junaid Mir"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a unified DINOv2-Unet architecture for medical image segmentation, achieving state-of-the-art performance with reduced computational cost.",
        "tldr_zh": "本文提出了一种用于医学图像分割的统一DINOv2-Unet架构，实现了最先进的性能并减少了计算成本。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VIRTUE: Visual-Interactive Text-Image Universal Embedder",
        "summary": "Multimodal representation learning models have demonstrated successful\noperation across complex tasks, and the integration of vision-language models\n(VLMs) has further enabled embedding models with instruction-following\ncapabilities. However, existing embedding models lack visual-interactive\ncapabilities to specify regions of interest from users (e.g., point, bounding\nbox, mask), which have been explored in generative models to broaden their\nhuman-interactive applicability. Equipping embedding models with visual\ninteractions not only would unlock new applications with localized grounding of\nuser intent, which remains unexplored, but also enable the models to learn\nentity-level information within images to complement their global\nrepresentations for conventional embedding tasks. In this paper, we propose a\nnovel Visual-InteRactive Text-Image Universal Embedder (VIRTUE) that extends\nthe capabilities of the segmentation model and the vision-language model to the\nrealm of representation learning. In VIRTUE, the segmentation model can process\nvisual prompts that pinpoint specific regions within an image, thereby enabling\nthe embedder to handle complex and ambiguous scenarios more precisely. To\nevaluate the visual-interaction ability of VIRTUE, we introduce a large-scale\nSegmentation-and-Scene Caption Retrieval (SCaR) benchmark comprising 1M samples\nthat aims to retrieve the text caption by jointly considering the entity with a\nspecific object and image scene. VIRTUE consistently achieves a\nstate-of-the-art performance with significant improvements across 36 universal\nMMEB (3.1%-8.5%) and five visual-interactive SCaR (15.2%-20.3%) tasks.",
        "url": "http://arxiv.org/abs/2510.00523v1",
        "published_date": "2025-10-01T05:11:54+00:00",
        "updated_date": "2025-10-01T05:11:54+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Wei-Yao Wang",
            "Kazuya Tateishi",
            "Qiyu Wu",
            "Shusuke Takahashi",
            "Yuki Mitsufuji"
        ],
        "ai_categories": [
            "Multimodality",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a Visual-Interactive Text-Image Universal Embedder (VIRTUE) that enhances embedding models with visual interaction capabilities. It achieves state-of-the-art performance on various tasks.",
        "tldr_zh": "该论文介绍了一种名为VIRTUE的视觉交互文本图像通用嵌入器，通过视觉交互功能增强了嵌入模型。它在各种任务中取得了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Efficient Multi-modal Large Language Models via Progressive Consistency Distillation",
        "summary": "Visual tokens consume substantial computational resources in multi-modal\nlarge models (MLLMs), significantly compromising their efficiency. Recent works\nhave attempted to improve efficiency by compressing visual tokens during\ntraining, either through modifications to model components or by introducing\nadditional parameters. However, they often overlook the increased learning\ndifficulty caused by such compression, as the model's parameter space struggles\nto quickly adapt to the substantial perturbations in the feature space induced\nby token compression. In this work, we propose to develop Efficient MLLMs via\nProgressive Consistency Distillation (EPIC), a progressive learning framework.\nSpecifically, by decomposing the feature space perturbations introduced by\ntoken compression along the token-wise and layer-wise dimensions, we introduce\ntoken consistency distillation and layer consistency distillation,\nrespectively, aiming to reduce the training difficulty by leveraging guidance\nfrom a teacher model and following a progressive learning trajectory. Extensive\nexperiments demonstrate the superior effectiveness, robustness, and\ngeneralization capabilities of our proposed framework.",
        "url": "http://arxiv.org/abs/2510.00515v1",
        "published_date": "2025-10-01T04:56:40+00:00",
        "updated_date": "2025-10-01T04:56:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zichen Wen",
            "Shaobo Wang",
            "Yufa Zhou",
            "Junyuan Zhang",
            "Qintong Zhang",
            "Yifeng Gao",
            "Zhaorun Chen",
            "Bin Wang",
            "Weijia Li",
            "Conghui He",
            "Linfeng Zhang"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer"
        ],
        "tldr": "The paper introduces an efficient learning framework for multi-modal large language models to improve efficiency by compressing visual tokens without compromising performance.",
        "tldr_zh": "该论文介绍了一种高效的学习框架，用于改善多模态大型语言模型的效率，通过压缩视觉标记而不影响性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Affordance-Guided Diffusion Prior for 3D Hand Reconstruction",
        "summary": "How can we reconstruct 3D hand poses when large portions of the hand are\nheavily occluded by itself or by objects? Humans often resolve such ambiguities\nby leveraging contextual knowledge -- such as affordances, where an object's\nshape and function suggest how the object is typically grasped. Inspired by\nthis observation, we propose a generative prior for hand pose refinement guided\nby affordance-aware textual descriptions of hand-object interactions (HOI). Our\nmethod employs a diffusion-based generative model that learns the distribution\nof plausible hand poses conditioned on affordance descriptions, which are\ninferred from a large vision-language model (VLM). This enables the refinement\nof occluded regions into more accurate and functionally coherent hand poses.\nExtensive experiments on HOGraspNet, a 3D hand-affordance dataset with severe\nocclusions, demonstrate that our affordance-guided refinement significantly\nimproves hand pose estimation over both recent regression methods and\ndiffusion-based refinement lacking contextual reasoning.",
        "url": "http://arxiv.org/abs/2510.00506v1",
        "published_date": "2025-10-01T04:36:11+00:00",
        "updated_date": "2025-10-01T04:36:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Naru Suzuki",
            "Takehiko Ohkawa",
            "Tatsuro Banno",
            "Jihyun Lee",
            "Ryosuke Furuta",
            "Yoichi Sato"
        ],
        "ai_categories": [
            "AIGC",
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper proposes a method for reconstructing 3D hand poses using affordance-guided diffusion, improving pose estimation when hands are heavily occluded by objects.",
        "tldr_zh": "本文提出了一种利用规避引导扩散的方法来重建3D手势，从而改善当手部受到物体重度遮挡时的姿势估计。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Relative-Absolute Fusion: Rethinking Feature Extraction in Image-Based Iterative Method Selection for Solving Sparse Linear Systems",
        "summary": "Iterative method selection is crucial for solving sparse linear systems\nbecause these methods inherently lack robustness. Though image-based selection\napproaches have shown promise, their feature extraction techniques might encode\ndistinct matrices into identical image representations, leading to the same\nselection and suboptimal method. In this paper, we introduce RAF\n(Relative-Absolute Fusion), an efficient feature extraction technique to\nenhance image-based selection approaches. By simultaneously extracting and\nfusing image representations as relative features with corresponding numerical\nvalues as absolute features, RAF achieves comprehensive matrix representations\nthat prevent feature ambiguity across distinct matrices, thus improving\nselection accuracy and unlocking the potential of image-based selection\napproaches. We conducted comprehensive evaluations of RAF on SuiteSparse and\nour developed BMCMat (Balanced Multi-Classification Matrix dataset),\ndemonstrating solution time reductions of 0.08s-0.29s for sparse linear\nsystems, which is 5.86%-11.50% faster than conventional image-based selection\napproaches and achieves state-of-the-art (SOTA) performance. BMCMat is\navailable at https://github.com/zkqq/BMCMat.",
        "url": "http://arxiv.org/abs/2510.00500v1",
        "published_date": "2025-10-01T04:33:23+00:00",
        "updated_date": "2025-10-01T04:33:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kaiqi Zhang",
            "Mingguan Yang",
            "Dali Chang",
            "Chun Chen",
            "Yuxiang Zhang",
            "Kexun He",
            "Jing Zhao"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces Relative-Absolute Fusion (RAF) to enhance image-based iterative method selection for solving sparse linear systems, showing significant performance improvements.",
        "tldr_zh": "本文引入了相对-绝对融合（RAF）技术，用于增强基于图像的迭代方法选择，以解决稀疏线性系统，表现出显著的性能改进。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "MathSticks: A Benchmark for Visual Symbolic Compositional Reasoning with Matchstick Puzzles",
        "summary": "We introduce \\textsc{MathSticks}, a benchmark for Visual Symbolic\nCompositional Reasoning (VSCR), which unifies visual perception, symbolic\nmanipulation, and arithmetic consistency. Each task presents an incorrect\nmatchstick equation that must be corrected by moving one or two sticks under\nstrict conservation rules. The benchmark includes both text-guided and purely\nvisual settings, systematically covering digit scale, move complexity, solution\nmultiplicity, and operator variation, with 1.4M generated instances and a\ncurated test set. Evaluations of 14 vision--language models reveal substantial\nlimitations: closed-source models succeed only on simple cases, open-source\nmodels fail in the visual regime, while humans exceed 90\\% accuracy. These\nfindings establish \\textsc{MathSticks} as a rigorous testbed for advancing\ncompositional reasoning across vision and symbols. Our code and dataset are\npublicly available at https://github.com/Yuheng2000/MathSticks.",
        "url": "http://arxiv.org/abs/2510.00483v1",
        "published_date": "2025-10-01T04:04:54+00:00",
        "updated_date": "2025-10-01T04:04:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuheng Ji",
            "Huajie Tan",
            "Cheng Chi",
            "Yijie Xu",
            "Yuting Zhao",
            "Enshen Zhou",
            "Huaihai Lyu",
            "Pengwei Wang",
            "Zhongyuan Wang",
            "Shanghang Zhang",
            "Xiaolong Zheng"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "MathSticks introduces a benchmark for Visual Symbolic Compositional Reasoning with matchstick puzzles, highlighting limitations of existing vision-language models and establishing itself as a testbed for advancing compositional reasoning.",
        "tldr_zh": "MathSticks引入了一个基于火柴棍谜题的视觉符号组合推理基准，突出了现有视觉语言模型的局限性，并将其确立为推进符号组合推理的实验平台。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Measuring and Controlling the Spectral Bias for Self-Supervised Image Denoising",
        "summary": "Current self-supervised denoising methods for paired noisy images typically\ninvolve mapping one noisy image through the network to the other noisy image.\nHowever, after measuring the spectral bias of such methods using our proposed\nImage Pair Frequency-Band Similarity, it suffers from two practical\nlimitations. Firstly, the high-frequency structural details in images are not\npreserved well enough. Secondly, during the process of fitting high\nfrequencies, the network learns high-frequency noise from the mapped noisy\nimages. To address these challenges, we introduce a Spectral Controlling\nnetwork (SCNet) to optimize self-supervised denoising of paired noisy images.\nFirst, we propose a selection strategy to choose frequency band components for\nnoisy images, to accelerate the convergence speed of training. Next, we present\na parameter optimization method that restricts the learning ability of\nconvolutional kernels to high-frequency noise using the Lipschitz constant,\nwithout changing the network structure. Finally, we introduce the Spectral\nSeparation and low-rank Reconstruction module (SSR module), which separates\nnoise and high-frequency details through frequency domain separation and\nlow-rank space reconstruction, to retain the high-frequency structural details\nof images. Experiments performed on synthetic and real-world datasets verify\nthe effectiveness of SCNet.",
        "url": "http://arxiv.org/abs/2510.00454v1",
        "published_date": "2025-10-01T03:07:05+00:00",
        "updated_date": "2025-10-01T03:07:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Wang Zhang",
            "Huaqiu Li",
            "Xiaowan Hu",
            "Tao Jiang",
            "Zikang Chen",
            "Haoqian Wang"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a Spectral Controlling network to optimize self-supervised denoising of paired noisy images, addressing challenges related to high-frequency structural details and noise learning",
        "tldr_zh": "本文介绍了一种谱控制网络，用于优化成对嘈杂图像的自监督去噪，解决了与高频结构细节和噪声学习相关的挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Plug-and-Play Prompt Refinement via Latent Feedback for Diffusion Model Alignment",
        "summary": "Despite the recent progress, reinforcement learning (RL)-based fine-tuning of\ndiffusion models often struggles with generalization, composability, and\nrobustness against reward hacking. Recent studies have explored prompt\nrefinement as a modular alternative, but most adopt a feed-forward approach\nthat applies a single refined prompt throughout the entire sampling trajectory,\nthereby failing to fully leverage the sequential nature of reinforcement\nlearning. To address this, here we introduce PromptLoop, a plug-and-play RL\nframework that incorporates latent feedback into step-wise prompt refinement.\nRather than modifying diffusion model weights, a multimodal large language\nmodel (MLLM) is trained with RL to iteratively update prompts based on\nintermediate latent states of diffusion models. This design achieves a\nstructural analogy to the Diffusion RL approach, while retaining the\nflexibility and generality of prompt-based alignment. Extensive experiments\nacross diverse reward functions and diffusion backbones demonstrate that\nPromptLoop (i) achieves effective reward optimization, (ii) generalizes\nseamlessly to unseen models, (iii) composes orthogonally with existing\nalignment methods, and (iv) mitigates over-optimization and reward hacking.",
        "url": "http://arxiv.org/abs/2510.00430v1",
        "published_date": "2025-10-01T02:18:58+00:00",
        "updated_date": "2025-10-01T02:18:58+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Suhyeon Lee",
            "Jong Chul Ye"
        ],
        "ai_categories": [
            "Diffusion",
            "Multimodality"
        ],
        "tldr": "The paper introduces PromptLoop, a framework that incorporates latent feedback into prompt refinement for diffusion models in reinforcement learning, achieving effective reward optimization, generalization, composability, and robustness.",
        "tldr_zh": "本文介绍了PromptLoop，这是一个将潜在反馈引入到强化学习中扩散模型的提示细化框架，实现了有效的奖励优化、泛化性、组合性和鲁棒性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Domain-Specialized Interactive Segmentation Framework for Meningioma Radiotherapy Planning",
        "summary": "Precise delineation of meningiomas is crucial for effective radiotherapy (RT)\nplanning, directly influencing treatment efficacy and preservation of adjacent\nhealthy tissues. While automated deep learning approaches have demonstrated\nconsiderable potential, achieving consistently accurate clinical segmentation\nremains challenging due to tumor heterogeneity. Interactive Medical Image\nSegmentation (IMIS) addresses this challenge by integrating advanced AI\ntechniques with clinical input. However, generic segmentation tools, despite\nwidespread applicability, often lack the specificity required for clinically\ncritical and disease-specific tasks like meningioma RT planning. To overcome\nthese limitations, we introduce Interactive-MEN-RT, a dedicated IMIS tool\nspecifically developed for clinician-assisted 3D meningioma segmentation in RT\nworkflows. The system incorporates multiple clinically relevant interaction\nmethods, including point annotations, bounding boxes, lasso tools, and\nscribbles, enhancing usability and clinical precision. In our evaluation\ninvolving 500 contrast-enhanced T1-weighted MRI scans from the BraTS 2025\nMeningioma RT Segmentation Challenge, Interactive-MEN-RT demonstrated\nsubstantial improvement compared to other segmentation methods, achieving Dice\nsimilarity coefficients of up to 77.6\\% and Intersection over Union scores of\n64.8\\%. These results emphasize the need for clinically tailored segmentation\nsolutions in critical applications such as meningioma RT planning. The code is\npublicly available at: https://github.com/snuh-rad-aicon/Interactive-MEN-RT",
        "url": "http://arxiv.org/abs/2510.00416v1",
        "published_date": "2025-10-01T01:57:10+00:00",
        "updated_date": "2025-10-01T01:57:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Junhyeok Lee",
            "Han Jang",
            "Kyu Sung Choi"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper introduces Interactive-MEN-RT, a specialized tool for meningioma segmentation in radiotherapy planning, showing significant improvement over existing methods.",
        "tldr_zh": "本文介绍了专门针对脑膜瘤分割的交互式工具Interactive-MEN-RT，在放射治疗规划中显示出明显的改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PAL-UI: Planning with Active Look-back for Vision-Based GUI Agents",
        "summary": "Graphical User Interface (GUI) agents powered by Multimodal Large Language\nModels (MLLMs) promise human-like interaction with software applications, yet\nlong-horizon tasks remain challenging due to memory limitations. Existing\napproaches either truncate history or rely on simple textual summaries, which\nrisk losing critical information when past visual details become necessary for\nfuture decisions. In this paper, we propose \\textbf{PAL-UI} (\\textbf{P}lanning\nwith \\textbf{A}ctive \\textbf{L}ook-back), a novel framework that enables GUI\nagents to adaptively retrieve past observations when required. PAL-UI combines\na dual-level summarization agent, capturing both observation-level cues and\naction-level outcomes, with a dedicated retrieval tool that allows the agent to\nrecall specific historical screenshots during planning. We curate a step-level\ninstruction dataset of 8.6K samples from mobile GUI navigation trajectories and\ntrain \\textbf{PAL-UI-3B} and \\textbf{PAL-UI-7B} models based on Qwen2.5-VL.\nExtensive experiments demonstrate that PAL-UI significantly outperforms\nbaseline models and prior methods in mobile GUI navigation tasks, even under\ndata-efficient settings. Moreover, PAL-UI exhibits strong cross-domain\ngeneralization, achieving notable improvements in web navigation without\nadditional training. Our work highlights the potential of active memory\nretrieval for long-horizon planning capabilities of vision-based GUI agents.",
        "url": "http://arxiv.org/abs/2510.00413v1",
        "published_date": "2025-10-01T01:48:39+00:00",
        "updated_date": "2025-10-01T01:48:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zikang Liu",
            "Junyi Li",
            "Wayne Xin Zhao",
            "Dawei Gao",
            "Yaliang Li",
            "Ji-rong Wen"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes PAL-UI, a framework for GUI agents to retrieve past visual details during planning, improving performance on long-horizon tasks.",
        "tldr_zh": "本文提出了PAL-UI框架，用于GUI代理在规划过程中检索过去的视觉细节，提高了长期任务的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators",
        "summary": "Vision-Language-Action (VLA) models enable embodied decision-making but rely\nheavily on imitation learning, leading to compounding errors and poor\nrobustness under distribution shift. Reinforcement learning (RL) can mitigate\nthese issues yet typically demands costly real-world interactions or suffers\nfrom sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning\nframework that leverages a data-driven world model as a controllable simulator.\nTrained from real interaction data, the simulator predicts future visual\nobservations conditioned on actions, allowing policy rollouts with dense,\ntrajectory-level rewards derived from goal-achieving references. This design\ndelivers an efficient and action-aligned learning signal, drastically lowering\nsample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses\nstrong supervised baselines and achieves greater efficiency than\nsimulator-based RL. Moreover, it exhibits strong robustness under perturbed\nconditions, sustaining stable task execution. Our results establish\nworld-model-based RFT as a practical post-training paradigm to enhance the\ngeneralization and robustness of VLA models. For more details, please refer to\nhttps://vla-rft.github.io/.",
        "url": "http://arxiv.org/abs/2510.00406v1",
        "published_date": "2025-10-01T01:33:10+00:00",
        "updated_date": "2025-10-01T01:33:10+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Hengtao Li",
            "Pengxiang Ding",
            "Runze Suo",
            "Yihao Wang",
            "Zirui Ge",
            "Dongyuan Zang",
            "Kexian Yu",
            "Mingyang Sun",
            "Hongyin Zhang",
            "Donglin Wang",
            "Weihua Su"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "VLA-RFT introduces a reinforcement fine-tuning framework leveraging a data-driven world model to enhance generalization and robustness of Vision-Language-Action models.",
        "tldr_zh": "VLA-RFT引入了一种强化微调框架，利用数据驱动的世界模型来增强Vision-Language-Action模型的泛化性和鲁棒性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Discrete Wavelet Transform as a Facilitator for Expressive Latent Space Representation in Variational Autoencoders in Satellite Imagery",
        "summary": "Latent Diffusion Models (LDM), a subclass of diffusion models, mitigate the\ncomputational complexity of pixel-space diffusion by operating within a\ncompressed latent space constructed by Variational Autoencoders (VAEs),\ndemonstrating significant advantages in Remote Sensing (RS) applications.\nThough numerous studies enhancing LDMs have been conducted, investigations\nexplicitly targeting improvements within the intrinsic latent space remain\nscarce. This paper proposes an innovative perspective, utilizing the Discrete\nWavelet Transform (DWT) to enhance the VAE's latent space representation,\ndesigned for satellite imagery. The proposed method, ExpDWT-VAE, introduces\ndual branches: one processes spatial domain input through convolutional\noperations, while the other extracts and processes frequency-domain features\nvia 2D Haar wavelet decomposition, convolutional operation, and inverse DWT\nreconstruction. These branches merge to create an integrated spatial-frequency\nrepresentation, further refined through convolutional and diagonal Gaussian\nmapping into a robust latent representation. We utilize a new satellite imagery\ndataset housed by the TerraFly mapping system to validate our method.\nExperimental results across several performance metrics highlight the efficacy\nof the proposed method at enhancing latent space representation.",
        "url": "http://arxiv.org/abs/2510.00376v1",
        "published_date": "2025-10-01T00:49:41+00:00",
        "updated_date": "2025-10-01T00:49:41+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Arpan Mahara",
            "Md Rezaul Karim Khan",
            "Naphtali Rishe",
            "Wenjia Wang",
            "Seyed Masoud Sadjadi"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes ExpDWT-VAE, a method that enhances the latent space representation in satellite imagery using Discrete Wavelet Transform within Variational Autoencoders.",
        "tldr_zh": "本文提出了一种利用离散小波变换改善卫星图像中潜在空间表示的方法ExpDWT-VAE。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Motion In-Betweening for Densely Interacting Characters",
        "summary": "Motion in-betweening is the problem to synthesize movement between keyposes.\nTraditional research focused primarily on single characters. Extending them to\ndensely interacting characters is highly challenging, as it demands precise\nspatial-temporal correspondence between the characters to maintain the\ninteraction, while creating natural transitions towards predefined keyposes. In\nthis research, we present a method for long-horizon interaction in-betweening\nthat enables two characters to engage and respond to one another naturally. To\neffectively represent and synthesize interactions, we propose a novel solution\ncalled Cross-Space In-Betweening, which models the interactions of each\ncharacter across different conditioning representation spaces. We further\nobserve that the significantly increased constraints in interacting characters\nheavily limit the solution space, leading to degraded motion quality and\ndiminished interaction over time. To enable long-horizon synthesis, we present\ntwo solutions to maintain long-term interaction and motion quality, thereby\nkeeping synthesis in the stable region of the solution space.We first sustain\ninteraction quality by identifying periodic interaction patterns through\nadversarial learning. We further maintain the motion quality by learning to\nrefine the drifted latent space and prevent pose error accumulation. We\ndemonstrate that our approach produces realistic, controllable, and\nlong-horizon in-between motions of two characters with dynamic boxing and\ndancing actions across multiple keyposes, supported by extensive quantitative\nevaluations and user studies.",
        "url": "http://arxiv.org/abs/2510.00314v1",
        "published_date": "2025-09-30T22:11:39+00:00",
        "updated_date": "2025-09-30T22:11:39+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Xiaotang Zhang",
            "Ziyi Chang",
            "Qianhui Men",
            "Hubert P. H. Shum"
        ],
        "ai_categories": [
            "GAN",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a method called Cross-Space In-Betweening for generating natural interactions between densely interacting characters in long-horizon scenarios, maintaining motion quality and interaction over time.",
        "tldr_zh": "本文提出了一种名为跨空间插帧的方法，用于在长时间范围内生成密集互动角色之间的自然互动，保持运动质量和互动。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MOLM: Mixture of LoRA Markers",
        "summary": "Generative models can generate photorealistic images at scale. This raises\nurgent concerns about the ability to detect synthetically generated images and\nattribute these images to specific sources. While watermarking has emerged as a\npossible solution, existing methods remain fragile to realistic distortions,\nsusceptible to adaptive removal, and expensive to update when the underlying\nwatermarking key changes. We propose a general watermarking framework that\nformulates the encoding problem as key-dependent perturbation of the parameters\nof a generative model. Within this framework, we introduce Mixture of LoRA\nMarkers (MOLM), a routing-based instantiation in which binary keys activate\nlightweight LoRA adapters inside residual and attention blocks. This design\navoids key-specific re-training and achieves the desired properties such as\nimperceptibility, fidelity, verifiability, and robustness. Experiments on\nStable Diffusion and FLUX show that MOLM preserves image quality while\nachieving robust key recovery against distortions, compression and\nregeneration, averaging attacks, and black-box adversarial attacks on the\nextractor.",
        "url": "http://arxiv.org/abs/2510.00293v1",
        "published_date": "2025-09-30T21:27:14+00:00",
        "updated_date": "2025-09-30T21:27:14+00:00",
        "categories": [
            "cs.CV",
            "cs.CR",
            "cs.LG"
        ],
        "authors": [
            "Samar Fares",
            "Nurbek Tastan",
            "Noor Hussein",
            "Karthik Nandakumar"
        ],
        "ai_categories": [
            "LoRA",
            "Diffusion",
            "GAN"
        ],
        "tldr": "The paper introduces a watermarking framework called MOLM, which preserves image quality while achieving robust key recovery against various attacks.",
        "tldr_zh": "该论文介绍了一种名为MOLM的数字水印框架，可在保留图像质量的同时抵抗各种攻击。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation",
        "summary": "Audio-video generation has often relied on complex multi-stage architectures\nor sequential synthesis of sound and visuals. We introduce Ovi, a unified\nparadigm for audio-video generation that models the two modalities as a single\ngenerative process. By using blockwise cross-modal fusion of twin-DiT modules,\nOvi achieves natural synchronization and removes the need for separate\npipelines or post hoc alignment. To facilitate fine-grained multimodal fusion\nmodeling, we initialize an audio tower with an architecture identical to that\nof a strong pretrained video model. Trained from scratch on hundreds of\nthousands of hours of raw audio, the audio tower learns to generate realistic\nsound effects, as well as speech that conveys rich speaker identity and\nemotion. Fusion is obtained by jointly training the identical video and audio\ntowers via blockwise exchange of timing (via scaled-RoPE embeddings) and\nsemantics (through bidirectional cross-attention) on a vast video corpus. Our\nmodel enables cinematic storytelling with natural speech and accurate,\ncontext-matched sound effects, producing movie-grade video clips. All the\ndemos, code and model weights are published at https://aaxwaz.github.io/Ovi",
        "url": "http://arxiv.org/abs/2510.01284v1",
        "published_date": "2025-09-30T21:03:50+00:00",
        "updated_date": "2025-09-30T21:03:50+00:00",
        "categories": [
            "cs.MM",
            "cs.CV",
            "cs.SD",
            "eess.AS"
        ],
        "authors": [
            "Chetwin Low",
            "Weimin Wang",
            "Calder Katyal"
        ],
        "ai_categories": [
            "GAN",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "Ovi proposes a unified paradigm for audio-video generation using blockwise cross-modal fusion of twin-DiT modules to achieve natural synchronization and remove the need for separate pipelines.",
        "tldr_zh": "Ovi提出了一种统一的音视频生成范式，使用双DiT模块的区块交叉模态融合来实现自然同步，消除了分开管道的需要。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Learning Energy-based Variational Latent Prior for VAEs",
        "summary": "Variational Auto-Encoders (VAEs) are known to generate blurry and\ninconsistent samples. One reason for this is the \"prior hole\" problem. A prior\nhole refers to regions that have high probability under the VAE's prior but low\nprobability under the VAE's posterior. This means that during data generation,\nhigh probability samples from the prior could have low probability under the\nposterior, resulting in poor quality data. Ideally, a prior needs to be\nflexible enough to match the posterior while retaining the ability to generate\nsamples fast. Generative models continue to address this tradeoff. This paper\nproposes to model the prior as an energy-based model (EBM). While EBMs are\nknown to offer the flexibility to match posteriors (and also improving the\nELBO), they are traditionally slow in sample generation due to their dependency\non MCMC methods. Our key idea is to bring a variational approach to tackle the\nnormalization constant in EBMs, thus bypassing the expensive MCMC approaches.\nThe variational form can be approximated with a sampler network, and we show\nthat such an approach to training priors can be formulated as an alternating\noptimization problem. Moreover, the same sampler reduces to an implicit\nvariational prior during generation, providing efficient and fast sampling. We\ncompare our Energy-based Variational Latent Prior (EVaLP) method to multiple\nSOTA baselines and show improvements in image generation quality, reduced prior\nholes, and better sampling efficiency.",
        "url": "http://arxiv.org/abs/2510.00260v1",
        "published_date": "2025-09-30T20:32:00+00:00",
        "updated_date": "2025-09-30T20:32:00+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Debottam Dutta",
            "Chaitanya Amballa",
            "Zhongweiyang Xu",
            "Yu-Lin Wei",
            "Romit Roy Choudhury"
        ],
        "ai_categories": [
            "GAN",
            "Transformer"
        ],
        "tldr": "The paper proposes a method called EVaLP to improve image generation quality and efficiency by using an energy-based model as a prior in VAEs.",
        "tldr_zh": "本文提出了一种名为EVaLP的方法，通过在VAE中使用能量模型作为先验来改善图像生成质量和效率。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Instant4D: 4D Gaussian Splatting in Minutes",
        "summary": "Dynamic view synthesis has seen significant advances, yet reconstructing\nscenes from uncalibrated, casual video remains challenging due to slow\noptimization and complex parameter estimation. In this work, we present\nInstant4D, a monocular reconstruction system that leverages native 4D\nrepresentation to efficiently process casual video sequences within minutes,\nwithout calibrated cameras or depth sensors. Our method begins with geometric\nrecovery through deep visual SLAM, followed by grid pruning to optimize scene\nrepresentation. Our design significantly reduces redundancy while maintaining\ngeometric integrity, cutting model size to under 10% of its original footprint.\nTo handle temporal dynamics efficiently, we introduce a streamlined 4D Gaussian\nrepresentation, achieving a 30x speed-up and reducing training time to within\ntwo minutes, while maintaining competitive performance across several\nbenchmarks. Our method reconstruct a single video within 10 minutes on the\nDycheck dataset or for a typical 200-frame video. We further apply our model to\nin-the-wild videos, showcasing its generalizability. Our project website is\npublished at https://instant4d.github.io/.",
        "url": "http://arxiv.org/abs/2510.01119v1",
        "published_date": "2025-10-01T17:07:21+00:00",
        "updated_date": "2025-10-01T17:07:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhanpeng Luo",
            "Haoxi Ran",
            "Li Lu"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces Instant4D, a system for reconstructing scenes from casual videos quickly without calibrated cameras or depth sensors, achieving significant speed-up and reducing model size.",
        "tldr_zh": "该论文介绍了Instant4D，这是一个系统，可以在没有校准相机或深度传感器的情况下，快速地从休闲视频中重建场景，实现了显著的加速和模型尺寸缩小。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "AI-CNet3D: An Anatomically-Informed Cross-Attention Network with Multi-Task Consistency Fine-tuning for 3D Glaucoma Classification",
        "summary": "Glaucoma is a progressive eye disease that leads to optic nerve damage,\ncausing irreversible vision loss if left untreated. Optical coherence\ntomography (OCT) has become a crucial tool for glaucoma diagnosis, offering\nhigh-resolution 3D scans of the retina and optic nerve. However, the\nconventional practice of condensing information from 3D OCT volumes into 2D\nreports often results in the loss of key structural details. To address this,\nwe propose a novel hybrid deep learning model that integrates cross-attention\nmechanisms into a 3D convolutional neural network (CNN), enabling the\nextraction of critical features from the superior and inferior hemiretinas, as\nwell as from the optic nerve head (ONH) and macula, within OCT volumes. We\nintroduce Channel Attention REpresentations (CAREs) to visualize\ncross-attention outputs and leverage them for consistency-based multi-task\nfine-tuning, aligning them with Gradient-Weighted Class Activation Maps\n(Grad-CAMs) from the CNN's final convolutional layer to enhance performance,\ninterpretability, and anatomical coherence. We have named this model AI-CNet3D\n(AI-`See'-Net3D) to reflect its design as an Anatomically-Informed\nCross-attention Network operating on 3D data. By dividing the volume along two\naxes and applying cross-attention, our model enhances glaucoma classification\nby capturing asymmetries between the hemiretinal regions while integrating\ninformation from the optic nerve head and macula. We validate our approach on\ntwo large datasets, showing that it outperforms state-of-the-art attention and\nconvolutional models across all key metrics. Finally, our model is\ncomputationally efficient, reducing the parameter count by one-hundred--fold\ncompared to other attention mechanisms while maintaining high diagnostic\nperformance and comparable GFLOPS.",
        "url": "http://arxiv.org/abs/2510.00882v1",
        "published_date": "2025-10-01T13:30:55+00:00",
        "updated_date": "2025-10-01T13:30:55+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Roshan Kenia",
            "Anfei Li",
            "Rishabh Srivastava",
            "Kaveri A. Thakoor"
        ],
        "ai_categories": [
            "Transformer",
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel deep learning model, AI-CNet3D, for 3D glaucoma classification using cross-attention mechanisms and multi-task consistency fine-tuning.",
        "tldr_zh": "本文引入了一种新颖的深度学习模型AI-CNet3D，用于使用交叉注意力机制和多任务一致性微调进行3D青光眼分类。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7.5
    },
    {
        "title": "What You See is What You Ask: Evaluating Audio Descriptions",
        "summary": "Audio descriptions (ADs) narrate important visual details in movies, enabling\nBlind and Low Vision (BLV) users to understand narratives and appreciate visual\ndetails. Existing works in automatic AD generation mostly focus on few-second\ntrimmed clips, and evaluate them by comparing against a single ground-truth\nreference AD. However, writing ADs is inherently subjective. Through alignment\nand analysis of two independent AD tracks for the same movies, we quantify the\nsubjectivity in when and whether to describe, and what and how to highlight.\nThus, we show that working with trimmed clips is inadequate. We propose ADQA, a\nQA benchmark that evaluates ADs at the level of few-minute long, coherent video\nsegments, testing whether they would help BLV users understand the story and\nappreciate visual details. ADQA features visual appreciation (VA) questions\nabout visual facts and narrative understanding (NU) questions based on the\nplot. Through ADQA, we show that current AD generation methods lag far behind\nhuman-authored ADs. We conclude with several recommendations for future work\nand introduce a public leaderboard for benchmarking.",
        "url": "http://arxiv.org/abs/2510.00808v1",
        "published_date": "2025-10-01T12:14:15+00:00",
        "updated_date": "2025-10-01T12:14:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Divy Kala",
            "Eshika Khandelwal",
            "Makarand Tapaswi"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper evaluates audio descriptions in movies for Blind and Low Vision users, proposing a QA benchmark to assess the quality of descriptions for better narrative understanding and visual appreciation.",
        "tldr_zh": "该论文评估电影中的音频描述，为盲人和视力低下用户提供理解故事情节和欣赏视觉细节的质量评估。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "HAMLET: Switch your Vision-Language-Action Model into a History-Aware Policy",
        "summary": "Inherently, robotic manipulation tasks are history-dependent: leveraging past\ncontext could be beneficial. However, most existing Vision-Language-Action\nmodels (VLAs) have been designed without considering this aspect, i.e., they\nrely solely on the current observation, ignoring preceding context. In this\npaper, we propose HAMLET, a scalable framework to adapt VLAs to attend to the\nhistorical context during action prediction. Specifically, we introduce moment\ntokens that compactly encode perceptual information at each timestep. Their\nrepresentations are initialized with time-contrastive learning, allowing them\nto better capture temporally distinctive aspects. Next, we employ a lightweight\nmemory module that integrates the moment tokens across past timesteps into\nmemory features, which are then leveraged for action prediction. Through\nempirical evaluation, we show that HAMLET successfully transforms a\nstate-of-the-art VLA into a history-aware policy, especially demonstrating\nsignificant improvements on long-horizon tasks that require historical context.\nIn particular, on top of GR00T N1.5, HAMLET achieves an average success rate of\n76.4% on history-dependent real-world tasks, surpassing the baseline\nperformance by 47.2%. Furthermore, HAMLET pushes prior art performance from\n64.1% to 66.4% on RoboCasa Kitchen (100-demo setup) and from 95.6% to 97.7% on\nLIBERO, highlighting its effectiveness even under generic robot-manipulation\nbenchmarks.",
        "url": "http://arxiv.org/abs/2510.00695v2",
        "published_date": "2025-10-01T09:15:52+00:00",
        "updated_date": "2025-10-02T06:41:44+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Myungkyu Koo",
            "Daewon Choi",
            "Taeyoung Kim",
            "Kyungmin Lee",
            "Changyeon Kim",
            "Younggyo Seo",
            "Jinwoo Shin"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "HAMLET proposes a framework to adapt Vision-Language-Action models to consider historical context for better action prediction in robotic manipulation tasks.",
        "tldr_zh": "HAMLET提出了一个框架，用于调整视觉-语言-动作模型以考虑历史上下文，以更好地预测机器人操作任务中的动作。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "LVLMs as inspectors: an agentic framework for category-level structural defect annotation",
        "summary": "Automated structural defect annotation is essential for ensuring\ninfrastructure safety while minimizing the high costs and inefficiencies of\nmanual labeling. A novel agentic annotation framework, Agent-based Defect\nPattern Tagger (ADPT), is introduced that integrates Large Vision-Language\nModels (LVLMs) with a semantic pattern matching module and an iterative\nself-questioning refinement mechanism. By leveraging optimized domain-specific\nprompting and a recursive verification process, ADPT transforms raw visual data\ninto high-quality, semantically labeled defect datasets without any manual\nsupervision. Experimental results demonstrate that ADPT achieves up to 98%\naccuracy in distinguishing defective from non-defective images, and 85%-98%\nannotation accuracy across four defect categories under class-balanced\nsettings, with 80%-92% accuracy on class-imbalanced datasets. The framework\noffers a scalable and cost-effective solution for high-fidelity dataset\nconstruction, providing strong support for downstream tasks such as transfer\nlearning and domain adaptation in structural damage assessment.",
        "url": "http://arxiv.org/abs/2510.00603v1",
        "published_date": "2025-10-01T07:31:42+00:00",
        "updated_date": "2025-10-01T07:31:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sheng Jiang",
            "Yuanmin Ning",
            "Bingxi Huang",
            "Peiyin Chen",
            "Zhaohui Chen"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper introduces an agentic annotation framework, ADPT, that uses LVLMs to annotate structural defects in visual data with high accuracy without manual supervision.",
        "tldr_zh": "本文介绍了一种新颖的注释框架，ADPT，使用LVLMs在视觉数据中对结构缺陷进行注释，精度高且无需人工监督。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Forestpest-YOLO: A High-Performance Detection Framework for Small Forestry Pests",
        "summary": "Detecting agricultural pests in complex forestry environments using remote\nsensing imagery is fundamental for ecological preservation, yet it is severely\nhampered by practical challenges. Targets are often minuscule, heavily\noccluded, and visually similar to the cluttered background, causing\nconventional object detection models to falter due to the loss of fine-grained\nfeatures and an inability to handle extreme data imbalance. To overcome these\nobstacles, this paper introduces Forestpest-YOLO, a detection framework\nmeticulously optimized for the nuances of forestry remote sensing. Building\nupon the YOLOv8 architecture, our framework introduces a synergistic trio of\ninnovations. We first integrate a lossless downsampling module, SPD-Conv, to\nensure that critical high-resolution details of small targets are preserved\nthroughout the network. This is complemented by a novel cross-stage feature\nfusion block, CSPOK, which dynamically enhances multi-scale feature\nrepresentation while suppressing background noise. Finally, we employ\nVarifocalLoss to refine the training objective, compelling the model to focus\non high-quality and hard-to-classify samples. Extensive experiments on our\nchallenging, self-constructed ForestPest dataset demonstrate that\nForestpest-YOLO achieves state-of-the-art performance, showing marked\nimprovements in detecting small, occluded pests and significantly outperforming\nestablished baseline models.",
        "url": "http://arxiv.org/abs/2510.00547v1",
        "published_date": "2025-10-01T06:06:40+00:00",
        "updated_date": "2025-10-01T06:06:40+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Aoduo Li",
            "Peikai Lin",
            "Jiancheng Li",
            "Zhen Zhang",
            "Shiting Wu",
            "Zexiao Liang",
            "Zhifa Jiang"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces Forestpest-YOLO, a high-performance detection framework for small forestry pests, achieving state-of-the-art performance in detecting small, occluded pests.",
        "tldr_zh": "该论文介绍了Forestpest-YOLO，一个针对小型林业害虫的高性能检测框架，在检测小型、被遮挡的害虫方面取得了最先进的性能。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "Feature Identification for Hierarchical Contrastive Learning",
        "summary": "Hierarchical classification is a crucial task in many applications, where\nobjects are organized into multiple levels of categories. However, conventional\nclassification approaches often neglect inherent inter-class relationships at\ndifferent hierarchy levels, thus missing important supervisory signals. Thus,\nwe propose two novel hierarchical contrastive learning (HMLC) methods. The\nfirst, leverages a Gaussian Mixture Model (G-HMLC) and the second uses an\nattention mechanism to capture hierarchy-specific features (A-HMLC), imitating\nhuman processing. Our approach explicitly models inter-class relationships and\nimbalanced class distribution at higher hierarchy levels, enabling fine-grained\nclustering across all hierarchy levels. On the competitive CIFAR100 and\nModelNet40 datasets, our method achieves state-of-the-art performance in linear\nevaluation, outperforming existing hierarchical contrastive learning methods by\n2 percentage points in terms of accuracy. The effectiveness of our approach is\nbacked by both quantitative and qualitative results, highlighting its potential\nfor applications in computer vision and beyond.",
        "url": "http://arxiv.org/abs/2510.00837v1",
        "published_date": "2025-10-01T12:46:47+00:00",
        "updated_date": "2025-10-01T12:46:47+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Julius Ott",
            "Nastassia Vysotskaya",
            "Huawei Sun",
            "Lorenzo Servadei",
            "Robert Wille"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes two hierarchical contrastive learning methods, G-HMLC and A-HMLC, to capture hierarchy-specific features and improve classification accuracy on CIFAR100 and ModelNet40 datasets.",
        "tldr_zh": "该论文提出了两种层次对比学习方法，G-HMLC和A-HMLC，以捕捉层次特定特征，并在CIFAR100和ModelNet40数据集上提高分类准确性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Defect Segmentation in OCT scans of ceramic parts for non-destructive inspection using deep learning",
        "summary": "Non-destructive testing (NDT) is essential in ceramic manufacturing to ensure\nthe quality of components without compromising their integrity. In this\ncontext, Optical Coherence Tomography (OCT) enables high-resolution internal\nimaging, revealing defects such as pores, delaminations, or inclusions. This\npaper presents an automatic defect detection system based on Deep Learning\n(DL), trained on OCT images with manually segmented annotations. A neural\nnetwork based on the U-Net architecture is developed, evaluating multiple\nexperimental configurations to enhance its performance. Post-processing\ntechniques enable both quantitative and qualitative evaluation of the\npredictions. The system shows an accurate behavior of 0.979 Dice Score,\noutperforming comparable studies. The inference time of 18.98 seconds per\nvolume supports its viability for detecting inclusions, enabling more\nefficient, reliable, and automated quality control.",
        "url": "http://arxiv.org/abs/2510.00745v1",
        "published_date": "2025-10-01T10:30:24+00:00",
        "updated_date": "2025-10-01T10:30:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Andrés Laveda-Martínez",
            "Natalia P. García-de-la-Puente",
            "Fernando García-Torres",
            "Niels Møller Israelsen",
            "Ole Bang",
            "Dominik Brouczek",
            "Niels Benson",
            "Adrián Colomer",
            "Valery Naranjo"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper presents an automatic defect detection system using deep learning for non-destructive inspection of ceramic parts in OCT scans, achieving high accuracy and efficiency.",
        "tldr_zh": "本文提出了一种使用深度学习进行陶瓷零件OCT扫描的非破坏性检验的自动缺陷检测系统，实现了高精度和效率。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Weakly Supervised Cloud Detection Combining Spectral Features and Multi-Scale Deep Network",
        "summary": "Clouds significantly affect the quality of optical satellite images, which\nseriously limits their precise application. Recently, deep learning has been\nwidely applied to cloud detection and has achieved satisfactory results.\nHowever, the lack of distinctive features in thin clouds and the low quality of\ntraining samples limit the cloud detection accuracy of deep learning methods,\nleaving space for further improvements. In this paper, we propose a weakly\nsupervised cloud detection method that combines spectral features and\nmulti-scale scene-level deep network (SpecMCD) to obtain highly accurate\npixel-level cloud masks. The method first utilizes a progressive training\nframework with a multi-scale scene-level dataset to train the multi-scale\nscene-level cloud detection network. Pixel-level cloud probability maps are\nthen obtained by combining the multi-scale probability maps and cloud thickness\nmap based on the characteristics of clouds in dense cloud coverage and large\ncloud-area coverage images. Finally, adaptive thresholds are generated based on\nthe differentiated regions of the scene-level cloud masks at different scales\nand combined with distance-weighted optimization to obtain binary cloud masks.\nTwo datasets, WDCD and GF1MS-WHU, comprising a total of 60 Gaofen-1\nmultispectral (GF1-MS) images, were used to verify the effectiveness of the\nproposed method. Compared to the other weakly supervised cloud detection\nmethods such as WDCD and WSFNet, the F1-score of the proposed SpecMCD method\nshows an improvement of over 7.82%, highlighting the superiority and potential\nof the SpecMCD method for cloud detection under different cloud coverage\nconditions.",
        "url": "http://arxiv.org/abs/2510.00654v1",
        "published_date": "2025-10-01T08:32:49+00:00",
        "updated_date": "2025-10-01T08:32:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shaocong Zhu",
            "Zhiwei Li",
            "Xinghua Li",
            "Huanfeng Shen"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper proposes a weakly supervised cloud detection method combining spectral features and multi-scale deep network to improve accuracy in cloud detection from optical satellite images.",
        "tldr_zh": "本文提出了一种弱监督的云检测方法，结合光谱特征和多尺度深度网络，以提高在光学卫星图像中的云检测精度。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Disentangling Foreground and Background for vision-Language Navigation via Online Augmentation",
        "summary": "Following language instructions, vision-language navigation (VLN) agents are\ntasked with navigating unseen environments. While augmenting multifaceted\nvisual representations has propelled advancements in VLN, the significance of\nforeground and background in visual observations remains underexplored.\nIntuitively, foreground regions provide semantic cues, whereas the background\nencompasses spatial connectivity information. Inspired on this insight, we\npropose a Consensus-driven Online Feature Augmentation strategy (COFA) with\nalternative foreground and background features to facilitate the navigable\ngeneralization. Specifically, we first leverage semantically-enhanced landmark\nidentification to disentangle foreground and background as candidate augmented\nfeatures. Subsequently, a consensus-driven online augmentation strategy\nencourages the agent to consolidate two-stage voting results on feature\npreferences according to diverse instructions and navigational locations.\nExperiments on REVERIE and R2R demonstrate that our online\nforeground-background augmentation boosts the generalization of baseline and\nattains state-of-the-art performance.",
        "url": "http://arxiv.org/abs/2510.00604v1",
        "published_date": "2025-10-01T07:32:36+00:00",
        "updated_date": "2025-10-01T07:32:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yunbo Xu",
            "Xuesong Zhang",
            "Jia Li",
            "Zhenzhen Hu",
            "Richang Hong"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes a strategy called COFA to disentangle foreground and background features for vision-language navigation, improving the generalization and achieving state-of-the-art performance.",
        "tldr_zh": "本文提出了一种名为COFA的策略，用于分离前景和背景特征，从而改善通用性并实现最先进的性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "VLOD-TTA: Test-Time Adaptation of Vision-Language Object Detectors",
        "summary": "Vision-language object detectors (VLODs) such as YOLO-World and Grounding\nDINO achieve impressive zero-shot recognition by aligning region proposals with\ntext representations. However, their performance often degrades under domain\nshift. We introduce VLOD-TTA, a test-time adaptation (TTA) framework for VLODs\nthat leverages dense proposal overlap and image-conditioned prompt scores.\nFirst, an IoU-weighted entropy objective is proposed that concentrates\nadaptation on spatially coherent proposal clusters and reduces confirmation\nbias from isolated boxes. Second, image-conditioned prompt selection is\nintroduced, which ranks prompts by image-level compatibility and fuses the most\ninformative prompts with the detector logits. Our benchmarking across diverse\ndistribution shifts -- including stylized domains, driving scenes, low-light\nconditions, and common corruptions -- shows the effectiveness of our method on\ntwo state-of-the-art VLODs, YOLO-World and Grounding DINO, with consistent\nimprovements over the zero-shot and TTA baselines. Code :\nhttps://github.com/imatif17/VLOD-TTA",
        "url": "http://arxiv.org/abs/2510.00458v1",
        "published_date": "2025-10-01T03:17:56+00:00",
        "updated_date": "2025-10-01T03:17:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Atif Belal",
            "Heitor R. Medeiros",
            "Marco Pedersoli",
            "Eric Granger"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "VLOD-TTA introduces a test-time adaptation framework for vision-language object detectors, showing improvements over existing methods under diverse distribution shifts.",
        "tldr_zh": "VLOD-TTA为视觉-语言目标检测器引入了一种测试时适应性框架，在各种分布转移情况下显示出改进。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "On-the-Fly Data Augmentation via Gradient-Guided and Sample-Aware Influence Estimation",
        "summary": "Data augmentation has been widely employed to improve the generalization of\ndeep neural networks. Most existing methods apply fixed or random\ntransformations. However, we find that sample difficulty evolves along with the\nmodel's generalization capabilities in dynamic training environments. As a\nresult, applying uniform or stochastic augmentations, without accounting for\nsuch dynamics, can lead to a mismatch between augmented data and the model's\nevolving training needs, ultimately degrading training effectiveness. To\naddress this, we introduce SADA, a Sample-Aware Dynamic Augmentation that\nperforms on-the-fly adjustment of augmentation strengths based on each sample's\nevolving influence on model optimization. Specifically, we estimate each\nsample's influence by projecting its gradient onto the accumulated model update\ndirection and computing the temporal variance within a local training window.\nSamples with low variance, indicating stable and consistent influence, are\naugmented more strongly to emphasize diversity, while unstable samples receive\nmilder transformations to preserve semantic fidelity and stabilize learning.\nOur method is lightweight, which does not require auxiliary models or policy\ntuning. It can be seamlessly integrated into existing training pipelines as a\nplug-and-play module. Experiments across various benchmark datasets and model\narchitectures show consistent improvements of SADA, including +7.3\\% on\nfine-grained tasks and +4.3\\% on long-tailed datasets, highlighting the\nmethod's effectiveness and practicality.",
        "url": "http://arxiv.org/abs/2510.00434v1",
        "published_date": "2025-10-01T02:26:52+00:00",
        "updated_date": "2025-10-01T02:26:52+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Suorong Yang",
            "Jie Zong",
            "Lihang Wang",
            "Ziheng Qin",
            "Hai Gan",
            "Pengfei Zhou",
            "Kai Wang",
            "Yang You",
            "Furao Shen"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces SADA, a method for on-the-fly data augmentation that adjusts augmentation strength based on each sample's influence on model optimization, leading to consistent improvements in various tasks.",
        "tldr_zh": "该论文介绍了SADA，一种根据样本对模型优化的影响调整数据增强强度的方法，在各种任务中实现了持续改进。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "EgoTraj-Bench: Towards Robust Trajectory Prediction Under Ego-view Noisy Observations",
        "summary": "Reliable trajectory prediction from an ego-centric perspective is crucial for\nrobotic navigation in human-centric environments. However, existing methods\ntypically assume idealized observation histories, failing to account for the\nperceptual artifacts inherent in first-person vision, such as occlusions, ID\nswitches, and tracking drift. This discrepancy between training assumptions and\ndeployment reality severely limits model robustness. To bridge this gap, we\nintroduce EgoTraj-Bench, the first real-world benchmark that grounds noisy,\nfirst-person visual histories in clean, bird's-eye-view future trajectories,\nenabling robust learning under realistic perceptual constraints. Building on\nthis benchmark, we propose BiFlow, a dual-stream flow matching model that\nconcurrently denoises historical observations and forecasts future motion by\nleveraging a shared latent representation. To better model agent intent, BiFlow\nincorporates our EgoAnchor mechanism, which conditions the prediction decoder\non distilled historical features via feature modulation. Extensive experiments\nshow that BiFlow achieves state-of-the-art performance, reducing minADE and\nminFDE by 10-15% on average and demonstrating superior robustness. We\nanticipate that our benchmark and model will provide a critical foundation for\ndeveloping trajectory forecasting systems truly resilient to the challenges of\nreal-world, ego-centric perception.",
        "url": "http://arxiv.org/abs/2510.00405v1",
        "published_date": "2025-10-01T01:30:13+00:00",
        "updated_date": "2025-10-01T01:30:13+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Jiayi Liu",
            "Jiaming Zhou",
            "Ke Ye",
            "Kun-Yu Lin",
            "Allan Wang",
            "Junwei Liang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a benchmark and model for robust trajectory prediction in noisy first-person observations, achieving state-of-the-art performance.",
        "tldr_zh": "本文介绍了一种基准和模型，用于在嘈杂的第一人称观察中做出稳健的轨迹预测，取得了最先进的性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Activation-Deactivation: A General Framework for Robust Post-hoc Explainable AI",
        "summary": "Black-box explainability methods are popular tools for explaining the\ndecisions of image classifiers. A major drawback of these tools is their\nreliance on mutants obtained by occluding parts of the input, leading to\nout-of-distribution images. This raises doubts about the quality of the\nexplanations. Moreover, choosing an appropriate occlusion value often requires\ndomain knowledge. In this paper we introduce a novel forward-pass paradigm\nActivation-Deactivation (AD), which removes the effects of occluded input\nfeatures from the model's decision-making by switching off the parts of the\nmodel that correspond to the occlusions. We introduce ConvAD, a drop-in\nmechanism that can be easily added to any trained Convolutional Neural Network\n(CNN), and which implements the AD paradigm. This leads to more robust\nexplanations without any additional training or fine-tuning. We prove that the\nConvAD mechanism does not change the decision-making process of the network. We\nprovide experimental evaluation across several datasets and model\narchitectures. We compare the quality of AD-explanations with explanations\nachieved using a set of masking values, using the proxies of robustness, size,\nand confidence drop-off. We observe a consistent improvement in robustness of\nAD explanations (up to 62.5%) compared to explanations obtained with\nocclusions, demonstrating that ConvAD extracts more robust explanations without\nthe need for domain knowledge.",
        "url": "http://arxiv.org/abs/2510.01038v1",
        "published_date": "2025-10-01T15:42:58+00:00",
        "updated_date": "2025-10-01T15:42:58+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Akchunya Chanchal",
            "David A. Kelly",
            "Hana Chockler"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a new explainability method, Activation-Deactivation (AD), for image classifiers, which removes the effects of occluded input features to provide more robust explanations without requiring domain knowledge.",
        "tldr_zh": "本文提出了一种新的解释方法，激活-去活化（AD），用于图像分类器，它通过关闭与遮挡对应的模型部分来消除遮挡输入特征的影响，提供更强大的解释，而无需领域知识。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Towards Adversarial Training under Hyperspectral Images",
        "summary": "Recent studies have revealed that hyperspectral classification models based\non deep learning are highly vulnerable to adversarial attacks, which pose\nsignificant security risks. Although several approaches have attempted to\nenhance adversarial robustness by modifying network architectures, these\nmethods often rely on customized designs that limit scalability and fail to\ndefend effectively against strong attacks. To address these challenges, we\nintroduce adversarial training to the hyperspectral domain, which is widely\nregarded as one of the most effective defenses against adversarial attacks.\nThrough extensive empirical analyses, we demonstrate that while adversarial\ntraining does enhance robustness across various models and datasets,\nhyperspectral data introduces unique challenges not seen in RGB images.\nSpecifically, we find that adversarial noise and the non-smooth nature of\nadversarial examples can distort or eliminate important spectral semantic\ninformation. To mitigate this issue, we employ data augmentation techniques and\npropose a novel hyperspectral adversarial training method, termed AT-RA. By\nincreasing the diversity of spectral information and ensuring spatial\nsmoothness, AT-RA preserves and corrects spectral semantics in hyperspectral\nimages. Experimental results show that AT-RA improves adversarial robustness by\n21.34% against AutoAttack and 18.78% against PGD-50 while boosting benign\naccuracy by 2.68%.",
        "url": "http://arxiv.org/abs/2510.01014v1",
        "published_date": "2025-10-01T15:19:39+00:00",
        "updated_date": "2025-10-01T15:19:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weihua Zhang",
            "Chengze Jiang",
            "Jie Gui",
            "Lu Dong"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces adversarial training to the hyperspectral domain to improve robustness against attacks, addressing unique challenges in hyperspectral images by employing data augmentation techniques and a novel training method called AT-RA.",
        "tldr_zh": "本文将对抗训练引入到高光谱域中，通过采用数据增强技术和一种名为AT-RA的新训练方法，提高针对攻击的鲁棒性，解决高光谱图像中的独特挑战。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Looking Alike From Far to Near: Enhancing Cross-Resolution Re-Identification via Feature Vector Panning",
        "summary": "In surveillance scenarios, varying camera distances cause significant\ndifferences among pedestrian image resolutions, making it hard to match\nlow-resolution (LR) images with high-resolution (HR) counterparts, limiting the\nperformance of Re-Identification (ReID) tasks. Most existing Cross-Resolution\nReID (CR-ReID) methods rely on super-resolution (SR) or joint learning for\nfeature compensation, which increases training and inference complexity and has\nreached a performance bottleneck in recent studies. Inspired by semantic\ndirections in the word embedding space, we empirically discover that semantic\ndirections implying resolution differences also emerge in the feature space of\nReID, and we substantiate this finding from a statistical perspective using\nCanonical Correlation Analysis and Pearson Correlation Analysis. Based on this\ninteresting finding, we propose a lightweight and effective Vector Panning\nFeature Alignment (VPFA) framework, which conducts CR-ReID from a novel\nperspective of modeling the resolution-specific feature discrepancy. Extensive\nexperimental results on multiple CR-ReID benchmarks show that our method\nsignificantly outperforms previous state-of-the-art baseline models while\nobtaining higher efficiency, demonstrating the effectiveness and superiority of\nour model based on the new finding in this paper.",
        "url": "http://arxiv.org/abs/2510.00936v1",
        "published_date": "2025-10-01T14:15:39+00:00",
        "updated_date": "2025-10-01T14:15:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zanwu Liu",
            "Chao Yuan",
            "Bo Li",
            "Xiaowei Zhang",
            "Guanglin Niu"
        ],
        "ai_categories": [
            "GAN"
        ],
        "tldr": "The paper introduces a new method for enhancing cross-resolution re-identification of pedestrians, improving performance and efficiency compared to existing methods.",
        "tldr_zh": "本文提出了一种新方法，用于增强行人的跨分辨率重新识别，相较于现有方法，提高了性能和效率。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Intuitions of Machine Learning Researchers about Transfer Learning for Medical Image Classification",
        "summary": "Transfer learning is crucial for medical imaging, yet the selection of source\ndatasets - which can impact the generalizability of algorithms, and thus\npatient outcomes - often relies on researchers' intuition rather than\nsystematic principles. This study investigates these decisions through a\ntask-based survey with machine learning practitioners. Unlike prior work that\nbenchmarks models and experimental setups, we take a human-centered HCI\nperspective on how practitioners select source datasets. Our findings indicate\nthat choices are task-dependent and influenced by community practices, dataset\nproperties, and computational (data embedding), or perceived visual or semantic\nsimilarity. However, similarity ratings and expected performance are not always\naligned, challenging a traditional \"more similar is better\" view. Participants\noften used ambiguous terminology, which suggests a need for clearer definitions\nand HCI tools to make them explicit and usable. By clarifying these heuristics,\nthis work provides practical insights for more systematic source selection in\ntransfer learning.",
        "url": "http://arxiv.org/abs/2510.00902v1",
        "published_date": "2025-10-01T13:44:46+00:00",
        "updated_date": "2025-10-01T13:44:46+00:00",
        "categories": [
            "cs.CV",
            "cs.CY",
            "cs.HC"
        ],
        "authors": [
            "Yucheng Lu",
            "Hubert Dariusz Zając",
            "Veronika Cheplygina",
            "Amelia Jiménez-Sánchez"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "The paper explores how machine learning researchers choose source datasets for medical image classification through a survey, highlighting the influence of task dependency and community practices.",
        "tldr_zh": "本文通过调查探讨了机器学习研究人员如何为医学图像分类选择源数据集，突出了任务依赖性和社区实践的影响。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Solar PV Installation Potential Assessment on Building Facades Based on Vision and Language Foundation Models",
        "summary": "Building facades represent a significant untapped resource for solar energy\ngeneration in dense urban environments, yet assessing their photovoltaic (PV)\npotential remains challenging due to complex geometries and semantic com\nponents. This study introduces SF-SPA (Semantic Facade Solar-PV Assessment), an\nautomated framework that transforms street-view photographs into quantitative\nPV deployment assessments. The approach combines com puter vision and\nartificial intelligence techniques to address three key challenges: perspective\ndistortion correction, semantic understanding of facade elements, and spatial\nreasoning for PV layout optimization. Our four-stage pipeline processes images\nthrough geometric rectification, zero-shot semantic segmentation, Large\nLanguage Model (LLM) guided spatial reasoning, and energy simulation.\nValidation across 80 buildings in four countries demonstrates ro bust\nperformance with mean area estimation errors of 6.2% &#177; 2.8% compared to\nexpert annotations. The auto mated assessment requires approximately 100\nseconds per building, a substantial gain in efficiency over manual methods.\nSimulated energy yield predictions confirm the method's reliability and\napplicability for regional poten tial studies, urban energy planning, and\nbuilding-integrated photovoltaic (BIPV) deployment. Code is available at:\nhttps:github.com/CodeAXu/Solar-PV-Installation",
        "url": "http://arxiv.org/abs/2510.00797v1",
        "published_date": "2025-10-01T11:51:28+00:00",
        "updated_date": "2025-10-01T11:51:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ruyu Liu",
            "Dongxu Zhuang",
            "Jianhua Zhang",
            "Arega Getaneh Abate",
            "Per Sieverts Nielsen",
            "Ben Wang",
            "Xiufeng Liu"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces an automated framework called SF-SPA that assesses the solar PV installation potential on building facades using computer vision and AI techniques, with validation showing promising results.",
        "tldr_zh": "该论文介绍了一个名为SF-SPA的自动化框架，利用计算机视觉和人工智能技术来评估建筑物立面上的太阳能光伏安装潜力，验证结果显示有希望的成果。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "DEAP DIVE: Dataset Investigation with Vision transformers for EEG evaluation",
        "summary": "Accurately predicting emotions from brain signals has the potential to\nachieve goals such as improving mental health, human-computer interaction, and\naffective computing. Emotion prediction through neural signals offers a\npromising alternative to traditional methods, such as self-assessment and\nfacial expression analysis, which can be subjective or ambiguous. Measurements\nof the brain activity via electroencephalogram (EEG) provides a more direct and\nunbiased data source. However, conducting a full EEG is a complex,\nresource-intensive process, leading to the rise of low-cost EEG devices with\nsimplified measurement capabilities. This work examines how subsets of EEG\nchannels from the DEAP dataset can be used for sufficiently accurate emotion\nprediction with low-cost EEG devices, rather than fully equipped\nEEG-measurements. Using Continuous Wavelet Transformation to convert EEG data\ninto scaleograms, we trained a vision transformer (ViT) model for emotion\nclassification. The model achieved over 91,57% accuracy in predicting 4\nquadrants (high/low per arousal and valence) with only 12 measuring points\n(also referred to as channels). Our work shows clearly, that a significant\nreduction of input channels yields high results compared to state-of-the-art\nresults of 96,9% with 32 channels. Training scripts to reproduce our code can\nbe found here:\nhttps://gitlab.kit.edu/kit/aifb/ATKS/public/AutoSMiLeS/DEAP-DIVE.",
        "url": "http://arxiv.org/abs/2510.00725v1",
        "published_date": "2025-10-01T10:07:07+00:00",
        "updated_date": "2025-10-01T10:07:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Annemarie Hoffsommer",
            "Helen Schneider",
            "Svetlana Pavlitska",
            "J. Marius Zöllner"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper investigates using subsets of EEG channels for emotion prediction with low-cost devices, achieving high accuracy with a vision transformer model.",
        "tldr_zh": "本文研究了如何使用低成本设备的EEG信道子集进行情绪预测，并通过视觉转换器模型实现了高准确性。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Adaptive Event Stream Slicing for Open-Vocabulary Event-Based Object Detection via Vision-Language Knowledge Distillation",
        "summary": "Event cameras offer advantages in object detection tasks due to high-speed\nresponse, low latency, and robustness to motion blur. However, event cameras\nlack texture and color information, making open-vocabulary detection\nparticularly challenging. Current event-based detection methods are typically\ntrained on predefined categories, limiting their ability to generalize to novel\nobjects, where encountering previously unseen objects is common.\nVision-language models (VLMs) have enabled open-vocabulary object detection in\nRGB images. However, the modality gap between images and event streams makes it\nineffective to directly transfer CLIP to event data, as CLIP was not designed\nfor event streams. To bridge this gap, we propose an event-image knowledge\ndistillation framework that leverages CLIP's semantic understanding to achieve\nopen-vocabulary object detection on event data. Instead of training CLIP\ndirectly on event streams, we use image frames as inputs to a teacher model,\nguiding the event-based student model to learn CLIP's rich visual\nrepresentations. Through spatial attention-based distillation, the student\nnetwork learns meaningful visual features directly from raw event inputs while\ninheriting CLIP's broad visual knowledge. Furthermore, to prevent information\nloss due to event data segmentation, we design a hybrid spiking neural network\n(SNN) and convolutional neural network (CNN) framework. Unlike fixed-group\nevent segmentation methods, which often discard crucial temporal information,\nour SNN adaptively determines the optimal event segmentation moments, ensuring\nthat key temporal features are extracted. The extracted event features are then\nprocessed by CNNs for object detection.",
        "url": "http://arxiv.org/abs/2510.00681v1",
        "published_date": "2025-10-01T09:03:30+00:00",
        "updated_date": "2025-10-01T09:03:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinchang Zhang",
            "Zijun Li",
            "Jiakai Lin",
            "Guoyu Lu"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "Proposes an adaptive event stream slicing approach for open-vocabulary event-based object detection using vision-language knowledge distillation, bridging the modality gap between event streams and CLIP.",
        "tldr_zh": "提出了一种自适应事件流切片方法，用于通过视觉语言知识蒸馏实现开放词汇事件驱动目标检测，以弥合事件流与CLIP之间的模态差距。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Color Models in Image Processing: A Review and Experimental Comparison",
        "summary": "Color representation is essential in computer vision and human-computer\ninteraction. There are multiple color models available. The choice of a\nsuitable color model is critical for various applications. This paper presents\na review of color models and spaces, analyzing their theoretical foundations,\ncomputational properties, and practical applications. We explore traditional\nmodels such as RGB, CMYK, and YUV, perceptually uniform spaces like CIELAB and\nCIELUV, and fuzzy-based approaches as well. Additionally, we conduct a series\nof experiments to evaluate color models from various perspectives, like device\ndependency, chromatic consistency, and computational complexity. Our\nexperimental results reveal gaps in existing color models and show that the HS*\nfamily is the most aligned with human perception. The review also identifies\nkey strengths and limitations of different models and outlines open challenges\nand future directions This study provides a reference for researchers in image\nprocessing, perceptual computing, digital media, and any other color-related\nfield.",
        "url": "http://arxiv.org/abs/2510.00584v1",
        "published_date": "2025-10-01T07:06:02+00:00",
        "updated_date": "2025-10-01T07:06:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Muragul Muratbekova",
            "Nuray Toganas",
            "Ayan Igali",
            "Maksat Shagyrov",
            "Elnara Kadyrgali",
            "Adilet Yerkin",
            "Pakizar Shamoi"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "The paper reviews and compares different color models in image processing, highlighting the strengths and limitations of each. It conducts experiments to evaluate color models and identifies gaps in existing models.",
        "tldr_zh": "本文回顾和比较了图像处理中不同的颜色模型，突出了每种模型的优缺点。它进行实验评估颜色模型，并识别了现有模型中的差距。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Adaptive Shared Experts with LoRA-Based Mixture of Experts for Multi-Task Learning",
        "summary": "Mixture-of-Experts (MoE) has emerged as a powerful framework for multi-task\nlearning (MTL). However, existing MoE-MTL methods often rely on single-task\npretrained backbones and suffer from redundant adaptation and inefficient\nknowledge sharing during the transition from single-task to multi-task learning\n(STL to MTL). To address these limitations, we propose adaptive shared experts\n(ASE) within a low-rank adaptation (LoRA) based MoE, where shared experts are\nassigned router-computed gating weights jointly normalized with sparse experts.\nThis design facilitates STL to MTL transition, enhances expert specialization,\nand cooperation. Furthermore, we incorporate fine-grained experts by increasing\nthe number of LoRA experts while proportionally reducing their rank, enabling\nmore effective knowledge sharing under a comparable parameter budget. Extensive\nexperiments on the PASCAL-Context benchmark, under unified training settings,\ndemonstrate that ASE consistently improves performance across diverse\nconfigurations and validates the effectiveness of fine-grained designs for MTL.",
        "url": "http://arxiv.org/abs/2510.00570v1",
        "published_date": "2025-10-01T06:49:19+00:00",
        "updated_date": "2025-10-01T06:49:19+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Minghao Yang",
            "Ren Togo",
            "Guang Li",
            "Takahiro Ogawa",
            "Miki Haseyama"
        ],
        "ai_categories": [
            "LoRA",
            "Multimodality"
        ],
        "tldr": "The paper proposes Adaptive Shared Experts within a LoRA-based Mixture of Experts framework for Multi-Task Learning, improving knowledge sharing and performance in transitioning from single-task to multi-task learning.",
        "tldr_zh": "本文提出了一种自适应共享专家与 LoRA 混合专家框架，用于多任务学习，提高了从单任务到多任务学习的知识共享和性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Cascaded Diffusion Framework for Probabilistic Coarse-to-Fine Hand Pose Estimation",
        "summary": "Deterministic models for 3D hand pose reconstruction, whether single-staged\nor cascaded, struggle with pose ambiguities caused by self-occlusions and\ncomplex hand articulations. Existing cascaded approaches refine predictions in\na coarse-to-fine manner but remain deterministic and cannot capture pose\nuncertainties. Recent probabilistic methods model pose distributions yet are\nrestricted to single-stage estimation, which often fails to produce accurate 3D\nreconstructions without refinement. To address these limitations, we propose a\ncoarse-to-fine cascaded diffusion framework that combines probabilistic\nmodeling with cascaded refinement. The first stage is a joint diffusion model\nthat samples diverse 3D joint hypotheses, and the second stage is a Mesh Latent\nDiffusion Model (Mesh LDM) that reconstructs a 3D hand mesh conditioned on a\njoint sample. By training Mesh LDM with diverse joint hypotheses in a learned\nlatent space, our framework learns distribution-aware joint-mesh relationships\nand robust hand priors. Furthermore, the cascaded design mitigates the\ndifficulty of directly mapping 2D images to dense 3D poses, enhancing accuracy\nthrough sequential refinement. Experiments on FreiHAND and HO3Dv2 demonstrate\nthat our method achieves state-of-the-art performance while effectively\nmodeling pose distributions.",
        "url": "http://arxiv.org/abs/2510.00527v1",
        "published_date": "2025-10-01T05:19:15+00:00",
        "updated_date": "2025-10-01T05:19:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Taeyun Woo",
            "Jinah Park",
            "Tae-Kyun Kim"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper presents a new framework for probabilistic hand pose estimation, combining cascaded refinement with probabilistic modeling to improve accuracy and capture pose uncertainties.",
        "tldr_zh": "该论文提出了一种新的概率手部姿势估计框架，将级联细化与概率建模相结合，以提高准确性并捕获姿势不确定性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CardioBench: Do Echocardiography Foundation Models Generalize Beyond the Lab?",
        "summary": "Foundation models (FMs) are reshaping medical imaging, yet their application\nin echocardiography remains limited. While several echocardiography-specific\nFMs have recently been introduced, no standardized benchmark exists to evaluate\nthem. Echocardiography poses unique challenges, including noisy acquisitions,\nhigh frame redundancy, and limited public datasets. Most existing solutions\nevaluate on private data, restricting comparability. To address this, we\nintroduce CardioBench, a comprehensive benchmark for echocardiography FMs.\nCardioBench unifies eight publicly available datasets into a standardized suite\nspanning four regression and five classification tasks, covering functional,\nstructural, diagnostic, and view recognition endpoints. We evaluate several\nleading FM, including cardiac-specific, biomedical, and general-purpose\nencoders, under consistent zero-shot, probing, and alignment protocols. Our\nresults highlight complementary strengths across model families: temporal\nmodeling is critical for functional regression, retrieval provides robustness\nunder distribution shift, and domain-specific text encoders capture\nphysiologically meaningful axes. General-purpose encoders transfer strongly and\noften close the gap with probing, but struggle with fine-grained distinctions\nlike view classification and subtle pathology recognition. By releasing\npreprocessing, splits, and public evaluation pipelines, CardioBench establishes\na reproducible reference point and offers actionable insights to guide the\ndesign of future echocardiography foundation models.",
        "url": "http://arxiv.org/abs/2510.00520v1",
        "published_date": "2025-10-01T05:09:48+00:00",
        "updated_date": "2025-10-01T05:09:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Darya Taratynova",
            "Ahmed Aly",
            "Numan Saeed",
            "Mohammad Yaqub"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "CardioBench introduces a benchmark for evaluating echocardiography foundation models using publicly available datasets and various tasks, showcasing strengths and weaknesses of different model families.",
        "tldr_zh": "CardioBench引入了一个用于评估超声心动图基础模型的基准测试，利用公开可用数据集和各种任务，展示了不同模型家族的优势和劣势。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "A Fast and Precise Method for Searching Rectangular Tumor Regions in Brain MR Images",
        "summary": "Purpose: To develop a fast and precise method for searching rectangular\nregions in brain tumor images. Methods: The authors propose a new method for\nsearching rectangular tumor regions in brain MR images. The proposed method\nconsisted of a segmentation network and a fast search method with a\nuser-controllable search metric. As the segmentation network, the U-Net whose\nencoder was replaced by the EfficientNet was used. In the fast search method,\nsummed-area tables were used for accelerating sums of voxels in rectangular\nregions. Use of the summed-area tables enabled exhaustive search of the 3D\noffset (3D full search). The search metric was designed for giving priority to\ncubes over oblongs, and assigning better values for higher tumor fractions even\nif they exceeded target tumor fractions. The proposed computation and metric\nwere compared with those used in a conventional method using the Brain Tumor\nImage Segmentation dataset. Results: When the 3D full search was used, the\nproposed computation (8 seconds) was 100-500 times faster than the conventional\ncomputation (11-40 minutes). When the user-controllable parts of the search\nmetrics were changed variously, the tumor fractions of the proposed metric were\nhigher than those of the conventional metric. In addition, the conventional\nmetric preferred oblongs whereas the proposed metric preferred cubes.\nConclusion: The proposed method is promising for implementing fast and precise\nsearch of rectangular tumor regions, which is useful for brain tumor diagnosis\nusing MRI systems. The proposed computation reduced processing times of the 3D\nfull search, and the proposed metric improved the quality of the assigned\nrectangular tumor regions.",
        "url": "http://arxiv.org/abs/2510.00505v1",
        "published_date": "2025-10-01T04:35:52+00:00",
        "updated_date": "2025-10-01T04:35:52+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Hidenori Takeshima",
            "Shuki Maruyama"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "This paper presents a fast and precise method for searching rectangular tumor regions in brain MR images using a segmentation network and a user-controllable search metric.",
        "tldr_zh": "本文提出了一种快速而精确的方法，用于在脑MR图像中搜索矩形肿瘤区域，采用分割网络和可控搜索度量。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Normal-Abnormal Guided Generalist Anomaly Detection",
        "summary": "Generalist Anomaly Detection (GAD) aims to train a unified model on an\noriginal domain that can detect anomalies in new target domains. Previous GAD\nmethods primarily use only normal samples as references, overlooking the\nvaluable information contained in anomalous samples that are often available in\nreal-world scenarios. To address this limitation, we propose a more practical\napproach: normal-abnormal-guided generalist anomaly detection, which leverages\nboth normal and anomalous samples as references to guide anomaly detection\nacross diverse domains. We introduce the Normal-Abnormal Generalist Learning\n(NAGL) framework, consisting of two key components: Residual Mining (RM) and\nAnomaly Feature Learning (AFL). RM extracts abnormal patterns from\nnormal-abnormal reference residuals to establish transferable anomaly\nrepresentations, while AFL adaptively learns anomaly features in query images\nthrough residual mapping to identify instance-aware anomalies. Our approach\neffectively utilizes both normal and anomalous references for more accurate and\nefficient cross-domain anomaly detection. Extensive experiments across multiple\nbenchmarks demonstrate that our method significantly outperforms existing GAD\napproaches. This work represents the first to adopt a mixture of normal and\nabnormal samples as references in generalist anomaly detection. The code and\ndatasets are available at https://github.com/JasonKyng/NAGL.",
        "url": "http://arxiv.org/abs/2510.00495v2",
        "published_date": "2025-10-01T04:27:10+00:00",
        "updated_date": "2025-10-02T06:22:11+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuexin Wang",
            "Xiaolei Wang",
            "Yizheng Gong",
            "Jimin Xiao"
        ],
        "ai_categories": [
            "AIGC"
        ],
        "tldr": "The paper introduces a new approach for anomaly detection using both normal and abnormal samples as references, outperforming existing methods.",
        "tldr_zh": "这篇论文引入了一种新的异常检测方法，利用正常和异常样本作为参考，优于现有方法。",
        "relevance_score": 2,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "A Deep Learning Pipeline for Epilepsy Genomic Analysis Using GPT-2 XL and NVIDIA H100",
        "summary": "Epilepsy is a chronic neurological condition characterized by recurrent\nseizures, with global prevalence estimated at 50 million people worldwide.\nWhile progress in high-throughput sequencing has allowed for broad-based\ntranscriptomic profiling of brain tissues, the deciphering of these highly\ncomplex datasets remains one of the challenges. To address this issue, in this\npaper we propose a new analysis pipeline that integrates the power of deep\nlearning strategies with GPU-acceleration computation for investigating Gene\nexpression patterns in epilepsy. Specifically, our proposed approach employs\nGPT-2 XL, a transformer-based Large Language Model (LLM) with 1.5 billion\nparameters for genomic sequence analysis over the latest NVIDIA H100 Tensor\nCore GPUs based on Hopper architecture. Our proposed method enables efficient\npreprocessing of RNA sequence data, gene sequence encoding, and subsequent\npattern identification. We conducted experiments on two epilepsy datasets\nincluding GEO accession GSE264537 and GSE275235. The obtained results reveal\nseveral significant transcriptomic modifications, including reduced hippocampal\nastrogliosis after ketogenic diet treatment as well as restored\nexcitatory-inhibitory signaling equilibrium in zebrafish epilepsy model.\nMoreover, our results highlight the effectiveness of leveraging LLMs in\ncombination with advanced hardware acceleration for transcriptomic\ncharacterization in neurological diseases.",
        "url": "http://arxiv.org/abs/2510.00392v1",
        "published_date": "2025-10-01T01:07:35+00:00",
        "updated_date": "2025-10-01T01:07:35+00:00",
        "categories": [
            "q-bio.GN",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Muhammad Omer Latif",
            "Hayat Ullah",
            "Muhammad Ali Shafique",
            "Zhihua Dong"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "This paper presents a deep learning pipeline using GPT-2 XL and NVIDIA H100 for analyzing genomic data related to epilepsy, showing significant transcriptomic modifications and highlighting the effectiveness of leveraging large language models in neurological disease research.",
        "tldr_zh": "本文提出了一种使用GPT-2 XL和NVIDIA H100进行癫痫相关基因组数据分析的深度学习管道，显示了显著的转录组修改，并强调了在神经系统疾病研究中利用大型语言模型的有效性。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Looking Beyond the Known: Towards a Data Discovery Guided Open-World Object Detection",
        "summary": "Open-World Object Detection (OWOD) enriches traditional object detectors by\nenabling continual discovery and integration of unknown objects via human\nguidance. However, existing OWOD approaches frequently suffer from semantic\nconfusion between known and unknown classes, alongside catastrophic forgetting,\nleading to diminished unknown recall and degraded known-class accuracy. To\novercome these challenges, we propose Combinatorial Open-World Detection\n(CROWD), a unified framework reformulating unknown object discovery and\nadaptation as an interwoven combinatorial (set-based) data-discovery\n(CROWD-Discover) and representation learning (CROWD-Learn) task. CROWD-Discover\nstrategically mines unknown instances by maximizing Submodular Conditional Gain\n(SCG) functions, selecting representative examples distinctly dissimilar from\nknown objects. Subsequently, CROWD-Learn employs novel combinatorial objectives\nthat jointly disentangle known and unknown representations while maintaining\ndiscriminative coherence among known classes, thus mitigating confusion and\nforgetting. Extensive evaluations on OWOD benchmarks illustrate that CROWD\nachieves improvements of 2.83% and 2.05% in known-class accuracy on M-OWODB and\nS-OWODB, respectively, and nearly 2.4x unknown recall compared to leading\nbaselines.",
        "url": "http://arxiv.org/abs/2510.00303v1",
        "published_date": "2025-09-30T21:48:08+00:00",
        "updated_date": "2025-09-30T21:48:08+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Anay Majee",
            "Amitesh Gangrade",
            "Rishabh Iyer"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a framework called CROWD for improving Open-World Object Detection (OWOD) by better handling unknown objects, resulting in higher accuracy and recall compared to existing methods.",
        "tldr_zh": "本文提出了一种名为CROWD的框架，通过更好地处理未知对象来改进开放世界目标检测(OWOD)，从而比现有方法具有更高的准确率和召回率。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Improved Hyperspectral Anomaly Detection via Unsupervised Subspace Modeling in the Signed Cumulative Distribution Transform Domain",
        "summary": "Hyperspectral anomaly detection (HAD), a crucial approach for many civilian\nand military applications, seeks to identify pixels with spectral signatures\nthat are anomalous relative to a preponderance of background signatures.\nSignificant effort has been made to improve HAD techniques, but challenges\narise due to complex real-world environments and, by definition, limited prior\nknowledge of potential signatures of interest. This paper introduces a novel\nHAD method by proposing a transport-based mathematical model to describe the\npixels comprising a given hyperspectral image. In this approach, hyperspectral\npixels are viewed as observations of a template pattern undergoing unknown\ndeformations that enables their representation in the signed cumulative\ndistribution transform (SCDT) domain. An unsupervised subspace modeling\ntechnique is then used to construct a model of abundant background signals in\nthis domain, whereupon anomalous signals are detected as deviations from the\nlearned model. Comprehensive evaluations across five distinct datasets\nillustrate the superiority of our approach compared to state-of-the-art\nmethods.",
        "url": "http://arxiv.org/abs/2510.00148v1",
        "published_date": "2025-09-30T18:20:36+00:00",
        "updated_date": "2025-09-30T18:20:36+00:00",
        "categories": [
            "cs.CV",
            "eess.SP"
        ],
        "authors": [
            "Abu Hasnat Mohammad Rubaiyat",
            "Jordan Vincent",
            "Colin Olson"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper introduces a novel method for hyperspectral anomaly detection using unsupervised subspace modeling in the signed cumulative distribution transform domain, showing superior performance compared to existing methods.",
        "tldr_zh": "本文介绍了一种新颖的高光谱异常检测方法，使用无监督子空间建模在带符号累积分布变换域，表现出比现有方法更好的性能。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Beyond one-hot encoding? Journey into compact encoding for large multi-class segmentation",
        "summary": "This work presents novel methods to reduce computational and memory\nrequirements for medical image segmentation with a large number of classes. We\ncuriously observe challenges in maintaining state-of-the-art segmentation\nperformance with all of the explored options. Standard learning-based methods\ntypically employ one-hot encoding of class labels. The computational complexity\nand memory requirements thus increase linearly with the number of classes. We\npropose a family of binary encoding approaches instead of one-hot encoding to\nreduce the computational complexity and memory requirements to logarithmic in\nthe number of classes. In addition to vanilla binary encoding, we investigate\nthe effects of error-correcting output codes (ECOCs), class weighting,\nhard/soft decoding, class-to-codeword assignment, and label embedding trees. We\napply the methods to the use case of whole brain parcellation with 108 classes\nbased on 3D MRI images. While binary encodings have proven efficient in\nso-called extreme classification problems in computer vision, we faced\nchallenges in reaching state-of-the-art segmentation quality with binary\nencodings. Compared to one-hot encoding (Dice Similarity Coefficient (DSC) =\n82.4 (2.8)), we report reduced segmentation performance with the binary\nsegmentation approaches, achieving DSCs in the range from 39.3 to 73.8.\nInformative negative results all too often go unpublished. We hope that this\nwork inspires future research of compact encoding strategies for large\nmulti-class segmentation tasks.",
        "url": "http://arxiv.org/abs/2510.00667v1",
        "published_date": "2025-10-01T08:53:39+00:00",
        "updated_date": "2025-10-01T08:53:39+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Aaron Kujawa",
            "Thomas Booth",
            "Tom Vercauteren"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "The paper explores compact encoding methods for large multi-class segmentation tasks, but faces challenges in achieving state-of-the-art results compared to one-hot encoding.",
        "tldr_zh": "本文探讨了针对大规模多类别分割任务的紧凑编码方法，但与one-hot编码相比在实现最先进结果方面遇到了挑战。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Rehearsal-free and Task-free Online Continual Learning With Contrastive Prompt",
        "summary": "The main challenge of continual learning is \\textit{catastrophic forgetting}.\nBecause of processing data in one pass, online continual learning (OCL) is one\nof the most difficult continual learning scenarios. To address catastrophic\nforgetting in OCL, some existing studies use a rehearsal buffer to store\nsamples and replay them in the later learning process, other studies do not\nstore samples but assume a sequence of learning tasks so that the task\nidentities can be explored. However, storing samples may raise data security or\nprivacy concerns and it is not always possible to identify the boundaries\nbetween learning tasks in one pass of data processing. It motivates us to\ninvestigate rehearsal-free and task-free OCL (F2OCL). By integrating prompt\nlearning with an NCM classifier, this study has effectively tackled\ncatastrophic forgetting without storing samples and without usage of task\nboundaries or identities. The extensive experimental results on two benchmarks\nhave demonstrated the effectiveness of the proposed method.",
        "url": "http://arxiv.org/abs/2510.00467v1",
        "published_date": "2025-10-01T03:39:29+00:00",
        "updated_date": "2025-10-01T03:39:29+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Aopeng Wang",
            "Ke Deng",
            "Yongli Ren",
            "Jun Luo"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer"
        ],
        "tldr": "The paper presents a method called F2OCL for online continual learning without storing samples or using task boundaries. It effectively addresses catastrophic forgetting in OCL scenarios.",
        "tldr_zh": "本文提出了一种名为F2OCL的方法，用于在线连续学习，无需存储样本或使用任务边界。它有效地解决了OCL场景中的灾难性遗忘。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Does Bigger Mean Better? Comparitive Analysis of CNNs and Biomedical Vision Language Modles in Medical Diagnosis",
        "summary": "The accurate interpretation of chest radiographs using automated methods is a\ncritical task in medical imaging. This paper presents a comparative analysis\nbetween a supervised lightweight Convolutional Neural Network (CNN) and a\nstate-of-the-art, zero-shot medical Vision-Language Model (VLM), BiomedCLIP,\nacross two distinct diagnostic tasks: pneumonia detection on the PneumoniaMNIST\nbenchmark and tuberculosis detection on the Shenzhen TB dataset. Our\nexperiments show that supervised CNNs serve as highly competitive baselines in\nboth cases. While the default zero-shot performance of the VLM is lower, we\ndemonstrate that its potential can be unlocked via a simple yet crucial remedy:\ndecision threshold calibration. By optimizing the classification threshold on a\nvalidation set, the performance of BiomedCLIP is significantly boosted across\nboth datasets. For pneumonia detection, calibration enables the zero-shot VLM\nto achieve a superior F1-score of 0.8841, surpassing the supervised CNN's\n0.8803. For tuberculosis detection, calibration dramatically improves the\nF1-score from 0.4812 to 0.7684, bringing it close to the supervised baseline's\n0.7834. This work highlights a key insight: proper calibration is essential for\nleveraging the full diagnostic power of zero-shot VLMs, enabling them to match\nor even outperform efficient, task-specific supervised models.",
        "url": "http://arxiv.org/abs/2510.00411v2",
        "published_date": "2025-10-01T01:46:09+00:00",
        "updated_date": "2025-10-02T04:22:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ran Tong",
            "Jiaqi Liu",
            "Su Liu",
            "Jiexi Xu",
            "Lanruo Wang",
            "Tong Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper compares the performance of a supervised CNN and a zero-shot medical Vision-Language Model for pneumonia and tuberculosis detection. It shows that with proper calibration, the VLM can outperform the CNN in some cases.",
        "tldr_zh": "本文比较了监督 CNN 和零射击医学视觉语言模型在肺炎和结核病检测中的表现。 结果表明，通过适当的校准，VLM 在某些情况下可以胜过CNN。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "A Scene is Worth a Thousand Features: Feed-Forward Camera Localization from a Collection of Image Features",
        "summary": "Visually localizing an image, i.e., estimating its camera pose, requires\nbuilding a scene representation that serves as a visual map. The representation\nwe choose has direct consequences towards the practicability of our system.\nEven when starting from mapping images with known camera poses,\nstate-of-the-art approaches still require hours of mapping time in the worst\ncase, and several minutes in the best. This work raises the question whether we\ncan achieve competitive accuracy much faster. We introduce FastForward, a\nmethod that creates a map representation and relocalizes a query image\non-the-fly in a single feed-forward pass. At the core, we represent multiple\nmapping images as a collection of features anchored in 3D space. FastForward\nutilizes these mapping features to predict image-to-scene correspondences for\nthe query image, enabling the estimation of its camera pose. We couple\nFastForward with image retrieval and achieve state-of-the-art accuracy when\ncompared to other approaches with minimal map preparation time. Furthermore,\nFastForward demonstrates robust generalization to unseen domains, including\nchallenging large-scale outdoor environments.",
        "url": "http://arxiv.org/abs/2510.00978v1",
        "published_date": "2025-10-01T14:52:12+00:00",
        "updated_date": "2025-10-01T14:52:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Axel Barroso-Laguna",
            "Tommaso Cavallari",
            "Victor Adrian Prisacariu",
            "Eric Brachmann"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces FastForward, a method for quickly estimating camera poses for query images using a feed-forward pass and image features.",
        "tldr_zh": "本文介绍了FastForward方法，通过使用前向传递和图像特征，快速估算查询图像的相机姿态。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.0
    },
    {
        "title": "Diagnosing Shortcut-Induced Rigidity in Continual Learning: The Einstellung Rigidity Index (ERI)",
        "summary": "Deep neural networks frequently exploit shortcut features, defined as\nincidental correlations between inputs and labels without causal meaning.\nShortcut features undermine robustness and reduce reliability under\ndistribution shifts. In continual learning (CL), the consequences of shortcut\nexploitation can persist and intensify: weights inherited from earlier tasks\nbias representation reuse toward whatever features most easily satisfied prior\nlabels, mirroring the cognitive Einstellung effect, a phenomenon where past\nhabits block optimal solutions. Whereas catastrophic forgetting erodes past\nskills, shortcut-induced rigidity throttles the acquisition of new ones. We\nintroduce the Einstellung Rigidity Index (ERI), a compact diagnostic that\ndisentangles genuine transfer from cue-inflated performance using three\ninterpretable facets: (i) Adaptation Delay (AD), (ii) Performance Deficit (PD),\nand (iii) Relative Suboptimal Feature Reliance (SFR_rel). On a two-phase\nCIFAR-100 CL benchmark with a deliberately spurious magenta patch in Phase 2,\nwe evaluate Naive fine-tuning (SGD), online Elastic Weight Consolidation\n(EWC_on), Dark Experience Replay (DER++), Gradient Projection Memory (GPM), and\nDeep Generative Replay (DGR). Across these continual learning methods, we\nobserve that CL methods reach accuracy thresholds earlier than a Scratch-T2\nbaseline (negative AD) but achieve slightly lower final accuracy on patched\nshortcut classes (positive PD). Masking the patch improves accuracy for CL\nmethods while slightly reducing Scratch-T2, yielding negative SFR_rel. This\npattern indicates the patch acted as a distractor for CL models in this setting\nrather than a helpful shortcut.",
        "url": "http://arxiv.org/abs/2510.00475v1",
        "published_date": "2025-10-01T03:52:40+00:00",
        "updated_date": "2025-10-01T03:52:40+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Kai Gu",
            "Weishi Shi"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces the Einstellung Rigidity Index to diagnose shortcut-induced rigidity in continual learning, highlighting how shortcut exploitation can hinder the acquisition of new skills.",
        "tldr_zh": "本文引入了Einstellung Rigidity Index来诊断持续学习中的捷径诱发的僵化，突出了捷径利用如何阻碍新技能的习得。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]