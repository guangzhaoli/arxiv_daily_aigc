[
    {
        "title": "Audio-Driven Universal Gaussian Head Avatars",
        "summary": "We introduce the first method for audio-driven universal photorealistic\navatar synthesis, combining a person-agnostic speech model with our novel\nUniversal Head Avatar Prior (UHAP). UHAP is trained on cross-identity\nmulti-view videos. In particular, our UHAP is supervised with neutral scan\ndata, enabling it to capture the identity-specific details at high fidelity. In\ncontrast to previous approaches, which predominantly map audio features to\ngeometric deformations only while ignoring audio-dependent appearance\nvariations, our universal speech model directly maps raw audio inputs into the\nUHAP latent expression space. This expression space inherently encodes, both,\ngeometric and appearance variations. For efficient personalization to new\nsubjects, we employ a monocular encoder, which enables lightweight regression\nof dynamic expression variations across video frames. By accounting for these\nexpression-dependent changes, it enables the subsequent model fine-tuning stage\nto focus exclusively on capturing the subject's global appearance and geometry.\nDecoding these audio-driven expression codes via UHAP generates highly\nrealistic avatars with precise lip synchronization and nuanced expressive\ndetails, such as eyebrow movement, gaze shifts, and realistic mouth interior\nappearance as well as motion. Extensive evaluations demonstrate that our method\nis not only the first generalizable audio-driven avatar model that can account\nfor detailed appearance modeling and rendering, but it also outperforms\ncompeting (geometry-only) methods across metrics measuring lip-sync accuracy,\nquantitative image quality, and perceptual realism.",
        "url": "http://arxiv.org/abs/2509.18924v1",
        "published_date": "2025-09-23T12:46:43+00:00",
        "updated_date": "2025-09-23T12:46:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kartik Teotia",
            "Helge Rhodin",
            "Mohit Mendiratta",
            "Hyeongwoo Kim",
            "Marc Habermann",
            "Christian Theobalt"
        ],
        "ai_categories": [
            "GAN",
            "Multimodality"
        ],
        "tldr": "The paper introduces a method for audio-driven photorealistic avatar synthesis that combines a speech model and a novel Universal Head Avatar Prior (UHAP) to capture identity-specific details. It outperforms existing methods in lip-sync accuracy and realism.",
        "tldr_zh": "本文介绍了一种结合语音模型和新型 Universal Head Avatar Prior (UHAP) 的音频驱动逼真头像合成方法，能够捕捉身份特定细节，超越现有方法在唇同步准确性和逼真度方面。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation",
        "summary": "Unified multimodal models have recently attracted considerable attention for\ntheir remarkable abilities in jointly understanding and generating diverse\ncontent. However, as contexts integrate increasingly numerous interleaved\nmultimodal tokens, the iterative processes of diffusion denoising and\nautoregressive decoding impose significant computational overhead. To address\nthis, we propose Hyper-Bagel, a unified acceleration framework designed to\nsimultaneously speed up both multimodal understanding and generation tasks. Our\napproach uses a divide-and-conquer strategy, employing speculative decoding for\nnext-token prediction and a multi-stage distillation process for diffusion\ndenoising. The framework delivers substantial performance gains, achieving over\na 2x speedup in multimodal understanding. For generative tasks, our resulting\nlossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a\n22x speedup in image editing, all while preserving the high-quality output of\nthe original model. We further develop a highly efficient 1-NFE model that\nenables near real-time interactive editing and generation. By combining\nadvanced adversarial distillation with human feedback learning, this model\nachieves ultimate cost-effectiveness and responsiveness, making complex\nmultimodal interactions seamless and instantaneous.",
        "url": "http://arxiv.org/abs/2509.18824v1",
        "published_date": "2025-09-23T09:12:46+00:00",
        "updated_date": "2025-09-23T09:12:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanzuo Lu",
            "Xin Xia",
            "Manlin Zhang",
            "Huafeng Kuang",
            "Jianbin Zheng",
            "Yuxi Ren",
            "Xuefeng Xiao"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "GAN"
        ],
        "tldr": "Hyper-Bagel is a framework that accelerates multimodal understanding and generation tasks, achieving significant speedups while maintaining high-quality output.",
        "tldr_zh": "Hyper-Bagel 是一个加速多模态理解和生成任务的框架，实现了显著的加速，同时保持高质量的输出。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction",
        "summary": "Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective\nsolution for novel view synthesis. Existing methods predominantly rely on a\npixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a\n3D Gaussian. We rethink this widely adopted formulation and identify several\ninherent limitations: it renders the reconstructed 3D models heavily dependent\non the number of input views, leads to view-biased density distributions, and\nintroduces alignment errors, particularly when source views contain occlusions\nor low texture. To address these challenges, we introduce VolSplat, a new\nmulti-view feed-forward paradigm that replaces pixel alignment with\nvoxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D\nvoxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature\nmatching, ensuring robust multi-view consistency. Furthermore, it enables\nadaptive control over Gaussian density based on 3D scene complexity, yielding\nmore faithful Gaussian point clouds, improved geometric consistency, and\nenhanced novel-view rendering quality. Experiments on widely used benchmarks\nincluding RealEstate10K and ScanNet demonstrate that VolSplat achieves\nstate-of-the-art performance while producing more plausible and view-consistent\nGaussian reconstructions. In addition to superior results, our approach\nestablishes a more scalable framework for feed-forward 3D reconstruction with\ndenser and more robust representations, paving the way for further research in\nwider communities. The video results, code and trained models are available on\nour project page: https://lhmd.top/volsplat.",
        "url": "http://arxiv.org/abs/2509.19297v1",
        "published_date": "2025-09-23T17:59:02+00:00",
        "updated_date": "2025-09-23T17:59:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weijie Wang",
            "Yeqing Chen",
            "Zeyu Zhang",
            "Hengyu Liu",
            "Haoxiao Wang",
            "Zhiyuan Feng",
            "Wenkang Qin",
            "Zheng Zhu",
            "Donny Y. Chen",
            "Bohan Zhuang"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces VolSplat, a new approach for 3D view synthesis using voxel-aligned Gaussians, leading to improved reconstruction quality and consistency.",
        "tldr_zh": "本文介绍了VolSplat，一种利用体素对齐的高斯方法进行3D视图合成，从而提高了重建质量和一致性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "Frequency-Domain Decomposition and Recomposition for Robust Audio-Visual Segmentation",
        "summary": "Audio-visual segmentation (AVS) plays a critical role in multimodal machine\nlearning by effectively integrating audio and visual cues to precisely segment\nobjects or regions within visual scenes. Recent AVS methods have demonstrated\nsignificant improvements. However, they overlook the inherent frequency-domain\ncontradictions between audio and visual modalities--the pervasively interfering\nnoise in audio high-frequency signals vs. the structurally rich details in\nvisual high-frequency signals. Ignoring these differences can result in\nsuboptimal performance. In this paper, we rethink the AVS task from a deeper\nperspective by reformulating AVS task as a frequency-domain decomposition and\nrecomposition problem. To this end, we introduce a novel Frequency-Aware\nAudio-Visual Segmentation (FAVS) framework consisting of two key modules:\nFrequency-Domain Enhanced Decomposer (FDED) module and Synergistic Cross-Modal\nConsistency (SCMC) module. FDED module employs a residual-based iterative\nfrequency decomposition to discriminate modality-specific semantics and\nstructural features, and SCMC module leverages a mixture-of-experts\narchitecture to reinforce semantic consistency and modality-specific feature\npreservation through dynamic expert routing. Extensive experiments demonstrate\nthat our FAVS framework achieves state-of-the-art performance on three\nbenchmark datasets, and abundant qualitative visualizations further verify the\neffectiveness of the proposed FDED and SCMC modules. The code will be released\nas open source upon acceptance of the paper.",
        "url": "http://arxiv.org/abs/2509.18912v1",
        "published_date": "2025-09-23T12:33:48+00:00",
        "updated_date": "2025-09-23T12:33:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yunzhe Shen",
            "Kai Peng",
            "Leiye Liu",
            "Wei Ji",
            "Jingjing Li",
            "Miao Zhang",
            "Yongri Piao",
            "Huchuan Lu"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel Frequency-Aware Audio-Visual Segmentation (FAVS) framework that decomposes and recomposes audio-visual signals in the frequency domain, achieving state-of-the-art performance on benchmark datasets.",
        "tldr_zh": "该论文提出了一种新颖的频域感知音视频分割（FAVS）框架，通过在频域中对音视频信号进行分解和重组，在基准数据集上取得了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Text Slider: Efficient and Plug-and-Play Continuous Concept Control for Image/Video Synthesis via LoRA Adapters",
        "summary": "Recent advances in diffusion models have significantly improved image and\nvideo synthesis. In addition, several concept control methods have been\nproposed to enable fine-grained, continuous, and flexible control over\nfree-form text prompts. However, these methods not only require intensive\ntraining time and GPU memory usage to learn the sliders or embeddings but also\nneed to be retrained for different diffusion backbones, limiting their\nscalability and adaptability. To address these limitations, we introduce Text\nSlider, a lightweight, efficient and plug-and-play framework that identifies\nlow-rank directions within a pre-trained text encoder, enabling continuous\ncontrol of visual concepts while significantly reducing training time, GPU\nmemory consumption, and the number of trainable parameters. Furthermore, Text\nSlider supports multi-concept composition and continuous control, enabling\nfine-grained and flexible manipulation in both image and video synthesis. We\nshow that Text Slider enables smooth and continuous modulation of specific\nattributes while preserving the original spatial layout and structure of the\ninput. Text Slider achieves significantly better efficiency: 5$\\times$ faster\ntraining than Concept Slider and 47$\\times$ faster than Attribute Control,\nwhile reducing GPU memory usage by nearly 2$\\times$ and 4$\\times$,\nrespectively.",
        "url": "http://arxiv.org/abs/2509.18831v1",
        "published_date": "2025-09-23T09:17:18+00:00",
        "updated_date": "2025-09-23T09:17:18+00:00",
        "categories": [
            "cs.GR",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.MM"
        ],
        "authors": [
            "Pin-Yen Chiu",
            "I-Sheng Fang",
            "Jun-Cheng Chen"
        ],
        "ai_categories": [
            "LoRA",
            "Transformer",
            "GAN"
        ],
        "tldr": "Text Slider is a new framework for fine-grained control of visual concepts in image and video synthesis, with faster training and reduced GPU memory usage compared to existing methods.",
        "tldr_zh": "Text Slider 是一个新的框架，用于在图像和视频合成中对视觉概念进行精细控制，与现有方法相比，训练更快，GPU 内存使用更少。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Towards Application Aligned Synthetic Surgical Image Synthesis",
        "summary": "The scarcity of annotated surgical data poses a significant challenge for\ndeveloping deep learning systems in computer-assisted interventions. While\ndiffusion models can synthesize realistic images, they often suffer from data\nmemorization, resulting in inconsistent or non-diverse samples that may fail to\nimprove, or even harm, downstream performance. We introduce \\emph{Surgical\nApplication-Aligned Diffusion} (SAADi), a new framework that aligns diffusion\nmodels with samples preferred by downstream models. Our method constructs pairs\nof \\emph{preferred} and \\emph{non-preferred} synthetic images and employs\nlightweight fine-tuning of diffusion models to align the image generation\nprocess with downstream objectives explicitly. Experiments on three surgical\ndatasets demonstrate consistent gains of $7$--$9\\%$ in classification and\n$2$--$10\\%$ in segmentation tasks, with the considerable improvements observed\nfor underrepresented classes. Iterative refinement of synthetic samples further\nboosts performance by $4$--$10\\%$. Unlike baseline approaches, our method\novercomes sample degradation and establishes task-aware alignment as a key\nprinciple for mitigating data scarcity and advancing surgical vision\napplications.",
        "url": "http://arxiv.org/abs/2509.18796v1",
        "published_date": "2025-09-23T08:40:40+00:00",
        "updated_date": "2025-09-23T08:40:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Danush Kumar Venkatesh",
            "Stefanie Speidel"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a new framework, SAADi, which aligns diffusion models with preferred samples for downstream models in surgical image synthesis. It shows consistent performance gains in classification and segmentation tasks with improvements for underrepresented classes.",
        "tldr_zh": "本文介绍了一种新的框架 SAADi，在外科图像合成中将扩散模型与下游模型的首选样本进行对齐。在分类和分割任务中表现出一致的性能提升，特别是对少数类的改进。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Knowledge Transfer from Interaction Learning",
        "summary": "Current visual foundation models (VFMs) face a fundamental limitation in\ntransferring knowledge from vision language models (VLMs), while VLMs excel at\nmodeling cross-modal interactions through unified representation spaces,\nexisting VFMs predominantly adopt result-oriented paradigms that neglect the\nunderlying interaction processes. This representational discrepancy hinders\neffective knowledge transfer and limits generalization across diverse vision\ntasks. We propose Learning from Interactions (LFI), a cognitive-inspired\nframework that addresses this gap by explicitly modeling visual understanding\nas an interactive process. Our key insight is that capturing the dynamic\ninteraction patterns encoded in pre-trained VLMs enables more faithful and\nefficient knowledge transfer to VFMs. The approach centers on two technical\ninnovations, Interaction Queries, which maintain persistent relational\nstructures across network layers, and interaction-based supervision, derived\nfrom the cross-modal attention mechanisms of VLMs. Comprehensive experiments\ndemonstrate consistent improvements across multiple benchmarks, achieving 3.3\nand 1.6mAP/2.4AP absolute gains on TinyImageNet classification and COCO\ndetection/segmentation respectively, with minimal parameter overhead and faster\nconvergence. The framework particularly excels in cross-domain settings,\ndelivering 2.4 and 9.3 zero-shot improvements on PACS and VLCS. Human\nevaluations further confirm its cognitive alignment, outperforming\nresult-oriented methods by 2.7 times in semantic consistency metrics.",
        "url": "http://arxiv.org/abs/2509.18733v1",
        "published_date": "2025-09-23T07:27:36+00:00",
        "updated_date": "2025-09-23T07:27:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yilin Gao",
            "Kangyi Chen",
            "Zhongxing Peng",
            "Hengjie Lu",
            "Shugong Xu"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer"
        ],
        "tldr": "The paper introduces Learning from Interactions (LFI) framework which improves knowledge transfer between visual language models, achieving significant gains on various benchmarks.",
        "tldr_zh": "本文介绍了Learning from Interactions（LFI）框架，该框架改善了视觉语言模型之间的知识转移，在各种基准测试中取得了显著的收益。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "AGSwap: Overcoming Category Boundaries in Object Fusion via Adaptive Group Swapping",
        "summary": "Fusing cross-category objects to a single coherent object has gained\nincreasing attention in text-to-image (T2I) generation due to its broad\napplications in virtual reality, digital media, film, and gaming. However,\nexisting methods often produce biased, visually chaotic, or semantically\ninconsistent results due to overlapping artifacts and poor integration.\nMoreover, progress in this field has been limited by the absence of a\ncomprehensive benchmark dataset. To address these problems, we propose\n\\textbf{Adaptive Group Swapping (AGSwap)}, a simple yet highly effective\napproach comprising two key components: (1) Group-wise Embedding Swapping,\nwhich fuses semantic attributes from different concepts through feature\nmanipulation, and (2) Adaptive Group Updating, a dynamic optimization mechanism\nguided by a balance evaluation score to ensure coherent synthesis.\nAdditionally, we introduce \\textbf{Cross-category Object Fusion (COF)}, a\nlarge-scale, hierarchically structured dataset built upon ImageNet-1K and\nWordNet. COF includes 95 superclasses, each with 10 subclasses, enabling\n451,250 unique fusion pairs. Extensive experiments demonstrate that AGSwap\noutperforms state-of-the-art compositional T2I methods, including GPT-Image-1\nusing simple and complex prompts.",
        "url": "http://arxiv.org/abs/2509.18699v1",
        "published_date": "2025-09-23T06:32:14+00:00",
        "updated_date": "2025-09-23T06:32:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zedong Zhang",
            "Ying Tai",
            "Jianjun Qian",
            "Jian Yang",
            "Jun Li"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces AGSwap, a method for fusing cross-category objects in text-to-image generation, outperforming state-of-the-art methods. They also present a new dataset for validation.",
        "tldr_zh": "该论文介绍了AGSwap，一种用于融合跨类别对象的方法，在文到图生成中表现优于现有技术。他们还呈现了一个新的验证数据集。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "OSDA: A Framework for Open-Set Discovery and Automatic Interpretation of Land-cover in Remote Sensing Imagery",
        "summary": "Open-set land-cover analysis in remote sensing requires the ability to\nachieve fine-grained spatial localization and semantically open categorization.\nThis involves not only detecting and segmenting novel objects without\ncategorical supervision but also assigning them interpretable semantic labels\nthrough multimodal reasoning. In this study, we introduce OSDA, an integrated\nthree-stage framework for annotation-free open-set land-cover discovery,\nsegmentation, and description. The pipeline consists of: (1) precise discovery\nand mask extraction with a promptable fine-tuned segmentation model (SAM), (2)\nsemantic attribution and contextual description via a two-phase fine-tuned\nmultimodal large language model (MLLM), and (3) LLM-as-judge and manual scoring\nof the MLLMs evaluation. By combining pixel-level accuracy with high-level\nsemantic understanding, OSDA addresses key challenges in open-world remote\nsensing interpretation. Designed to be architecture-agnostic and label-free,\nthe framework supports robust evaluation across diverse satellite imagery\nwithout requiring manual annotation. Our work provides a scalable and\ninterpretable solution for dynamic land-cover monitoring, showing strong\npotential for automated cartographic updating and large-scale earth observation\nanalysis.",
        "url": "http://arxiv.org/abs/2509.18693v1",
        "published_date": "2025-09-23T06:23:56+00:00",
        "updated_date": "2025-09-23T06:23:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Siyi Chen",
            "Kai Wang",
            "Weicong Pang",
            "Ruiming Yang",
            "Ziru Chen",
            "Renjun Gao",
            "Alexis Kai Hon Lau",
            "Dasa Gu",
            "Chenchen Zhang",
            "Cheng Li"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "OSDA is a framework for open-set land-cover analysis in remote sensing imagery, providing annotation-free discovery, segmentation, and description of land-cover with semantic labels.",
        "tldr_zh": "OSDA是一个用于遥感图像中开放式土地覆盖分析的框架，提供了无需标注的土地覆盖的发现、分割和描述，带有语义标签。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "Training-Free Multi-Style Fusion Through Reference-Based Adaptive Modulation",
        "summary": "We propose Adaptive Multi-Style Fusion (AMSF), a reference-based\ntraining-free framework that enables controllable fusion of multiple reference\nstyles in diffusion models. Most of the existing reference-based methods are\nlimited by (a) acceptance of only one style image, thus prohibiting hybrid\naesthetics and scalability to more styles, and (b) lack of a principled\nmechanism to balance several stylistic influences. AMSF mitigates these\nchallenges by encoding all style images and textual hints with a semantic token\ndecomposition module that is adaptively injected into every cross-attention\nlayer of an frozen diffusion model. A similarity-aware re-weighting module then\nrecalibrates, at each denoising step, the attention allocated to every style\ncomponent, yielding balanced and user-controllable blends without any\nfine-tuning or external adapters. Both qualitative and quantitative evaluations\nshow that AMSF produces multi-style fusion results that consistently outperform\nthe state-of-the-art approaches, while its fusion design scales seamlessly to\ntwo or more styles. These capabilities position AMSF as a practical step toward\nexpressive multi-style generation in diffusion models.",
        "url": "http://arxiv.org/abs/2509.18602v1",
        "published_date": "2025-09-23T03:47:59+00:00",
        "updated_date": "2025-09-23T03:47:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xu Liu",
            "Yibo Lu",
            "Xinxian Wang",
            "Xinyu Wu"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "Proposes Adaptive Multi-Style Fusion (AMSF) for training-free multi-style fusion in diffusion models, outperforming existing methods in producing balanced and user-controllable blends.",
        "tldr_zh": "提出了适用于扩散模型的无需训练的多样式融合技术AMSF，优于现有方法在产生平衡且可控混合方面。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation",
        "summary": "The ability to generate virtual environments is crucial for applications\nranging from gaming to physical AI domains such as robotics, autonomous\ndriving, and industrial AI. Current learning-based 3D reconstruction methods\nrely on the availability of captured real-world multi-view data, which is not\nalways readily available. Recent advancements in video diffusion models have\nshown remarkable imagination capabilities, yet their 2D nature limits the\napplications to simulation where a robot needs to navigate and interact with\nthe environment. In this paper, we propose a self-distillation framework that\naims to distill the implicit 3D knowledge in the video diffusion models into an\nexplicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for\nmulti-view training data. Specifically, we augment the typical RGB decoder with\na 3DGS decoder, which is supervised by the output of the RGB decoder. In this\napproach, the 3DGS decoder can be purely trained with synthetic data generated\nby video diffusion models. At inference time, our model can synthesize 3D\nscenes from either a text prompt or a single image for real-time rendering. Our\nframework further extends to dynamic 3D scene generation from a monocular input\nvideo. Experimental results show that our framework achieves state-of-the-art\nperformance in static and dynamic 3D scene generation.",
        "url": "http://arxiv.org/abs/2509.19296v1",
        "published_date": "2025-09-23T17:58:01+00:00",
        "updated_date": "2025-09-23T17:58:01+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Sherwin Bahmani",
            "Tianchang Shen",
            "Jiawei Ren",
            "Jiahui Huang",
            "Yifeng Jiang",
            "Haithem Turki",
            "Andrea Tagliasacchi",
            "David B. Lindell",
            "Zan Gojcic",
            "Sanja Fidler",
            "Huan Ling",
            "Jun Gao",
            "Xuanchi Ren"
        ],
        "ai_categories": [
            "LoRA",
            "Diffusion",
            "GAN",
            "AIGC"
        ],
        "tldr": "The paper introduces a self-distillation framework to convert 2D video diffusion models into 3D representations for scene generation, achieving state-of-the-art performance in static and dynamic 3D scene generation.",
        "tldr_zh": "该论文介绍了一种自我蒸馏框架，将2D视频扩散模型转换为3D表示，用于场景生成，在静态和动态3D场景生成中取得了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps",
        "summary": "Continuous human motion understanding remains a core challenge in computer\nvision due to its high dimensionality and inherent redundancy. Efficient\ncompression and representation are crucial for analyzing complex motion\ndynamics. In this work, we introduce an adversarially-refined VQ-GAN framework\nwith dense motion tokenization for compressing spatio-temporal heatmaps while\npreserving the fine-grained traces of human motion. Our approach combines dense\nmotion tokenization with adversarial refinement, which eliminates\nreconstruction artifacts like motion smearing and temporal misalignment\nobserved in non-adversarial baselines. Our experiments on the CMU Panoptic\ndataset provide conclusive evidence of our method's superiority, outperforming\nthe dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.\nFurthermore, our dense tokenization strategy enables a novel analysis of motion\ncomplexity, revealing that 2D motion can be optimally represented with a\ncompact 128-token vocabulary, while 3D motion's complexity demands a much\nlarger 1024-token codebook for faithful reconstruction. These results establish\npractical deployment feasibility across diverse motion analysis applications.\nThe code base for this work is available at\nhttps://github.com/TeCSAR-UNCC/Pose-Quantization.",
        "url": "http://arxiv.org/abs/2509.19252v1",
        "published_date": "2025-09-23T17:12:20+00:00",
        "updated_date": "2025-09-23T17:12:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Gabriel Maldonado",
            "Narges Rashvand",
            "Armin Danesh Pazho",
            "Ghazal Alinezhad Noghre",
            "Vinit Katariya",
            "Hamed Tabkhi"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces an adversarially-refined VQ-GAN framework with dense motion tokenization to compress spatio-temporal heatmaps while preserving human motion details, showing superiority over existing methods.",
        "tldr_zh": "本文介绍了一种具有稠密运动标记的对抗优化VQ-GAN框架，用于压缩时空热图，同时保留人体运动细节，表现出对现有方法的优越性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models",
        "summary": "Autonomous vehicles (AVs) are expected to revolutionize transportation by\nimproving efficiency and safety. Their success relies on 3D vision systems that\neffectively sense the environment and detect traffic agents. Among sensors AVs\nuse to create a comprehensive view of surroundings, LiDAR provides\nhigh-resolution depth data enabling accurate object detection, safe navigation,\nand collision avoidance. However, collecting real-world LiDAR data is\ntime-consuming and often affected by noise and sparsity due to adverse weather\nor sensor limitations. This work applies a denoising diffusion probabilistic\nmodel (DDPM), enhanced with novel noise scheduling and time-step embedding\ntechniques to generate high-quality synthetic data for augmentation, thereby\nimproving performance across a range of computer vision tasks, particularly in\nAV perception. These modifications impact the denoising process and the model's\ntemporal awareness, allowing it to produce more realistic point clouds based on\nthe projection. The proposed method was extensively evaluated under various\nconfigurations using the IAMCV and KITTI-360 datasets, with four performance\nmetrics compared against state-of-the-art (SOTA) methods. The results\ndemonstrate the model's superior performance over most existing baselines and\nits effectiveness in mitigating the effects of noisy and sparse LiDAR data,\nproducing diverse point clouds with rich spatial relationships and structural\ndetail.",
        "url": "http://arxiv.org/abs/2509.18917v1",
        "published_date": "2025-09-23T12:35:07+00:00",
        "updated_date": "2025-09-23T12:35:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Amirhesam Aghanouri",
            "Cristina Olaverri-Monreal"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper presents a method using denoising diffusion probabilistic models to generate high-quality synthetic LiDAR point cloud data for autonomous vehicles, improving performance in computer vision tasks.",
        "tldr_zh": "本文提出了一种使用去噪扩散概率模型生成高质量合成LiDAR点云数据的方法，改善了自动驾驶车辆在计算机视觉任务中的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps",
        "summary": "Despite steady progress in layout-to-image generation, current methods still\nstruggle with layouts containing significant overlap between bounding boxes. We\nidentify two primary challenges: (1) large overlapping regions and (2)\noverlapping instances with minimal semantic distinction. Through both\nqualitative examples and quantitative analysis, we demonstrate how these\nfactors degrade generation quality. To systematically assess this issue, we\nintroduce OverLayScore, a novel metric that quantifies the complexity of\noverlapping bounding boxes. Our analysis reveals that existing benchmarks are\nbiased toward simpler cases with low OverLayScore values, limiting their\neffectiveness in evaluating model performance under more challenging\nconditions. To bridge this gap, we present OverLayBench, a new benchmark\nfeaturing high-quality annotations and a balanced distribution across different\nlevels of OverLayScore. As an initial step toward improving performance on\ncomplex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on a\ncurated amodal mask dataset. Together, our contributions lay the groundwork for\nmore robust layout-to-image generation under realistic and challenging\nscenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench.",
        "url": "http://arxiv.org/abs/2509.19282v1",
        "published_date": "2025-09-23T17:50:00+00:00",
        "updated_date": "2025-09-23T17:50:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bingnan Li",
            "Chen-Yu Wang",
            "Haiyang Xu",
            "Xiang Zhang",
            "Ethan Armand",
            "Divyansh Srivastava",
            "Xiaojun Shan",
            "Zeyuan Chen",
            "Jianwen Xie",
            "Zhuowen Tu"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper introduces OverLayBench, a benchmark for layout-to-image generation that addresses challenges with overlapping bounding boxes. It also presents a new metric, OverLayScore, to quantify the complexity of overlap and proposes a model, CreatiLayout-AM, to improve performance on complex overlaps.",
        "tldr_zh": "本文介绍了OverLayBench，一种针对布局生成图像的基准，解决了重叠边界框的挑战。它还提出了一种新的指标，OverLayScore，来量化重叠的复杂性，并提出了一个模型，CreatiLayout-AM，以改善复杂重叠的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Moving by Looking: Towards Vision-Driven Avatar Motion Generation",
        "summary": "The way we perceive the world fundamentally shapes how we move, whether it is\nhow we navigate in a room or how we interact with other humans. Current human\nmotion generation methods, neglect this interdependency and use task-specific\n``perception'' that differs radically from that of humans. We argue that the\ngeneration of human-like avatar behavior requires human-like perception.\nConsequently, in this work we present CLOPS, the first human avatar that solely\nuses egocentric vision to perceive its surroundings and navigate. Using vision\nas the primary driver of motion however, gives rise to a significant challenge\nfor training avatars: existing datasets have either isolated human motion,\nwithout the context of a scene, or lack scale. We overcome this challenge by\ndecoupling the learning of low-level motion skills from learning of high-level\ncontrol that maps visual input to motion. First, we train a motion prior model\non a large motion capture dataset. Then, a policy is trained using Q-learning\nto map egocentric visual inputs to high-level control commands for the motion\nprior. Our experiments empirically demonstrate that egocentric vision can give\nrise to human-like motion characteristics in our avatars. For example, the\navatars walk such that they avoid obstacles present in their visual field.\nThese findings suggest that equipping avatars with human-like sensors,\nparticularly egocentric vision, holds promise for training avatars that behave\nlike humans.",
        "url": "http://arxiv.org/abs/2509.19259v1",
        "published_date": "2025-09-23T17:18:56+00:00",
        "updated_date": "2025-09-23T17:18:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Markos Diomataris",
            "Berat Mert Albaba",
            "Giorgio Becherini",
            "Partha Ghosh",
            "Omid Taheri",
            "Michael J. Black"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper presents CLOPS, the first human avatar that uses egocentric vision to perceive its surroundings and navigate, demonstrating human-like motion characteristics.",
        "tldr_zh": "本文介绍了CLOPS，这是第一个使用自我的视觉来感知环境和导航的人类化身，展示了类似人类的运动特征。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Graph-Radiomic Learning (GrRAiL) Descriptor to Characterize Imaging Heterogeneity in Confounding Tumor Pathologies",
        "summary": "A significant challenge in solid tumors is reliably distinguishing\nconfounding pathologies from malignant neoplasms on routine imaging. While\nradiomics methods seek surrogate markers of lesion heterogeneity on CT/MRI,\nmany aggregate features across the region of interest (ROI) and miss complex\nspatial relationships among varying intensity compositions. We present a new\nGraph-Radiomic Learning (GrRAiL) descriptor for characterizing intralesional\nheterogeneity (ILH) on clinical MRI scans. GrRAiL (1) identifies clusters of\nsub-regions using per-voxel radiomic measurements, then (2) computes\ngraph-theoretic metrics to quantify spatial associations among clusters. The\nresulting weighted graphs encode higher-order spatial relationships within the\nROI, aiming to reliably capture ILH and disambiguate confounding pathologies\nfrom malignancy. To assess efficacy and clinical feasibility, GrRAiL was\nevaluated in n=947 subjects spanning three use cases: differentiating tumor\nrecurrence from radiation effects in glioblastoma (GBM; n=106) and brain\nmetastasis (n=233), and stratifying pancreatic intraductal papillary mucinous\nneoplasms (IPMNs) into no+low vs high risk (n=608). In a multi-institutional\nsetting, GrRAiL consistently outperformed state-of-the-art baselines - Graph\nNeural Networks (GNNs), textural radiomics, and intensity-graph analysis. In\nGBM, cross-validation (CV) and test accuracies for recurrence vs\npseudo-progression were 89% and 78% with >10% test-accuracy gains over\ncomparators. In brain metastasis, CV and test accuracies for recurrence vs\nradiation necrosis were 84% and 74% (>13% improvement). For IPMN risk\nstratification, CV and test accuracies were 84% and 75%, showing >10%\nimprovement.",
        "url": "http://arxiv.org/abs/2509.19258v1",
        "published_date": "2025-09-23T17:18:33+00:00",
        "updated_date": "2025-09-23T17:18:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dheerendranath Battalapalli",
            "Apoorva Safai",
            "Maria Jaramillo",
            "Hyemin Um",
            "Gustavo Adalfo Pineda Ortiz",
            "Ulas Bagci",
            "Manmeet Singh Ahluwalia",
            "Marwa Ismail",
            "Pallavi Tiwari"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a Graph-Radiomic Learning (GrRAiL) method to characterize imaging heterogeneity and distinguish confounding pathologies from malignancy in tumor imaging.",
        "tldr_zh": "本文提出了一种图放射组学学习（GrRAiL）方法，用于表征肿瘤影像中的异质性，并区分混淆的病理诊断。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ConViS-Bench: Estimating Video Similarity Through Semantic Concepts",
        "summary": "What does it mean for two videos to be similar? Videos may appear similar\nwhen judged by the actions they depict, yet entirely different if evaluated\nbased on the locations where they were filmed. While humans naturally compare\nvideos by taking different aspects into account, this ability has not been\nthoroughly studied and presents a challenge for models that often depend on\nbroad global similarity scores. Large Multimodal Models (LMMs) with video\nunderstanding capabilities open new opportunities for leveraging natural\nlanguage in comparative video tasks. We introduce Concept-based Video\nSimilarity estimation (ConViS), a novel task that compares pairs of videos by\ncomputing interpretable similarity scores across a predefined set of key\nsemantic concepts. ConViS allows for human-like reasoning about video\nsimilarity and enables new applications such as concept-conditioned video\nretrieval. To support this task, we also introduce ConViS-Bench, a new\nbenchmark comprising carefully annotated video pairs spanning multiple domains.\nEach pair comes with concept-level similarity scores and textual descriptions\nof both differences and similarities. Additionally, we benchmark several\nstate-of-the-art models on ConViS, providing insights into their alignment with\nhuman judgments. Our results reveal significant performance differences on\nConViS, indicating that some concepts present greater challenges for estimating\nvideo similarity. We believe that ConViS-Bench will serve as a valuable\nresource for advancing research in language-driven video understanding.",
        "url": "http://arxiv.org/abs/2509.19245v1",
        "published_date": "2025-09-23T17:06:11+00:00",
        "updated_date": "2025-09-23T17:06:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Benedetta Liberatori",
            "Alessandro Conti",
            "Lorenzo Vaquero",
            "Yiming Wang",
            "Elisa Ricci",
            "Paolo Rota"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "Introducing ConViS-Bench, a benchmark for estimating video similarity using semantic concepts. This approach allows for human-like reasoning about video similarity and enables new applications.",
        "tldr_zh": "引入 ConViS-Bench，一个使用语义概念来估计视频相似性的基准。这种方法允许人类般地推理视频相似性，并开启新的应用。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal Understanding and Generation",
        "summary": "We proposed Lavida-O, a unified multi-modal Masked Diffusion Model (MDM)\ncapable of image understanding and generation tasks. Unlike existing multimodal\ndiffsion language models such as MMaDa and Muddit which only support simple\nimage-level understanding tasks and low-resolution image generation, Lavida-O\nexhibits many new capabilities such as object grounding, image-editing, and\nhigh-resolution (1024px) image synthesis. It is also the first unified MDM that\nuses its understanding capabilities to improve image generation and editing\nresults through planning and iterative self-reflection. To allow effective and\nefficient training and sampling, Lavida-O ntroduces many novel techniques such\nas Elastic Mixture-of-Transformer architecture, universal text conditioning,\nand stratified sampling. \\ours~achieves state-of-the-art performance on a wide\nrange of benchmarks such as RefCOCO object grounding, GenEval text-to-image\ngeneration, and ImgEdit image editing, outperforming existing autoregressive\nand continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while\noffering considerable speedup at inference.",
        "url": "http://arxiv.org/abs/2509.19244v1",
        "published_date": "2025-09-23T17:05:46+00:00",
        "updated_date": "2025-09-23T17:05:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shufan Li",
            "Jiuxiang Gu",
            "Kangning Liu",
            "Zhe Lin",
            "Zijun Wei",
            "Aditya Grover",
            "Jason Kuen"
        ],
        "ai_categories": [
            "Multimodality",
            "Diffusion",
            "Transformer",
            "GAN"
        ],
        "tldr": "Lavida-O is a unified multi-modal Masked Diffusion Model that excels in image understanding and generation tasks with capabilities like object grounding and high-resolution image synthesis.",
        "tldr_zh": "Lavida-O是一个统一的多模态遮罩扩散模型，擅长图像理解和生成任务，具有物体定位和高分辨率图像合成等功能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "DevFD: Developmental Face Forgery Detection by Learning Shared and Orthogonal LoRA Subspaces",
        "summary": "The rise of realistic digital face generation and manipulation poses\nsignificant social risks. The primary challenge lies in the rapid and diverse\nevolution of generation techniques, which often outstrip the detection\ncapabilities of existing models. To defend against the ever-evolving new types\nof forgery, we need to enable our model to quickly adapt to new domains with\nlimited computation and data while avoiding forgetting previously learned\nforgery types. In this work, we posit that genuine facial samples are abundant\nand relatively stable in acquisition methods, while forgery faces continuously\nevolve with the iteration of manipulation techniques. Given the practical\ninfeasibility of exhaustively collecting all forgery variants, we frame face\nforgery detection as a continual learning problem and allow the model to\ndevelop as new forgery types emerge. Specifically, we employ a Developmental\nMixture of Experts (MoE) architecture that uses LoRA models as its individual\nexperts. These experts are organized into two groups: a Real-LoRA to learn and\nrefine knowledge of real faces, and multiple Fake-LoRAs to capture incremental\ninformation from different forgery types. To prevent catastrophic forgetting,\nwe ensure that the learning direction of Fake-LoRAs is orthogonal to the\nestablished subspace. Moreover, we integrate orthogonal gradients into the\northogonal loss of Fake-LoRAs, preventing gradient interference throughout the\ntraining process of each task. Experimental results under both the datasets and\nmanipulation types incremental protocols demonstrate the effectiveness of our\nmethod.",
        "url": "http://arxiv.org/abs/2509.19230v1",
        "published_date": "2025-09-23T16:52:27+00:00",
        "updated_date": "2025-09-23T16:52:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianshuo Zhang",
            "Li Gao",
            "Siran Peng",
            "Xiangyu Zhu",
            "Zhen Lei"
        ],
        "ai_categories": [
            "LoRA",
            "Diffusion",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a developmental approach for detecting face forgery using a mixture of experts architecture with LoRA models, allowing the model to adapt to new forgery types as they emerge.",
        "tldr_zh": "本文介绍了一种利用LoRA模型的专家混合体系结构来检测面部伪造的发展性方法，使模型能够随着新的伪造类型的出现而适应。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HyKid: An Open MRI Dataset with Expert-Annotated Multi-Structure and Choroid Plexus in Pediatric Hydrocephalus",
        "summary": "Evaluation of hydrocephalus in children is challenging, and the related\nresearch is limited by a lack of publicly available, expert-annotated datasets,\nparticularly those with segmentation of the choroid plexus. To address this, we\npresent HyKid, an open-source dataset from 48 pediatric patients with\nhydrocephalus. 3D MRIs were provided with 1mm isotropic resolution, which was\nreconstructed from routine low-resolution images using a slice-to-volume\nalgorithm. Manually corrected segmentations of brain tissues, including white\nmatter, grey matter, lateral ventricle, external CSF, and the choroid plexus,\nwere provided by an experienced neurologist. Additionally, structured data was\nextracted from clinical radiology reports using a Retrieval-Augmented\nGeneration framework. The strong correlation between choroid plexus volume and\ntotal CSF volume provided a potential biomarker for hydrocephalus evaluation,\nachieving excellent performance in a predictive model (AUC = 0.87). The\nproposed HyKid dataset provided a high-quality benchmark for neuroimaging\nalgorithms development, and it revealed the choroid plexus-related features in\nhydrocephalus assessments. Our datasets are publicly available at\nhttps://www.synapse.org/Synapse:syn68544889.",
        "url": "http://arxiv.org/abs/2509.19218v1",
        "published_date": "2025-09-23T16:42:16+00:00",
        "updated_date": "2025-09-23T16:42:16+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yunzhi Xu",
            "Yushuang Ding",
            "Hu Sun",
            "Hongxi Zhang",
            "Li Zhao"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper introduces HyKid, an open MRI dataset for pediatric hydrocephalus with expert-annotated segmentation of brain tissues and choroid plexus, showing potential biomarkers for hydrocephalus evaluation.",
        "tldr_zh": "该论文介绍了HyKid，这是一个专门针对小儿脑积水的开放MRI数据集，包含专家注释的脑组织和脉络丛分割，展示了脑积水评估的潜在生物标志物。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enabling Plant Phenotyping in Weedy Environments using Multi-Modal Imagery via Synthetic and Generated Training Data",
        "summary": "Accurate plant segmentation in thermal imagery remains a significant\nchallenge for high throughput field phenotyping, particularly in outdoor\nenvironments where low contrast between plants and weeds and frequent\nocclusions hinder performance. To address this, we present a framework that\nleverages synthetic RGB imagery, a limited set of real annotations, and\nGAN-based cross-modality alignment to enhance semantic segmentation in thermal\nimages. We trained models on 1,128 synthetic images containing complex mixtures\nof crop and weed plants in order to generate image segmentation masks for crop\nand weed plants. We additionally evaluated the benefit of integrating as few as\nfive real, manually segmented field images within the training process using\nvarious sampling strategies. When combining all the synthetic images with a few\nlabeled real images, we observed a maximum relative improvement of 22% for the\nweed class and 17% for the plant class compared to the full real-data baseline.\nCross-modal alignment was enabled by translating RGB to thermal using\nCycleGAN-turbo, allowing robust template matching without calibration. Results\ndemonstrated that combining synthetic data with limited manual annotations and\ncross-domain translation via generative models can significantly boost\nsegmentation performance in complex field environments for multi-model imagery.",
        "url": "http://arxiv.org/abs/2509.19208v1",
        "published_date": "2025-09-23T16:29:13+00:00",
        "updated_date": "2025-09-23T16:29:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Earl Ranario",
            "Ismael Mayanja",
            "Heesup Yun",
            "Brian N. Bailey",
            "J. Mason Earles"
        ],
        "ai_categories": [
            "GAN",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper presents a framework that combines synthetic RGB imagery and real annotations with GAN-based cross-modality alignment to improve plant segmentation in thermal images in weedy environments.",
        "tldr_zh": "该论文提出了一个框架，结合了合成的RGB图像和真实的注释，通过基于GAN的跨模态对齐来改善杂草环境中热像图中的植物分割。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Long Story Short: Disentangling Compositionality and Long-Caption Understanding in VLMs",
        "summary": "Contrastive vision-language models (VLMs) have made significant progress in\nbinding visual and textual information, but understanding long, dense captions\nremains an open challenge. We hypothesize that compositionality, the capacity\nto reason about object-attribute bindings and inter-object relationships, is\nkey to understanding longer captions. In this paper, we investigate the\ninteraction between compositionality and long-caption understanding, asking\nwhether training for one property enhances the other. We train and evaluate a\nrange of models that target each of these capabilities. Our results reveal a\nbidirectional relationship: compositional training improves performance on\nlong-caption retrieval, and training on long captions promotes\ncompositionality. However, these gains are sensitive to data quality and model\ndesign. We find that training on poorly structured captions, or with limited\nparameter updates, fails to support generalization. Likewise, strategies that\naim at retaining general alignment, such as freezing positional embeddings, do\nnot improve compositional understanding. Overall, we find that compositional\nunderstanding and long-caption understanding are intertwined capabilities that\ncan be jointly learned through training on dense, grounded descriptions.\nDespite these challenges, we show that models trained on high-quality,\nlong-caption data can achieve strong performance in both tasks, offering\npractical guidance for improving VLM generalization.",
        "url": "http://arxiv.org/abs/2509.19207v1",
        "published_date": "2025-09-23T16:28:51+00:00",
        "updated_date": "2025-09-23T16:28:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Israfel Salazar",
            "Desmond Elliott",
            "Yova Kementchedjhieva"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality"
        ],
        "tldr": "The paper explores the interaction between compositionality and long-caption understanding in vision-language models, showing a bidirectional relationship that can be jointly learned through training on dense descriptions.",
        "tldr_zh": "本文探讨了视觉语言模型中组合性和长说明理解之间的互动关系，显示出可以通过在密集描述上进行训练来共同学习的双向关系。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions",
        "summary": "Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have\nbecome the standard approach for learning discriminative vision-language\nrepresentations. However, these models often exhibit shallow language\nunderstanding, manifesting bag-of-words behaviour. These limitations are\nreinforced by their dual-encoder design, which induces a modality gap.\nAdditionally, the reliance on vast web-collected data corpora for training\nmakes the process computationally expensive and introduces significant privacy\nconcerns. To address these limitations, in this work, we challenge the\nnecessity of vision encoders for retrieval tasks by introducing a vision-free,\nsingle-encoder retrieval pipeline. Departing from the traditional text-to-image\nretrieval paradigm, we migrate to a text-to-text paradigm with the assistance\nof VLLM-generated structured image descriptions. We demonstrate that this\nparadigm shift has significant advantages, including a substantial reduction of\nthe modality gap, improved compositionality, and better performance on short\nand long caption queries, all attainable with only a few hours of calibration\non two GPUs. Additionally, substituting raw images with textual descriptions\nintroduces a more privacy-friendly alternative for retrieval. To further assess\ngeneralisation and address some of the shortcomings of prior compositionality\nbenchmarks, we release two benchmarks derived from Flickr30k and COCO,\ncontaining diverse compositional queries made of short captions, which we coin\nsubFlickr and subCOCO. Our vision-free retriever matches and often surpasses\ntraditional multimodal models. Importantly, our approach achieves\nstate-of-the-art zero-shot performance on multiple retrieval and\ncompositionality benchmarks, with models as small as 0.3B parameters. Code is\navailable at: https://github.com/IoannaNti/LexiCLIP",
        "url": "http://arxiv.org/abs/2509.19203v1",
        "published_date": "2025-09-23T16:22:27+00:00",
        "updated_date": "2025-09-23T16:22:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ioanna Ntinou",
            "Alexandros Xenos",
            "Yassine Ouali",
            "Adrian Bulat",
            "Georgios Tzimiropoulos"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a vision-free retrieval approach for multimodal search using textual scene descriptions, demonstrating significant advantages over traditional models.",
        "tldr_zh": "本文介绍了一种利用文本场景描述进行视觉无关检索的方法，在传统模型之上展现出明显优势。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models",
        "summary": "Vision-Language Models (VLMs) have demonstrated remarkable performance across\na variety of real-world tasks. However, existing VLMs typically process visual\ninformation by serializing images, a method that diverges significantly from\nthe parallel nature of human vision. Moreover, their opaque internal mechanisms\nhinder both deeper understanding and architectural innovation. Inspired by the\ndual-stream hypothesis of human vision, which distinguishes the \"what\" and\n\"where\" pathways, we deconstruct the visual processing in VLMs into object\nrecognition and spatial perception for separate study. For object recognition,\nwe convert images into text token maps and find that the model's perception of\nimage content unfolds as a two-stage process from shallow to deep layers,\nbeginning with attribute recognition and culminating in semantic\ndisambiguation. For spatial perception, we theoretically derive and empirically\nverify the geometric structure underlying the positional representation in\nVLMs. Based on these findings, we introduce an instruction-agnostic token\ncompression algorithm based on a plug-and-play visual decoder to improve\ndecoding efficiency, and a RoPE scaling technique to enhance spatial reasoning.\nThrough rigorous experiments, our work validates these analyses, offering a\ndeeper understanding of VLM internals and providing clear principles for\ndesigning more capable future architectures.",
        "url": "http://arxiv.org/abs/2509.19191v1",
        "published_date": "2025-09-23T16:07:18+00:00",
        "updated_date": "2025-09-23T16:07:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yueyan Li",
            "Chenggong Zhao",
            "Zeyuan Zang",
            "Caixia Yuan",
            "Xiaojie Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality"
        ],
        "tldr": "The paper breaks down how Vision-Language Models process visual information into object recognition and spatial perception, introducing techniques to improve decoding efficiency and spatial reasoning.",
        "tldr_zh": "该论文将视觉语言模型处理视觉信息的过程分解为物体识别和空间感知，介绍了提高解码效率和空间推理的技术。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "The 1st Solution for MOSEv2 Challenge 2025: Long-term and Concept-aware Video Segmentation via SeC",
        "summary": "This technical report explores the MOSEv2 track of the LSVOS Challenge, which\ntargets complex semi-supervised video object segmentation. By analysing and\nadapting SeC, an enhanced SAM-2 framework, we conduct a detailed study of its\nlong-term memory and concept-aware memory, showing that long-term memory\npreserves temporal continuity under occlusion and reappearance, while\nconcept-aware memory supplies semantic priors that suppress distractors;\ntogether, these traits directly benefit several MOSEv2's core challenges. Our\nsolution achieves a JF score of 39.89% on the test set, ranking 1st in the\nMOSEv2 track of the LSVOS Challenge.",
        "url": "http://arxiv.org/abs/2509.19183v1",
        "published_date": "2025-09-23T15:58:13+00:00",
        "updated_date": "2025-09-23T15:58:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mingqi Gao",
            "Jingkun Chen",
            "Yunqi Miao",
            "Gengshen Wu",
            "Zhijin Qin",
            "Jungong Han"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a solution for the MOSEv2 Challenge 2025, focusing on video segmentation using a framework called SeC, achieving top rank in the challenge.",
        "tldr_zh": "本文介绍了MOSEv2挑战2025的解决方案，重点关注使用SeC框架进行视频分割，在挑战中取得了前列成绩。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions",
        "summary": "Recent self-supervised stereo matching methods have made significant\nprogress, but their performance significantly degrades under adverse weather\nconditions such as night, rain, and fog. We identify two primary weaknesses\ncontributing to this performance degradation. First, adverse weather introduces\nnoise and reduces visibility, making CNN-based feature extractors struggle with\ndegraded regions like reflective and textureless areas. Second, these degraded\nregions can disrupt accurate pixel correspondences, leading to ineffective\nsupervision based on the photometric consistency assumption. To address these\nchallenges, we propose injecting robust priors derived from the visual\nfoundation model into the CNN-based feature extractor to improve feature\nrepresentation under adverse weather conditions. We then introduce scene\ncorrespondence priors to construct robust supervisory signals rather than\nrelying solely on the photometric consistency assumption. Specifically, we\ncreate synthetic stereo datasets with realistic weather degradations. These\ndatasets feature clear and adverse image pairs that maintain the same semantic\ncontext and disparity, preserving the scene correspondence property. With this\nknowledge, we propose a robust self-supervised training paradigm, consisting of\ntwo key steps: robust self-supervised scene correspondence learning and adverse\nweather distillation. Both steps aim to align underlying scene results from\nclean and adverse image pairs, thus improving model disparity estimation under\nadverse weather effects. Extensive experiments demonstrate the effectiveness\nand versatility of our proposed solution, which outperforms existing\nstate-of-the-art self-supervised methods. Codes are available at\n\\textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}.",
        "url": "http://arxiv.org/abs/2509.19165v1",
        "published_date": "2025-09-23T15:41:40+00:00",
        "updated_date": "2025-09-23T15:41:40+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yun Wang",
            "Junjie Hu",
            "Junhui Hou",
            "Chenghao Zhang",
            "Renwei Yang",
            "Dapeng Oliver Wu"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper addresses the challenge of self-supervised stereo matching under adverse weather conditions by proposing robust priors and scene correspondence learning.",
        "tldr_zh": "本文通过提出强健的先验和场景对应学习来解决自监督立体匹配在恶劣天气条件下的挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "NeuCODEX: Edge-Cloud Co-Inference with Spike-Driven Compression and Dynamic Early-Exit",
        "summary": "Spiking Neural Networks (SNNs) offer significant potential for enabling\nenergy-efficient intelligence at the edge. However, performing full SNN\ninference at the edge can be challenging due to the latency and energy\nconstraints arising from fixed and high timestep overheads. Edge-cloud\nco-inference systems present a promising solution, but their deployment is\noften hindered by high latency and feature transmission costs. To address these\nissues, we introduce NeuCODEX, a neuromorphic co-inference architecture that\njointly optimizes both spatial and temporal redundancy. NeuCODEX incorporates a\nlearned spike-driven compression module to reduce data transmission and employs\na dynamic early-exit mechanism to adaptively terminate inference based on\noutput confidence. We evaluated NeuCODEX on both static images (CIFAR10 and\nCaltech) and neuromorphic event streams (CIFAR10-DVS and N-Caltech). To\ndemonstrate practicality, we prototyped NeuCODEX on ResNet-18 and VGG-16\nbackbones in a real edge-to-cloud testbed. Our proposed system reduces data\ntransfer by up to 2048x and edge energy consumption by over 90%, while reducing\nend-to-end latency by up to 3x compared to edge-only inference, all with a\nnegligible accuracy drop of less than 2%. In doing so, NeuCODEX enables\npractical, high-performance SNN deployment in resource-constrained\nenvironments.",
        "url": "http://arxiv.org/abs/2509.19156v1",
        "published_date": "2025-09-23T15:34:33+00:00",
        "updated_date": "2025-09-23T15:34:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Maurf Hassan",
            "Steven Davy",
            "Muhammad Zawish",
            "Owais Bin Zuber",
            "Nouman Ashraf"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset",
            "Transformer"
        ],
        "tldr": "NeuCODEX is a neuromorphic co-inference architecture that optimizes spatial and temporal redundancy for energy-efficient intelligence at the edge, reducing data transfer, energy consumption, and latency while maintaining high accuracy.",
        "tldr_zh": "NeuCODEX是一种神经形态协同推理架构，通过优化空间和时间冗余实现边缘能效智能，减少数据传输、能耗和延迟，同时保持高准确性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Investigating Traffic Accident Detection Using Multimodal Large Language Models",
        "summary": "Traffic safety remains a critical global concern, with timely and accurate\naccident detection essential for hazard reduction and rapid emergency response.\nInfrastructure-based vision sensors offer scalable and efficient solutions for\ncontinuous real-time monitoring, facilitating automated detection of acci-\ndents directly from captured images. This research investigates the zero-shot\ncapabilities of multimodal large language models (MLLMs) for detecting and\ndescribing traffic accidents using images from infrastructure cameras, thus\nminimizing reliance on extensive labeled datasets. Main contributions include:\n(1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA,\nexplicitly addressing the scarcity of diverse, realistic, infrastructure-based\naccident data through controlled simulations; (2) Comparative performance\nanalysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in acci- dent\nidentification and descriptive capabilities without prior fine-tuning; and (3)\nIntegration of advanced visual analytics, specifically YOLO for object\ndetection, Deep SORT for multi- object tracking, and Segment Anything (SAM) for\ninstance segmentation, into enhanced prompts to improve model accuracy and\nexplainability. Key numerical results show Pixtral as the top performer with an\nF1-score of 0.71 and 83% recall, while Gemini models gained precision with\nenhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and\nrecall losses. Gemma 3 offered the most balanced performance with minimal\nmetric fluctuation. These findings demonstrate the substantial potential of\nintegrating MLLMs with advanced visual analytics techniques, enhancing their\napplicability in real-world automated traffic monitoring systems.",
        "url": "http://arxiv.org/abs/2509.19096v1",
        "published_date": "2025-09-23T14:47:33+00:00",
        "updated_date": "2025-09-23T14:47:33+00:00",
        "categories": [
            "cs.CV",
            "cs.SE"
        ],
        "authors": [
            "Ilhan Skender",
            "Kailin Tong",
            "Selim Solmaz",
            "Daniel Watzenig"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper explores using multimodal large language models for traffic accident detection using images from infrastructure cameras, showing promising results in reducing reliance on labeled datasets.",
        "tldr_zh": "本文探讨了使用多模态大型语言模型来进行交通事故检测，使用基础设施摄像头捕获的图像，显示出在减少对标记数据集依赖方面有希望的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Citrus-V: Advancing Medical Foundation Models with Unified Medical Image Grounding for Clinical Reasoning",
        "summary": "Medical imaging provides critical evidence for clinical diagnosis, treatment\nplanning, and surgical decisions, yet most existing imaging models are narrowly\nfocused and require multiple specialized networks, limiting their\ngeneralization. Although large-scale language and multimodal models exhibit\nstrong reasoning and multi-task capabilities, real-world clinical applications\ndemand precise visual grounding, multimodal integration, and chain-of-thought\nreasoning. We introduce Citrus-V, a multimodal medical foundation model that\ncombines image analysis with textual reasoning. The model integrates detection,\nsegmentation, and multimodal chain-of-thought reasoning, enabling pixel-level\nlesion localization, structured report generation, and physician-like\ndiagnostic inference in a single framework. We propose a novel multimodal\ntraining approach and release a curated open-source data suite covering\nreasoning, detection, segmentation, and document understanding tasks.\nEvaluations demonstrate that Citrus-V outperforms existing open-source medical\nmodels and expert-level imaging systems across multiple benchmarks, delivering\na unified pipeline from visual grounding to clinical reasoning and supporting\nprecise lesion quantification, automated reporting, and reliable second\nopinions.",
        "url": "http://arxiv.org/abs/2509.19090v1",
        "published_date": "2025-09-23T14:42:31+00:00",
        "updated_date": "2025-09-23T14:42:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Guoxin Wang",
            "Jun Zhao",
            "Xinyi Liu",
            "Yanbo Liu",
            "Xuyang Cao",
            "Chao Li",
            "Zhuoyun Liu",
            "Qintian Sun",
            "Fangru Zhou",
            "Haoqiang Xing",
            "Zhenhong Yang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces Citrus-V, a multimodal medical model that combines image analysis with textual reasoning for clinical diagnosis. It outperforms existing models in lesion localization, report generation, and diagnostic inference.",
        "tldr_zh": "本文介绍了 Citrus-V，这是一个将图像分析与文本推理相结合的多模态医学模型，用于临床诊断。在病变定位、报告生成和诊断推理方面优于现有模型。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications",
        "summary": "Multi-spectral imagery plays a crucial role in diverse Remote Sensing\napplications including land-use classification, environmental monitoring and\nurban planning. These images are widely adopted because their additional\nspectral bands correlate strongly with physical materials on the ground, such\nas ice, water, and vegetation. This allows for more accurate identification,\nand their public availability from missions, such as Sentinel-2 and Landsat,\nonly adds to their value. Currently, the automatic analysis of such data is\npredominantly managed through machine learning models specifically trained for\nmulti-spectral input, which are costly to train and support. Furthermore,\nalthough providing a lot of utility for Remote Sensing, such additional inputs\ncannot be used with powerful generalist large multimodal models, which are\ncapable of solving many visual problems, but are not able to understand\nspecialized multi-spectral signals.\n  To address this, we propose a training-free approach which introduces new\nmulti-spectral data in a Zero-Shot-only mode, as inputs to generalist\nmultimodal models, trained on RGB-only inputs. Our approach leverages the\nmultimodal models' understanding of the visual space, and proposes to adapt to\ninputs to that space, and to inject domain-specific information as instructions\ninto the model. We exemplify this idea with the Gemini2.5 model and observe\nstrong Zero-Shot performance gains of the approach on popular Remote Sensing\nbenchmarks for land cover and land use classification and demonstrate the easy\nadaptability of Gemini2.5 to new inputs. These results highlight the potential\nfor geospatial professionals, working with non-standard specialized inputs, to\neasily leverage powerful multimodal models, such as Gemini2.5, to accelerate\ntheir work, benefiting from their rich reasoning and contextual capabilities,\ngrounded in the specialized sensor data.",
        "url": "http://arxiv.org/abs/2509.19087v1",
        "published_date": "2025-09-23T14:40:52+00:00",
        "updated_date": "2025-09-23T14:40:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ganesh Mallya",
            "Yotam Gigi",
            "Dahun Kim",
            "Maxim Neumann",
            "Genady Beryozkin",
            "Tomer Shekel",
            "Anelia Angelova"
        ],
        "ai_categories": [
            "Multimodality",
            "Other"
        ],
        "tldr": "The paper proposes a training-free approach to incorporate multi-spectral data into generalist multimodal models for remote sensing applications, showing strong performance gains.",
        "tldr_zh": "本文提出了一种无需训练的方法，将多光谱数据整合到通用多模态模型中，用于遥感应用，并展示出较强的性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "3rd Place Report of LSVOS 2025 MeViS Track: Sa2VA-i: Improving Sa2VA Results with Consistent Training and Inference",
        "summary": "Sa2VA is a recent model for language-guided dense grounding in images and\nvideo that achieves state-of-the-art results on multiple segmentation\nbenchmarks and that has become widely popular. However, we found that Sa2VA\ndoes not perform according to its full potential for referring video object\nsegmentation tasks. We identify inconsistencies between training and inference\nprocedures as the key factor holding it back. To mitigate this issue, we\npropose an improved version of Sa2VA, Sa2VA-i, that rectifies these issues and\nimproves the results. In fact, Sa2VA-i sets a new state of the art for multiple\nvideo benchmarks and achieves improvements of up to +11.6 J&F on MeViS, +1.4 on\nRef-YT-VOS, +3.3 on Ref-DAVIS and +4.1 on ReVOS using the same Sa2VA\ncheckpoints. With our fixes, the Sa2VA-i-1B model even performs on par with the\noriginal Sa2VA-26B model on the MeViS benchmark. We hope that this work will\nshow the importance of seemingly trivial implementation details and that it\nwill provide valuable insights for the referring video segmentation field. We\nprovide the code and updated models at https://github.com/kumuji/sa2va-i",
        "url": "http://arxiv.org/abs/2509.19082v1",
        "published_date": "2025-09-23T14:38:25+00:00",
        "updated_date": "2025-09-23T14:38:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Alexey Nekrasov",
            "Ali Athar",
            "Daan de Geus",
            "Alexander Hermans",
            "Bastian Leibe"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "Sa2VA-i is an improved version of a model for language-guided dense grounding in images and video, setting new state-of-the-art results on video benchmarks by addressing inconsistencies in training and inference procedures.",
        "tldr_zh": "Sa2VA-i是一个改进版本的模型，用于语言引导的图像和视频密集定位，在视频基准测试中取得了新的最先进结果，通过解决训练和推断程序中的不一致性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction",
        "summary": "3D Gaussian Splatting (3DGS) has become a powerful representation for\nimage-based object reconstruction, yet its performance drops sharply in\nsparse-view settings. Prior works address this limitation by employing\ndiffusion models to repair corrupted renders, subsequently using them as pseudo\nground truths for later optimization. While effective, such approaches incur\nheavy computation from the diffusion fine-tuning and repair steps. We present\nWaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object\nreconstruction. Our key idea is to shift diffusion into the wavelet domain:\ndiffusion is applied only to the low-resolution LL subband, while\nhigh-frequency subbands are refined with a lightweight network. We further\npropose an efficient online random masking strategy to curate training pairs\nfor diffusion fine-tuning, replacing the commonly used, but inefficient,\nleave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360\nand OmniObject3D, show WaveletGaussian achieves competitive rendering quality\nwhile substantially reducing training time.",
        "url": "http://arxiv.org/abs/2509.19073v1",
        "published_date": "2025-09-23T14:34:10+00:00",
        "updated_date": "2025-09-23T14:34:10+00:00",
        "categories": [
            "cs.CV",
            "eess.IV",
            "eess.SP"
        ],
        "authors": [
            "Hung Nguyen",
            "Runfa Li",
            "An Le",
            "Truong Nguyen"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "WaveletGaussian presents a more efficient approach for 3D Gaussian object reconstruction in sparse-view settings by shifting diffusion into the wavelet domain and employing an online random masking strategy for training pairs.",
        "tldr_zh": "WaveletGaussian通过将扩散应用于小波域，并利用在线随机遮罩策略来生成训练对，提出了一种更高效的在稀疏视图设置中进行3D高斯物体重建的方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A DyL-Unet framework based on dynamic learning for Temporally Consistent Echocardiographic Segmentation",
        "summary": "Accurate segmentation of cardiac anatomy in echocardiography is essential for\ncardiovascular diagnosis and treatment. Yet echocardiography is prone to\ndeformation and speckle noise, causing frame-to-frame segmentation jitter. Even\nwith high accuracy in single-frame segmentation, temporal instability can\nweaken functional estimates and impair clinical interpretability. To address\nthese issues, we propose DyL-UNet, a dynamic learning-based temporal\nconsistency U-Net segmentation architecture designed to achieve temporally\nstable and precise echocardiographic segmentation. The framework constructs an\nEcho-Dynamics Graph (EDG) through dynamic learning to extract dynamic\ninformation from videos. DyL-UNet incorporates multiple Swin-Transformer-based\nencoder-decoder branches for processing single-frame images. It further\nintroduces Cardiac Phase-Dynamics Attention (CPDA) at the skip connections,\nwhich uses EDG-encoded dynamic features and cardiac-phase cues to enforce\ntemporal consistency during segmentation. Extensive experiments on the CAMUS\nand EchoNet-Dynamic datasets demonstrate that DyL-UNet maintains segmentation\naccuracy comparable to existing methods while achieving superior temporal\nconsistency, providing a reliable solution for automated clinical\nechocardiography.",
        "url": "http://arxiv.org/abs/2509.19052v1",
        "published_date": "2025-09-23T14:17:01+00:00",
        "updated_date": "2025-09-23T14:17:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jierui Qu",
            "Jianchun Zhao"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces DyL-UNet, a dynamic learning-based temporal consistency U-Net segmentation architecture for echocardiographic segmentation, achieving superior temporal consistency and maintaining segmentation accuracy.",
        "tldr_zh": "该论文提出了DyL-UNet，一种基于动态学习的时间一致性U-Net分割架构，用于心射超声分割，实现了卓越的时间一致性和保持分割准确性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Latent Danger Zone: Distilling Unified Attention for Cross-Architecture Black-box Attacks",
        "summary": "Black-box adversarial attacks remain challenging due to limited access to\nmodel internals. Existing methods often depend on specific network\narchitectures or require numerous queries, resulting in limited\ncross-architecture transferability and high query costs. To address these\nlimitations, we propose JAD, a latent diffusion model framework for black-box\nadversarial attacks. JAD generates adversarial examples by leveraging a latent\ndiffusion model guided by attention maps distilled from both a convolutional\nneural network (CNN) and a Vision Transformer (ViT) models. By focusing on\nimage regions that are commonly sensitive across architectures, this approach\ncrafts adversarial perturbations that transfer effectively between different\nmodel types. This joint attention distillation strategy enables JAD to be\narchitecture-agnostic, achieving superior attack generalization across diverse\nmodels. Moreover, the generative nature of the diffusion framework yields high\nadversarial sample generation efficiency by reducing reliance on iterative\nqueries. Experiments demonstrate that JAD offers improved attack\ngeneralization, generation efficiency, and cross-architecture transferability\ncompared to existing methods, providing a promising and effective paradigm for\nblack-box adversarial attacks.",
        "url": "http://arxiv.org/abs/2509.19044v1",
        "published_date": "2025-09-23T14:12:41+00:00",
        "updated_date": "2025-09-23T14:12:41+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Yang Li",
            "Chenyu Wang",
            "Tingrui Wang",
            "Yongwei Wang",
            "Haonan Li",
            "Zhunga Liu",
            "Quan Pan"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "GAN"
        ],
        "tldr": "The paper introduces a framework called JAD for black-box adversarial attacks that leverages attention maps from CNN and ViT models to generate efficient and architecture-agnostic adversarial examples.",
        "tldr_zh": "该论文介绍了一种名为JAD的框架，用于黑盒敌对攻击，利用CNN和ViT模型的注意力图生成高效且与架构无关的对抗样本。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Weakly Supervised Food Image Segmentation using Vision Transformers and Segment Anything Model",
        "summary": "In this paper, we propose a weakly supervised semantic segmentation approach\nfor food images which takes advantage of the zero-shot capabilities and\npromptability of the Segment Anything Model (SAM) along with the attention\nmechanisms of Vision Transformers (ViTs). Specifically, we use class activation\nmaps (CAMs) from ViTs to generate prompts for SAM, resulting in masks suitable\nfor food image segmentation. The ViT model, a Swin Transformer, is trained\nexclusively using image-level annotations, eliminating the need for pixel-level\nannotations during training. Additionally, to enhance the quality of the\nSAM-generated masks, we examine the use of image preprocessing techniques in\ncombination with single-mask and multi-mask SAM generation strategies. The\nmethodology is evaluated on the FoodSeg103 dataset, generating an average of\n2.4 masks per image (excluding background), and achieving an mIoU of 0.54 for\nthe multi-mask scenario. We envision the proposed approach as a tool to\naccelerate food image annotation tasks or as an integrated component in food\nand nutrition tracking applications.",
        "url": "http://arxiv.org/abs/2509.19028v1",
        "published_date": "2025-09-23T14:01:51+00:00",
        "updated_date": "2025-09-23T14:01:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ioannis Sarafis",
            "Alexandros Papadopoulos",
            "Anastasios Delopoulos"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "Proposes a weakly supervised semantic segmentation method using Vision Transformers and Segment Anything Model for food images, achieving promising results on the FoodSeg103 dataset.",
        "tldr_zh": "提出了一种利用视觉变换器和分割任何模型进行弱监督语义分割的方法，对FoodSeg103数据集取得了令人期待的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards",
        "summary": "Chain of thought reasoning has demonstrated remarkable success in large\nlanguage models, yet its adaptation to vision-language reasoning remains an\nopen challenge with unclear best practices. Existing attempts typically employ\nreasoning chains at a coarse-grained level, which struggles to perform\nfine-grained structured reasoning and, more importantly, are difficult to\nevaluate the reward and quality of intermediate reasoning. In this work, we\ndelve into chain of step reasoning for vision-language models, enabling\nassessing reasoning step quality accurately and leading to effective\nreinforcement learning and inference-time scaling with fine-grained rewards. We\npresent a simple, effective, and fully transparent framework, including the\nstep-level reasoning data, process reward model (PRM), and reinforcement\nlearning training. With the proposed approaches, our models set strong\nbaselines with consistent improvements on challenging vision-language\nbenchmarks. More importantly, we conduct a thorough empirical analysis and\nablation study, unveiling the impact of each component and several intriguing\nproperties of inference-time scaling. We believe this paper serves as a\nbaseline for vision-language models and offers insights into more complex\nmultimodal reasoning. Our dataset, PRM, and code will be available at\nhttps://github.com/baaivision/CoS.",
        "url": "http://arxiv.org/abs/2509.19003v1",
        "published_date": "2025-09-23T13:47:32+00:00",
        "updated_date": "2025-09-23T13:47:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Honghao Chen",
            "Xingzhou Lou",
            "Xiaokun Feng",
            "Kaiqi Huang",
            "Xinlong Wang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a framework for fine-grained step reasoning in vision-language models, showing improvements in benchmarks and offering insights into multimodal reasoning.",
        "tldr_zh": "该论文引入了一个细粒度步骤推理的框架，展示了在基准测试中的改进，并提供了多模态推理的见解。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Prompt-DAS: Annotation-Efficient Prompt Learning for Domain Adaptive Semantic Segmentation of Electron Microscopy Images",
        "summary": "Domain adaptive segmentation (DAS) of numerous organelle instances from\nlarge-scale electron microscopy (EM) is a promising way to enable\nannotation-efficient learning. Inspired by SAM, we propose a promptable\nmultitask framework, namely Prompt-DAS, which is flexible enough to utilize any\nnumber of point prompts during the adaptation training stage and testing stage.\nThus, with varying prompt configurations, Prompt-DAS can perform unsupervised\ndomain adaptation (UDA) and weakly supervised domain adaptation (WDA), as well\nas interactive segmentation during testing. Unlike the foundation model SAM,\nwhich necessitates a prompt for each individual object instance, Prompt-DAS is\nonly trained on a small dataset and can utilize full points on all instances,\nsparse points on partial instances, or even no points at all, facilitated by\nthe incorporation of an auxiliary center-point detection task. Moreover, a\nnovel prompt-guided contrastive learning is proposed to enhance discriminative\nfeature learning. Comprehensive experiments conducted on challenging benchmarks\ndemonstrate the effectiveness of the proposed approach over existing UDA, WDA,\nand SAM-based approaches.",
        "url": "http://arxiv.org/abs/2509.18973v1",
        "published_date": "2025-09-23T13:26:06+00:00",
        "updated_date": "2025-09-23T13:26:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiabao Chen",
            "Shan Xiong",
            "Jialin Peng"
        ],
        "ai_categories": [
            "GAN",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "Prompt-DAS is a domain adaptive segmentation method for electron microscopy images that allows for annotation-efficient learning through the use of point prompts.",
        "tldr_zh": "Prompt-DAS是一种用于电子显微镜图像的领域自适应分割方法，可以通过使用点提示实现有效的标注学习。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Generative data augmentation for biliary tract detection on intraoperative images",
        "summary": "Cholecystectomy is one of the most frequently performed procedures in\ngastrointestinal surgery, and the laparoscopic approach is the gold standard\nfor symptomatic cholecystolithiasis and acute cholecystitis. In addition to the\nadvantages of a significantly faster recovery and better cosmetic results, the\nlaparoscopic approach bears a higher risk of bile duct injury, which has a\nsignificant impact on quality of life and survival. To avoid bile duct injury,\nit is essential to improve the intraoperative visualization of the bile duct.\nThis work aims to address this problem by leveraging a deep-learning approach\nfor the localization of the biliary tract from white-light images acquired\nduring the surgical procedures. To this end, the construction and annotation of\nan image database to train the Yolo detection algorithm has been employed.\nBesides classical data augmentation techniques, the paper proposes Generative\nAdversarial Network (GAN) for the generation of a synthetic portion of the\ntraining dataset. Experimental results have been discussed along with ethical\nconsiderations.",
        "url": "http://arxiv.org/abs/2509.18958v1",
        "published_date": "2025-09-23T13:11:53+00:00",
        "updated_date": "2025-09-23T13:11:53+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Cristina Iacono",
            "Mariarosaria Meola",
            "Federica Conte",
            "Laura Mecozzi",
            "Umberto Bracale",
            "Pietro Falco",
            "Fanny Ficuciello"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper addresses the problem of improving intraoperative visualization of the bile duct during laparoscopic surgery using deep learning and data augmentation techniques.",
        "tldr_zh": "本文利用深度学习和数据增强技术解决了腹腔镜手术中改善胆道可视化的问题。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "One-shot Embroidery Customization via Contrastive LoRA Modulation",
        "summary": "Diffusion models have significantly advanced image manipulation techniques,\nand their ability to generate photorealistic images is beginning to transform\nretail workflows, particularly in presale visualization. Beyond artistic style\ntransfer, the capability to perform fine-grained visual feature transfer is\nbecoming increasingly important. Embroidery is a textile art form characterized\nby intricate interplay of diverse stitch patterns and material properties,\nwhich poses unique challenges for existing style transfer methods. To explore\nthe customization for such fine-grained features, we propose a novel\ncontrastive learning framework that disentangles fine-grained style and content\nfeatures with a single reference image, building on the classic concept of\nimage analogy. We first construct an image pair to define the target style, and\nthen adopt a similarity metric based on the decoupled representations of\npretrained diffusion models for style-content separation. Subsequently, we\npropose a two-stage contrastive LoRA modulation technique to capture\nfine-grained style features. In the first stage, we iteratively update the\nwhole LoRA and the selected style blocks to initially separate style from\ncontent. In the second stage, we design a contrastive learning strategy to\nfurther decouple style and content through self-knowledge distillation.\nFinally, we build an inference pipeline to handle image or text inputs with\nonly the style blocks. To evaluate our method on fine-grained style transfer,\nwe build a benchmark for embroidery customization. Our approach surpasses prior\nmethods on this task and further demonstrates strong generalization to three\nadditional domains: artistic style transfer, sketch colorization, and\nappearance transfer.",
        "url": "http://arxiv.org/abs/2509.18948v1",
        "published_date": "2025-09-23T12:58:15+00:00",
        "updated_date": "2025-09-23T12:58:15+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Jun Ma",
            "Qian He",
            "Gaofeng He",
            "Huang Chen",
            "Chen Liu",
            "Xiaogang Jin",
            "Huamin Wang"
        ],
        "ai_categories": [
            "LoRA",
            "Diffusion"
        ],
        "tldr": "The paper proposes a novel contrastive learning framework for fine-grained style transfer in embroidery customization and other image manipulation tasks.",
        "tldr_zh": "本文提出了一种新颖的对比学习框架，用于细粒度的刺绣个性化定制和其他图像操作任务。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MoiréNet: A Compact Dual-Domain Network for Image Demoiréing",
        "summary": "Moir\\'e patterns arise from spectral aliasing between display pixel lattices\nand camera sensor grids, manifesting as anisotropic, multi-scale artifacts that\npose significant challenges for digital image demoir\\'eing. We propose\nMoir\\'eNet, a convolutional neural U-Net-based framework that synergistically\nintegrates frequency and spatial domain features for effective artifact\nremoval. Moir\\'eNet introduces two key components: a Directional\nFrequency-Spatial Encoder (DFSE) that discerns moir\\'e orientation via\ndirectional difference convolution, and a Frequency-Spatial Adaptive Selector\n(FSAS) that enables precise, feature-adaptive suppression. Extensive\nexperiments demonstrate that Moir\\'eNet achieves state-of-the-art performance\non public and actively used datasets while being highly parameter-efficient.\nWith only 5.513M parameters, representing a 48% reduction compared to ESDNet-L,\nMoir\\'eNet combines superior restoration quality with parameter efficiency,\nmaking it well-suited for resource-constrained applications including\nsmartphone photography, industrial imaging, and augmented reality.",
        "url": "http://arxiv.org/abs/2509.18910v1",
        "published_date": "2025-09-23T12:33:23+00:00",
        "updated_date": "2025-09-23T12:33:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuwei Guo",
            "Simin Luan",
            "Yan Ke",
            "Zeyd Boukhers",
            "John See",
            "Cong Yang"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "MoiréNet is a convolutional neural network designed to remove moiré patterns from images by integrating frequency and spatial domain features, achieving state-of-the-art performance with high parameter efficiency.",
        "tldr_zh": "MoiréNet是一个卷积神经网络，通过整合频率和空间域特征来去除图像中的moire图案，实现了在参数效率高的情况下实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring",
        "summary": "In this paper, we propose the first Structure-from-Motion (SfM)-free\ndeblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat.\nWe address the motion-deblurring problem in two ways. First, we leverage the\npretrained capability of the dense stereo module (DUSt3R) to directly obtain\naccurate initial point clouds from blurred images. Without calculating camera\nposes as an intermediate result, we avoid the cumulative errors transfer from\ninaccurate camera poses to the initial point clouds' positions. Second, we\nintroduce the event stream into the deblur pipeline for its high sensitivity to\ndynamic change. By decoding the latent sharp images from the event stream and\nblurred images, we can provide a fine-grained supervision signal for scene\nreconstruction optimization. Extensive experiments across a range of scenes\ndemonstrate that DeblurSplat not only excels in generating high-fidelity novel\nviews but also achieves significant rendering efficiency compared to the SOTAs\nin deblur 3D-GS.",
        "url": "http://arxiv.org/abs/2509.18898v1",
        "published_date": "2025-09-23T11:21:54+00:00",
        "updated_date": "2025-09-23T11:21:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pengteng Li",
            "Yunfan Lu",
            "Pinhao Song",
            "Weiyu Guo",
            "Huizai Yao",
            "F. Richard Yu",
            "Hui Xiong"
        ],
        "ai_categories": [
            "GAN",
            "Other"
        ],
        "tldr": "The paper introduces DeblurSplat, a 3D Gaussian Splatting method for motion deblurring using event cameras without Structure-from-Motion (SfM), showing improved efficiency and accuracy compared to existing methods.",
        "tldr_zh": "本文引入了DeblurSplat，一种使用事件相机进行运动去模糊的3D高斯处理方法，无需结构-运动（SfM），显示出比现有方法更高的效率和准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SmartWilds: Multimodal Wildlife Monitoring Dataset",
        "summary": "We present the first release of SmartWilds, a multimodal wildlife monitoring\ndataset. SmartWilds is a synchronized collection of drone imagery, camera trap\nphotographs and videos, and bioacoustic recordings collected during summer 2025\nat The Wilds safari park in Ohio. This dataset supports multimodal AI research\nfor comprehensive environmental monitoring, addressing critical needs in\nendangered species research, conservation ecology, and habitat management. Our\npilot deployment captured four days of synchronized monitoring across three\nmodalities in a 220-acre pasture containing Pere David's deer, Sichuan takin,\nPrzewalski's horses, as well as species native to Ohio, including bald eagles,\nwhite-tailed deer, and coyotes. We provide a comparative analysis of sensor\nmodality performance, demonstrating complementary strengths for landuse\npatterns, species detection, behavioral analysis, and habitat monitoring. This\nwork establishes reproducible protocols for multimodal wildlife monitoring\nwhile contributing open datasets to advance conservation computer vision\nresearch. Future releases will include synchronized GPS tracking data from\ntagged individuals, citizen science data, and expanded temporal coverage across\nmultiple seasons.",
        "url": "http://arxiv.org/abs/2509.18894v1",
        "published_date": "2025-09-23T11:07:18+00:00",
        "updated_date": "2025-09-23T11:07:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jenna Kline",
            "Anirudh Potlapally",
            "Bharath Pillai",
            "Tanishka Wani",
            "Rugved Katole",
            "Vedant Patil",
            "Penelope Covey",
            "Hari Subramoni",
            "Tanya Berger-Wolf",
            "Christopher Stewart"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces the SmartWilds dataset, a synchronized collection of drone imagery, camera trap photos and videos, and bioacoustic recordings for wildlife monitoring, with a focus on conservation ecology and habitat management.",
        "tldr_zh": "本文介绍了SmartWilds数据集，这是一种用于野生动物监测的同步集合，包括无人机图像、相机陷阱照片和视频以及生物声学记录，重点关注保护生态学和栖息地管理。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Attack for Defense: Adversarial Agents for Point Prompt Optimization Empowering Segment Anything Model",
        "summary": "Prompt quality plays a critical role in the performance of the Segment\nAnything Model (SAM), yet existing approaches often rely on heuristic or\nmanually crafted prompts, limiting scalability and generalization. In this\npaper, we propose Point Prompt Defender, an adversarial reinforcement learning\nframework that adopts an attack-for-defense paradigm to automatically optimize\npoint prompts. We construct a task-agnostic point prompt environment by\nrepresenting image patches as nodes in a dual-space graph, where edges encode\nboth physical and semantic distances. Within this environment, an attacker\nagent learns to activate a subset of prompts that maximally degrade SAM's\nsegmentation performance, while a defender agent learns to suppress these\ndisruptive prompts and restore accuracy. Both agents are trained using Deep\nQ-Networks with a reward signal based on segmentation quality variation. During\ninference, only the defender is deployed to refine arbitrary coarse prompt\nsets, enabling enhanced SAM segmentation performance across diverse tasks\nwithout retraining. Extensive experiments show that Point Prompt Defender\neffectively improves SAM's robustness and generalization, establishing a\nflexible, interpretable, and plug-and-play framework for prompt-based\nsegmentation.",
        "url": "http://arxiv.org/abs/2509.18891v1",
        "published_date": "2025-09-23T10:59:24+00:00",
        "updated_date": "2025-09-23T10:59:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xueyu Liu",
            "Xiaoyi Zhang",
            "Guangze Shi",
            "Meilin Liu",
            "Yexin Lai",
            "Yongfei Wu",
            "Mingqiang Wei"
        ],
        "ai_categories": [
            "AIGC"
        ],
        "tldr": "The paper proposes an adversarial reinforcement learning framework to automatically optimize point prompts for the Segment Anything Model (SAM) to improve segmentation performance across diverse tasks without retraining.",
        "tldr_zh": "本文提出了一种对抗性强化学习框架，以自动优化点提示，从而改善Segment Anything Model（SAM）在各种任务中的分割性能，无需重新训练。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ViG-LRGC: Vision Graph Neural Networks with Learnable Reparameterized Graph Construction",
        "summary": "Image Representation Learning is an important problem in Computer Vision.\nTraditionally, images were processed as grids, using Convolutional Neural\nNetworks or as a sequence of visual tokens, using Vision Transformers.\nRecently, Vision Graph Neural Networks (ViG) have proposed the treatment of\nimages as a graph of nodes; which provides a more intuitive image\nrepresentation. The challenge is to construct a graph of nodes in each layer\nthat best represents the relations between nodes and does not need a\nhyper-parameter search. ViG models in the literature depend on\nnon-parameterized and non-learnable statistical methods that operate on the\nlatent features of nodes to create a graph. This might not select the best\nneighborhood for each node. Starting from k-NN graph construction to HyperGraph\nConstruction and Similarity-Thresholded graph construction, these methods lack\nthe ability to provide a learnable hyper-parameter-free graph construction\nmethod. To overcome those challenges, we present the Learnable Reparameterized\nGraph Construction (LRGC) for Vision Graph Neural Networks. LRGC applies\nkey-query attention between every pair of nodes; then uses soft-threshold\nreparameterization for edge selection, which allows the use of a differentiable\nmathematical model for training. Using learnable parameters to select the\nneighborhood removes the bias that is induced by any clustering or thresholding\nmethods previously introduced in the literature. In addition, LRGC allows\ntuning the threshold in each layer to the training data since the thresholds\nare learnable through training and are not provided as hyper-parameters to the\nmodel. We demonstrate that the proposed ViG-LRGC approach outperforms\nstate-of-the-art ViG models of similar sizes on the ImageNet-1k benchmark\ndataset.",
        "url": "http://arxiv.org/abs/2509.18840v1",
        "published_date": "2025-09-23T09:25:22+00:00",
        "updated_date": "2025-09-23T09:25:22+00:00",
        "categories": [
            "cs.CV",
            "I.2.10"
        ],
        "authors": [
            "Ismael Elsharkawi",
            "Hossam Sharara",
            "Ahmed Rafea"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a new method called ViG-LRGC for constructing graphs in Vision Graph Neural Networks, outperforming existing methods on the ImageNet-1k dataset.",
        "tldr_zh": "本文介绍了一种名为ViG-LRGC的新方法，用于构建视觉图神经网络中的图，其在ImageNet-1k数据集上表现优异。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Reconstruction of Optical Coherence Tomography Images from Wavelength-space Using Deep-learning",
        "summary": "Conventional Fourier-domain Optical Coherence Tomography (FD-OCT) systems\ndepend on resampling into wavenumber (k) domain to extract the depth profile.\nThis either necessitates additional hardware resources or amplifies the\nexisting computational complexity. Moreover, the OCT images also suffer from\nspeckle noise, due to systemic reliance on low coherence interferometry. We\npropose a streamlined and computationally efficient approach based on\nDeep-Learning (DL) which enables reconstructing speckle-reduced OCT images\ndirectly from the wavelength domain. For reconstruction, two encoder-decoder\nstyled networks namely Spatial Domain Convolution Neural Network (SD-CNN) and\nFourier Domain CNN (FD-CNN) are used sequentially. The SD-CNN exploits the\nhighly degraded images obtained by Fourier transforming the domain fringes to\nreconstruct the deteriorated morphological structures along with suppression of\nunwanted noise. The FD-CNN leverages this output to enhance the image quality\nfurther by optimization in Fourier domain (FD). We quantitatively and visually\ndemonstrate the efficacy of the method in obtaining high-quality OCT images.\nFurthermore, we illustrate the computational complexity reduction by harnessing\nthe power of DL models. We believe that this work lays the framework for\nfurther innovations in the realm of OCT image reconstruction.",
        "url": "http://arxiv.org/abs/2509.18783v1",
        "published_date": "2025-09-23T08:21:53+00:00",
        "updated_date": "2025-09-23T08:21:53+00:00",
        "categories": [
            "physics.optics",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Maryam Viqar",
            "Erdem Sahin",
            "Elena Stoykova",
            "Violeta Madjarova"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a Deep-Learning approach to reconstruct Optical Coherence Tomography images directly from the wavelength domain, reducing computational complexity and noise, leading to high-quality images.",
        "tldr_zh": "本文提出了一种深度学习方法，直接从波长域重建光学相干断层扫描图像，降低计算复杂度和噪音，实现高质量图像重建。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Real-time Deer Detection and Warning in Connected Vehicles via Thermal Sensing and Deep Learning",
        "summary": "Deer-vehicle collisions represent a critical safety challenge in the United\nStates, causing nearly 2.1 million incidents annually and resulting in\napproximately 440 fatalities, 59,000 injuries, and 10 billion USD in economic\ndamages. These collisions also contribute significantly to declining deer\npopulations. This paper presents a real-time detection and driver warning\nsystem that integrates thermal imaging, deep learning, and\nvehicle-to-everything communication to help mitigate deer-vehicle collisions.\nOur system was trained and validated on a custom dataset of over 12,000 thermal\ndeer images collected in Mars Hill, North Carolina. Experimental evaluation\ndemonstrates exceptional performance with 98.84 percent mean average precision,\n95.44 percent precision, and 95.96 percent recall. The system was field tested\nduring a follow-up visit to Mars Hill and readily sensed deer providing the\ndriver with advanced warning. Field testing validates robust operation across\ndiverse weather conditions, with thermal imaging maintaining between 88 and 92\npercent detection accuracy in challenging scenarios where conventional visible\nlight based cameras achieve less than 60 percent effectiveness. When a high\nprobability threshold is reached sensor data sharing messages are broadcast to\nsurrounding vehicles and roadside units via cellular vehicle to everything\n(CV2X) communication devices. Overall, our system achieves end to end latency\nconsistently under 100 milliseconds from detection to driver alert. This\nresearch establishes a viable technological pathway for reducing deer-vehicle\ncollisions through thermal imaging and connected vehicles.",
        "url": "http://arxiv.org/abs/2509.18779v1",
        "published_date": "2025-09-23T08:16:25+00:00",
        "updated_date": "2025-09-23T08:16:25+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Hemanth Puppala",
            "Wayne Sarasua",
            "Srinivas Biyaguda",
            "Farhad Farzinpour",
            "Mashrur Chowdhury"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper presents a real-time deer detection and warning system using thermal sensing and deep learning in connected vehicles to reduce deer-vehicle collisions.",
        "tldr_zh": "本文提出了一种利用热感知和深度学习在联网车辆中实时检测和警示鹿的系统，以减少鹿车碰撞。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DiSSECT: Structuring Transfer-Ready Medical Image Representations through Discrete Self-Supervision",
        "summary": "Self-supervised learning (SSL) has emerged as a powerful paradigm for medical\nimage representation learning, particularly in settings with limited labeled\ndata. However, existing SSL methods often rely on complex architectures,\nanatomy-specific priors, or heavily tuned augmentations, which limit their\nscalability and generalizability. More critically, these models are prone to\nshortcut learning, especially in modalities like chest X-rays, where anatomical\nsimilarity is high and pathology is subtle. In this work, we introduce DiSSECT\n-- Discrete Self-Supervision for Efficient Clinical Transferable\nRepresentations, a framework that integrates multi-scale vector quantization\ninto the SSL pipeline to impose a discrete representational bottleneck. This\nconstrains the model to learn repeatable, structure-aware features while\nsuppressing view-specific or low-utility patterns, improving representation\ntransfer across tasks and domains. DiSSECT achieves strong performance on both\nclassification and segmentation tasks, requiring minimal or no fine-tuning, and\nshows particularly high label efficiency in low-label regimes. We validate\nDiSSECT across multiple public medical imaging datasets, demonstrating its\nrobustness and generalizability compared to existing state-of-the-art\napproaches.",
        "url": "http://arxiv.org/abs/2509.18765v1",
        "published_date": "2025-09-23T07:58:21+00:00",
        "updated_date": "2025-09-23T07:58:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Azad Singh",
            "Deepak Mishra"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "DiSSECT introduces a framework for efficient medical image representation learning through discrete self-supervision, showing strong performance on various tasks and datasets with minimal fine-tuning.",
        "tldr_zh": "DiSSECT通过离散的自监督学习引入了一个高效的医学图像表示学习框架，在各种任务和数据集上表现出色，无需或只需少量微调。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Bi-VLM: Pushing Ultra-Low Precision Post-Training Quantization Boundaries in Vision-Language Models",
        "summary": "We address the critical gap between the computational demands of\nvision-language models and the possible ultra-low-bit weight precision\n(bitwidth $\\leq2$ bits) we can use for higher efficiency. Our work is motivated\nby the substantial computational cost and memory requirements of VLMs, which\nrestrict their applicability in hardware-constrained environments. We propose\nBi-VLM, which separates model weights non-uniformly based on the Gaussian\nquantiles. Our formulation groups the model weights into outlier (salient) and\nmultiple inlier (unsalient) subsets, ensuring that each subset contains a\nproportion of weights corresponding to its quantile in the distribution. We\npropose a saliency-aware hybrid quantization algorithm and use it to quantize\nweights by imposing different constraints on the scaler and binary matrices\nbased on the saliency metric and compression objective. We have evaluated our\napproach on different VLMs. For the language model part of the VLM, our Bi-VLM\noutperforms the SOTA by 3%-47% on the visual question answering task in terms\nof four different benchmarks and three different models. For the overall VLM,\nour Bi-VLM outperforms the SOTA by 4%-45%. We also perform token pruning on the\nquantized models and observe that there is redundancy of image tokens 90% - 99%\nin the quantized models. This helps us to further prune the visual tokens to\nimprove efficiency.",
        "url": "http://arxiv.org/abs/2509.18763v1",
        "published_date": "2025-09-23T07:55:48+00:00",
        "updated_date": "2025-09-23T07:55:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xijun Wang",
            "Junyun Huang",
            "Rayyan Abdalla",
            "Chengyuan Zhang",
            "Ruiqi Xian",
            "Dinesh Manocha"
        ],
        "ai_categories": [
            "Transformer",
            "AIGC"
        ],
        "tldr": "The paper proposes a novel approach, Bi-VLM, to push the boundaries of ultra-low precision quantization in vision-language models, achieving significant performance improvements over the state-of-the-art.",
        "tldr_zh": "本文提出了一种新颖的方法，Bi-VLM，以推动视觉语言模型中超低精度量化的边界，实现了显著的性能提升，超越了现有技术水平。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score Distillation",
        "summary": "Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in\n3D reconstruction and novel view synthesis. However, reconstructing 3D scenes\nfrom sparse viewpoints remains highly challenging due to insufficient visual\ninformation, which results in noticeable artifacts persisting across the 3D\nrepresentation. To address this limitation, recent methods have resorted to\ngenerative priors to remove artifacts and complete missing content in\nunder-constrained areas. Despite their effectiveness, these approaches struggle\nto ensure multi-view consistency, resulting in blurred structures and\nimplausible details. In this work, we propose FixingGS, a training-free method\nthat fully exploits the capabilities of the existing diffusion model for\nsparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our\ndistillation approach, which delivers more accurate and cross-view coherent\ndiffusion priors, thereby enabling effective artifact removal and inpainting.\nIn addition, we propose an adaptive progressive enhancement scheme that further\nrefines reconstructions in under-constrained regions. Extensive experiments\ndemonstrate that FixingGS surpasses existing state-of-the-art methods with\nsuperior visual quality and reconstruction performance. Our code will be\nreleased publicly.",
        "url": "http://arxiv.org/abs/2509.18759v1",
        "published_date": "2025-09-23T07:53:46+00:00",
        "updated_date": "2025-09-23T07:53:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhaorui Wang",
            "Yi Gu",
            "Deming Zhou",
            "Renjing Xu"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "FixingGS proposes a training-free method to enhance 3D Gaussian Splatting for reconstruction and view synthesis, outperforming existing methods in visual quality and performance.",
        "tldr_zh": "FixingGS 提出了一种无需训练的方法，用于增强 3D 高斯打点技术，用于重建和视图合成，在视觉质量和性能方面胜过现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "COLT: Enhancing Video Large Language Models with Continual Tool Usage",
        "summary": "The success of Large Language Models (LLMs) has significantly propelled the\nresearch of video understanding. To harvest the benefits of well-trained expert\nmodels (i.e., tools), video LLMs prioritize the exploration of tool usage\ncapabilities. Existing methods either prompt closed-source LLMs or employ the\ninstruction tuning paradigm for tool-use fine-tuning. These methods, however,\nassume an established repository of fixed tools and struggle to generalize to\nreal-world environments where tool data is perpetually evolving and streaming\nin. To this end, we propose to enhance open-source video LLMs with COntinuaL\nTool usage (termed COLT), which automatically acquires tool-use ability in a\nsuccessive tool stream without suffering 'catastrophic forgetting' of the past\nlearned tools. Specifically, our COLT incorporates a learnable tool codebook as\na tool-specific memory system. Then relevant tools are dynamically selected\nbased on the similarity between user instruction and tool features within the\ncodebook. To unleash the tool usage potential of video LLMs, we collect a\nvideo-centric tool-use instruction tuning dataset VideoToolBench. Extensive\nexperiments on both previous video LLM benchmarks and the tool-use-specific\nVideoToolBench dataset demonstrate the state-of-the-art performance of our\nproposed COLT.",
        "url": "http://arxiv.org/abs/2509.18754v1",
        "published_date": "2025-09-23T07:49:30+00:00",
        "updated_date": "2025-09-23T07:49:30+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuyang Liu",
            "Xinyuan Shi",
            "Bang Yang",
            "Peilin Zhou",
            "Jiahua Dong",
            "Long Chen",
            "Ian Reid",
            "Xiaondan Liang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes COLT, a method to enhance video Large Language Models with continual tool usage, achieving state-of-the-art performance on video understanding tasks.",
        "tldr_zh": "本文提出了COLT方法，用于增强视频大语言模型的持续工具使用，在视频理解任务上取得了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HyPSAM: Hybrid Prompt-driven Segment Anything Model for RGB-Thermal Salient Object Detection",
        "summary": "RGB-thermal salient object detection (RGB-T SOD) aims to identify prominent\nobjects by integrating complementary information from RGB and thermal\nmodalities. However, learning the precise boundaries and complete objects\nremains challenging due to the intrinsic insufficient feature fusion and the\nextrinsic limitations of data scarcity. In this paper, we propose a novel\nhybrid prompt-driven segment anything model (HyPSAM), which leverages the\nzero-shot generalization capabilities of the segment anything model (SAM) for\nRGB-T SOD. Specifically, we first propose a dynamic fusion network (DFNet) that\ngenerates high-quality initial saliency maps as visual prompts. DFNet employs\ndynamic convolution and multi-branch decoding to facilitate adaptive\ncross-modality interaction, overcoming the limitations of fixed-parameter\nkernels and enhancing multi-modal feature representation. Moreover, we propose\na plug-and-play refinement network (P2RNet), which serves as a general\noptimization strategy to guide SAM in refining saliency maps by using hybrid\nprompts. The text prompt ensures reliable modality input, while the mask and\nbox prompts enable precise salient object localization. Extensive experiments\non three public datasets demonstrate that our method achieves state-of-the-art\nperformance. Notably, HyPSAM has remarkable versatility, seamlessly integrating\nwith different RGB-T SOD methods to achieve significant performance gains,\nthereby highlighting the potential of prompt engineering in this field. The\ncode and results of our method are available at:\nhttps://github.com/milotic233/HyPSAM.",
        "url": "http://arxiv.org/abs/2509.18738v1",
        "published_date": "2025-09-23T07:32:11+00:00",
        "updated_date": "2025-09-23T07:32:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruichao Hou",
            "Xingyuan Li",
            "Tongwei Ren",
            "Dongming Zhou",
            "Gangshan Wu",
            "Jinde Cao"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel model for RGB-thermal salient object detection that leverages the capabilities of the segment anything model for improved performance. The proposed model achieves state-of-the-art results on public datasets.",
        "tldr_zh": "本文介绍了一种新颖的模型，用于 RGB-热成像显著物体检测，利用了段任意模型的能力，以提高性能。提出的模型在公共数据集上取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Pre-training CLIP against Data Poisoning with Optimal Transport-based Matching and Alignment",
        "summary": "Recent studies have shown that Contrastive Language-Image Pre-training (CLIP)\nmodels are threatened by targeted data poisoning and backdoor attacks due to\nmassive training image-caption pairs crawled from the Internet. Previous\ndefense methods correct poisoned image-caption pairs by matching a new caption\nfor each image. However, the matching process relies solely on the global\nrepresentations of images and captions, overlooking fine-grained features of\nvisual and textual features. It may introduce incorrect image-caption pairs and\nharm the CLIP pre-training. To address their limitations, we propose an Optimal\nTransport-based framework to reconstruct image-caption pairs, named OTCCLIP. We\npropose a new optimal transport-based distance measure between fine-grained\nvisual and textual feature sets and re-assign new captions based on the\nproposed optimal transport distance. Additionally, to further reduce the\nnegative impact of mismatched pairs, we encourage the inter- and intra-modality\nfine-grained alignment by employing optimal transport-based objective\nfunctions. Our experiments demonstrate that OTCCLIP can successfully decrease\nthe attack success rates of poisoning attacks. Also, compared to previous\nmethods, OTCCLIP significantly improves CLIP's zero-shot and linear probing\nperformance trained on poisoned datasets.",
        "url": "http://arxiv.org/abs/2509.18717v1",
        "published_date": "2025-09-23T07:05:43+00:00",
        "updated_date": "2025-09-23T07:05:43+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Tong Zhang",
            "Kuofeng Gao",
            "Jiawang Bai",
            "Leo Yu Zhang",
            "Xin Yin",
            "Zonghui Wang",
            "Shouling Ji",
            "Wenzhi Chen"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a framework named OTCCLIP to protect CLIP models from data poisoning attacks by using optimal transport-based matching and alignment.",
        "tldr_zh": "本文提出了一个名为OTCCLIP的框架，通过使用基于最优传输的匹配和对齐来保护CLIP模型免受数据毒化攻击。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "What Makes You Unique? Attribute Prompt Composition for Object Re-Identification",
        "summary": "Object Re-IDentification (ReID) aims to recognize individuals across\nnon-overlapping camera views. While recent advances have achieved remarkable\nprogress, most existing models are constrained to either single-domain or\ncross-domain scenarios, limiting their real-world applicability. Single-domain\nmodels tend to overfit to domain-specific features, whereas cross-domain models\noften rely on diverse normalization strategies that may inadvertently suppress\nidentity-specific discriminative cues. To address these limitations, we propose\nan Attribute Prompt Composition (APC) framework, which exploits textual\nsemantics to jointly enhance discrimination and generalization. Specifically,\nwe design an Attribute Prompt Generator (APG) consisting of a Semantic\nAttribute Dictionary (SAD) and a Prompt Composition Module (PCM). SAD is an\nover-complete attribute dictionary to provide rich semantic descriptions, while\nPCM adaptively composes relevant attributes from SAD to generate discriminative\nattribute-aware features. In addition, motivated by the strong generalization\nability of Vision-Language Models (VLM), we propose a Fast-Slow Training\nStrategy (FSTS) to balance ReID-specific discrimination and generalizable\nrepresentation learning. Specifically, FSTS adopts a Fast Update Stream (FUS)\nto rapidly acquire ReID-specific discriminative knowledge and a Slow Update\nStream (SUS) to retain the generalizable knowledge inherited from the\npre-trained VLM. Through a mutual interaction, the framework effectively\nfocuses on ReID-relevant features while mitigating overfitting. Extensive\nexperiments on both conventional and Domain Generalized (DG) ReID datasets\ndemonstrate that our framework surpasses state-of-the-art methods, exhibiting\nsuperior performances in terms of both discrimination and generalization. The\nsource code is available at https://github.com/AWangYQ/APC.",
        "url": "http://arxiv.org/abs/2509.18715v1",
        "published_date": "2025-09-23T07:03:08+00:00",
        "updated_date": "2025-09-23T07:03:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yingquan Wang",
            "Pingping Zhang",
            "Chong Sun",
            "Dong Wang",
            "Huchuan Lu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes an Attribute Prompt Composition framework for Object Re-Identification using textual semantics to enhance discrimination and generalization, achieving state-of-the-art results.",
        "tldr_zh": "本文提出了一种利用文本语义增强鉴别性和泛化性的对象再识别属性提示组合框架，取得了最新技术的成果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Zero-shot Monocular Metric Depth for Endoscopic Images",
        "summary": "Monocular relative and metric depth estimation has seen a tremendous boost in\nthe last few years due to the sharp advancements in foundation models and in\nparticular transformer based networks. As we start to see applications to the\ndomain of endoscopic images, there is still a lack of robust benchmarks and\nhigh-quality datasets in that area. This paper addresses these limitations by\npresenting a comprehensive benchmark of state-of-the-art (metric and relative)\ndepth estimation models evaluated on real, unseen endoscopic images, providing\ncritical insights into their generalisation and performance in clinical\nscenarios. Additionally, we introduce and publish a novel synthetic dataset\n(EndoSynth) of endoscopic surgical instruments paired with ground truth metric\ndepth and segmentation masks, designed to bridge the gap between synthetic and\nreal-world data. We demonstrate that fine-tuning depth foundation models using\nour synthetic dataset boosts accuracy on most unseen real data by a significant\nmargin. By providing both a benchmark and a synthetic dataset, this work\nadvances the field of depth estimation for endoscopic images and serves as an\nimportant resource for future research. Project page, EndoSynth dataset and\ntrained weights are available at https://github.com/TouchSurgery/EndoSynth.",
        "url": "http://arxiv.org/abs/2509.18642v1",
        "published_date": "2025-09-23T04:56:25+00:00",
        "updated_date": "2025-09-23T04:56:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nicolas Toussaint",
            "Emanuele Colleoni",
            "Ricardo Sanchez-Matilla",
            "Joshua Sutcliffe",
            "Vanessa Thompson",
            "Muhammad Asad",
            "Imanol Luengo",
            "Danail Stoyanov"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper presents a benchmark and synthetic dataset for depth estimation in endoscopic images, improving accuracy on real data by fine-tuning models with the synthetic dataset.",
        "tldr_zh": "本文提出了一个深度估计的基准和合成数据集，通过使用合成数据集对模型进行微调，提高了对真实数据的准确性.",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Understanding-in-Generation: Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation",
        "summary": "Recent works have made notable advancements in enhancing unified models for\ntext-to-image generation through the Chain-of-Thought (CoT). However, these\nreasoning methods separate the processes of understanding and generation, which\nlimits their ability to guide the reasoning of unified models in addressing the\ndeficiencies of their generative capabilities. To this end, we propose a novel\nreasoning framework for unified models, Understanding-in-Generation (UiG),\nwhich harnesses the robust understanding capabilities of unified models to\nreinforce their performance in image generation. The core insight of our UiG is\nto integrate generative guidance by the strong understanding capabilities\nduring the reasoning process, thereby mitigating the limitations of generative\nabilities. To achieve this, we introduce \"Image Editing\" as a bridge to infuse\nunderstanding into the generation process. Initially, we verify the generated\nimage and incorporate the understanding of unified models into the editing\ninstructions. Subsequently, we enhance the generated image step by step,\ngradually infusing the understanding into the generation process. Our UiG\nframework demonstrates a significant performance improvement in text-to-image\ngeneration over existing text-to-image reasoning methods, e.g., a 3.92% gain on\nthe long prompt setting of the TIIF benchmark. The project code:\nhttps://github.com/QC-LY/UiG",
        "url": "http://arxiv.org/abs/2509.18639v1",
        "published_date": "2025-09-23T04:52:39+00:00",
        "updated_date": "2025-09-23T04:52:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuanhuiyi Lyu",
            "Chi Kit Wong",
            "Chenfei Liao",
            "Lutao Jiang",
            "Xu Zheng",
            "Zexin Lu",
            "Linfeng Zhang",
            "Xuming Hu"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a new framework, Understanding-in-Generation (UiG), to improve text-to-image generation by integrating understanding capabilities into the generation process, showcasing significant performance gains.",
        "tldr_zh": "该论文介绍了一种新框架，名为Understanding-in-Generation (UiG)，通过将理解能力整合到生成过程中，显著提高了文本到图像的生成性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Learning neuroimaging models from health system-scale data",
        "summary": "Neuroimaging is a ubiquitous tool for evaluating patients with neurological\ndiseases. The global demand for magnetic resonance imaging (MRI) studies has\nrisen steadily, placing significant strain on health systems, prolonging\nturnaround times, and intensifying physician burnout \\cite{Chen2017-bt,\nRula2024-qp-1}. These challenges disproportionately impact patients in\nlow-resource and rural settings. Here, we utilized a large academic health\nsystem as a data engine to develop Prima, the first vision language model (VLM)\nserving as an AI foundation for neuroimaging that supports real-world, clinical\nMRI studies as input. Trained on over 220,000 MRI studies, Prima uses a\nhierarchical vision architecture that provides general and transferable MRI\nfeatures. Prima was tested in a 1-year health system-wide study that included\n30K MRI studies. Across 52 radiologic diagnoses from the major neurologic\ndisorders, including neoplastic, inflammatory, infectious, and developmental\nlesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0,\noutperforming other state-of-the-art general and medical AI models. Prima\noffers explainable differential diagnoses, worklist priority for radiologists,\nand clinical referral recommendations across diverse patient demographics and\nMRI systems. Prima demonstrates algorithmic fairness across sensitive groups\nand can help mitigate health system biases, such as prolonged turnaround times\nfor low-resource populations. These findings highlight the transformative\npotential of health system-scale VLMs and Prima's role in advancing AI-driven\nhealthcare.",
        "url": "http://arxiv.org/abs/2509.18638v1",
        "published_date": "2025-09-23T04:49:59+00:00",
        "updated_date": "2025-09-23T04:49:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yiwei Lyu",
            "Samir Harake",
            "Asadur Chowdury",
            "Soumyanil Banerjee",
            "Rachel Gologorsky",
            "Shixuan Liu",
            "Anna-Katharina Meissner",
            "Akshay Rao",
            "Chenhui Zhao",
            "Akhil Kondepudi",
            "Cheng Jiang",
            "Xinhai Hou",
            "Rushikesh S. Joshi",
            "Volker Neuschmelting",
            "Ashok Srinivasan",
            "Dawn Kleindorfer",
            "Brian Athey",
            "Vikas Gulani",
            "Aditya Pandey",
            "Honglak Lee",
            "Todd Hollon"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces Prima, a vision language model trained on MRI studies to assist in neuroimaging diagnosis and decision-making in healthcare.",
        "tldr_zh": "该论文介绍了Prima，这是一个在MRI研究中训练的视觉语言模型，用于辅助神经影像诊断和医疗决策。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Prompt-Guided Dual Latent Steering for Inversion Problems",
        "summary": "Inverting corrupted images into the latent space of diffusion models is\nchallenging. Current methods, which encode an image into a single latent\nvector, struggle to balance structural fidelity with semantic accuracy, leading\nto reconstructions with semantic drift, such as blurred details or incorrect\nattributes. To overcome this, we introduce Prompt-Guided Dual Latent Steering\n(PDLS), a novel, training-free framework built upon Rectified Flow models for\ntheir stable inversion paths. PDLS decomposes the inversion process into two\ncomplementary streams: a structural path to preserve source integrity and a\nsemantic path guided by a prompt. We formulate this dual guidance as an optimal\ncontrol problem and derive a closed-form solution via a Linear Quadratic\nRegulator (LQR). This controller dynamically steers the generative trajectory\nat each step, preventing semantic drift while ensuring the preservation of fine\ndetail without costly, per-image optimization. Extensive experiments on FFHQ-1K\nand ImageNet-1K under various inversion tasks, including Gaussian deblurring,\nmotion deblurring, super-resolution and freeform inpainting, demonstrate that\nPDLS produces reconstructions that are both more faithful to the original image\nand better aligned with the semantic information than single-latent baselines.",
        "url": "http://arxiv.org/abs/2509.18619v1",
        "published_date": "2025-09-23T04:11:06+00:00",
        "updated_date": "2025-09-23T04:11:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yichen Wu",
            "Xu Liu",
            "Chenxuan Zhao",
            "Xinyu Wu"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Diffusion"
        ],
        "tldr": "The paper introduces a novel framework, PDLS, for inverting corrupted images into diffusion models' latent space, balancing structural fidelity and semantic accuracy using dual guidance streams.",
        "tldr_zh": "该论文引入了一种新的框架，PDLS，用于将损坏的图像反转到扩散模型的潜在空间，利用双重引导流平衡结构保真度和语义准确度。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MLF-4DRCNet: Multi-Level Fusion with 4D Radar and Camera for 3D Object Detection in Autonomous Driving",
        "summary": "The emerging 4D millimeter-wave radar, measuring the range, azimuth,\nelevation, and Doppler velocity of objects, is recognized for its\ncost-effectiveness and robustness in autonomous driving. Nevertheless, its\npoint clouds exhibit significant sparsity and noise, restricting its standalone\napplication in 3D object detection. Recent 4D radar-camera fusion methods have\nprovided effective perception. Most existing approaches, however, adopt\nexplicit Bird's-Eye-View fusion paradigms originally designed for LiDAR-camera\nfusion, neglecting radar's inherent drawbacks. Specifically, they overlook the\nsparse and incomplete geometry of radar point clouds and restrict fusion to\ncoarse scene-level integration. To address these problems, we propose\nMLF-4DRCNet, a novel two-stage framework for 3D object detection via\nmulti-level fusion of 4D radar and camera images. Our model incorporates the\npoint-, scene-, and proposal-level multi-modal information, enabling\ncomprehensive feature representation. It comprises three crucial components:\nthe Enhanced Radar Point Encoder (ERPE) module, the Hierarchical Scene Fusion\nPooling (HSFP) module, and the Proposal-Level Fusion Enhancement (PLFE) module.\nOperating at the point-level, ERPE densities radar point clouds with 2D image\ninstances and encodes them into voxels via the proposed Triple-Attention Voxel\nFeature Encoder. HSFP dynamically integrates multi-scale voxel features with 2D\nimage features using deformable attention to capture scene context and adopts\npooling to the fused features. PLFE refines region proposals by fusing image\nfeatures, and further integrates with the pooled features from HSFP.\nExperimental results on the View-of-Delft (VoD) and TJ4DRadSet datasets\ndemonstrate that MLF-4DRCNet achieves the state-of-the-art performance.\nNotably, it attains performance comparable to LiDAR-based models on the VoD\ndataset.",
        "url": "http://arxiv.org/abs/2509.18613v1",
        "published_date": "2025-09-23T04:02:28+00:00",
        "updated_date": "2025-09-23T04:02:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuzhi Wu",
            "Li Xiao",
            "Jun Liu",
            "Guangfeng Jiang",
            "XiangGen Xia"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel method, MLF-4DRCNet, for 3D object detection in autonomous driving by fusing data from 4D radar and camera images, achieving state-of-the-art performance compared to LiDAR-based models.",
        "tldr_zh": "该论文提出了一种新颖的方法，MLF-4DRCNet，通过融合4D雷达和摄像头图像的数据，在自动驾驶中进行三维物体检测，在VoD数据集上达到了与基于LiDAR的模型相当的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "SSCM: A Spatial-Semantic Consistent Model for Multi-Contrast MRI Super-Resolution",
        "summary": "Multi-contrast Magnetic Resonance Imaging super-resolution (MC-MRI SR) aims\nto enhance low-resolution (LR) contrasts leveraging high-resolution (HR)\nreferences, shortening acquisition time and improving imaging efficiency while\npreserving anatomical details. The main challenge lies in maintaining\nspatial-semantic consistency, ensuring anatomical structures remain\nwell-aligned and coherent despite structural discrepancies and motion between\nthe target and reference images. Conventional methods insufficiently model\nspatial-semantic consistency and underuse frequency-domain information, which\nleads to poor fine-grained alignment and inadequate recovery of high-frequency\ndetails. In this paper, we propose the Spatial-Semantic Consistent Model\n(SSCM), which integrates a Dynamic Spatial Warping Module for inter-contrast\nspatial alignment, a Semantic-Aware Token Aggregation Block for long-range\nsemantic consistency, and a Spatial-Frequency Fusion Block for fine structure\nrestoration. Experiments on public and private datasets show that SSCM achieves\nstate-of-the-art performance with fewer parameters while ensuring spatially and\nsemantically consistent reconstructions.",
        "url": "http://arxiv.org/abs/2509.18593v1",
        "published_date": "2025-09-23T03:24:32+00:00",
        "updated_date": "2025-09-23T03:24:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoman Wu",
            "Lubin Gan",
            "Siying Wu",
            "Jing Zhang",
            "Yunwei Ou",
            "Xiaoyan Sun"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces SSCM, a model for improving low-resolution MRI images by leveraging high-resolution references. It focuses on spatial-semantic consistency to maintain anatomical details.",
        "tldr_zh": "本文介绍了SSCM模型，旨在通过利用高分辨率参考图像来改善低分辨率MRI图像。它侧重于空间语义一致性，以保持解剖细节。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing Video Object Segmentation in TrackRAD Using XMem Memory Network",
        "summary": "This paper presents an advanced tumor segmentation framework for real-time\nMRI-guided radiotherapy, designed for the TrackRAD2025 challenge. Our method\nleverages the XMem model, a memory-augmented architecture, to segment tumors\nacross long cine-MRI sequences. The proposed system efficiently integrates\nmemory mechanisms to track tumor motion in real-time, achieving high\nsegmentation accuracy even under challenging conditions with limited annotated\ndata. Unfortunately, the detailed experimental records have been lost,\npreventing us from reporting precise quantitative results at this stage.\nNevertheless, From our preliminary impressions during development, the\nXMem-based framework demonstrated reasonable segmentation performance and\nsatisfied the clinical real-time requirement. Our work contributes to improving\nthe precision of tumor tracking during MRI-guided radiotherapy, which is\ncrucial for enhancing the accuracy and safety of cancer treatments.",
        "url": "http://arxiv.org/abs/2509.18591v1",
        "published_date": "2025-09-23T03:22:06+00:00",
        "updated_date": "2025-09-23T03:22:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pengchao Deng",
            "Shengqi Chen"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper presents a tumor segmentation framework using XMem model for real-time MRI-guided radiotherapy, achieving high segmentation accuracy even with limited data.",
        "tldr_zh": "本文介绍了一种使用XMem模型的肿瘤分割框架，用于实时MRI引导放疗，即使数据有限也能实现高的分割准确度。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "The Photographer Eye: Teaching Multimodal Large Language Models to See and Critique like Photographers",
        "summary": "While editing directly from life, photographers have found it too difficult\nto see simultaneously both the blue and the sky. Photographer and curator,\nSzarkowski insightfully revealed one of the notable gaps between general and\naesthetic visual understanding: while the former focuses on identifying the\nfactual element in an image (sky), the latter transcends such object\nidentification, viewing it instead as an aesthetic component--a pure color\nblock (blue). Such fundamental distinctions between general (detection,\nlocalization, etc.) and aesthetic (color, lighting, composition, etc.) visual\nunderstanding present a significant challenge for Multimodal Large Language\nModels (MLLMs). Although some recent works have made initial explorations, they\nare often limited to general and basic aesthetic commonsense. As a result, they\nfrequently fall short in real-world scenarios (Fig. 1), which require extensive\nexpertise--including photographic techniques, photo pre/post-processing\nknowledge, and more, to provide a detailed analysis and description. To\nfundamentally enhance the aesthetics understanding of MLLMs, we first introduce\na novel dataset, PhotoCritique, derived from extensive discussions among\nprofessional photographers and enthusiasts, and characterized by the large\nscale, expertise, and diversity. Then, to better learn visual aesthetics from\nPhotoCritique, we furthur propose a novel model, PhotoEye, featuring a\nlanguageguided multi-view vision fusion mechanism to understand image\naesthetics from multiple perspectives. Finally, we present a novel benchmark,\nPhotoBench, a comprehensive and professional benchmark for aesthetic visual\nunderstanding. On existing benchmarks and PhotoBench, our model demonstrates\nclear advantages over existing models.",
        "url": "http://arxiv.org/abs/2509.18582v1",
        "published_date": "2025-09-23T02:59:41+00:00",
        "updated_date": "2025-09-23T02:59:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Daiqing Qi",
            "Handong Zhao",
            "Jing Shi",
            "Simon Jenni",
            "Yifei Fan",
            "Franck Dernoncourt",
            "Scott Cohen",
            "Sheng Li"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a new dataset, model, and benchmark to enhance the aesthetics understanding of Multimodal Large Language Models (MLLMs) in the context of photography.",
        "tldr_zh": "本文引入了一个新的数据集，模型和基准，以提高多模态大语言模型（MLLMs）在摄影领域中对美学的理解。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Live-E2T: Real-time Threat Monitoring in Video via Deduplicated Event Reasoning and Chain-of-Thought",
        "summary": "Real-time threat monitoring identifies threatening behaviors in video streams\nand provides reasoning and assessment of threat events through explanatory\ntext. However, prevailing methodologies, whether based on supervised learning\nor generative models, struggle to concurrently satisfy the demanding\nrequirements of real-time performance and decision explainability. To bridge\nthis gap, we introduce Live-E2T, a novel framework that unifies these two\nobjectives through three synergistic mechanisms. First, we deconstruct video\nframes into structured Human-Object-Interaction-Place semantic tuples. This\napproach creates a compact, semantically focused representation, circumventing\nthe information degradation common in conventional feature compression. Second,\nan efficient online event deduplication and updating mechanism is proposed to\nfilter spatio-temporal redundancies, ensuring the system's real time\nresponsiveness. Finally, we fine-tune a Large Language Model using a\nChain-of-Thought strategy, endow it with the capability for transparent and\nlogical reasoning over event sequences to produce coherent threat assessment\nreports. Extensive experiments on benchmark datasets, including XD-Violence and\nUCF-Crime, demonstrate that Live-E2T significantly outperforms state-of-the-art\nmethods in terms of threat detection accuracy, real-time efficiency, and the\ncrucial dimension of explainability.",
        "url": "http://arxiv.org/abs/2509.18571v1",
        "published_date": "2025-09-23T02:53:43+00:00",
        "updated_date": "2025-09-23T02:53:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuhan Wang",
            "Cheng Liu",
            "Zihan Zhao",
            "Weichao Wu"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "Live-E2T is a framework for real-time threat monitoring in videos that combines efficient event reasoning and chain-of-thought processing for improved threat detection accuracy, efficiency, and explainability.",
        "tldr_zh": "Live-E2T是一个用于实时视频威胁监控的框架，结合了高效事件推理和思维链处理，以提高威胁检测准确性、效率和可解释性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction",
        "summary": "Reconstructing dynamic humans together with static scenes from monocular\nvideos remains difficult, especially under fast motion, where RGB frames suffer\nfrom motion blur. Event cameras exhibit distinct advantages, e.g., microsecond\ntemporal resolution, making them a superior sensing choice for dynamic human\nreconstruction. Accordingly, we present a novel event-guided human-scene\nreconstruction framework that jointly models human and scene from a single\nmonocular event camera via 3D Gaussian Splatting. Specifically, a unified set\nof 3D Gaussians carries a learnable semantic attribute; only Gaussians\nclassified as human undergo deformation for animation, while scene Gaussians\nstay static. To combat blur, we propose an event-guided loss that matches\nsimulated brightness changes between consecutive renderings with the event\nstream, improving local fidelity in fast-moving regions. Our approach removes\nthe need for external human masks and simplifies managing separate Gaussian\nsets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers\nstate-of-the-art human-scene reconstruction, with notable gains over strong\nbaselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.",
        "url": "http://arxiv.org/abs/2509.18566v1",
        "published_date": "2025-09-23T02:50:56+00:00",
        "updated_date": "2025-09-23T02:50:56+00:00",
        "categories": [
            "cs.CV",
            "cs.RO",
            "eess.IV"
        ],
        "authors": [
            "Xiaoting Yin",
            "Hao Shi",
            "Kailun Yang",
            "Jiajun Zhai",
            "Shangwei Guo",
            "Lin Wang",
            "Kaiwei Wang"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel event-guided human-scene reconstruction framework using 3D Gaussian Splatting, achieving state-of-the-art results on benchmark datasets for dynamic human and scene reconstruction.",
        "tldr_zh": "本文介绍了一种新颖的事件引导的人体场景重建框架，使用3D高斯喷涂，在动态人体和场景重建方面取得了基准数据集上的最先进结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HadaSmileNet: Hadamard fusion of handcrafted and deep-learning features for enhancing facial emotion recognition of genuine smiles",
        "summary": "The distinction between genuine and posed emotions represents a fundamental\npattern recognition challenge with significant implications for data mining\napplications in social sciences, healthcare, and human-computer interaction.\nWhile recent multi-task learning frameworks have shown promise in combining\ndeep learning architectures with handcrafted D-Marker features for smile facial\nemotion recognition, these approaches exhibit computational inefficiencies due\nto auxiliary task supervision and complex loss balancing requirements. This\npaper introduces HadaSmileNet, a novel feature fusion framework that directly\nintegrates transformer-based representations with physiologically grounded\nD-Markers through parameter-free multiplicative interactions. Through\nsystematic evaluation of 15 fusion strategies, we demonstrate that Hadamard\nmultiplicative fusion achieves optimal performance by enabling direct feature\ninteractions while maintaining computational efficiency. The proposed approach\nestablishes new state-of-the-art results for deep learning methods across four\nbenchmark datasets: UvA-NEMO (88.7 percent, +0.8), MMI (99.7 percent), SPOS\n(98.5 percent, +0.7), and BBC (100 percent, +5.0). Comprehensive computational\nanalysis reveals 26 percent parameter reduction and simplified training\ncompared to multi-task alternatives, while feature visualization demonstrates\nenhanced discriminative power through direct domain knowledge integration. The\nframework's efficiency and effectiveness make it particularly suitable for\npractical deployment in multimedia data mining applications that require\nreal-time affective computing capabilities.",
        "url": "http://arxiv.org/abs/2509.18550v1",
        "published_date": "2025-09-23T02:20:43+00:00",
        "updated_date": "2025-09-23T02:20:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mohammad Junayed Hasan",
            "Nabeel Mohammed",
            "Shafin Rahman",
            "Philipp Koehn"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces HadaSmileNet, a framework that combines deep learning with handcrafted features for facial emotion recognition, achieving state-of-the-art results with improved efficiency.",
        "tldr_zh": "本文引入了HadaSmileNet，一个框架将深度学习与手工特征相结合，用于面部情绪识别，取得了效率提升的最新成果。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "GeoRemover: Removing Objects and Their Causal Visual Artifacts",
        "summary": "Towards intelligent image editing, object removal should eliminate both the\ntarget object and its causal visual artifacts, such as shadows and reflections.\nHowever, existing image appearance-based methods either follow strictly\nmask-aligned training and fail to remove these causal effects which are not\nexplicitly masked, or adopt loosely mask-aligned strategies that lack\ncontrollability and may unintentionally over-erase other objects. We identify\nthat these limitations stem from ignoring the causal relationship between an\nobject's geometry presence and its visual effects. To address this limitation,\nwe propose a geometry-aware two-stage framework that decouples object removal\ninto (1) geometry removal and (2) appearance rendering. In the first stage, we\nremove the object directly from the geometry (e.g., depth) using strictly\nmask-aligned supervision, enabling structure-aware editing with strong\ngeometric constraints. In the second stage, we render a photorealistic RGB\nimage conditioned on the updated geometry, where causal visual effects are\nconsidered implicitly as a result of the modified 3D geometry. To guide\nlearning in the geometry removal stage, we introduce a preference-driven\nobjective based on positive and negative sample pairs, encouraging the model to\nremove objects as well as their causal visual artifacts while avoiding new\nstructural insertions. Extensive experiments demonstrate that our method\nachieves state-of-the-art performance in removing both objects and their\nassociated artifacts on two popular benchmarks. The code is available at\nhttps://github.com/buxiangzhiren/GeoRemover.",
        "url": "http://arxiv.org/abs/2509.18538v1",
        "published_date": "2025-09-23T02:04:19+00:00",
        "updated_date": "2025-09-23T02:04:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zixin Zhu",
            "Haoxiang Li",
            "Xuelu Feng",
            "He Wu",
            "Chunming Qiao",
            "Junsong Yuan"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a two-stage framework, GeoRemover, for object removal in images that considers both object geometry and appearance rendering to remove causal visual artifacts.",
        "tldr_zh": "本文提出了一个两阶段框架，GeoRemover，在图像中进行物体移除，同时考虑物体几何和外观渲染来去除因果视觉伪影。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dynamical Modeling of Behaviorally Relevant Spatiotemporal Patterns in Neural Imaging Data",
        "summary": "High-dimensional imaging of neural activity, such as widefield calcium and\nfunctional ultrasound imaging, provide a rich source of information for\nunderstanding the relationship between brain activity and behavior. Accurately\nmodeling neural dynamics in these modalities is crucial for understanding this\nrelationship but is hindered by the high-dimensionality, complex spatiotemporal\ndependencies, and prevalent behaviorally irrelevant dynamics in these\nmodalities. Existing dynamical models often employ preprocessing steps to\nobtain low-dimensional representations from neural image modalities. However,\nthis process can discard behaviorally relevant information and miss\nspatiotemporal structure. We propose SBIND, a novel data-driven deep learning\nframework to model spatiotemporal dependencies in neural images and disentangle\ntheir behaviorally relevant dynamics from other neural dynamics. We validate\nSBIND on widefield imaging datasets, and show its extension to functional\nultrasound imaging, a recent modality whose dynamical modeling has largely\nremained unexplored. We find that our model effectively identifies both local\nand long-range spatial dependencies across the brain while also dissociating\nbehaviorally relevant neural dynamics. Doing so, SBIND outperforms existing\nmodels in neural-behavioral prediction. Overall, SBIND provides a versatile\ntool for investigating the neural mechanisms underlying behavior using imaging\nmodalities.",
        "url": "http://arxiv.org/abs/2509.18507v1",
        "published_date": "2025-09-23T01:16:23+00:00",
        "updated_date": "2025-09-23T01:16:23+00:00",
        "categories": [
            "q-bio.NC",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Mohammad Hosseini",
            "Maryam M. Shanechi"
        ],
        "ai_categories": [
            "Dataset",
            "Multimodality",
            "Transformer"
        ],
        "tldr": "The paper proposes a novel deep learning framework, SBIND, to model behaviorally relevant spatiotemporal patterns in neural imaging data, outperforming existing models in neural-behavioral prediction.",
        "tldr_zh": "本文提出了一种新颖的深度学习框架SBIND用于模拟神经成像数据中的行为相关时空模式，优于现有模型在神经行为预测方面的表现。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment",
        "summary": "Research on unsupervised domain adaptation (UDA) for semantic segmentation of\nremote sensing images has been extensively conducted. However, research on how\nto achieve domain adaptation in practical scenarios where source domain data is\ninaccessible namely, source-free domain adaptation (SFDA) remains limited.\nSelf-training has been widely used in SFDA, which requires obtaining as many\nhigh-quality pseudo-labels as possible to train models on target domain data.\nMost existing methods optimize the entire pseudo-label set to obtain more\nsupervisory information. However, as pseudo-label sets often contain\nsubstantial noise, simultaneously optimizing all labels is challenging. This\nlimitation undermines the effectiveness of optimization approaches and thus\nrestricts the performance of self-training. To address this, we propose a novel\npseudo-label optimization framework called Diffusion-Guided Label Enrichment\n(DGLE), which starts from a few easily obtained high-quality pseudo-labels and\npropagates them to a complete set of pseudo-labels while ensuring the quality\nof newly generated labels. Firstly, a pseudo-label fusion method based on\nconfidence filtering and super-resolution enhancement is proposed, which\nutilizes cross-validation of details and contextual information to obtain a\nsmall number of high-quality pseudo-labels as initial seeds. Then, we leverage\nthe diffusion model to propagate incomplete seed pseudo-labels with irregular\ndistributions due to its strong denoising capability for randomly distributed\nnoise and powerful modeling capacity for complex distributions, thereby\ngenerating complete and high-quality pseudo-labels. This method effectively\navoids the difficulty of directly optimizing the complete set of pseudo-labels,\nsignificantly improves the quality of pseudo-labels, and thus enhances the\nmodel's performance in the target domain.",
        "url": "http://arxiv.org/abs/2509.18502v1",
        "published_date": "2025-09-23T01:10:01+00:00",
        "updated_date": "2025-09-23T01:10:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenjie Liu",
            "Hongmin Liu",
            "Lixin Zhang",
            "Bin Fan"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper introduces a new method called Diffusion-Guided Label Enrichment for improving pseudo-label quality in source-free domain adaptive semantic segmentation of remote sensing images.",
        "tldr_zh": "该论文提出了一种新方法，称为扩散引导标签丰富，用于改进遥感图像的无源领域自适应语义分割中的伪标签质量。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting for Deformable Intraoperative Surgical Navigation",
        "summary": "We introduce BridgeSplat, a novel approach for deformable surgical navigation\nthat couples intraoperative 3D reconstruction with preoperative CT data to\nbridge the gap between surgical video and volumetric patient data. Our method\nrigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian\nparameters and mesh deformation through photometric supervision. By\nparametrizing each Gaussian relative to its parent mesh triangle, we enforce\nalignment between Gaussians and mesh and obtain deformations that can be\npropagated back to update the CT. We demonstrate BridgeSplat's effectiveness on\nvisceral pig surgeries and synthetic data of a human liver under simulation,\nshowing sensible deformations of the preoperative CT on monocular RGB data.\nCode, data, and additional resources can be found at\nhttps://maxfehrentz.github.io/ct-informed-splatting/ .",
        "url": "http://arxiv.org/abs/2509.18501v1",
        "published_date": "2025-09-23T01:09:36+00:00",
        "updated_date": "2025-09-23T01:09:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Maximilian Fehrentz",
            "Alexander Winkler",
            "Thomas Heiliger",
            "Nazim Haouchine",
            "Christian Heiliger",
            "Nassir Navab"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "BridgeSplat is a new approach for surgical navigation that combines 3D reconstruction with preoperative CT data to align surgical video with patient data, enabling sensible deformations of preoperative CT on RGB data.",
        "tldr_zh": "BridgeSplat是一种新的手术导航方法，将3D重建与术前CT数据结合起来，使手术视频与病人数据对齐，能够在RGB数据上实现术前CT的合理变形。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Differentiable Light Transport with Gaussian Surfels via Adapted Radiosity for Efficient Relighting and Geometry Reconstruction",
        "summary": "Radiance fields have gained tremendous success with applications ranging from\nnovel view synthesis to geometry reconstruction, especially with the advent of\nGaussian splatting. However, they sacrifice modeling of material reflective\nproperties and lighting conditions, leading to significant geometric\nambiguities and the inability to easily perform relighting. One way to address\nthese limitations is to incorporate physically-based rendering, but it has been\nprohibitively expensive to include full global illumination within the inner\nloop of the optimization. Therefore, previous works adopt simplifications that\nmake the whole optimization with global illumination effects efficient but less\naccurate. In this work, we adopt Gaussian surfels as the primitives and build\nan efficient framework for differentiable light transport, inspired from the\nclassic radiosity theory. The whole framework operates in the coefficient space\nof spherical harmonics, enabling both diffuse and specular materials. We extend\nthe classic radiosity into non-binary visibility and semi-opaque primitives,\npropose novel solvers to efficiently solve the light transport, and derive the\nbackward pass for gradient optimizations, which is more efficient than\nauto-differentiation. During inference, we achieve view-independent rendering\nwhere light transport need not be recomputed under viewpoint changes, enabling\nhundreds of FPS for global illumination effects, including view-dependent\nreflections using a spherical harmonics representation. Through extensive\nqualitative and quantitative experiments, we demonstrate superior geometry\nreconstruction, view synthesis and relighting than previous inverse rendering\nbaselines, or data-driven baselines given relatively sparse datasets with known\nor unknown lighting conditions.",
        "url": "http://arxiv.org/abs/2509.18497v1",
        "published_date": "2025-09-23T01:02:31+00:00",
        "updated_date": "2025-09-23T01:02:31+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Kaiwen Jiang",
            "Jia-Mu Sun",
            "Zilu Li",
            "Dan Wang",
            "Tzu-Mao Li",
            "Ravi Ramamoorthi"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper proposes a framework using Gaussian surfels for efficient relighting and geometry reconstruction through differentiable light transport with adapted radiosity.",
        "tldr_zh": "本文提出了一种使用高斯surfel的框架，通过自适应辐射度的可微光传输实现了高效的照明和几何重建。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MK-UNet: Multi-kernel Lightweight CNN for Medical Image Segmentation",
        "summary": "In this paper, we introduce MK-UNet, a paradigm shift towards\nultra-lightweight, multi-kernel U-shaped CNNs tailored for medical image\nsegmentation. Central to MK-UNet is the multi-kernel depth-wise convolution\nblock (MKDC) we design to adeptly process images through multiple kernels,\nwhile capturing complex multi-resolution spatial relationships. MK-UNet also\nemphasizes the images salient features through sophisticated attention\nmechanisms, including channel, spatial, and grouped gated attention. Our\nMK-UNet network, with a modest computational footprint of only 0.316M\nparameters and 0.314G FLOPs, represents not only a remarkably lightweight, but\nalso significantly improved segmentation solution that provides higher accuracy\nover state-of-the-art (SOTA) methods across six binary medical imaging\nbenchmarks. Specifically, MK-UNet outperforms TransUNet in DICE score with\nnearly 333$\\times$ and 123$\\times$ fewer parameters and FLOPs, respectively.\nSimilarly, when compared against UNeXt, MK-UNet exhibits superior segmentation\nperformance, improving the DICE score up to 6.7% margins while operating with\n4.7$\\times$ fewer #Params. Our MK-UNet also outperforms other recent\nlightweight networks, such as MedT, CMUNeXt, EGE-UNet, and Rolling-UNet, with\nmuch lower computational resources. This leap in performance, coupled with\ndrastic computational gains, positions MK-UNet as an unparalleled solution for\nreal-time, high-fidelity medical diagnostics in resource-limited settings, such\nas point-of-care devices. Our implementation is available at\nhttps://github.com/SLDGroup/MK-UNet.",
        "url": "http://arxiv.org/abs/2509.18493v1",
        "published_date": "2025-09-23T00:54:40+00:00",
        "updated_date": "2025-09-23T00:54:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Md Mostafijur Rahman",
            "Radu Marculescu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "MK-UNet is a lightweight CNN designed for medical image segmentation, outperforming state-of-the-art methods with significantly fewer parameters and FLOPs.",
        "tldr_zh": "MK-UNet是一种为医学图像分割设计的轻量级CNN，通过显著减少参数和FLOPs，优于现有技术。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "MoCrop: Training Free Motion Guided Cropping for Efficient Video Action Recognition",
        "summary": "We introduce MoCrop, a motion-aware adaptive cropping module for efficient\nvideo action recognition in the compressed domain. MoCrop uses motion vectors\nthat are available in H.264 video to locate motion-dense regions and produces a\nsingle clip-level crop that is applied to all I-frames at inference. The module\nis training free, adds no parameters, and can be plugged into diverse\nbackbones. A lightweight pipeline that includes denoising & merge (DM), Monte\nCarlo sampling (MCS), and adaptive cropping (AC) via a motion-density submatrix\nsearch yields robust crops with negligible overhead. On UCF101, MoCrop improves\naccuracy or reduces compute. With ResNet-50, it delivers +3.5% Top-1 accuracy\nat equal FLOPs (attention setting), or +2.4% Top-1 accuracy with 26.5% fewer\nFLOPs (efficiency setting). Applied to CoViAR, it reaches 89.2% Top-1 accuracy\nat the original cost and 88.5% Top-1 accuracy while reducing compute from 11.6\nto 8.5 GFLOPs. Consistent gains on MobileNet-V3, EfficientNet-B1, and Swin-B\nindicate strong generality and make MoCrop practical for real-time deployment\nin the compressed domain. Our code and models are available at\nhttps://github.com/microa/MoCrop.",
        "url": "http://arxiv.org/abs/2509.18473v1",
        "published_date": "2025-09-22T23:23:04+00:00",
        "updated_date": "2025-09-22T23:23:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Binhua Huang",
            "Wendong Yao",
            "Shaowu Chen",
            "Guoxin Wang",
            "Qingyuan Wang",
            "Soumyabrata Dev"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "MoCrop is a motion-aware adaptive cropping module for video action recognition that uses motion vectors to locate motion-dense regions without the need for training, adding no parameters. It improves accuracy and reduces computation cost across different backbone networks.",
        "tldr_zh": "MoCrop是一个运动感知自适应裁剪模块，用于视频动作识别，利用运动矢量定位运动密集区域，无需训练和添加参数。它在不同骨干网络中提高准确性并降低计算成本。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Zero-Shot Visual Deepfake Detection: Can AI Predict and Prevent Fake Content Before It's Created?",
        "summary": "Generative adversarial networks (GANs) and diffusion models have dramatically\nadvanced deepfake technology, and its threats to digital security, media\nintegrity, and public trust have increased rapidly. This research explored\nzero-shot deepfake detection, an emerging method even when the models have\nnever seen a particular deepfake variation. In this work, we studied\nself-supervised learning, transformer-based zero-shot classifier, generative\nmodel fingerprinting, and meta-learning techniques that better adapt to the\never-evolving deepfake threat. In addition, we suggested AI-driven prevention\nstrategies that mitigated the underlying generation pipeline of the deepfakes\nbefore they occurred. They consisted of adversarial perturbations for creating\ndeepfake generators, digital watermarking for content authenticity\nverification, real-time AI monitoring for content creation pipelines, and\nblockchain-based content verification frameworks. Despite these advancements,\nzero-shot detection and prevention faced critical challenges such as\nadversarial attacks, scalability constraints, ethical dilemmas, and the absence\nof standardized evaluation benchmarks. These limitations were addressed by\ndiscussing future research directions on explainable AI for deepfake detection,\nmultimodal fusion based on image, audio, and text analysis, quantum AI for\nenhanced security, and federated learning for privacy-preserving deepfake\ndetection. This further highlighted the need for an integrated defense\nframework for digital authenticity that utilized zero-shot learning in\ncombination with preventive deepfake mechanisms. Finally, we highlighted the\nimportant role of interdisciplinary collaboration between AI researchers,\ncybersecurity experts, and policymakers to create resilient defenses against\nthe rising tide of deepfake attacks.",
        "url": "http://arxiv.org/abs/2509.18461v1",
        "published_date": "2025-09-22T22:33:16+00:00",
        "updated_date": "2025-09-22T22:33:16+00:00",
        "categories": [
            "cs.GR",
            "cs.AI",
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Ayan Sar",
            "Sampurna Roy",
            "Tanupriya Choudhury",
            "Ajith Abraham"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Diffusion",
            "AIGC",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper explores zero-shot deepfake detection and prevention using various AI techniques and strategies to combat the evolving threat of deepfake technology.",
        "tldr_zh": "该论文探讨了使用各种人工智能技术和策略来抵御不断发展的深度伪造技术所采用的零样本深伪造检测和预防。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "CPT-4DMR: Continuous sPatial-Temporal Representation for 4D-MRI Reconstruction",
        "summary": "Four-dimensional MRI (4D-MRI) is an promising technique for capturing\nrespiratory-induced motion in radiation therapy planning and delivery.\nConventional 4D reconstruction methods, which typically rely on phase binning\nor separate template scans, struggle to capture temporal variability,\ncomplicate workflows, and impose heavy computational loads. We introduce a\nneural representation framework that considers respiratory motion as a smooth,\ncontinuous deformation steered by a 1D surrogate signal, completely replacing\nthe conventional discrete sorting approach. The new method fuses motion\nmodeling with image reconstruction through two synergistic networks: the\nSpatial Anatomy Network (SAN) encodes a continuous 3D anatomical\nrepresentation, while a Temporal Motion Network (TMN), guided by\nTransformer-derived respiratory signals, produces temporally consistent\ndeformation fields. Evaluation using a free-breathing dataset of 19 volunteers\ndemonstrates that our template- and phase-free method accurately captures both\nregular and irregular respiratory patterns, while preserving vessel and\nbronchial continuity with high anatomical fidelity. The proposed method\nsignificantly improves efficiency, reducing the total processing time from\napproximately five hours required by conventional discrete sorting methods to\njust 15 minutes of training. Furthermore, it enables inference of each 3D\nvolume in under one second. The framework accurately reconstructs 3D images at\nany respiratory state, achieves superior performance compared to conventional\nmethods, and demonstrates strong potential for application in 4D radiation\ntherapy planning and real-time adaptive treatment.",
        "url": "http://arxiv.org/abs/2509.18427v1",
        "published_date": "2025-09-22T21:18:26+00:00",
        "updated_date": "2025-09-22T21:18:26+00:00",
        "categories": [
            "cs.CV",
            "physics.med-ph"
        ],
        "authors": [
            "Xinyang Wu",
            "Muheng Li",
            "Xia Li",
            "Orso Pusterla",
            "Sairos Safai",
            "Philippe C. Cattin",
            "Antony J. Lomax",
            "Ye Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a new method for 4D-MRI reconstruction that improves efficiency and accuracy by considering respiratory motion as a continuous deformation guided by a surrogate signal, achieving high anatomical fidelity and reducing processing time significantly.",
        "tldr_zh": "本文介绍了一种新的4D-MRI重建方法，通过将呼吸运动视为连续变形并由替代信号引导，提高了效率和准确性，实现了高解剖学保真度并显著减少了处理时间。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Check Field Detection Agent (CFD-Agent) using Multimodal Large Language and Vision Language Models",
        "summary": "Checks remain a foundational instrument in the financial ecosystem,\nfacilitating substantial transaction volumes across institutions. However,\ntheir continued use also renders them a persistent target for fraud,\nunderscoring the importance of robust check fraud detection mechanisms. At the\ncore of such systems lies the accurate identification and localization of\ncritical fields, such as the signature, magnetic ink character recognition\n(MICR) line, courtesy amount, legal amount, payee, and payer, which are\nessential for subsequent verification against reference checks belonging to the\nsame customer. This field-level detection is traditionally dependent on object\ndetection models trained on large, diverse, and meticulously labeled datasets,\na resource that is scarce due to proprietary and privacy concerns. In this\npaper, we introduce a novel, training-free framework for automated check field\ndetection, leveraging the power of a vision language model (VLM) in conjunction\nwith a multimodal large language model (MLLM). Our approach enables zero-shot\ndetection of check components, significantly lowering the barrier to deployment\nin real-world financial settings. Quantitative evaluation of our model on a\nhand-curated dataset of 110 checks spanning multiple formats and layouts\ndemonstrates strong performance and generalization capability. Furthermore,\nthis framework can serve as a bootstrap mechanism for generating high-quality\nlabeled datasets, enabling the development of specialized real-time object\ndetection models tailored to institutional needs.",
        "url": "http://arxiv.org/abs/2509.18405v1",
        "published_date": "2025-09-22T20:43:59+00:00",
        "updated_date": "2025-09-22T20:43:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sourav Halder",
            "Jinjun Tong",
            "Xinyu Wu"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a novel framework for automated check field detection using vision and language models, enabling zero-shot detection of check components and generating high-quality labeled datasets.",
        "tldr_zh": "本文介绍了一种利用视觉和语言模型进行自动支票字段检测的新框架，实现了支票元素的零拍摄检测，并生成了高质量的标记数据集。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Improving the color accuracy of lighting estimation models",
        "summary": "Advances in high dynamic range (HDR) lighting estimation from a single image\nhave opened new possibilities for augmented reality (AR) applications.\nPredicting complex lighting environments from a single input image allows for\nthe realistic rendering and compositing of virtual objects. In this work, we\ninvestigate the color robustness of such methods -- an often overlooked yet\ncritical factor for achieving visual realism. While most evaluations conflate\ncolor with other lighting attributes (e.g., intensity, direction), we isolate\ncolor as the primary variable of interest. Rather than introducing a new\nlighting estimation algorithm, we explore whether simple adaptation techniques\ncan enhance the color accuracy of existing models. Using a novel HDR dataset\nfeaturing diverse lighting colors, we systematically evaluate several\nadaptation strategies. Our results show that preprocessing the input image with\na pre-trained white balance network improves color robustness, outperforming\nother strategies across all tested scenarios. Notably, this approach requires\nno retraining of the lighting estimation model. We further validate the\ngenerality of this finding by applying the technique to three state-of-the-art\nlighting estimation methods from recent literature.",
        "url": "http://arxiv.org/abs/2509.18390v1",
        "published_date": "2025-09-22T20:23:33+00:00",
        "updated_date": "2025-09-22T20:23:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zitian Zhang",
            "Joshua Urban Davis",
            "Jeanne Phuong Anh Vu",
            "Jiangtao Kuang",
            "Jean-François Lalonde"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper investigates techniques to improve the color accuracy of lighting estimation models for augmented reality applications without requiring retraining of the existing models.",
        "tldr_zh": "该论文研究了一种在增强现实应用中改进光照估计模型颜色准确性的技术，而无需重复训练现有模型。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Neural Network-Driven Direct CBCT-Based Dose Calculation for Head-and-Neck Proton Treatment Planning",
        "summary": "Accurate dose calculation on cone beam computed tomography (CBCT) images is\nessential for modern proton treatment planning workflows, particularly when\naccounting for inter-fractional anatomical changes in adaptive treatment\nscenarios. Traditional CBCT-based dose calculation suffers from image quality\nlimitations, requiring complex correction workflows. This study develops and\nvalidates a deep learning approach for direct proton dose calculation from CBCT\nimages using extended Long Short-Term Memory (xLSTM) neural networks. A\nretrospective dataset of 40 head-and-neck cancer patients with paired planning\nCT and treatment CBCT images was used to train an xLSTM-based neural network\n(CBCT-NN). The architecture incorporates energy token encoding and\nbeam's-eye-view sequence modelling to capture spatial dependencies in proton\ndose deposition patterns. Training utilized 82,500 paired beam configurations\nwith Monte Carlo-generated ground truth doses. Validation was performed on 5\nindependent patients using gamma analysis, mean percentage dose error\nassessment, and dose-volume histogram comparison. The CBCT-NN achieved gamma\npass rates of 95.1 $\\pm$ 2.7% using 2mm/2% criteria. Mean percentage dose\nerrors were 2.6 $\\pm$ 1.4% in high-dose regions ($>$90% of max dose) and 5.9\n$\\pm$ 1.9% globally. Dose-volume histogram analysis showed excellent\npreservation of target coverage metrics (Clinical Target Volume V95%\ndifference: -0.6 $\\pm$ 1.1%) and organ-at-risk constraints (parotid mean dose\ndifference: -0.5 $\\pm$ 1.5%). Computation time is under 3 minutes without\nsacrificing Monte Carlo-level accuracy. This study demonstrates the\nproof-of-principle of direct CBCT-based proton dose calculation using xLSTM\nneural networks. The approach eliminates traditional correction workflows while\nachieving comparable accuracy and computational efficiency suitable for\nadaptive protocols.",
        "url": "http://arxiv.org/abs/2509.18378v1",
        "published_date": "2025-09-22T20:01:32+00:00",
        "updated_date": "2025-09-22T20:01:32+00:00",
        "categories": [
            "physics.med-ph",
            "cs.CV"
        ],
        "authors": [
            "Muheng Li",
            "Evangelia Choulilitsa",
            "Lisa Fankhauser",
            "Francesca Albertini",
            "Antony Lomax",
            "Ye Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper presents a deep learning approach for direct proton dose calculation from CBCT images using xLSTM neural networks, achieving high accuracy and computational efficiency for adaptive treatment planning.",
        "tldr_zh": "该论文提出了一种深度学习方法，使用xLSTM神经网络从CBCT图像直接计算质子剂量，实现高精度和计算效率，适用于自适应治疗规划。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Single Image Is All You Need: Zero-Shot Anomaly Localization Without Training Data",
        "summary": "Anomaly detection in images is typically addressed by learning from\ncollections of training data or relying on reference samples. In many\nreal-world scenarios, however, such training data may be unavailable, and only\nthe test image itself is provided. We address this zero-shot setting by\nproposing a single-image anomaly localization method that leverages the\ninductive bias of convolutional neural networks, inspired by Deep Image Prior\n(DIP). Our method is named Single Shot Decomposition Network (SSDnet). Our key\nassumption is that natural images often exhibit unified textures and patterns,\nand that anomalies manifest as localized deviations from these repetitive or\nstochastic patterns. To learn the deep image prior, we design a patch-based\ntraining framework where the input image is fed directly into the network for\nself-reconstruction, rather than mapping random noise to the image as done in\nDIP. To avoid the model simply learning an identity mapping, we apply masking,\npatch shuffling, and small Gaussian noise. In addition, we use a perceptual\nloss based on inner-product similarity to capture structure beyond pixel\nfidelity. Our approach needs no external training data, labels, or references,\nand remains robust in the presence of noise or missing pixels. SSDnet achieves\n0.99 AUROC and 0.60 AUPRC on MVTec-AD and 0.98 AUROC and 0.67 AUPRC on the\nfabric dataset, outperforming state-of-the-art methods. The implementation code\nwill be released at https://github.com/mehrdadmoradi124/SSDnet",
        "url": "http://arxiv.org/abs/2509.18354v1",
        "published_date": "2025-09-22T19:29:20+00:00",
        "updated_date": "2025-09-22T19:29:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "eess.IV",
            "62H35, 68T07, 62M40, 68T45",
            "I.2.6; I.2.10; I.4.6; I.4.8; I.5.1; I.5.4"
        ],
        "authors": [
            "Mehrdad Moradi",
            "Shengzhe Chen",
            "Hao Yan",
            "Kamran Paynabar"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper presents a method, SSDnet, for anomaly detection in images without the need for training data, achieving high performance on benchmark datasets.",
        "tldr_zh": "该论文提出了一种名为SSDnet的方法，用于在图像中检测异常，无需训练数据，在基准数据集上表现出色。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Learning Contrastive Multimodal Fusion with Improved Modality Dropout for Disease Detection and Prediction",
        "summary": "As medical diagnoses increasingly leverage multimodal data, machine learning\nmodels are expected to effectively fuse heterogeneous information while\nremaining robust to missing modalities. In this work, we propose a novel\nmultimodal learning framework that integrates enhanced modalities dropout and\ncontrastive learning to address real-world limitations such as modality\nimbalance and missingness. Our approach introduces learnable modality tokens\nfor improving missingness-aware fusion of modalities and augments conventional\nunimodal contrastive objectives with fused multimodal representations. We\nvalidate our framework on large-scale clinical datasets for disease detection\nand prediction tasks, encompassing both visual and tabular modalities.\nExperimental results demonstrate that our method achieves state-of-the-art\nperformance, particularly in challenging and practical scenarios where only a\nsingle modality is available. Furthermore, we show its adaptability through\nsuccessful integration with a recent CT foundation model. Our findings\nhighlight the effectiveness, efficiency, and generalizability of our approach\nfor multimodal learning, offering a scalable, low-cost solution with\nsignificant potential for real-world clinical applications. The code is\navailable at https://github.com/omron-sinicx/medical-modality-dropout.",
        "url": "http://arxiv.org/abs/2509.18284v1",
        "published_date": "2025-09-22T18:12:12+00:00",
        "updated_date": "2025-09-22T18:12:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yi Gu",
            "Kuniaki Saito",
            "Jiaxin Ma"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a multimodal learning framework that combines modalities dropout and contrastive learning for disease detection and prediction, showing state-of-the-art performance and adaptability to single modality scenarios.",
        "tldr_zh": "本文引入一种多模态学习框架，结合了模态丢失和对比学习，用于疾病检测和预测，展示出了在单模态场景下的领先性能和适应性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching",
        "summary": "Conditional generative modeling aims to learn a conditional data distribution\nfrom samples containing data-condition pairs. For this, diffusion and\nflow-based methods have attained compelling results. These methods use a\nlearned (flow) model to transport an initial standard Gaussian noise that\nignores the condition to the conditional data distribution. The model is hence\nrequired to learn both mass transport and conditional injection. To ease the\ndemand on the model, we propose Condition-Aware Reparameterization for Flow\nMatching (CAR-Flow) -- a lightweight, learned shift that conditions the source,\nthe target, or both distributions. By relocating these distributions, CAR-Flow\nshortens the probability path the model must learn, leading to faster training\nin practice. On low-dimensional synthetic data, we visualize and quantify the\neffects of CAR. On higher-dimensional natural image data (ImageNet-256),\nequipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while\nintroducing less than 0.6% additional parameters.",
        "url": "http://arxiv.org/abs/2509.19300v1",
        "published_date": "2025-09-23T17:59:31+00:00",
        "updated_date": "2025-09-23T17:59:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chen Chen",
            "Pengsheng Guo",
            "Liangchen Song",
            "Jiasen Lu",
            "Rui Qian",
            "Xinze Wang",
            "Tsu-Jui Fu",
            "Wei Liu",
            "Yinfei Yang",
            "Alex Schwing"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "AIGC"
        ],
        "tldr": "CAR-Flow proposes a lightweight shift method to improve flow matching by conditioning the source and target distributions, leading to faster training and improved performance on ImageNet-256 data.",
        "tldr_zh": "CAR-Flow提出了一种轻量级位移方法，通过使源和目标分布适应条件来改进流匹配，在ImageNet-256数据上实现了更快的训练和更好的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.75
    },
    {
        "title": "Benchmarking Vision-Language and Multimodal Large Language Models in Zero-shot and Few-shot Scenarios: A study on Christian Iconography",
        "summary": "This study evaluates the capabilities of Multimodal Large Language Models\n(LLMs) and Vision Language Models (VLMs) in the task of single-label\nclassification of Christian Iconography. The goal was to assess whether\ngeneral-purpose VLMs (CLIP and SigLIP) and LLMs, such as GPT-4o and Gemini 2.5,\ncan interpret the Iconography, typically addressed by supervised classifiers,\nand evaluate their performance. Two research questions guided the analysis:\n(RQ1) How do multimodal LLMs perform on image classification of Christian\nsaints? And (RQ2), how does performance vary when enriching input with\ncontextual information or few-shot exemplars? We conducted a benchmarking study\nusing three datasets supporting Iconclass natively: ArtDL, ICONCLASS, and\nWikidata, filtered to include the top 10 most frequent classes. Models were\ntested under three conditions: (1) classification using class labels, (2)\nclassification with Iconclass descriptions, and (3) few-shot learning with five\nexemplars. Results were compared against ResNet50 baselines fine-tuned on the\nsame datasets. The findings show that Gemini-2.5 Pro and GPT-4o outperformed\nthe ResNet50 baselines. Accuracy dropped significantly on the Wikidata dataset,\nwhere Siglip reached the highest accuracy score, suggesting model sensitivity\nto image size and metadata alignment. Enriching prompts with class descriptions\ngenerally improved zero-shot performance, while few-shot learning produced\nlower results, with only occasional and minimal increments in accuracy. We\nconclude that general-purpose multimodal LLMs are capable of classification in\nvisually complex cultural heritage domains. These results support the\napplication of LLMs as metadata curation tools in digital humanities workflows,\nsuggesting future research on prompt optimization and the expansion of the\nstudy to other classification strategies and models.",
        "url": "http://arxiv.org/abs/2509.18839v1",
        "published_date": "2025-09-23T09:23:31+00:00",
        "updated_date": "2025-09-23T09:23:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Gianmarco Spinaci",
            "Lukas Klic",
            "Giovanni Colavizza"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper evaluates the performance of Multimodal Large Language Models and Vision Language Models in classifying Christian Iconography using different datasets and conditions.",
        "tldr_zh": "本文评估了使用不同数据集和条件，多模态大语言模型和视觉语言模型在对基督教图像进行分类时的表现。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.75
    },
    {
        "title": "YOLO-LAN: Precise Polyp Detection via Optimized Loss, Augmentations and Negatives",
        "summary": "Colorectal cancer (CRC), a lethal disease, begins with the growth of abnormal\nmucosal cell proliferation called polyps in the inner wall of the colon. When\nleft undetected, polyps can become malignant tumors. Colonoscopy is the\nstandard procedure for detecting polyps, as it enables direct visualization and\nremoval of suspicious lesions. Manual detection by colonoscopy can be\ninconsistent and is subject to oversight. Therefore, object detection based on\ndeep learning offers a better solution for a more accurate and real-time\ndiagnosis during colonoscopy. In this work, we propose YOLO-LAN, a YOLO-based\npolyp detection pipeline, trained using M2IoU loss, versatile data\naugmentations and negative data to replicate real clinical situations. Our\npipeline outperformed existing methods for the Kvasir-seg and BKAI-IGH NeoPolyp\ndatasets, achieving mAP$_{50}$ of 0.9619, mAP$_{50:95}$ of 0.8599 with YOLOv12\nand mAP$_{50}$ of 0.9540, mAP$_{50:95}$ of 0.8487 with YOLOv8 on the Kvasir-seg\ndataset. The significant increase is achieved in mAP$_{50:95}$ score, showing\nthe precision of polyp detection. We show robustness based on polyp size and\nprecise location detection, making it clinically relevant in AI-assisted\ncolorectal screening.",
        "url": "http://arxiv.org/abs/2509.19166v1",
        "published_date": "2025-09-23T15:41:44+00:00",
        "updated_date": "2025-09-23T15:41:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Siddharth Gupta",
            "Jitin Singla"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes YOLO-LAN, a YOLO-based polyp detection pipeline trained using optimized loss, augmentations, and negatives, outperforming existing methods for polyp detection.",
        "tldr_zh": "该论文提出了YOLO-LAN，一个基于YOLO的息肉检测管道，使用优化的损失、数据增强和负样本训练，优于现有的息肉检测方法。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Seeing Through Reflections: Advancing 3D Scene Reconstruction in Mirror-Containing Environments with Gaussian Splatting",
        "summary": "Mirror-containing environments pose unique challenges for 3D reconstruction\nand novel view synthesis (NVS), as reflective surfaces introduce view-dependent\ndistortions and inconsistencies. While cutting-edge methods such as Neural\nRadiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical\nscenes, their performance deteriorates in the presence of mirrors. Existing\nsolutions mainly focus on handling mirror surfaces through symmetry mapping but\noften overlook the rich information carried by mirror reflections. These\nreflections offer complementary perspectives that can fill in absent details\nand significantly enhance reconstruction quality. To advance 3D reconstruction\nin mirror-rich environments, we present MirrorScene3D, a comprehensive dataset\nfeaturing diverse indoor scenes, 1256 high-quality images, and annotated mirror\nmasks, providing a benchmark for evaluating reconstruction methods in\nreflective settings. Building on this, we propose ReflectiveGS, an extension of\n3D Gaussian Splatting that utilizes mirror reflections as complementary\nviewpoints rather than simple symmetry artifacts, enhancing scene geometry and\nrecovering absent details. Experiments on MirrorScene3D show that\nReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and\ntraining speed, setting a new benchmark for 3D reconstruction in mirror-rich\nenvironments.",
        "url": "http://arxiv.org/abs/2509.18956v1",
        "published_date": "2025-09-23T13:06:00+00:00",
        "updated_date": "2025-09-23T13:06:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zijing Guo",
            "Yunyang Zhao",
            "Lin Wang"
        ],
        "ai_categories": [
            "Dataset",
            "GAN"
        ],
        "tldr": "The paper presents a new method, ReflectiveGS, that uses mirror reflections to improve 3D scene reconstruction in mirror-rich environments, outperforming existing methods and setting a new benchmark in benchmarks.",
        "tldr_zh": "该论文提出了一种新方法，ReflectiveGS，利用镜面反射改进镜面丰富环境中的3D场景重建，在基准测试中表现优越，树立了新的标杆。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation",
        "summary": "LiDAR-based localization and SLAM often rely on iterative matching\nalgorithms, particularly the Iterative Closest Point (ICP) algorithm, to align\nsensor data with pre-existing maps or previous scans. However, ICP is prone to\nerrors in featureless environments and dynamic scenes, leading to inaccurate\npose estimation. Accurately predicting the uncertainty associated with ICP is\ncrucial for robust state estimation but remains challenging, as existing\napproaches often rely on handcrafted models or simplified assumptions.\nMoreover, a few deep learning-based methods for localizability estimation\neither depend on a pre-built map, which may not always be available, or provide\na binary classification of localizable versus non-localizable, which fails to\nproperly model uncertainty. In this work, we propose a data-driven framework\nthat leverages deep learning to estimate the registration error covariance of\nICP before matching, even in the absence of a reference map. By associating\neach LiDAR scan with a reliable 6-DoF error covariance estimate, our method\nenables seamless integration of ICP within Kalman filtering, enhancing\nlocalization accuracy and robustness. Extensive experiments on the KITTI\ndataset demonstrate the effectiveness of our approach, showing that it\naccurately predicts covariance and, when applied to localization using a\npre-built map or SLAM, reduces localization errors and improves robustness.",
        "url": "http://arxiv.org/abs/2509.18954v1",
        "published_date": "2025-09-23T13:02:44+00:00",
        "updated_date": "2025-09-23T13:02:44+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Minoo Dolatabadi",
            "Fardin Ayar",
            "Ehsan Javanmardi",
            "Manabu Tsukada",
            "Mahdi Javanmardi"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a deep learning-based framework to estimate registration errors in LiDAR localization without the need for a pre-built map, improving accuracy and robustness.",
        "tldr_zh": "本文提出了一种基于深度学习的框架，用于估计LiDAR定位中的注册错误，无需预先构建地图，从而提高准确性和鲁棒性。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning",
        "summary": "While deep learning, including Convolutional Neural Networks (CNNs) and\nVision Transformers (ViTs), has significantly advanced classification\nperformance, its typical reliance on extensive annotated datasets presents a\nmajor obstacle in many practical scenarios where such data is scarce.\nVision-language models (VLMs) and transfer learning with pre-trained visual\nmodels appear as promising techniques to deal with this problem. This paper\nproposes a novel zero-shot image classification framework that combines a VLM\nand a pre-trained visual model within a self-learning cycle. Requiring only the\nset of class names and no labeled training data, our method utilizes a\nconfidence-based pseudo-labeling strategy to train a lightweight classifier\ndirectly on the test data, enabling dynamic adaptation. The VLM identifies\nhigh-confidence samples, and the pre-trained visual model enhances their visual\nrepresentations. These enhanced features then iteratively train the classifier,\nallowing the system to capture complementary semantic and visual cues without\nsupervision. Notably, our approach avoids VLM fine-tuning and the use of large\nlanguage models, relying on the visual-only model to reduce the dependence on\nsemantic representation. Experimental evaluations on ten diverse datasets\ndemonstrate that our approach outperforms the baseline zero-shot method.",
        "url": "http://arxiv.org/abs/2509.18938v1",
        "published_date": "2025-09-23T12:54:52+00:00",
        "updated_date": "2025-09-23T12:54:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Matheus Vinícius Todescato",
            "Joel Luís Carbonera"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "Proposed novel zero-shot image classification framework combining Vision-language models and pre-trained visual models without labeled training data to outperform baseline methods on diverse datasets.",
        "tldr_zh": "提出了一种新型的零样本图像分类框架，结合了视觉语言模型和预训练视觉模型，在不需要标记的训练数据的情况下，在多样的数据集上优于基线方法。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "RS3DBench: A Comprehensive Benchmark for 3D Spatial Perception in Remote Sensing",
        "summary": "In this paper, we introduce a novel benchmark designed to propel the\nadvancement of general-purpose, large-scale 3D vision models for remote sensing\nimagery. While several datasets have been proposed within the realm of remote\nsensing, many existing collections either lack comprehensive depth information\nor fail to establish precise alignment between depth data and remote sensing\nimages. To address this deficiency, we present a visual Benchmark for 3D\nunderstanding of Remotely Sensed images, dubbed RS3DBench. This dataset\nencompasses 54,951 pairs of remote sensing images and pixel-level aligned depth\nmaps, accompanied by corresponding textual descriptions, spanning a broad array\nof geographical contexts. It serves as a tool for training and assessing 3D\nvisual perception models within remote sensing image spatial understanding\ntasks. Furthermore, we introduce a remotely sensed depth estimation model\nderived from stable diffusion, harnessing its multimodal fusion capabilities,\nthereby delivering state-of-the-art performance on our dataset. Our endeavor\nseeks to make a profound contribution to the evolution of 3D visual perception\nmodels and the advancement of geographic artificial intelligence within the\nremote sensing domain. The dataset, models and code will be accessed on the\nhttps://rs3dbench.github.io.",
        "url": "http://arxiv.org/abs/2509.18897v1",
        "published_date": "2025-09-23T11:20:51+00:00",
        "updated_date": "2025-09-23T11:20:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiayu Wang",
            "Ruizhi Wang",
            "Jie Song",
            "Haofei Zhang",
            "Mingli Song",
            "Zunlei Feng",
            "Li Sun"
        ],
        "ai_categories": [
            "Dataset",
            "Multimodality",
            "Diffusion",
            "AIGC"
        ],
        "tldr": "The paper introduces a new benchmark, RS3DBench, for 3D spatial perception in remote sensing, along with a depth estimation model using stable diffusion for improved performance.",
        "tldr_zh": "本文介绍了一个新的基准，RS3DBench，用于遥感中的3D空间感知，并提出了一种使用稳定扩散的深度估计模型，以提高性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "TriFusion-AE: Language-Guided Depth and LiDAR Fusion for Robust Point Cloud Processing",
        "summary": "LiDAR-based perception is central to autonomous driving and robotics, yet raw\npoint clouds remain highly vulnerable to noise, occlusion, and adversarial\ncorruptions. Autoencoders offer a natural framework for denoising and\nreconstruction, but their performance degrades under challenging real-world\nconditions. In this work, we propose TriFusion-AE, a multimodal cross-attention\nautoencoder that integrates textual priors, monocular depth maps from\nmulti-view images, and LiDAR point clouds to improve robustness. By aligning\nsemantic cues from text, geometric (depth) features from images, and spatial\nstructure from LiDAR, TriFusion-AE learns representations that are resilient to\nstochastic noise and adversarial perturbations. Interestingly, while showing\nlimited gains under mild perturbations, our model achieves significantly more\nrobust reconstruction under strong adversarial attacks and heavy noise, where\nCNN-based autoencoders collapse. We evaluate on the nuScenes-mini dataset to\nreflect realistic low-data deployment scenarios. Our multimodal fusion\nframework is designed to be model-agnostic, enabling seamless integration with\nany CNN-based point cloud autoencoder for joint representation learning.",
        "url": "http://arxiv.org/abs/2509.18743v1",
        "published_date": "2025-09-23T07:37:28+00:00",
        "updated_date": "2025-09-23T07:37:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Susmit Neogi"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "TriFusion-AE proposes a multimodal cross-attention autoencoder for robust point cloud processing by integrating textual priors, monocular depth maps, and LiDAR point clouds. It shows improved robustness to noise and adversarial attacks compared to traditional CNN-based methods.",
        "tldr_zh": "TriFusion-AE提出了一种多模态交叉注意力自动编码器，用于融合文本先验信息、单眼深度图和LiDAR点云，以提高对噪声和对抗攻击的鲁棒性。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7.5
    },
    {
        "title": "Lightweight Vision Transformer with Window and Spatial Attention for Food Image Classification",
        "summary": "With the rapid development of society and continuous advances in science and\ntechnology, the food industry increasingly demands higher production quality\nand efficiency. Food image classification plays a vital role in enabling\nautomated quality control on production lines, supporting food safety\nsupervision, and promoting intelligent agricultural production. However, this\ntask faces challenges due to the large number of parameters and high\ncomputational complexity of Vision Transformer models. To address these issues,\nwe propose a lightweight food image classification algorithm that integrates a\nWindow Multi-Head Attention Mechanism (WMHAM) and a Spatial Attention Mechanism\n(SAM). The WMHAM reduces computational cost by capturing local and global\ncontextual features through efficient window partitioning, while the SAM\nadaptively emphasizes key spatial regions to improve discriminative feature\nrepresentation. Experiments conducted on the Food-101 and Vireo Food-172\ndatasets demonstrate that our model achieves accuracies of 95.24% and 94.33%,\nrespectively, while significantly reducing parameters and FLOPs compared with\nbaseline methods. These results confirm that the proposed approach achieves an\neffective balance between computational efficiency and classification\nperformance, making it well-suited for deployment in resource-constrained\nenvironments.",
        "url": "http://arxiv.org/abs/2509.18692v1",
        "published_date": "2025-09-23T06:23:50+00:00",
        "updated_date": "2025-09-23T06:23:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinle Gao",
            "Linghui Ye",
            "Zhiyong Xiao"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a lightweight food image classification algorithm using Window Multi-Head Attention and Spatial Attention mechanisms to improve efficiency and accuracy.",
        "tldr_zh": "本文提出了一种轻量级食品图像分类算法，利用窗口多头注意力和空间注意力机制来提高效率和准确性。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "OraPO: Oracle-educated Reinforcement Learning for Data-efficient and Factual Radiology Report Generation",
        "summary": "Radiology report generation (RRG) aims to automatically produce clinically\nfaithful reports from chest X-ray images. Prevailing work typically follows a\nscale-driven paradigm, by multi-stage training over large paired corpora and\noversized backbones, making pipelines highly data- and compute-intensive. In\nthis paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based\nreward (FactS) to tackle the RRG task under constrained budgets. OraPO enables\nsingle-stage, RL-only training by converting failed GRPO explorations on rare\nor difficult studies into direct preference supervision via a lightweight\noracle step. FactS grounds learning in diagnostic evidence by extracting atomic\nclinical facts and checking entailment against ground-truth labels, yielding\ndense, interpretable sentence-level rewards. Together, OraPO and FactS create a\ncompact and powerful framework that significantly improves learning efficiency\non clinically challenging cases, setting the new SOTA performance on the\nCheXpert Plus dataset (0.341 in F1) with 2--3 orders of magnitude less training\ndata using a small base VLM on modest hardware.",
        "url": "http://arxiv.org/abs/2509.18600v1",
        "published_date": "2025-09-23T03:42:26+00:00",
        "updated_date": "2025-09-23T03:42:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Zhuoxiao Chen",
            "Hongyang Yu",
            "Ying Xu",
            "Yadan Luo",
            "Long Duong",
            "Yuan-Fang Li"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces OraPO, a novel framework for generating radiology reports using reinforcement learning and diagnostic evidence, achieving state-of-the-art performance with significantly less training data.",
        "tldr_zh": "本文介绍了OraPO，一种利用强化学习和诊断证据生成放射学报告的新框架，使用显著较少的训练数据实现了最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "Efficient Breast and Ovarian Cancer Classification via ViT-Based Preprocessing and Transfer Learning",
        "summary": "Cancer is one of the leading health challenges for women, specifically breast\nand ovarian cancer. Early detection can help improve the survival rate through\ntimely intervention and treatment. Traditional methods of detecting cancer\ninvolve manually examining mammograms, CT scans, ultrasounds, and other imaging\ntypes. However, this makes the process labor-intensive and requires the\nexpertise of trained pathologists. Hence, making it both time-consuming and\nresource-intensive. In this paper, we introduce a novel vision transformer\n(ViT)-based method for detecting and classifying breast and ovarian cancer. We\nuse a pre-trained ViT-Base-Patch16-224 model, which is fine-tuned for both\nbinary and multi-class classification tasks using publicly available\nhistopathological image datasets. Further, we use a preprocessing pipeline that\nconverts raw histophological images into standardized PyTorch tensors, which\nare compatible with the ViT architecture and also help improve the model\nperformance. We evaluated the performance of our model on two benchmark\ndatasets: the BreakHis dataset for binary classification and the UBC-OCEAN\ndataset for five-class classification without any data augmentation. Our model\nsurpasses existing CNN, ViT, and topological data analysis-based approaches in\nbinary classification. For multi-class classification, it is evaluated against\nrecent topological methods and demonstrates superior performance. Our study\nhighlights the effectiveness of Vision Transformer-based transfer learning\ncombined with efficient preprocessing in oncological diagnostics.",
        "url": "http://arxiv.org/abs/2509.18553v1",
        "published_date": "2025-09-23T02:25:44+00:00",
        "updated_date": "2025-09-23T02:25:44+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Richa Rawat",
            "Faisal Ahmed"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel method using Vision Transformer for detecting and classifying breast and ovarian cancer with superior performance compared to existing approaches.",
        "tldr_zh": "本文介绍了一种使用Vision Transformer的新方法，用于检测和分类乳腺和卵巢癌，在二元分类和多类分类任务中表现优异。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "TinyBEV: Cross Modal Knowledge Distillation for Efficient Multi Task Bird's Eye View Perception and Planning",
        "summary": "We present TinyBEV, a unified, camera only Bird's Eye View (BEV) framework\nthat distills the full-stack capabilities of a large planning-oriented teacher\n(UniAD [19]) into a compact, real-time student model. Unlike prior efficient\ncamera only baselines such as VAD[23] and VADv2[7], TinyBEV supports the\ncomplete autonomy stack 3D detection, HD-map segmentation, motion forecasting,\noccupancy prediction, and goal-directed planning within a streamlined\n28M-parameter backbone, achieving a 78% reduction in parameters over UniAD\n[19]. Our model-agnostic, multi-stage distillation strategy combines\nfeature-level, output-level, and adaptive region-aware supervision to\neffectively transfer high-capacity multi-modal knowledge to a lightweight BEV\nrepresentation. On nuScenes[4], Tiny-BEV achieves 39.0 mAP for detection, 1.08\nminADE for motion forecasting, and a 0.32 collision rate, while running 5x\nfaster (11 FPS) and requiring only camera input. These results demonstrate that\nfull-stack driving intelligence can be retained in resource-constrained\nsettings, bridging the gap between large-scale, multi-modal perception-planning\nmodels and deployment-ready real-time autonomy.",
        "url": "http://arxiv.org/abs/2509.18372v1",
        "published_date": "2025-09-22T19:54:02+00:00",
        "updated_date": "2025-09-22T19:54:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Reeshad Khan",
            "John Gauch"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "TinyBEV is a compact, real-time student model for multi-task bird's eye view perception and planning, achieving significant reduction in parameters while maintaining driving intelligence in resource-constrained settings.",
        "tldr_zh": "TinyBEV是一个紧凑的实时学生模型，用于多任务鸟瞰图感知和规划，在资源受限环境中实现参数显著减少，同时保持驾驶智能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Improving Handshape Representations for Sign Language Processing: A Graph Neural Network Approach",
        "summary": "Handshapes serve a fundamental phonological role in signed languages, with\nAmerican Sign Language employing approximately 50 distinct shapes.\nHowever,computational approaches rarely model handshapes explicitly, limiting\nboth recognition accuracy and linguistic analysis.We introduce a novel graph\nneural network that separates temporal dynamics from static handshape\nconfigurations. Our approach combines anatomically-informed graph structures\nwith contrastive learning to address key challenges in handshape recognition,\nincluding subtle interclass distinctions and temporal variations. We establish\nthe first benchmark for structured handshape recognition in signing sequences,\nachieving 46% accuracy across 37 handshape classes (with baseline methods\nachieving 25%).",
        "url": "http://arxiv.org/abs/2509.18309v1",
        "published_date": "2025-09-22T18:35:16+00:00",
        "updated_date": "2025-09-22T18:35:16+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "I.2.10"
        ],
        "authors": [
            "Alessa Carbo",
            "Eric Nalisnick"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel graph neural network approach to improve handshape representations for sign language processing, achieving significant accuracy improvements in recognizing handshapes.",
        "tldr_zh": "该论文介绍了一种新颖的图神经网络方法，用于改进手势语言处理中的手势表示，显著提高了手势识别的准确性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Track-On2: Enhancing Online Point Tracking with Memory",
        "summary": "In this paper, we consider the problem of long-term point tracking, which\nrequires consistent identification of points across video frames under\nsignificant appearance changes, motion, and occlusion. We target the online\nsetting, i.e. tracking points frame-by-frame, making it suitable for real-time\nand streaming applications. We extend our prior model Track-On into Track-On2,\na simple and efficient transformer-based model for online long-term tracking.\nTrack-On2 improves both performance and efficiency through architectural\nrefinements, more effective use of memory, and improved synthetic training\nstrategies. Unlike prior approaches that rely on full-sequence access or\niterative updates, our model processes frames causally and maintains temporal\ncoherence via a memory mechanism, which is key to handling drift and occlusions\nwithout requiring future frames. At inference, we perform coarse patch-level\nclassification followed by refinement. Beyond architecture, we systematically\nstudy synthetic training setups and their impact on memory behavior, showing\nhow they shape temporal robustness over long sequences. Through comprehensive\nexperiments, Track-On2 achieves state-of-the-art results across five synthetic\nand real-world benchmarks, surpassing prior online trackers and even strong\noffline methods that exploit bidirectional context. These results highlight the\neffectiveness of causal, memory-based architectures trained purely on synthetic\ndata as scalable solutions for real-world point tracking. Project page:\nhttps://kuis-ai.github.io/track_on2",
        "url": "http://arxiv.org/abs/2509.19115v1",
        "published_date": "2025-09-23T15:00:18+00:00",
        "updated_date": "2025-09-23T15:00:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Görkay Aydemir",
            "Weidi Xie",
            "Fatma Güney"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "Track-On2 is a new transformer-based model for online long-term point tracking that outperforms existing methods by processing frames causally and maintaining temporal coherence via memory mechanisms.",
        "tldr_zh": "Track-On2是一种新的基于transformer的模型，用于在线长期点追踪，通过处理帧时序并通过内存机制保持时间连贯性，在性能上超越了现有方法。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction",
        "summary": "Recent advances in multimodal large language models (MLLMs) have\nsignificantly enhanced video understanding capabilities, opening new\npossibilities for practical applications. Yet current video benchmarks focus\nlargely on indoor scenes or short-range outdoor activities, leaving the\nchallenges associated with long-distance travel largely unexplored. Mastering\nextended geospatial-temporal trajectories is critical for next-generation\nMLLMs, underpinning real-world tasks such as embodied-AI planning and\nnavigation. To bridge this gap, we present VIR-Bench, a novel benchmark\nconsisting of 200 travel videos that frames itinerary reconstruction as a\nchallenging task designed to evaluate and push forward MLLMs'\ngeospatial-temporal intelligence. Experimental results reveal that\nstate-of-the-art MLLMs, including proprietary ones, struggle to achieve high\nscores, underscoring the difficulty of handling videos that span extended\nspatial and temporal scales. Moreover, we conduct an in-depth case study in\nwhich we develop a prototype travel-planning agent that leverages the insights\ngained from VIR-Bench. The agent's markedly improved itinerary recommendations\nverify that our evaluation protocol not only benchmarks models effectively but\nalso translates into concrete performance gains in user-facing applications.",
        "url": "http://arxiv.org/abs/2509.19002v1",
        "published_date": "2025-09-23T13:46:31+00:00",
        "updated_date": "2025-09-23T13:46:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Hao Wang",
            "Eiki Murata",
            "Lingfang Zhang",
            "Ayako Sato",
            "So Fukuda",
            "Ziqi Yin",
            "Wentao Hu",
            "Keisuke Nakao",
            "Yusuke Nakamura",
            "Sebastian Zwirner",
            "Yi-Chia Chen",
            "Hiroyuki Otomo",
            "Hiroki Ouchi",
            "Daisuke Kawahara"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a new benchmark called VIR-Bench to evaluate the geospatial and temporal understanding of large language models using travel videos, highlighting the challenges of handling extended spatial and temporal scales.",
        "tldr_zh": "本文介绍了一个名为VIR-Bench的新基准，用于评估大型语言模型对旅行视频的地理空间和时间理解，突出了处理扩展空间和时间尺度的挑战。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "SynapFlow: A Modular Framework Towards Large-Scale Analysis of Dendritic Spines",
        "summary": "Dendritic spines are key structural components of excitatory synapses in the\nbrain. Given the size of dendritic spines provides a proxy for synaptic\nefficacy, their detection and tracking across time is important for studies of\nthe neural basis of learning and memory. Despite their relevance, large-scale\nanalyses of the structural dynamics of dendritic spines in 3D+time microscopy\ndata remain challenging and labor-intense. Here, we present a modular machine\nlearning-based pipeline designed to automate the detection, time-tracking, and\nfeature extraction of dendritic spines in volumes chronically recorded with\ntwo-photon microscopy. Our approach tackles the challenges posed by biological\ndata by combining a transformer-based detection module, a depth-tracking\ncomponent that integrates spatial features, a time-tracking module to associate\n3D spines across time by leveraging spatial consistency, and a feature\nextraction unit that quantifies biologically relevant spine properties. We\nvalidate our method on open-source labeled spine data, and on two complementary\nannotated datasets that we publish alongside this work: one for detection and\ndepth-tracking, and one for time-tracking, which, to the best of our knowledge,\nis the first data of this kind. To encourage future research, we release our\ndata, code, and pre-trained weights at\nhttps://github.com/pamelaosuna/SynapFlow, establishing a baseline for scalable,\nend-to-end analysis of dendritic spine dynamics.",
        "url": "http://arxiv.org/abs/2509.18926v1",
        "published_date": "2025-09-23T12:47:43+00:00",
        "updated_date": "2025-09-23T12:47:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pamela Osuna-Vargas",
            "Altug Kamacioglu",
            "Dominik F. Aschauer",
            "Petros E. Vlachos",
            "Sercan Alipek",
            "Jochen Triesch",
            "Simon Rumpel",
            "Matthias Kaschube"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper presents a modular machine learning-based pipeline for automated detection, tracking, and analysis of dendritic spines in 3D+time microscopy data.",
        "tldr_zh": "该论文提出了一个模块化的基于机器学习的流程，用于在3D+时间显微镜数据中自动检测，跟踪和分析树突棘。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images",
        "summary": "Remote sensing visual grounding (RSVG) aims to localize objects in remote\nsensing images based on free-form natural language expressions. Existing\napproaches are typically constrained to closed-set vocabularies, limiting their\napplicability in open-world scenarios. While recent attempts to leverage\ngeneric foundation models for open-vocabulary RSVG, they overly rely on\nexpensive high-quality datasets and time-consuming fine-tuning. To address\nthese limitations, we propose \\textbf{RSVG-ZeroOV}, a training-free framework\nthat aims to explore the potential of frozen generic foundation models for\nzero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key\nstages: (i) Overview: We utilize a vision-language model (VLM) to obtain\ncross-attention\\footnote[1]{In this paper, although decoder-only VLMs use\nself-attention over all tokens, we refer to the image-text interaction part as\ncross-attention to distinguish it from pure visual self-attention.}maps that\ncapture semantic correlations between text queries and visual regions. (ii)\nFocus: By leveraging the fine-grained modeling priors of a diffusion model\n(DM), we fill in gaps in structural and shape information of objects, which are\noften overlooked by VLM. (iii) Evolve: A simple yet effective attention\nevolution module is introduced to suppress irrelevant activations, yielding\npurified segmentation masks over the referred objects. Without cumbersome\ntask-specific training, RSVG-ZeroOV offers an efficient and scalable solution.\nExtensive experiments demonstrate that the proposed framework consistently\noutperforms existing weakly-supervised and zero-shot methods.",
        "url": "http://arxiv.org/abs/2509.18711v1",
        "published_date": "2025-09-23T06:52:15+00:00",
        "updated_date": "2025-09-23T06:52:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ke Li",
            "Di Wang",
            "Ting Wang",
            "Fuyu Dong",
            "Yiming Zhang",
            "Luyao Zhang",
            "Xiangyu Wang",
            "Shaofeng Li",
            "Quan Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "RSVG-ZeroOV is a training-free framework for zero-shot open-vocabulary visual grounding in remote sensing images, outperforming existing methods without task-specific training.",
        "tldr_zh": "RSVG-ZeroOV是一个零训练的框架，用于远程传感图像中的零阶开放词汇视觉定位，不需要任务特定训练，胜过现有方法。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for RGB-D Salient Object Detection",
        "summary": "RGB-D salient object detection (SOD) aims to identify the most conspicuous\nobjects in a scene with the incorporation of depth cues. Existing methods\nmainly rely on CNNs, limited by the local receptive fields, or Vision\nTransformers that suffer from the cost of quadratic complexity, posing a\nchallenge in balancing performance and computational efficiency. Recently,\nstate space models (SSM), Mamba, have shown great potential for modeling\nlong-range dependency with linear complexity. However, directly applying SSM to\nRGB-D SOD may lead to deficient local semantics as well as the inadequate\ncross-modality fusion. To address these issues, we propose a Local Emphatic and\nAdaptive Fusion state space model (LEAF-Mamba) that contains two novel\ncomponents: 1) a local emphatic state space module (LE-SSM) to capture\nmulti-scale local dependencies for both modalities. 2) an SSM-based adaptive\nfusion module (AFM) for complementary cross-modality interaction and reliable\ncross-modality integration. Extensive experiments demonstrate that the\nLEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in\nboth efficacy and efficiency. Moreover, our method can achieve excellent\nperformance on the RGB-T SOD task, proving a powerful generalization ability.",
        "url": "http://arxiv.org/abs/2509.18683v1",
        "published_date": "2025-09-23T06:08:17+00:00",
        "updated_date": "2025-09-23T06:08:17+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "authors": [
            "Lanhu Wu",
            "Zilin Gao",
            "Hao Fei",
            "Mong-Li Lee",
            "Wynne Hsu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a new model, LEAF-Mamba, for RGB-D salient object detection that outperforms existing methods in both efficacy and efficiency.",
        "tldr_zh": "该论文提出了一种新模型LEAF-Mamba，用于RGB-D显著对象检测，在有效性和效率方面均优于现有方法。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Codebook-Based Adaptive Feature Compression With Semantic Enhancement for Edge-Cloud Systems",
        "summary": "Coding images for machines with minimal bitrate and strong analysis\nperformance is key to effective edge-cloud systems. Several approaches deploy\nan image codec and perform analysis on the reconstructed image. Other methods\ncompress intermediate features using entropy models and subsequently perform\nanalysis on the decoded features. Nevertheless, these methods both perform\npoorly under low-bitrate conditions, as they retain many redundant details or\nlearn over-concentrated symbol distributions. In this paper, we propose a\nCodebook-based Adaptive Feature Compression framework with Semantic\nEnhancement, named CAFC-SE. It maps continuous visual features to discrete\nindices with a codebook at the edge via Vector Quantization (VQ) and\nselectively transmits them to the cloud. The VQ operation that projects feature\nvectors onto the nearest visual primitives enables us to preserve more\ninformative visual patterns under low-bitrate conditions. Hence, CAFC-SE is\nless vulnerable to low-bitrate conditions. Extensive experiments demonstrate\nthe superiority of our method in terms of rate and accuracy.",
        "url": "http://arxiv.org/abs/2509.18481v1",
        "published_date": "2025-09-23T00:34:12+00:00",
        "updated_date": "2025-09-23T00:34:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinyu Wang",
            "Zikun Zhou",
            "Yingjian Li",
            "Xin An",
            "Hongpeng Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a Codebook-based Adaptive Feature Compression framework with Semantic Enhancement for edge-cloud systems, which aims to map continuous visual features to discrete indices with a codebook for better compression and analysis.",
        "tldr_zh": "该论文提出了一种基于码书的自适应特征压缩框架，通过语义增强将连续视觉特征映射到码书中的离散指数，以实现更好的压缩和分析。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7.25
    },
    {
        "title": "MOIS-SAM2: Exemplar-based Segment Anything Model 2 for multilesion interactive segmentation of neurobromas in whole-body MRI",
        "summary": "Background and Objectives: Neurofibromatosis type 1 is a genetic disorder\ncharacterized by the development of numerous neurofibromas (NFs) throughout the\nbody. Whole-body MRI (WB-MRI) is the clinical standard for detection and\nlongitudinal surveillance of NF tumor growth. Existing interactive segmentation\nmethods fail to combine high lesion-wise precision with scalability to hundreds\nof lesions. This study proposes a novel interactive segmentation model tailored\nto this challenge.\n  Methods: We introduce MOIS-SAM2, a multi-object interactive segmentation\nmodel that extends the state-of-the-art, transformer-based, promptable Segment\nAnything Model 2 (SAM2) with exemplar-based semantic propagation. MOIS-SAM2 was\ntrained and evaluated on 119 WB-MRI scans from 84 NF1 patients acquired using\nT2-weighted fat-suppressed sequences. The dataset was split at the patient\nlevel into a training set and four test sets (one in-domain and three\nreflecting different domain shift scenarios, e.g., MRI field strength\nvariation, low tumor burden, differences in clinical site and scanner vendor).\n  Results: On the in-domain test set, MOIS-SAM2 achieved a scan-wise DSC of\n0.60 against expert manual annotations, outperforming baseline 3D nnU-Net (DSC:\n0.54) and SAM2 (DSC: 0.35). Performance of the proposed model was maintained\nunder MRI field strength shift (DSC: 0.53) and scanner vendor variation (DSC:\n0.50), and improved in low tumor burden cases (DSC: 0.61). Lesion detection F1\nscores ranged from 0.62 to 0.78 across test sets. Preliminary inter-reader\nvariability analysis showed model-to-expert agreement (DSC: 0.62-0.68),\ncomparable to inter-expert agreement (DSC: 0.57-0.69).\n  Conclusions: The proposed MOIS-SAM2 enables efficient and scalable\ninteractive segmentation of NFs in WB-MRI with minimal user input and strong\ngeneralization, supporting integration into clinical workflows.",
        "url": "http://arxiv.org/abs/2509.19277v1",
        "published_date": "2025-09-23T17:42:24+00:00",
        "updated_date": "2025-09-23T17:42:24+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Georgii Kolokolnikov",
            "Marie-Lena Schmalhofer",
            "Sophie Götz",
            "Lennart Well",
            "Said Farschtschi",
            "Victor-Felix Mautner",
            "Inka Ristow",
            "Rene Werner"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "A new interactive segmentation model, MOIS-SAM2, was proposed for efficient and scalable segmentation of neurofibromas in whole-body MRI, showing promising results in lesion detection and inter-reader variability.",
        "tldr_zh": "提出了一种新的交互式分割模型MOIS-SAM2，用于在全身MRI中高效可扩展地分割神经纤维瘤，表现出在病变检测和读者间变异性方面的有希望结果。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "KAMERA: Enhancing Aerial Surveys of Ice-associated Seals in Arctic Environments",
        "summary": "We introduce KAMERA: a comprehensive system for multi-camera, multi-spectral\nsynchronization and real-time detection of seals and polar bears. Utilized in\naerial surveys for ice-associated seals in the Bering, Chukchi, and Beaufort\nseas around Alaska, KAMERA provides up to an 80% reduction in dataset\nprocessing time over previous methods. Our rigorous calibration and hardware\nsynchronization enable using multiple spectra for object detection. All\ncollected data are annotated with metadata so they can be easily referenced\nlater. All imagery and animal detections from a survey are mapped onto a world\nplane for accurate surveyed area estimates and quick assessment of survey\nresults. We hope KAMERA will inspire other mapping and detection efforts in the\nscientific community, with all software, models, and schematics fully\nopen-sourced.",
        "url": "http://arxiv.org/abs/2509.19129v1",
        "published_date": "2025-09-23T15:15:37+00:00",
        "updated_date": "2025-09-23T15:15:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Adam Romlein",
            "Benjamin X. Hou",
            "Yuval Boss",
            "Cynthia L. Christman",
            "Stacie Koslovsky",
            "Erin E. Moreland",
            "Jason Parham",
            "Anthony Hoogs"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "KAMERA is a system for real-time detection of seals and polar bears in aerial surveys, providing significant time reduction and accurate area estimates.",
        "tldr_zh": "KAMERA 是一个用于实时检测北极环境中海豹和北极熊的系统，在空中调查中提供了显著的时间减少和准确的面积估算。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation",
        "summary": "General-purpose robotic skills from end-to-end demonstrations often leads to\ntask-specific policies that fail to generalize beyond the training\ndistribution. Therefore, we introduce FunCanon, a framework that converts\nlong-horizon manipulation tasks into sequences of action chunks, each defined\nby an actor, verb, and object. These chunks focus policy learning on the\nactions themselves, rather than isolated tasks, enabling compositionality and\nreuse. To make policies pose-aware and category-general, we perform functional\nobject canonicalization for functional alignment and automatic manipulation\ntrajectory transfer, mapping objects into shared functional frames using\naffordance cues from large vision language models. An object centric and action\ncentric diffusion policy FuncDiffuser trained on this aligned data naturally\nrespects object affordances and poses, simplifying learning and improving\ngeneralization ability. Experiments on simulated and real-world benchmarks\ndemonstrate category-level generalization, cross-task behavior reuse, and\nrobust sim2real deployment, showing that functional canonicalization provides a\nstrong inductive bias for scalable imitation learning in complex manipulation\ndomains. Details of the demo and supplemental material are available on our\nproject website https://sites.google.com/view/funcanon.",
        "url": "http://arxiv.org/abs/2509.19102v1",
        "published_date": "2025-09-23T14:49:05+00:00",
        "updated_date": "2025-09-23T14:49:05+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Hongli Xu",
            "Lei Zhang",
            "Xiaoyue Hu",
            "Boyang Zhong",
            "Kaixin Bai",
            "Zoltán-Csaba Márton",
            "Zhenshan Bing",
            "Zhaopeng Chen",
            "Alois Christian Knoll",
            "Jianwei Zhang"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "FunCanon introduces a framework for converting manipulation tasks into action chunks to improve generalization in robotic manipulation through pose-aware and category-general policies.",
        "tldr_zh": "FunCanon引入了一个框架，将操纵任务转换为动作块，通过姿态感知和通用类策略来改善机器人操纵的泛化能力。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness Tests?",
        "summary": "This paper presents ColorBlindnessEval, a novel benchmark designed to\nevaluate the robustness of Vision-Language Models (VLMs) in visually\nadversarial scenarios inspired by the Ishihara color blindness test. Our\ndataset comprises 500 Ishihara-like images featuring numbers from 0 to 99 with\nvarying color combinations, challenging VLMs to accurately recognize numerical\ninformation embedded in complex visual patterns. We assess 9 VLMs using Yes/No\nand open-ended prompts and compare their performance with human participants.\nOur experiments reveal limitations in the models' ability to interpret numbers\nin adversarial contexts, highlighting prevalent hallucination issues. These\nfindings underscore the need to improve the robustness of VLMs in complex\nvisual environments. ColorBlindnessEval serves as a valuable tool for\nbenchmarking and improving the reliability of VLMs in real-world applications\nwhere accuracy is critical.",
        "url": "http://arxiv.org/abs/2509.19070v1",
        "published_date": "2025-09-23T14:33:21+00:00",
        "updated_date": "2025-09-23T14:33:21+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Zijian Ling",
            "Han Zhang",
            "Yazhuo Zhou",
            "Jiahao Cui"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a new benchmark, ColorBlindnessEval, to test the robustness of Vision-Language Models in visually challenging scenarios inspired by color blindness tests.",
        "tldr_zh": "这篇论文介绍了一个新的基准，ColorBlindnessEval，用于测试在受色盲测试启发的视觉挑战情况下视觉-语言模型的稳健性。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Advancing Metallic Surface Defect Detection via Anomaly-Guided Pretraining on a Large Industrial Dataset",
        "summary": "The pretraining-finetuning paradigm is a crucial strategy in metallic surface\ndefect detection for mitigating the challenges posed by data scarcity. However,\nits implementation presents a critical dilemma. Pretraining on natural image\ndatasets such as ImageNet, faces a significant domain gap. Meanwhile, naive\nself-supervised pretraining on in-domain industrial data is often ineffective\ndue to the inability of existing learning objectives to distinguish subtle\ndefect patterns from complex background noise and textures. To resolve this, we\nintroduce Anomaly-Guided Self-Supervised Pretraining (AGSSP), a novel paradigm\nthat explicitly guides representation learning through anomaly priors. AGSSP\nemploys a two-stage framework: (1) it first pretrains the model's backbone by\ndistilling knowledge from anomaly maps, encouraging the network to capture\ndefect-salient features; (2) it then pretrains the detector using pseudo-defect\nboxes derived from these maps, aligning it with localization tasks. To enable\nthis, we develop a knowledge-enhanced method to generate high-quality anomaly\nmaps and collect a large-scale industrial dataset of 120,000 images.\nAdditionally, we present two small-scale, pixel-level labeled metallic surface\ndefect datasets for validation. Extensive experiments demonstrate that AGSSP\nconsistently enhances performance across various settings, achieving up to a\n10\\% improvement in mAP@0.5 and 11.4\\% in mAP@0.5:0.95 compared to\nImageNet-based models. All code, pretrained models, and datasets are publicly\navailable at https://clovermini.github.io/AGSSP-Dev/.",
        "url": "http://arxiv.org/abs/2509.18919v1",
        "published_date": "2025-09-23T12:35:32+00:00",
        "updated_date": "2025-09-23T12:35:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chuni Liu",
            "Hongjie Li",
            "Jiaqi Du",
            "Yangyang Hou",
            "Qian Sun",
            "Lei Jin",
            "Ke Xu"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces Anomaly-Guided Self-Supervised Pretraining (AGSSP) to improve metallic surface defect detection by guiding representation learning through anomaly priors.",
        "tldr_zh": "本文引入了异常引导的自监督预训练（AGSSP）方法，通过异常先验指导表示学习，从而提高金属表面缺陷检测的效果。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "xAI-CV: An Overview of Explainable Artificial Intelligence in Computer Vision",
        "summary": "Deep learning has become the de facto standard and dominant paradigm in image\nanalysis tasks, achieving state-of-the-art performance. However, this approach\noften results in \"black-box\" models, whose decision-making processes are\ndifficult to interpret, raising concerns about reliability in critical\napplications. To address this challenge and provide human a method to\nunderstand how AI model process and make decision, the field of xAI has\nemerged. This paper surveys four representative approaches in xAI for visual\nperception tasks: (i) Saliency Maps, (ii) Concept Bottleneck Models (CBM),\n(iii) Prototype-based methods, and (iv) Hybrid approaches. We analyze their\nunderlying mechanisms, strengths and limitations, as well as evaluation\nmetrics, thereby providing a comprehensive overview to guide future research\nand applications.",
        "url": "http://arxiv.org/abs/2509.18913v1",
        "published_date": "2025-09-23T12:33:54+00:00",
        "updated_date": "2025-09-23T12:33:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nguyen Van Tu",
            "Pham Nguyen Hai Long",
            "Vo Hoai Viet"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality"
        ],
        "tldr": "The paper provides an overview of explainable Artificial Intelligence in computer vision, focusing on four different approaches to make AI models more interpretable.",
        "tldr_zh": "本文概述了计算机视觉中可解释人工智能的概况，重点介绍了四种不同的方法，以使AI模型更易解释。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions",
        "summary": "Tool-augmented large language models (LLMs) are usually trained with\nsupervised imitation or coarse-grained reinforcement learning that optimizes\nsingle tool calls. Current self-reflection practices rely on heuristic prompts\nor one-way reasoning: the model is urged to 'think more' instead of learning\nerror diagnosis and repair. This is fragile in multi-turn interactions; after a\nfailure the model often repeats the same mistake. We propose structured\nreflection, which turns the path from error to repair into an explicit,\ncontrollable, and trainable action. The agent produces a short yet precise\nreflection: it diagnoses the failure using evidence from the previous step and\nthen proposes a correct, executable follow-up call. For training we combine\nDAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing\nthe stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce\nTool-Reflection-Bench, a lightweight benchmark that programmatically checks\nstructural validity, executability, parameter correctness, and result\nconsistency. Tasks are built as mini trajectories of erroneous call,\nreflection, and corrected call, with disjoint train and test splits.\nExperiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn\ntool-call success and error recovery, and a reduction of redundant calls. These\nresults indicate that making reflection explicit and optimizing it directly\nimproves the reliability of tool interaction and offers a reproducible path for\nagents to learn from failure.",
        "url": "http://arxiv.org/abs/2509.18847v1",
        "published_date": "2025-09-23T09:35:49+00:00",
        "updated_date": "2025-09-23T09:35:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Junhao Su",
            "Yuanliang Wan",
            "Junwei Yang",
            "Hengyu Shi",
            "Tianyang Han",
            "Junfeng Luo",
            "Yurui Qiu"
        ],
        "ai_categories": [
            "Transformer",
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper proposes structured reflection in large language models to improve tool interactions by diagnosing failures and proposing correct follow-up calls, leading to better error recovery and reduced redundant calls.",
        "tldr_zh": "本文提出了在大型语言模型中引入结构化反思，通过诊断失败并提出正确的后续调用来改善工具交互，从而提高错误恢复能力和减少冗余调用。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Surgical Video Understanding with Label Interpolation",
        "summary": "Robot-assisted surgery (RAS) has become a critical paradigm in modern\nsurgery, promoting patient recovery and reducing the burden on surgeons through\nminimally invasive approaches. To fully realize its potential, however, a\nprecise understanding of the visual data generated during surgical procedures\nis essential. Previous studies have predominantly focused on single-task\napproaches, but real surgical scenes involve complex temporal dynamics and\ndiverse instrument interactions that limit comprehensive understanding.\nMoreover, the effective application of multi-task learning (MTL) requires\nsufficient pixel-level segmentation data, which are difficult to obtain due to\nthe high cost and expertise required for annotation. In particular, long-term\nannotations such as phases and steps are available for every frame, whereas\nshort-term annotations such as surgical instrument segmentation and action\ndetection are provided only for key frames, resulting in a significant\ntemporal-spatial imbalance. To address these challenges, we propose a novel\nframework that combines optical flow-based segmentation label interpolation\nwith multi-task learning. optical flow estimated from annotated key frames is\nused to propagate labels to adjacent unlabeled frames, thereby enriching sparse\nspatial supervision and balancing temporal and spatial information for\ntraining. This integration improves both the accuracy and efficiency of\nsurgical scene understanding and, in turn, enhances the utility of RAS.",
        "url": "http://arxiv.org/abs/2509.18802v1",
        "published_date": "2025-09-23T08:49:07+00:00",
        "updated_date": "2025-09-23T08:49:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Garam Kim",
            "Tae Kyeong Jeong",
            "Juyoun Park"
        ],
        "ai_categories": [
            "Multimodality",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a framework combining optical flow-based segmentation label interpolation with multi-task learning to improve surgical video understanding and enhance robot-assisted surgery.",
        "tldr_zh": "本文提出了将光流分割标签插值与多任务学习相结合的框架，以提高外科视频理解能力，增强机器辅助外科手术的效果。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Human-Interpretable Uncertainty Explanations for Point Cloud Registration",
        "summary": "In this paper, we address the point cloud registration problem, where\nwell-known methods like ICP fail under uncertainty arising from sensor noise,\npose-estimation errors, and partial overlap due to occlusion. We develop a\nnovel approach, Gaussian Process Concept Attribution (GP-CA), which not only\nquantifies registration uncertainty but also explains it by attributing\nuncertainty to well-known sources of errors in registration problems. Our\napproach leverages active learning to discover new uncertainty sources in the\nwild by querying informative instances. We validate GP-CA on three publicly\navailable datasets and in our real-world robot experiment. Extensive ablations\nsubstantiate our design choices. Our approach outperforms other\nstate-of-the-art methods in terms of runtime, high sample-efficiency with\nactive learning, and high accuracy. Our real-world experiment clearly\ndemonstrates its applicability. Our video also demonstrates that GP-CA enables\neffective failure-recovery behaviors, yielding more robust robotic perception.",
        "url": "http://arxiv.org/abs/2509.18786v1",
        "published_date": "2025-09-23T08:23:51+00:00",
        "updated_date": "2025-09-23T08:23:51+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Johannes A. Gaus",
            "Loris Schneider",
            "Yitian Shi",
            "Jongseok Lee",
            "Rania Rayyes",
            "Rudolph Triebel"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel approach for addressing uncertainty in point cloud registration problems, outperforming state-of-the-art methods in terms of efficiency and accuracy on various datasets and real-world experiments.",
        "tldr_zh": "本文介绍了一种新颖的方法，用于解决点云配准问题，在各种数据集和实际实验中表现出色，效率和准确性均超过现有技术。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation",
        "summary": "Rapid adaptation in unseen environments is essential for scalable real-world\nautonomy, yet existing approaches rely on exhaustive exploration or rigid\nnavigation policies that fail to generalize. We present VLN-Zero, a two-phase\nvision-language navigation framework that leverages vision-language models to\nefficiently construct symbolic scene graphs and enable zero-shot neurosymbolic\nnavigation. In the exploration phase, structured prompts guide VLM-based search\ntoward informative and diverse trajectories, yielding compact scene graph\nrepresentations. In the deployment phase, a neurosymbolic planner reasons over\nthe scene graph and environmental observations to generate executable plans,\nwhile a cache-enabled execution module accelerates adaptation by reusing\npreviously computed task-location trajectories. By combining rapid exploration,\nsymbolic reasoning, and cache-enabled execution, the proposed framework\novercomes the computational inefficiency and poor generalization of prior\nvision-language navigation methods, enabling robust and scalable\ndecision-making in unseen environments. VLN-Zero achieves 2x higher success\nrate compared to state-of-the-art zero-shot models, outperforms most fine-tuned\nbaselines, and reaches goal locations in half the time with 55% fewer VLM calls\non average compared to state-of-the-art models across diverse environments.\nCodebase, datasets, and videos for VLN-Zero are available at:\nhttps://vln-zero.github.io/.",
        "url": "http://arxiv.org/abs/2509.18592v1",
        "published_date": "2025-09-23T03:23:03+00:00",
        "updated_date": "2025-09-23T03:23:03+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.SY",
            "eess.SY"
        ],
        "authors": [
            "Neel P. Bhatt",
            "Yunhao Yang",
            "Rohan Siva",
            "Pranay Samineni",
            "Daniel Milan",
            "Zhangyang Wang",
            "Ufuk Topcu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "VLN-Zero is a vision-language navigation framework that enables rapid adaptation in unseen environments by leveraging symbolic reasoning and efficient exploration methods.",
        "tldr_zh": "VLN-Zero是一种视觉语言导航框架，通过利用符号推理和高效的探索方法，在未知环境中实现快速适应。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.0
    },
    {
        "title": "SEGA: A Transferable Signed Ensemble Gaussian Black-Box Attack against No-Reference Image Quality Assessment Models",
        "summary": "No-Reference Image Quality Assessment (NR-IQA) models play an important role\nin various real-world applications. Recently, adversarial attacks against\nNR-IQA models have attracted increasing attention, as they provide valuable\ninsights for revealing model vulnerabilities and guiding robust system design.\nSome effective attacks have been proposed against NR-IQA models in white-box\nsettings, where the attacker has full access to the target model. However,\nthese attacks often suffer from poor transferability to unknown target models\nin more realistic black-box scenarios, where the target model is inaccessible.\nThis work makes the first attempt to address the challenge of low\ntransferability in attacking NR-IQA models by proposing a transferable Signed\nEnsemble Gaussian black-box Attack (SEGA). The main idea is to approximate the\ngradient of the target model by applying Gaussian smoothing to source models\nand ensembling their smoothed gradients. To ensure the imperceptibility of\nadversarial perturbations, SEGA further removes inappropriate perturbations\nusing a specially designed perturbation filter mask. Experimental results on\nthe CLIVE dataset demonstrate the superior transferability of SEGA, validating\nits effectiveness in enabling successful transfer-based black-box attacks\nagainst NR-IQA models.",
        "url": "http://arxiv.org/abs/2509.18546v1",
        "published_date": "2025-09-23T02:10:42+00:00",
        "updated_date": "2025-09-23T02:10:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yujia Liu",
            "Dingquan Li",
            "Tiejun Huang"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a transferable black-box attack method called SEGA against image quality assessment models, demonstrating superior transferability and effectiveness in attacking these models.",
        "tldr_zh": "本文介绍了一种名为SEGA的可转移黑盒攻击方法，针对图像质量评估模型，展示了其在攻击这些模型方面的出色可转移性和有效性。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Latent Action Pretraining Through World Modeling",
        "summary": "Vision-Language-Action (VLA) models have gained popularity for learning\nrobotic manipulation tasks that follow language instructions. State-of-the-art\nVLAs, such as OpenVLA and $\\pi_{0}$, were trained on large-scale, manually\nlabeled action datasets collected through teleoperation. More recent\napproaches, including LAPA and villa-X, introduce latent action representations\nthat enable unsupervised pretraining on unlabeled datasets by modeling abstract\nvisual changes between frames. Although these methods have shown strong\nresults, their large model sizes make deployment in real-world settings\nchallenging. In this work, we propose LAWM, a model-agnostic framework to\npretrain imitation learning models in a self-supervised way, by learning latent\naction representations from unlabeled video data through world modeling. These\nvideos can be sourced from robot recordings or videos of humans performing\nactions with everyday objects. Our framework is designed to be effective for\ntransferring across tasks, environments, and embodiments. It outperforms models\ntrained with ground-truth robotics actions and similar pretraining methods on\nthe LIBERO benchmark and real-world setup, while being significantly more\nefficient and practical for real-world settings.",
        "url": "http://arxiv.org/abs/2509.18428v1",
        "published_date": "2025-09-22T21:19:10+00:00",
        "updated_date": "2025-09-22T21:19:10+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Bahey Tharwat",
            "Yara Nasser",
            "Ali Abouzeid",
            "Ian Reid"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces LAWM, a model-agnostic framework for pretraining imitation learning models by learning latent action representations from unlabeled video data through world modeling, outperforming existing methods in real-world settings.",
        "tldr_zh": "本文介绍了LAWM，这是一个模型无关的框架，通过世界建模从未标记的视频数据中学习潜在动作表示，优于现有方法在真实世界环境中的表现。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "MVP: Motion Vector Propagation for Zero-Shot Video Object Detection",
        "summary": "Running a large open-vocabulary (Open-vocab) detector on every video frame is\naccurate but expensive. We introduce a training-free pipeline that invokes\nOWLv2 only on fixed-interval keyframes and propagates detections to\nintermediate frames using compressed-domain motion vectors (MV). A simple 3x3\ngrid aggregation of motion vectors provides translation and uniform-scale\nupdates, augmented with an area-growth check and an optional single-class\nswitch. The method requires no labels, no fine-tuning, and uses the same prompt\nlist for all open-vocabulary methods. On ILSVRC2015-VID (validation dataset),\nour approach (MVP) attains mAP@0.5=0.609 and mAP@[0.5:0.95]=0.316. At loose\nintersection-over-union (IoU) thresholds it remains close to framewise\nOWLv2-Large (0.747/0.721 at 0.2/0.3 versus 0.784/0.780), reflecting that coarse\nlocalization is largely preserved. Under the same keyframe schedule, MVP\noutperforms tracker-based propagation (MOSSE, KCF, CSRT) at mAP@0.5. A\nsupervised reference (YOLOv12x) reaches 0.631 at mAP@0.5 but requires labeled\ntraining, whereas our method remains label-free and open-vocabulary. These\nresults indicate that compressed-domain propagation is a practical way to\nreduce detector invocations while keeping strong zero-shot coverage in videos.\nOur code and models are available at https://github.com/microa/MVP.",
        "url": "http://arxiv.org/abs/2509.18388v1",
        "published_date": "2025-09-22T20:18:27+00:00",
        "updated_date": "2025-09-22T20:18:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Binhua Huang",
            "Ni Wang",
            "Wendong Yao",
            "Soumyabrata Dev"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a training-free pipeline for zero-shot video object detection that only runs a detector on keyframes and propagates detections to other frames using motion vectors.",
        "tldr_zh": "该论文介绍了一种无需训练的零样本视频目标检测管道，仅在关键帧上运行检测器，并利用运动矢量将检测结果传播到其他帧。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "BlurBall: Joint Ball and Motion Blur Estimation for Table Tennis Ball Tracking",
        "summary": "Motion blur reduces the clarity of fast-moving objects, posing challenges for\ndetection systems, especially in racket sports, where balls often appear as\nstreaks rather than distinct points. Existing labeling conventions mark the\nball at the leading edge of the blur, introducing asymmetry and ignoring\nvaluable motion cues correlated with velocity. This paper introduces a new\nlabeling strategy that places the ball at the center of the blur streak and\nexplicitly annotates blur attributes. Using this convention, we release a new\ntable tennis ball detection dataset. We demonstrate that this labeling approach\nconsistently enhances detection performance across various models. Furthermore,\nwe introduce BlurBall, a model that jointly estimates ball position and motion\nblur attributes. By incorporating attention mechanisms such as\nSqueeze-and-Excitation over multi-frame inputs, we achieve state-of-the-art\nresults in ball detection. Leveraging blur not only improves detection accuracy\nbut also enables more reliable trajectory prediction, benefiting real-time\nsports analytics.",
        "url": "http://arxiv.org/abs/2509.18387v1",
        "published_date": "2025-09-22T20:16:50+00:00",
        "updated_date": "2025-09-22T20:16:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Thomas Gossard",
            "Filip Radovic",
            "Andreas Ziegler",
            "Andrea Zell"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a new labeling strategy for tracking fast-moving table tennis balls by considering motion blur attributes. It also presents a model, BlurBall, that jointly estimates ball position and blur attributes to improve detection performance and trajectory prediction.",
        "tldr_zh": "本文介绍了一种新的标记策略，以考虑运动模糊属性来跟踪快速移动的乒乓球。它还提出了一种模型，BlurBall，它联合估计球的位置和模糊属性，以提高检测性能和轨迹预测。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata",
        "summary": "Accurate visual localization from aerial views is a fundamental problem with\napplications in mapping, large-area inspection, and search-and-rescue\noperations. In many scenarios, these systems require high-precision\nlocalization while operating with limited resources (e.g., no internet\nconnection or GNSS/GPS support), making large image databases or heavy 3D\nmodels impractical. Surprisingly, little attention has been given to leveraging\northographic geodata as an alternative paradigm, which is lightweight and\nincreasingly available through free releases by governmental authorities (e.g.,\nthe European Union). To fill this gap, we propose OrthoLoC, the first\nlarge-scale dataset comprising 16,425 UAV images from Germany and the United\nStates with multiple modalities. The dataset addresses domain shifts between\nUAV imagery and geospatial data. Its paired structure enables fair benchmarking\nof existing solutions by decoupling image retrieval from feature matching,\nallowing isolated evaluation of localization and calibration performance.\nThrough comprehensive evaluation, we examine the impact of domain shifts, data\nresolutions, and covisibility on localization accuracy. Finally, we introduce a\nrefinement technique called AdHoP, which can be integrated with any feature\nmatcher, improving matching by up to 95% and reducing translation error by up\nto 63%. The dataset and code are available at:\nhttps://deepscenario.github.io/OrthoLoC.",
        "url": "http://arxiv.org/abs/2509.18350v1",
        "published_date": "2025-09-22T19:22:32+00:00",
        "updated_date": "2025-09-22T19:22:32+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Oussema Dhaouadi",
            "Riccardo Marin",
            "Johannes Meier",
            "Jacques Kaiser",
            "Daniel Cremers"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper introduces OrthoLoC, a large dataset for UAV 6-DoF localization and calibration using orthographic geodata, with a refinement technique called AdHoP.",
        "tldr_zh": "该论文介绍了使用正交地理数据进行UAV 6-DoF定位和校准的大型数据集OrthoLoC，以及一种称为AdHoP的改进技术。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Semantic-Aware Particle Filter for Reliable Vineyard Robot Localisation",
        "summary": "Accurate localisation is critical for mobile robots in structured outdoor\nenvironments, yet LiDAR-based methods often fail in vineyards due to repetitive\nrow geometry and perceptual aliasing. We propose a semantic particle filter\nthat incorporates stable object-level detections, specifically vine trunks and\nsupport poles into the likelihood estimation process. Detected landmarks are\nprojected into a birds eye view and fused with LiDAR scans to generate semantic\nobservations. A key innovation is the use of semantic walls, which connect\nadjacent landmarks into pseudo-structural constraints that mitigate row\naliasing. To maintain global consistency in headland regions where semantics\nare sparse, we introduce a noisy GPS prior that adaptively supports the filter.\nExperiments in a real vineyard demonstrate that our approach maintains\nlocalisation within the correct row, recovers from deviations where AMCL fails,\nand outperforms vision-based SLAM methods such as RTAB-Map.",
        "url": "http://arxiv.org/abs/2509.18342v1",
        "published_date": "2025-09-22T19:04:31+00:00",
        "updated_date": "2025-09-22T19:04:31+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Rajitha de Silva",
            "Jonathan Cox",
            "James R. Heselden",
            "Marija Popovic",
            "Cesar Cadena",
            "Riccardo Polvara"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a semantic-aware particle filter for accurate localization of vineyard robots by incorporating stable object-level detections and semantic observations to mitigate aliasing issues.",
        "tldr_zh": "本文介绍了一种语义感知的粒子滤波器，通过结合稳定的对象级别检测和语义观察以减轻别名问题，实现葡萄园机器人的准确定位。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Quantum Random Synthetic Skyrmion Texture Generation, a Qiskit Simulation",
        "summary": "An integer winding, i.e., topological charge, is a characteristic of\nskyrmions, which are topologically nontrivial spin patterns in magnets. They\nemerge when smooth two-dimensional spin configurations are stabilized by\nconflicting interactions such as exchange, anisotropy, the\nDzyaloshinskii-Moriya interaction, or geometric frustration. These nanoscale\ntextures, which are typically a few to tens of nanometers in size, are strong\n'particle-like' excitations because they are shielded by energy barriers\nconnected to their topology. By exploiting their helicity, i.e., spin rotation\nangle or associated internal modes, as a two-level system, skyrmions can\nfunction as quantum bits or qubits. Two quantized helicity states of a\nnanometer-scale skyrmion encode the logical value states in a 'skyrmion qubit.'\nInterestingly, skyrmion qubits are topologically protected and macroscopic,\ni.e., they involve a large number of spins; however, external influences can\nstill affect them. When the texture is tiny and disconnected, the helicity\nangle of the skyrmion becomes quantized. A qubit basis is made up of the lowest\ntwo energy eigenstates, i.e., symmetric or antisymmetric superpositions of\nopposite helicity, for example. Therefore, Skyrmion textures can provide\nvaluable insights for different purposes. However, is it possible to\nsynthetically generate skyrmion textures using quantum computing? This paper\ninvestigates the possibility and generates a few hundred different textures,\nproducing sample comparisons from various types, which indicate a novel\ndirection for skyrmion-based research based on quantum randomness and other\ncriteria.",
        "url": "http://arxiv.org/abs/2509.18947v1",
        "published_date": "2025-09-23T12:58:12+00:00",
        "updated_date": "2025-09-23T12:58:12+00:00",
        "categories": [
            "quant-ph",
            "cs.CV"
        ],
        "authors": [
            "Hillol Biswas"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Transformer",
            "Other"
        ],
        "tldr": "This paper investigates the synthetic generation of skyrmion textures using quantum computing, potentially opening up new research directions in skyrmion-based studies.",
        "tldr_zh": "本文研究了使用量子计算生成skyrmion纹理，可能在基于skyrmion的研究中开辟新的方向。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "A Kernel Space-based Multidimensional Sparse Model for Dynamic PET Image Denoising",
        "summary": "Achieving high image quality for temporal frames in dynamic positron emission\ntomography (PET) is challenging due to the limited statistic especially for the\nshort frames. Recent studies have shown that deep learning (DL) is useful in a\nwide range of medical image denoising tasks. In this paper, we propose a\nmodel-based neural network for dynamic PET image denoising. The inter-frame\nspatial correlation and intra-frame structural consistency in dynamic PET are\nused to establish the kernel space-based multidimensional sparse (KMDS) model.\nWe then substitute the inherent forms of the parameter estimation with neural\nnetworks to enable adaptive parameters optimization, forming the end-to-end\nneural KMDS-Net. Extensive experimental results from simulated and real data\ndemonstrate that the neural KMDS-Net exhibits strong denoising performance for\ndynamic PET, outperforming previous baseline methods. The proposed method may\nbe used to effectively achieve high temporal and spatial resolution for dynamic\nPET. Our source code is available at\nhttps://github.com/Kuangxd/Neural-KMDS-Net/tree/main.",
        "url": "http://arxiv.org/abs/2509.18801v1",
        "published_date": "2025-09-23T08:48:36+00:00",
        "updated_date": "2025-09-23T08:48:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kuang Xiaodong",
            "Li Bingxuan",
            "Li Yuan",
            "Rao Fan",
            "Ma Gege",
            "Xie Qingguo",
            "Mok Greta S P",
            "Liu Huafeng",
            "Zhu Wentao"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a model-based neural network for denoising dynamic PET images using a novel kernel space-based multidimensional sparse model, outperforming previous methods.",
        "tldr_zh": "本文提出了一种基于模型的神经网络，使用新型基于内核空间的多维稀疏模型对动态PET图像进行去噪，优于先前方法。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Overview of LifeCLEF Plant Identification task 2019: diving into data deficient tropical countries",
        "summary": "Automated identification of plants has improved considerably thanks to the\nrecent progress in deep learning and the availability of training data.\nHowever, this profusion of data only concerns a few tens of thousands of\nspecies, while the planet has nearly 369K. The LifeCLEF 2019 Plant\nIdentification challenge (or \"PlantCLEF 2019\") was designed to evaluate\nautomated identification on the flora of data deficient regions. It is based on\na dataset of 10K species mainly focused on the Guiana shield and the Northern\nAmazon rainforest, an area known to have one of the greatest diversity of\nplants and animals in the world. As in the previous edition, a comparison of\nthe performance of the systems evaluated with the best tropical flora experts\nwas carried out. This paper presents the resources and assessments of the\nchallenge, summarizes the approaches and systems employed by the participating\nresearch groups, and provides an analysis of the main outcomes.",
        "url": "http://arxiv.org/abs/2509.18705v1",
        "published_date": "2025-09-23T06:42:30+00:00",
        "updated_date": "2025-09-23T06:42:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Herve Goeau",
            "Pierre Bonnet",
            "Alexis Joly"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper presents the LifeCLEF 2019 Plant Identification challenge focused on data-deficient tropical regions, comparing automated identification systems with expert performance.",
        "tldr_zh": "该论文介绍了LifeCLEF 2019植物识别挑战，重点关注数据不足的热带地区，比较了自动化识别系统与专家表现。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Hyperbolic Coarse-to-Fine Few-Shot Class-Incremental Learning",
        "summary": "In the field of machine learning, hyperbolic space demonstrates superior\nrepresentation capabilities for hierarchical data compared to conventional\nEuclidean space. This work focuses on the Coarse-To-Fine Few-Shot\nClass-Incremental Learning (C2FSCIL) task. Our study follows the Knowe\napproach, which contrastively learns coarse class labels and subsequently\nnormalizes and freezes the classifier weights of learned fine classes in the\nembedding space. To better interpret the \"coarse-to-fine\" paradigm, we propose\nembedding the feature extractor into hyperbolic space. Specifically, we employ\nthe Poincar\\'e ball model of hyperbolic space, enabling the feature extractor\nto transform input images into feature vectors within the Poincar\\'e ball\ninstead of Euclidean space. We further introduce hyperbolic contrastive loss\nand hyperbolic fully-connected layers to facilitate model optimization and\nclassification in hyperbolic space. Additionally, to enhance performance under\nfew-shot conditions, we implement maximum entropy distribution in hyperbolic\nspace to estimate the probability distribution of fine-class feature vectors.\nThis allows generation of augmented features from the distribution to mitigate\noverfitting during training with limited samples. Experiments on C2FSCIL\nbenchmarks show that our method effectively improves both coarse and fine class\naccuracies.",
        "url": "http://arxiv.org/abs/2509.18504v1",
        "published_date": "2025-09-23T01:12:21+00:00",
        "updated_date": "2025-09-23T01:12:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "stat.ML"
        ],
        "authors": [
            "Jiaxin Dai",
            "Xiang Xiang"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a method for hyperbolic coarse-to-fine few-shot class-incremental learning using hyperbolic space, showing improved performance on class accuracies.",
        "tldr_zh": "这篇论文介绍了一种利用双曲空间进行粗到细的少样本类增量学习的方法，展示了在类别准确性上的改进。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Machine learning approach to single-shot multiparameter estimation for the non-linear Schrödinger equation",
        "summary": "The nonlinear Schr\\\"odinger equation (NLSE) is a fundamental model for wave\ndynamics in nonlinear media ranging from optical fibers to Bose-Einstein\ncondensates. Accurately estimating its parameters, which are often strongly\ncorrelated, from a single measurement remains a significant challenge. We\naddress this problem by treating parameter estimation as an inverse problem and\ntraining a neural network to invert the NLSE mapping. We combine a fast\nnumerical solver with a machine learning approach based on the ConvNeXt\narchitecture and a multivariate Gaussian negative log-likelihood loss function.\nFrom single-shot field (density and phase) images, our model estimates three\nkey parameters: the nonlinear coefficient $n_2$, the saturation intensity\n$I_{sat}$, and the linear absorption coefficient $\\alpha$. Trained on 100,000\nsimulated images, the model achieves a mean absolute error of $3.22\\%$ on\n12,500 unseen test samples, demonstrating strong generalization and close\nagreement with ground-truth values. This approach provides an efficient route\nfor characterizing nonlinear systems and has the potential to bridge\ntheoretical modeling and experimental data when realistic noise is\nincorporated.",
        "url": "http://arxiv.org/abs/2509.18479v1",
        "published_date": "2025-09-23T00:32:37+00:00",
        "updated_date": "2025-09-23T00:32:37+00:00",
        "categories": [
            "quant-ph",
            "cs.CV",
            "physics.optics"
        ],
        "authors": [
            "Louis Rossignol",
            "Tangui Aladjidi",
            "Myrann Baker-Rasooli",
            "Quentin Glorieux"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a machine learning approach to estimate parameters in the nonlinear Schrödinger equation from single-shot field images, achieving strong generalization and close agreement with ground-truth values.",
        "tldr_zh": "本文提出了一种机器学习方法，用于从单次拍摄的场图像中估算非线性薛定谔方程的参数，实现了强大的泛化能力和与实际值的接近一致。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Losing the Plot: How VLM responses degrade on imperfect charts",
        "summary": "Vision language models (VLMs) show strong results on chart understanding, yet\nexisting benchmarks assume clean figures and fact based queries. Real world\ncharts often contain distortions and demand reasoning beyond simple matching.\nWe evaluate ChatGPT 4o, Claude Sonnet 4, and Gemini 2.5 Pro, finding sharp\nperformance drops under corruption or occlusion, with hallucinations such as\nvalue fabrication, trend misinterpretation, and entity confusion becoming more\nfrequent. Models remain overconfident in degraded settings, generating\nplausible but unsupported explanations.\n  To address this gap, we introduce CHART NOISe(Chart Hallucinations, Answers,\nand Reasoning Testing on Noisy and Occluded Input Selections), a dataset\ncombining chart corruptions, occlusions, and exam style multiple choice\nquestions inspired by Korea's CSAT English section. A key innovation is prompt\nreverse inconsistency, where models contradict themselves when asked to confirm\nversus deny the same statement. Our contributions are threefold: (1)\nbenchmarking state of the art VLMs, exposing systematic vulnerabilities in\nchart reasoning; (2) releasing CHART NOISe, the first dataset unifying\ncorruption, occlusion, and reverse inconsistency; and (3) proposing baseline\nmitigation strategies such as quality filtering and occlusion detection.\nTogether, these efforts establish a rigorous testbed for advancing robustness\nand reliability in chart understanding.",
        "url": "http://arxiv.org/abs/2509.18425v1",
        "published_date": "2025-09-22T21:12:20+00:00",
        "updated_date": "2025-09-22T21:12:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Philip Wootaek Shin",
            "Jack Sampson",
            "Vijaykrishnan Narayanan",
            "Andres Marquez",
            "Mahantesh Halappanavar"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper explores how vision language models degrade on imperfect charts, introducing a new dataset to test their robustness and reliability in chart understanding.",
        "tldr_zh": "本文探讨了视觉语言模型在不完美图表上的降级情况，并提出了一个新的数据集来测试它们在图表理解方面的稳健性和可靠性。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Align Where the Words Look: Cross-Attention-Guided Patch Alignment with Contrastive and Transport Regularization for Bengali Captioning",
        "summary": "Grounding vision--language models in low-resource languages remains\nchallenging, as they often produce fluent text about the wrong objects. This\nstems from scarce paired data, translation pivots that break alignment, and\nEnglish-centric pretraining that ignores target-language semantics. We address\nthis with a compute-aware Bengali captioning pipeline trained on LaBSE-verified\nEN--BN pairs and 110k bilingual-prompted synthetic images. A frozen MaxViT\nyields stable visual patches, a Bengali-native mBART-50 decodes, and a\nlightweight bridge links the modalities. Our core novelty is a tri-loss\nobjective: Patch-Alignment Loss (PAL) aligns real and synthetic patch\ndescriptors using decoder cross-attention, InfoNCE enforces global\nreal--synthetic separation, and Sinkhorn-based OT ensures balanced fine-grained\npatch correspondence. This PAL+InfoNCE+OT synergy improves grounding, reduces\nspurious matches, and drives strong gains on Flickr30k-1k (BLEU-4 12.29, METEOR\n27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14,\nBERTScore-F1 75.40), outperforming strong CE baselines and narrowing the\nreal--synthetic centroid gap by 41%.",
        "url": "http://arxiv.org/abs/2509.18369v1",
        "published_date": "2025-09-22T19:49:35+00:00",
        "updated_date": "2025-09-22T19:49:35+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Riad Ahmed Anonto",
            "Sardar Md. Saffat Zabin",
            "M. Saifur Rahman"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a compute-aware Bengali captioning pipeline that uses a tri-loss objective to improve grounding and reduce spurious matches, showing strong gains on evaluation metrics for image captioning.",
        "tldr_zh": "本文引入了一种计算感知的孟加拉语字幕生成管道，使用三损失目标来改善 grounding 并减少虚假匹配，在图像字幕评估指标上表现出较大进展。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Rethinking Pulmonary Embolism Segmentation: A Study of Current Approaches and Challenges with an Open Weight Model",
        "summary": "In this study, we curated a densely annotated in-house dataset comprising 490\nCTPA scans. Using this dataset, we systematically evaluated nine widely used\nsegmentation architectures from both the CNN and Vision Transformer (ViT)\nfamilies, initialized with either pretrained or random weights, under a unified\ntesting framework as a performance audit. Our study leads to several important\nobservations: (1) 3D U-Net with a ResNet encoder remains a highly effective\narchitecture for PE segmentation; (2) 3D models are particularly well-suited to\nthis task given the morphological characteristics of emboli; (3) CNN-based\nmodels generally yield superior performance compared to their ViT-based\ncounterparts in PE segmentation; (4) classification-based pretraining, even on\nlarge PE datasets, can adversely impact segmentation performance compared to\ntraining from scratch, suggesting that PE classification and segmentation may\nrely on different sets of discriminative features; (5) different model\narchitectures show a highly consistent pattern of segmentation performance when\ntrained on the same data; and (6) while central and large emboli can be\nsegmented with satisfactory accuracy, distal emboli remain challenging due to\nboth task complexity and the scarcity of high-quality datasets. Besides these\nfindings, our best-performing model achieves a mean Dice score of 0.7131 for\nsegmentation. It detects 181 emboli with 49 false positives and 28 false\nnegatives from 60 in-house testing scans. Its generalizability is further\nvalidated on public datasets.",
        "url": "http://arxiv.org/abs/2509.18308v1",
        "published_date": "2025-09-22T18:34:30+00:00",
        "updated_date": "2025-09-22T18:34:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yixin Zhang",
            "Ryan Chamberlain",
            "Lawrance Ngo",
            "Kevin Kramer",
            "Maciej A. Mazurowski"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper evaluates different segmentation architectures for pulmonary embolism detection using a curated dataset, highlighting the effectiveness of certain models and the challenges in segmenting distal emboli.",
        "tldr_zh": "该论文评估不同的分割架构用于肺栓塞检测，使用精心策划的数据集，突出了某些模型的有效性和分割远端栓塞的挑战。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "MsFIN: Multi-scale Feature Interaction Network for Traffic Accident Anticipation",
        "summary": "With the widespread deployment of dashcams and advancements in computer\nvision, developing accident prediction models from the dashcam perspective has\nbecome critical for proactive safety interventions. However, two key challenges\npersist: modeling feature-level interactions among traffic participants (often\noccluded in dashcam views) and capturing complex, asynchronous multi-temporal\nbehavioral cues preceding accidents. To deal with these two challenges, a\nMulti-scale Feature Interaction Network (MsFIN) is proposed for early-stage\naccident anticipation from dashcam videos. MsFIN has three layers for\nmulti-scale feature aggregation, temporal feature processing and multi-scale\nfeature post fusion, respectively. For multi-scale feature aggregation, a\nMulti-scale Module is designed to extract scene representations at short-term,\nmid-term and long-term temporal scales. Meanwhile, the Transformer architecture\nis leveraged to facilitate comprehensive feature interactions. Temporal feature\nprocessing captures the sequential evolution of scene and object features under\ncausal constraints. In the multi-scale feature post fusion stage, the network\nfuses scene and object features across multiple temporal scales to generate a\ncomprehensive risk representation. Experiments on DAD and DADA datasets show\nthat MsFIN significantly outperforms state-of-the-art models with single-scale\nfeature extraction in both prediction correctness and earliness. Ablation\nstudies validate the effectiveness of each module in MsFIN, highlighting how\nthe network achieves superior performance through multi-scale feature fusion\nand contextual interaction modeling.",
        "url": "http://arxiv.org/abs/2509.19227v1",
        "published_date": "2025-09-23T16:49:25+00:00",
        "updated_date": "2025-09-23T16:49:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Tongshuai Wu",
            "Chao Lu",
            "Ze Song",
            "Yunlong Lin",
            "Sizhe Fan",
            "Xuemei Chen"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a Multi-scale Feature Interaction Network (MsFIN) for anticipating traffic accidents from dashcam videos, outperforming existing models in prediction correctness and earliness.",
        "tldr_zh": "本文提出了一种用于从行车记录仪视频中预测交通事故的多尺度特征交互网络(MsFIN)，在预测正确性和提前性方面优于现有模型。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "Category-Level Object Shape and Pose Estimation in Less Than a Millisecond",
        "summary": "Object shape and pose estimation is a foundational robotics problem,\nsupporting tasks from manipulation to scene understanding and navigation. We\npresent a fast local solver for shape and pose estimation which requires only\ncategory-level object priors and admits an efficient certificate of global\noptimality. Given an RGB-D image of an object, we use a learned front-end to\ndetect sparse, category-level semantic keypoints on the target object. We\nrepresent the target object's unknown shape using a linear active shape model\nand pose a maximum a posteriori optimization problem to solve for position,\norientation, and shape simultaneously. Expressed in unit quaternions, this\nproblem admits first-order optimality conditions in the form of an eigenvalue\nproblem with eigenvector nonlinearities. Our primary contribution is to solve\nthis problem efficiently with self-consistent field iteration, which only\nrequires computing a 4-by-4 matrix and finding its minimum eigenvalue-vector\npair at each iterate. Solving a linear system for the corresponding Lagrange\nmultipliers gives a simple global optimality certificate. One iteration of our\nsolver runs in about 100 microseconds, enabling fast outlier rejection. We test\nour method on synthetic data and a variety of real-world settings, including\ntwo public datasets and a drone tracking scenario. Code is released at\nhttps://github.com/MIT-SPARK/Fast-ShapeAndPose.",
        "url": "http://arxiv.org/abs/2509.18979v1",
        "published_date": "2025-09-23T13:29:32+00:00",
        "updated_date": "2025-09-23T13:29:32+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Lorenzo Shaikewitz",
            "Tim Nguyen",
            "Luca Carlone"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a fast local solver for shape and pose estimation using category-level object priors, achieving global optimality. It runs in less than a millisecond and is tested on synthetic and real-world data, including a drone tracking scenario.",
        "tldr_zh": "该论文介绍了一种使用类别级别对象先验的快速本地解算器，实现了全局最优性。运行时间不到一毫秒，已在合成和真实数据上进行了测试，包括无人机跟踪场景。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6.0
    },
    {
        "title": "DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation",
        "summary": "Human skin provides a rich tactile sensing stream, localizing intentional and\nunintentional contact events over a large and contoured region. Replicating\nthese tactile sensing capabilities for dexterous robotic manipulation systems\nremains a longstanding challenge. In this work, we take a step towards this\ngoal by introducing DexSkin. DexSkin is a soft, conformable capacitive\nelectronic skin that enables sensitive, localized, and calibratable tactile\nsensing, and can be tailored to varying geometries. We demonstrate its efficacy\nfor learning downstream robotic manipulation by sensorizing a pair of parallel\njaw gripper fingers, providing tactile coverage across almost the entire finger\nsurfaces. We empirically evaluate DexSkin's capabilities in learning\nchallenging manipulation tasks that require sensing coverage across the entire\nsurface of the fingers, such as reorienting objects in hand and wrapping\nelastic bands around boxes, in a learning-from-demonstration framework. We then\nshow that, critically for data-driven approaches, DexSkin can be calibrated to\nenable model transfer across sensor instances, and demonstrate its\napplicability to online reinforcement learning on real robots. Our results\nhighlight DexSkin's suitability and practicality for learning real-world,\ncontact-rich manipulation. Please see our project webpage for videos and\nvisualizations: https://dex-skin.github.io/.",
        "url": "http://arxiv.org/abs/2509.18830v1",
        "published_date": "2025-09-23T09:16:34+00:00",
        "updated_date": "2025-09-23T09:16:34+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Suzannah Wistreich",
            "Baiyu Shi",
            "Stephen Tian",
            "Samuel Clarke",
            "Michael Nath",
            "Chengyi Xu",
            "Zhenan Bao",
            "Jiajun Wu"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces DexSkin, a conformable robotic skin for tactile sensing in manipulation tasks, demonstrating its efficacy in learning contact-rich manipulation tasks.",
        "tldr_zh": "该论文介绍了DexSkin，一种适用于触觉感知的可塑性机器人皮肤，在学习接触丰富的操纵任务中展示了其有效性。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "Overview of PlantCLEF 2021: cross-domain plant identification",
        "summary": "Automated plant identification has improved considerably thanks to recent\nadvances in deep learning and the availability of training data with more and\nmore field photos. However, this profusion of data concerns only a few tens of\nthousands of species, mainly located in North America and Western Europe, much\nless in the richest regions in terms of biodiversity such as tropical\ncountries. On the other hand, for several centuries, botanists have\nsystematically collected, catalogued and stored plant specimens in herbaria,\nespecially in tropical regions, and recent efforts by the biodiversity\ninformatics community have made it possible to put millions of digitised\nrecords online. The LifeCLEF 2021 plant identification challenge (or \"PlantCLEF\n2021\") was designed to assess the extent to which automated identification of\nflora in data-poor regions can be improved by using herbarium collections. It\nis based on a dataset of about 1,000 species mainly focused on the Guiana\nShield of South America, a region known to have one of the highest plant\ndiversities in the world. The challenge was evaluated as a cross-domain\nclassification task where the training set consisted of several hundred\nthousand herbarium sheets and a few thousand photos to allow learning a\ncorrespondence between the two domains. In addition to the usual metadata\n(location, date, author, taxonomy), the training data also includes the values\nof 5 morphological and functional traits for each species. The test set\nconsisted exclusively of photos taken in the field. This article presents the\nresources and evaluations of the assessment carried out, summarises the\napproaches and systems used by the participating research groups and provides\nan analysis of the main results.",
        "url": "http://arxiv.org/abs/2509.18697v1",
        "published_date": "2025-09-23T06:26:24+00:00",
        "updated_date": "2025-09-23T06:26:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Herve Goeau",
            "Pierre Bonnet",
            "Alexis Joly"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper introduces PlantCLEF 2021, a challenge for improving plant identification in data-poor regions by utilizing herbarium collections, focusing on a diverse region in South America.",
        "tldr_zh": "本文介绍了PlantCLEF 2021挑战赛，旨在通过利用植物标本馆藏品，提升数据匮乏地区的植物识别能力，重点关注南美洲多样性地区。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "An Analysis of Kalman Filter based Object Tracking Methods for Fast-Moving Tiny Objects",
        "summary": "Unpredictable movement patterns and small visual mark make precise tracking\nof fast-moving tiny objects like a racquetball one of the challenging problems\nin computer vision. This challenge is particularly relevant for sport robotics\napplications, where lightweight and accurate tracking systems can improve robot\nperception and planning capabilities. While Kalman filter-based tracking\nmethods have shown success in general object tracking scenarios, their\nperformance degrades substantially when dealing with rapidly moving objects\nthat exhibit irregular bouncing behavior. In this study, we evaluate the\nperformance of five state-of-the-art Kalman filter-based tracking\nmethods-OCSORT, DeepOCSORT, ByteTrack, BoTSORT, and StrongSORT-using a custom\ndataset containing 10,000 annotated racquetball frames captured at 720p-1280p\nresolution. We focus our analysis on two critical performance factors:\ninference speed and update frequency per image, examining how these parameters\naffect tracking accuracy and reliability for fast-moving tiny objects. Our\nexperimental evaluation across four distinct scenarios reveals that DeepOCSORT\nachieves the lowest tracking error with an average ADE of 31.15 pixels compared\nto ByteTrack's 114.3 pixels, while ByteTrack demonstrates the fastest\nprocessing at 26.6ms average inference time versus DeepOCSORT's 26.8ms.\nHowever, our results show that all Kalman filter-based trackers exhibit\nsignificant tracking drift with spatial errors ranging from 3-11cm (ADE values:\n31-114 pixels), indicating fundamental limitations in handling the\nunpredictable motion patterns of fast-moving tiny objects like racquetballs.\nOur analysis demonstrates that current tracking approaches require substantial\nimprovements, with error rates 3-4x higher than standard object tracking\nbenchmarks, highlighting the need for specialized methodologies for fast-moving\ntiny object tracking applications.",
        "url": "http://arxiv.org/abs/2509.18451v1",
        "published_date": "2025-09-22T22:12:48+00:00",
        "updated_date": "2025-09-22T22:12:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Prithvi Raj Singh",
            "Raju Gottumukkala",
            "Anthony Maida"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer",
            "Other"
        ],
        "tldr": "The paper evaluates Kalman filter-based object tracking methods for fast-moving tiny objects like racquetballs, highlighting the challenges and limitations of current approaches.",
        "tldr_zh": "本文评估了基于卡尔曼滤波器的物体追踪方法，用于快速移动的小物体，如球拍球，突显当前方法的挑战和局限性。",
        "relevance_score": 4,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "Does Embodiment Matter to Biomechanics and Function? A Comparative Analysis of Head-Mounted and Hand-Held Assistive Devices for Individuals with Blindness and Low Vision",
        "summary": "Visual assistive technologies, such as Microsoft Seeing AI, can improve\naccess to environmental information for persons with blindness or low vision\n(pBLV). Yet, the physical and functional implications of different device\nembodiments remain unclear. In this study, 11 pBLV participants used Seeing AI\non a hand-held smartphone and on a head-mounted ARx Vision system to perform\nsix activities of daily living, while their movements were captured with Xsens\nmotion capture. Functional outcomes included task time, success rate, and\nnumber of attempts, and biomechanical measures included joint range of motion,\nangular path length, working volume, and movement smoothness. The head-mounted\nsystem generally reduced upper-body movement and task time, especially for\ndocument-scanning style tasks, whereas the hand-held system yielded higher\nsuccess rates for tasks involving small or curved text. These findings indicate\nthat both embodiments are viable, but they differ in terms of physical demands\nand ease of use. Incorporating biomechanical measures into assistive technology\nevaluations can inform designs that optimise user experience by balancing\nfunctional efficiency, physical sustainability, and intuitive interaction.",
        "url": "http://arxiv.org/abs/2509.18391v1",
        "published_date": "2025-09-22T20:23:33+00:00",
        "updated_date": "2025-09-22T20:23:33+00:00",
        "categories": [
            "cs.HC",
            "cs.CV",
            "K.4.2; H.5.2; I.3.6"
        ],
        "authors": [
            "Gaurav Seth",
            "Hoa Pham",
            "Giles Hamilton-Fletcher",
            "Charles Leclercq",
            "John-Ross Rizzo"
        ],
        "ai_categories": [
            "Multimodality"
        ],
        "tldr": "The paper compares the use of head-mounted and hand-held visual assistive devices for individuals with blindness and low vision, focusing on biomechanics and functionality.",
        "tldr_zh": "该论文比较了对于盲人和低视力人士，使用头戴式和手持式视觉辅助设备在生物力学和功能方面的影响。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "Influence of Classification Task and Distribution Shift Type on OOD Detection in Fetal Ultrasound",
        "summary": "Reliable out-of-distribution (OOD) detection is important for safe deployment\nof deep learning models in fetal ultrasound amidst heterogeneous image\ncharacteristics and clinical settings. OOD detection relies on estimating a\nclassification model's uncertainty, which should increase for OOD samples.\nWhile existing research has largely focused on uncertainty quantification\nmethods, this work investigates the impact of the classification task itself.\nThrough experiments with eight uncertainty quantification methods across four\nclassification tasks, we demonstrate that OOD detection performance\nsignificantly varies with the task, and that the best task depends on the\ndefined ID-OOD criteria; specifically, whether the OOD sample is due to: i) an\nimage characteristic shift or ii) an anatomical feature shift. Furthermore, we\nreveal that superior OOD detection does not guarantee optimal abstained\nprediction, underscoring the necessity to align task selection and uncertainty\nstrategies with the specific downstream application in medical image analysis.",
        "url": "http://arxiv.org/abs/2509.18326v1",
        "published_date": "2025-09-22T18:49:25+00:00",
        "updated_date": "2025-09-22T18:49:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chun Kit Wong",
            "Anders N. Christensen",
            "Cosmin I. Bercea",
            "Julia A. Schnabel",
            "Martin G. Tolsgaard",
            "Aasa Feragen"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper explores how different classification tasks impact the detection of out-of-distribution samples in fetal ultrasound images, highlighting the importance of aligning task selection with uncertainty strategies for optimal performance.",
        "tldr_zh": "本文探讨了不同分类任务对胎儿超声图像中异常样本检测的影响，强调了将任务选择与不确定性策略相结合以获得最佳性能的重要性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]