[
    {
        "title": "Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models",
        "summary": "Reasoning over dynamic visual content remains a central challenge for multimodal large language models. Recent thinking models generate explicit reasoning traces for interpretability; however, their reasoning often appears convincing while being logically inconsistent or weakly grounded in visual evidence. We identify and formalize these issues through two diagnostic metrics: Think Answer Consistency (TAC), which measures the alignment between reasoning and answers, and Video Attention Score (VAS), which captures the extent to which reasoning depends on visual versus textual cues. Analysis across 11 video reasoning benchmarks shows that current models rely heavily on linguistic priors rather than visual content. To address this, we propose a reinforcement learning approach that enhances both temporal precision and reasoning consistency. Our approach combines timestamp aware supervised fine tuning with Group Relative Policy Optimization (GRPO) guided by a novel Temporal Alignment Reward (TAR). This dual step post training stage encourages temporally aligned and causally coherent video reasoning. The resulting model, Video R2, achieves consistently higher TAC, VAS, and accuracy across multiple benchmarks, demonstrating that improvements in temporal alignment and reasoning coherence lead to more accurate and trustworthy video understanding. Our code, dataset, and model will be open sourced.",
        "url": "http://arxiv.org/abs/2511.23478v1",
        "published_date": "2025-11-28T18:59:58+00:00",
        "updated_date": "2025-11-28T18:59:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Muhammad Maaz",
            "Hanoona Rasheed",
            "Fahad Shahbaz Khan",
            "Salman Khan"
        ],
        "ai_categories": []
    },
    {
        "title": "Video-CoM: Interactive Video Reasoning via Chain of Manipulations",
        "summary": "Recent multimodal large language models (MLLMs) have advanced video understanding, yet most still \"think about videos\" ie once a video is encoded, reasoning unfolds entirely in text, treating visual input as a static context. This passive paradigm creates a semantic bottleneck: models cannot rewatch, refocus, or verify evidence, leading to shallow visual reasoning on tasks requiring fine grained spatio temporal understanding. In this work, we introduce Interactive Video Reasoning, a new paradigm that transforms video into an active cognitive workspace, enabling models to \"think with videos\". Our model, Video CoM, reasons through a Chain of Manipulations (CoM), performing iterative visual actions to gather and refine evidence. To support this behavior, we construct Video CoM Instruct, an 18K instruction tuning dataset curated for multi step manipulation reasoning. Beyond supervised learning, we further optimize the manipulation policy via reinforcement learning with reasoning aware Group Relative Policy Optimization (GRPO). Unlike prior work that relies solely on sparse answer rewards, our method introduces step level reasoning rewards, guiding the model toward grounded and consistent reasoning. Video CoM achieves strong results across nine video reasoning benchmarks, improving average performance by 3.6 percent over recent state of the art models, while training on only 25K SFT and 3K GRPO video samples, significantly fewer than comparable large scale models. Ablation studies demonstrate that reasoning aware rewards improve both accuracy and interpretability. Code: https://github.com/mbzuai-oryx/Video-CoM",
        "url": "http://arxiv.org/abs/2511.23477v1",
        "published_date": "2025-11-28T18:59:57+00:00",
        "updated_date": "2025-11-28T18:59:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hanoona Rasheed",
            "Mohammed Zumri",
            "Muhammad Maaz",
            "Ming-Hsuan Yang",
            "Fahad Shahbaz Khan",
            "Salman Khan"
        ],
        "ai_categories": []
    },
    {
        "title": "AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement",
        "summary": "Recently, multi-person video generation has started to gain prominence. While a few preliminary works have explored audio-driven multi-person talking video generation, they often face challenges due to the high costs of diverse multi-person data collection and the difficulty of driving multiple identities with coherent interactivity. To address these challenges, we propose AnyTalker, a multi-person generation framework that features an extensible multi-stream processing architecture. Specifically, we extend Diffusion Transformer's attention block with a novel identity-aware attention mechanism that iteratively processes identity-audio pairs, allowing arbitrary scaling of drivable identities. Besides, training multi-person generative models demands massive multi-person data. Our proposed training pipeline depends solely on single-person videos to learn multi-person speaking patterns and refines interactivity with only a few real multi-person clips. Furthermore, we contribute a targeted metric and dataset designed to evaluate the naturalness and interactivity of the generated multi-person videos. Extensive experiments demonstrate that AnyTalker achieves remarkable lip synchronization, visual quality, and natural interactivity, striking a favorable balance between data costs and identity scalability.",
        "url": "http://arxiv.org/abs/2511.23475v1",
        "published_date": "2025-11-28T18:59:01+00:00",
        "updated_date": "2025-11-28T18:59:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhizhou Zhong",
            "Yicheng Ji",
            "Zhe Kong",
            "Yiying Liu",
            "Jiarui Wang",
            "Jiasun Feng",
            "Lupeng Liu",
            "Xiangyi Wang",
            "Yanjia Li",
            "Yuqing She",
            "Ying Qin",
            "Huan Li",
            "Shuiyang Mao",
            "Wei Liu",
            "Wenhan Luo"
        ],
        "ai_categories": []
    },
    {
        "title": "Visual Generation Tuning",
        "summary": "Large Vision Language Models (VLMs) effectively bridge the modality gap through extensive pretraining, acquiring sophisticated visual representations aligned with language. However, it remains underexplored whether these representations, optimized for multimodal understanding tasks, harbor an inherent potential for visual generation. In this paper, we propose VGT, Visual Generation Tuning, a novel paradigm designed to stimulate the underlying capabilities of visual generation within any vision language models. By performing efficient visual generation tuning on well-pretrained VLMs, we significantly mitigate the alignment costs and accelerate the convergence of autoregressive modeling in the continuous space (20x speedup). Specifically, we dismiss the entangled pixel-level VAEs designed for diffusion transformers and formulate VGT-AE through aligning the semantic encoders from pretrained VLMs with the latent representations of pixel decoders. In image reconstruction tasks, we achieve 26.67 PSNR and 0.50 rFID at a 28x compression ratio, outperforming specialized VAEs; in visual generation tasks, we achieve state-of-the-art outcomes among autoregressive models, 0.77 on GenEval and 78.73 on DPG-Bench. Furthermore, our proposed VGT showcases significant scaling promise and is versatile for endowing any VLMs trained for multimodal understanding with the capabilities of visual generation, which paves the new avenue to explore next-generation unified multimodal foundation models. Models and codes are available at https://github.com/hustvl/VGT.",
        "url": "http://arxiv.org/abs/2511.23469v1",
        "published_date": "2025-11-28T18:57:13+00:00",
        "updated_date": "2025-11-28T18:57:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahao Guo",
            "Sinan Du",
            "Jingfeng Yao",
            "Wenyu Liu",
            "Bo Li",
            "Haoxiang Cao",
            "Kun Gai",
            "Chun Yuan",
            "Kai Wu",
            "Xinggang Wang"
        ],
        "ai_categories": []
    },
    {
        "title": "Object-Centric Data Synthesis for Category-level Object Detection",
        "summary": "Deep learning approaches to object detection have achieved reliable detection of specific object classes in images. However, extending a model's detection capability to new object classes requires large amounts of annotated training data, which is costly and time-consuming to acquire, especially for long-tailed classes with insufficient representation in existing datasets. Here, we introduce the object-centric data setting, when limited data is available in the form of object-centric data (multi-view images or 3D models), and systematically evaluate the performance of four different data synthesis methods to finetune object detection models on novel object categories in this setting. The approaches are based on simple image processing techniques, 3D rendering, and image diffusion models, and use object-centric data to synthesize realistic, cluttered images with varying contextual coherence and complexity. We assess how these methods enable models to achieve category-level generalization in real-world data, and demonstrate significant performance boosts within this data-constrained experimental setting.",
        "url": "http://arxiv.org/abs/2511.23450v1",
        "published_date": "2025-11-28T18:41:46+00:00",
        "updated_date": "2025-11-28T18:41:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Vikhyat Agarwal",
            "Jiayi Cora Guo",
            "Declan Hoban",
            "Sissi Zhang",
            "Nicholas Moran",
            "Peter Cho",
            "Srilakshmi Pattabiraman",
            "Shantanu Joshi"
        ],
        "ai_categories": []
    },
    {
        "title": "Physics-Informed Neural Networks for Thermophysical Property Retrieval",
        "summary": "Inverse heat problems refer to the estimation of material thermophysical properties given observed or known heat diffusion behaviour. Inverse heat problems have wide-ranging uses, but a critical application lies in quantifying how building facade renovation reduces thermal transmittance, a key determinant of building energy efficiency. However, solving inverse heat problems with non-invasive data collected in situ is error-prone due to environmental variability or deviations from theoretically assumed conditions. Hence, current methods for measuring thermal conductivity are either invasive, require lengthy observation periods, or are sensitive to environmental and experimental conditions. Here, we present a PINN-based iterative framework to estimate the thermal conductivity k of a wall from a set of thermographs; our framework alternates between estimating the forward heat problem with a PINN for a fixed k, and optimizing k by comparing the thermographs and surface temperatures predicted by the PINN, repeating until the estimated k's convergence. Using both environmental data captured by a weather station and data generated from Finite-Volume-Method software simulations, we accurately predict k across different environmental conditions and data collection sampling times, given the temperature profile of the wall at dawn is close to steady state. Although violating the steady-state assumption impacts the accuracy of k's estimation, we show that our proposed framework still only exhibits a maximum MAE of 4.0851. Our work demonstrates the potential of PINN-based methods for reliable estimation of material properties in situ and under realistic conditions, without lengthy measurement campaigns. Given the lack of research on using machine learning, and more specifically on PINNs, for solving in-situ inverse problems, we expect our work to be a starting point for more research on the topic.",
        "url": "http://arxiv.org/abs/2511.23449v1",
        "published_date": "2025-11-28T18:41:08+00:00",
        "updated_date": "2025-11-28T18:41:08+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CE",
            "cs.CV"
        ],
        "authors": [
            "Ali Waseem",
            "Malcolm Mielle"
        ],
        "ai_categories": []
    },
    {
        "title": "Hunyuan-GameCraft-2: Instruction-following Interactive Game World Model",
        "summary": "Recent advances in generative world models have enabled remarkable progress in creating open-ended game environments, evolving from static scene synthesis toward dynamic, interactive simulation. However, current approaches remain limited by rigid action schemas and high annotation costs, restricting their ability to model diverse in-game interactions and player-driven dynamics. To address these challenges, we introduce Hunyuan-GameCraft-2, a new paradigm of instruction-driven interaction for generative game world modeling. Instead of relying on fixed keyboard inputs, our model allows users to control game video contents through natural language prompts, keyboard, or mouse signals, enabling flexible and semantically rich interaction within generated worlds. We formally defined the concept of interactive video data and developed an automated process to transform large-scale, unstructured text-video pairs into causally aligned interactive datasets. Built upon a 14B image-to-video Mixture-of-Experts(MoE) foundation model, our model incorporates a text-driven interaction injection mechanism for fine-grained control over camera motion, character behavior, and environment dynamics. We introduce an interaction-focused benchmark, InterBench, to evaluate interaction performance comprehensively. Extensive experiments demonstrate that our model generates temporally coherent and causally grounded interactive game videos that faithfully respond to diverse and free-form user instructions such as \"open the door\", \"draw a torch\", or \"trigger an explosion\".",
        "url": "http://arxiv.org/abs/2511.23429v1",
        "published_date": "2025-11-28T18:26:39+00:00",
        "updated_date": "2025-11-28T18:26:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junshu Tang",
            "Jiacheng Liu",
            "Jiaqi Li",
            "Longhuang Wu",
            "Haoyu Yang",
            "Penghao Zhao",
            "Siruis Gong",
            "Xiang Yuan",
            "Shuai Shao",
            "Qinglin Lu"
        ],
        "ai_categories": []
    },
    {
        "title": "DisMo: Disentangled Motion Representations for Open-World Motion Transfer",
        "summary": "Recent advances in text-to-video (T2V) and image-to-video (I2V) models, have enabled the creation of visually compelling and dynamic videos from simple textual descriptions or initial frames. However, these models often fail to provide an explicit representation of motion separate from content, limiting their applicability for content creators. To address this gap, we propose DisMo, a novel paradigm for learning abstract motion representations directly from raw video data via an image-space reconstruction objective. Our representation is generic and independent of static information such as appearance, object identity, or pose. This enables open-world motion transfer, allowing motion to be transferred across semantically unrelated entities without requiring object correspondences, even between vastly different categories. Unlike prior methods, which trade off motion fidelity and prompt adherence, are overfitting to source structure or drifting from the described action, our approach disentangles motion semantics from appearance, enabling accurate transfer and faithful conditioning. Furthermore, our motion representation can be combined with any existing video generator via lightweight adapters, allowing us to effortlessly benefit from future advancements in video models. We demonstrate the effectiveness of our method through a diverse set of motion transfer tasks. Finally, we show that the learned representations are well-suited for downstream motion understanding tasks, consistently outperforming state-of-the-art video representation models such as V-JEPA in zero-shot action classification on benchmarks including Something-Something v2 and Jester. Project page: https://compvis.github.io/DisMo",
        "url": "http://arxiv.org/abs/2511.23428v1",
        "published_date": "2025-11-28T18:25:54+00:00",
        "updated_date": "2025-11-28T18:25:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Thomas Ressler-Antal",
            "Frank Fundel",
            "Malek Ben Alaya",
            "Stefan Andreas Baumann",
            "Felix Krause",
            "Ming Gui",
            "Bj√∂rn Ommer"
        ],
        "ai_categories": []
    }
]