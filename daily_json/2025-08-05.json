[
    {
        "title": "Text2Lip: Progressive Lip-Synced Talking Face Generation from Text via Viseme-Guided Rendering",
        "summary": "Generating semantically coherent and visually accurate talking faces requires\nbridging the gap between linguistic meaning and facial articulation. Although\naudio-driven methods remain prevalent, their reliance on high-quality paired\naudio visual data and the inherent ambiguity in mapping acoustics to lip motion\npose significant challenges in terms of scalability and robustness. To address\nthese issues, we propose Text2Lip, a viseme-centric framework that constructs\nan interpretable phonetic-visual bridge by embedding textual input into\nstructured viseme sequences. These mid-level units serve as a linguistically\ngrounded prior for lip motion prediction. Furthermore, we design a progressive\nviseme-audio replacement strategy based on curriculum learning, enabling the\nmodel to gradually transition from real audio to pseudo-audio reconstructed\nfrom enhanced viseme features via cross-modal attention. This allows for robust\ngeneration in both audio-present and audio-free scenarios. Finally, a\nlandmark-guided renderer synthesizes photorealistic facial videos with accurate\nlip synchronization. Extensive evaluations show that Text2Lip outperforms\nexisting approaches in semantic fidelity, visual realism, and modality\nrobustness, establishing a new paradigm for controllable and flexible talking\nface generation. Our project homepage is https://plyon1.github.io/Text2Lip/.",
        "url": "http://arxiv.org/abs/2508.02362v1",
        "published_date": "2025-08-04T12:50:22+00:00",
        "updated_date": "2025-08-04T12:50:22+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xu Wang",
            "Shengeng Tang",
            "Fei Wang",
            "Lechao Cheng",
            "Dan Guo",
            "Feng Xue",
            "Richang Hong"
        ],
        "ai_categories": [
            "GAN",
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "Text2Lip is a framework for generating talking faces from text, using a viseme-centric approach for robust audio-free generation with realistic lip synchronization.",
        "tldr_zh": "Text2Lip是一个从文本生成说话人脸的框架，采用以视音节为中心的方法，实现了无需音频的鲁棒生成，同时具有逼真的唇部同步。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "RL-U$^2$Net: A Dual-Branch UNet with Reinforcement Learning-Assisted Multimodal Feature Fusion for Accurate 3D Whole-Heart Segmentation",
        "summary": "Accurate whole-heart segmentation is a critical component in the precise\ndiagnosis and interventional planning of cardiovascular diseases. Integrating\ncomplementary information from modalities such as computed tomography (CT) and\nmagnetic resonance imaging (MRI) can significantly enhance segmentation\naccuracy and robustness. However, existing multi-modal segmentation methods\nface several limitations: severe spatial inconsistency between modalities\nhinders effective feature fusion; fusion strategies are often static and lack\nadaptability; and the processes of feature alignment and segmentation are\ndecoupled and inefficient. To address these challenges, we propose a\ndual-branch U-Net architecture enhanced by reinforcement learning for feature\nalignment, termed RL-U$^2$Net, designed for precise and efficient multi-modal\n3D whole-heart segmentation. The model employs a dual-branch U-shaped network\nto process CT and MRI patches in parallel, and introduces a novel RL-XAlign\nmodule between the encoders. The module employs a cross-modal attention\nmechanism to capture semantic correspondences between modalities and a\nreinforcement-learning agent learns an optimal rotation strategy that\nconsistently aligns anatomical pose and texture features. The aligned features\nare then reconstructed through their respective decoders. Finally, an\nensemble-learning-based decision module integrates the predictions from\nindividual patches to produce the final segmentation result. Experimental\nresults on the publicly available MM-WHS 2017 dataset demonstrate that the\nproposed RL-U$^2$Net outperforms existing state-of-the-art methods, achieving\nDice coefficients of 93.1% on CT and 87.0% on MRI, thereby validating the\neffectiveness and superiority of the proposed approach.",
        "url": "http://arxiv.org/abs/2508.02557v1",
        "published_date": "2025-08-04T16:12:06+00:00",
        "updated_date": "2025-08-04T16:12:06+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Jierui Qu",
            "Jianchun Zhao"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "RL-U$^2$Net is a dual-branch UNet model with reinforcement learning for accurate 3D whole-heart segmentation by integrating information from CT and MRI modalities.",
        "tldr_zh": "RL-U$^2$Net是一种双分支UNet模型，通过集成CT和MRI模态的信息，通过强化学习实现准确的3D整心分割。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "Free-MoRef: Instantly Multiplexing Context Perception Capabilities of Video-MLLMs within Single Inference",
        "summary": "Video Multimodal Large Language Models~(Video-MLLM) have achieved remarkable\nadvancements in video understanding tasks. However, constrained by the context\nlength limitation in the underlying LLMs, existing Video-MLLMs typically\nexhibit suboptimal performance on long video scenarios. To understand extended\ninput frames, common solutions span token compression and streaming inference\ntechniques, which sacrifice feature granularity or inference efficiency.\nDifferently, to efficiently achieve comprehensive understanding of longer frame\ninputs, we draw ideas from MoE and propose a training-free approach\n\\textbf{Free-MoRef}, which instantly multiplexes the context perception\ncapabilities of Video-MLLMs within one inference pass. Specifically, Free-MoRef\nreconstructs the vision tokens into several short sequences as\nmulti-references. Subsequently, we introduce MoRef-attention, which gathers\nclues from the multi-reference chunks in parallel to summarize unified query\nactivations. After the shadow layers in LLMs, a reference fusion step is\nderived to compose a final mixed reasoning sequence with key tokens from\nparallel chunks, which compensates the cross-reference vision interactions that\nare neglected in MoRef-attention. By splitting and fusing the long vision token\nsequences, Free-MoRef achieves improved performance under much lower computing\ncosts in reasoning multiplexed context length, demonstrating strong efficiency\nand effectiveness. Experiments on VideoMME, MLVU, LongVideoBench show that\nFree-MoRef achieves full perception of 2$\\times$ to 8$\\times$ longer input\nframes without compression on a single A100 GPU while keeping instant\nresponses, thereby bringing significant performance gains, even surpassing\ndedicatedly trained long-video-MLLMs. Codes are available at\nhttps://github.com/wkfdb/Free-MoRef",
        "url": "http://arxiv.org/abs/2508.02134v1",
        "published_date": "2025-08-04T07:31:10+00:00",
        "updated_date": "2025-08-04T07:31:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kuo Wang",
            "Quanlong Zheng",
            "Junlin Xie",
            "Yanhao Zhang",
            "Jinguo Luo",
            "Haonan Lu",
            "Liang Lin",
            "Fan Zhou",
            "Guanbin Li"
        ],
        "ai_categories": [
            "Transformer",
            "AIGC"
        ],
        "tldr": "The paper introduces Free-MoRef, a training-free approach that improves the performance of Video-MLLMs on long videos by instantly multiplexing context perception capabilities within one inference pass.",
        "tldr_zh": "本文引入了Free-MoRef，一种无需训练的方法，通过在单次推理中即时复用上下文感知能力，提高了Video-MLLM在长视频上的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "Protego: User-Centric Pose-Invariant Privacy Protection Against Face Recognition-Induced Digital Footprint Exposure",
        "summary": "Face recognition (FR) technologies are increasingly used to power large-scale\nimage retrieval systems, raising serious privacy concerns. Services like\nClearview AI and PimEyes allow anyone to upload a facial photo and retrieve a\nlarge amount of online content associated with that person. This not only\nenables identity inference but also exposes their digital footprint, such as\nsocial media activity, private photos, and news reports, often without their\nconsent. In response to this emerging threat, we propose Protego, a\nuser-centric privacy protection method that safeguards facial images from such\nretrieval-based privacy intrusions. Protego encapsulates a user's 3D facial\nsignatures into a pose-invariant 2D representation, which is dynamically\ndeformed into a natural-looking 3D mask tailored to the pose and expression of\nany facial image of the user, and applied prior to online sharing. Motivated by\na critical limitation of existing methods, Protego amplifies the sensitivity of\nFR models so that protected images cannot be matched even among themselves.\nExperiments show that Protego significantly reduces retrieval accuracy across a\nwide range of black-box FR models and performs at least 2x better than existing\nmethods. It also offers unprecedented visual coherence, particularly in video\nsettings where consistency and natural appearance are essential. Overall,\nProtego contributes to the fight against the misuse of FR for mass surveillance\nand unsolicited identity tracing.",
        "url": "http://arxiv.org/abs/2508.02034v1",
        "published_date": "2025-08-04T04:03:01+00:00",
        "updated_date": "2025-08-04T04:03:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziling Wang",
            "Shuya Yang",
            "Jialin Lu",
            "Ka-Ho Chow"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset",
            "Transformer"
        ],
        "tldr": "Protego is a privacy protection method against face recognition technology that creates pose-invariant masks for facial images, reducing accuracy of FR models and enhancing visual coherence.",
        "tldr_zh": "Protego是一种针对人脸识别技术的隐私保护方法，为面部图像创建姿势不变的面具，降低FR模型的准确性并增强视觉一致性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "DreamPainter: Image Background Inpainting for E-commerce Scenarios",
        "summary": "Although diffusion-based image genenation has been widely explored and\napplied, background generation tasks in e-commerce scenarios still face\nsignificant challenges. The first challenge is to ensure that the generated\nproducts are consistent with the given product inputs while maintaining a\nreasonable spatial arrangement, harmonious shadows, and reflections between\nforeground products and backgrounds. Existing inpainting methods fail to\naddress this due to the lack of domain-specific data. The second challenge\ninvolves the limitation of relying solely on text prompts for image control, as\neffective integrating visual information to achieve precise control in\ninpainting tasks remains underexplored. To address these challenges, we\nintroduce DreamEcom-400K, a high-quality e-commerce dataset containing accurate\nproduct instance masks, background reference images, text prompts, and\naesthetically pleasing product images. Based on this dataset, we propose\nDreamPainter, a novel framework that not only utilizes text prompts for control\nbut also flexibly incorporates reference image information as an additional\ncontrol signal. Extensive experiments demonstrate that our approach\nsignificantly outperforms state-of-the-art methods, maintaining high product\nconsistency while effectively integrating both text prompt and reference image\ninformation.",
        "url": "http://arxiv.org/abs/2508.02155v1",
        "published_date": "2025-08-04T07:54:37+00:00",
        "updated_date": "2025-08-04T07:54:37+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sijie Zhao",
            "Jing Cheng",
            "Yaoyao Wu",
            "Hao Xu",
            "Shaohui Jiao"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a novel framework, DreamPainter, for image background inpainting in e-commerce scenarios, utilizing text prompts and reference image information to achieve high product consistency and control.",
        "tldr_zh": "该论文介绍了一种新颖的框架DreamPainter，用于在电子商务场景中进行图像背景修复，利用文本提示和参考图像信息实现高产品一致性和控制。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "MedVLThinker: Simple Baselines for Multimodal Medical Reasoning",
        "summary": "Large Reasoning Models (LRMs) have introduced a new paradigm in AI by\nenabling models to ``think before responding\" via chain-of-thought reasoning.\nHowever, the absence of open and reproducible recipes for building\nreasoning-centric medical LMMs hinders community-wide research, analysis, and\ncomparison. In this paper, we present MedVLThinker, a suite of simple yet\nstrong baselines. Our fully open recipe consists of: (1) systematic data\ncuration for both text-only and image-text medical data, filtered according to\nvarying levels of reasoning difficulty, and (2) two training paradigms:\nSupervised Fine-Tuning (SFT) on distilled reasoning traces and Reinforcement\nLearning with Verifiable Rewards (RLVR) based on final answer correctness.\nAcross extensive experiments on the Qwen2.5-VL model family (3B, 7B) and six\nmedical QA benchmarks, we find that RLVR consistently and significantly\noutperforms SFT. Additionally, under the RLVR framework, a key,\ncounter-intuitive finding is that training on our curated text-only reasoning\ndata provides a more substantial performance boost than training on multimodal\nimage-text data. Our best open 7B model, trained using the RLVR recipe on\ntext-only data, establishes a new state-of-the-art on existing public VQA\nbenchmarks, surpassing all previous open-source medical LMMs. Furthermore,\nscaling our model to 32B achieves performance on par with the proprietary\nGPT-4o. We release all curated data, models, and code to provide the community\nwith a strong, open foundation for future research in multimodal medical\nreasoning.",
        "url": "http://arxiv.org/abs/2508.02669v1",
        "published_date": "2025-08-04T17:59:38+00:00",
        "updated_date": "2025-08-04T17:59:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoke Huang",
            "Juncheng Wu",
            "Hui Liu",
            "Xianfeng Tang",
            "Yuyin Zhou"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces MedVLThinker, a set of simple baselines for multimodal medical reasoning, achieving state-of-the-art results and releasing all data and code to the community.",
        "tldr_zh": "本文介绍了MedVLThinker，这是一组用于多模态医学推理的简单基线，实现了最新领域的结果，并向社区发布了所有数据和代码。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "CLIP-IN: Enhancing Fine-Grained Visual Understanding in CLIP via Instruction Editing Data and Long Captions",
        "summary": "Despite the success of Vision-Language Models (VLMs) like CLIP in aligning\nvision and language, their proficiency in detailed, fine-grained visual\ncomprehension remains a key challenge. We present CLIP-IN, a novel framework\nthat bolsters CLIP's fine-grained perception through two core innovations.\nFirstly, we leverage instruction-editing datasets, originally designed for\nimage manipulation, as a unique source of hard negative image-text pairs.\nCoupled with a symmetric hard negative contrastive loss, this enables the model\nto effectively distinguish subtle visual-semantic differences. Secondly,\nCLIP-IN incorporates long descriptive captions, utilizing rotary positional\nencodings to capture rich semantic context often missed by standard CLIP. Our\nexperiments demonstrate that CLIP-IN achieves substantial gains on the MMVP\nbenchmark and various fine-grained visual recognition tasks, without\ncompromising robust zero-shot performance on broader classification and\nretrieval tasks. Critically, integrating CLIP-IN's visual representations into\nMultimodal Large Language Models significantly reduces visual hallucinations\nand enhances reasoning abilities. This work underscores the considerable\npotential of synergizing targeted, instruction-based contrastive learning with\ncomprehensive descriptive information to elevate the fine-grained understanding\nof VLMs.",
        "url": "http://arxiv.org/abs/2508.02329v1",
        "published_date": "2025-08-04T11:57:10+00:00",
        "updated_date": "2025-08-04T11:57:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziteng Wang",
            "Siqi Yang",
            "Limeng Qiao",
            "Lin Ma"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "CLIP-IN is a framework that enhances CLIP's fine-grained visual understanding by utilizing instruction-editing datasets and long descriptive captions.",
        "tldr_zh": "CLIP-IN通过利用指令编辑数据集和长描述性字幕增强CLIP的细粒度视觉理解。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Welcome New Doctor: Continual Learning with Expert Consultation and Autoregressive Inference for Whole Slide Image Analysis",
        "summary": "Whole Slide Image (WSI) analysis, with its ability to reveal detailed tissue\nstructures in magnified views, plays a crucial role in cancer diagnosis and\nprognosis. Due to their giga-sized nature, WSIs require substantial storage and\ncomputational resources for processing and training predictive models. With the\nrapid increase in WSIs used in clinics and hospitals, there is a growing need\nfor a continual learning system that can efficiently process and adapt existing\nmodels to new tasks without retraining or fine-tuning on previous tasks. Such a\nsystem must balance resource efficiency with high performance. In this study,\nwe introduce COSFormer, a Transformer-based continual learning framework\ntailored for multi-task WSI analysis. COSFormer is designed to learn\nsequentially from new tasks wile avoiding the need to revisit full historical\ndatasets. We evaluate COSFormer on a sequence of seven WSI datasets covering\nseven organs and six WSI-related tasks under both class-incremental and\ntask-incremental settings. The results demonstrate COSFormer's superior\ngeneralizability and effectiveness compared to existing continual learning\nframeworks, establishing it as a robust solution for continual WSI analysis in\nclinical applications.",
        "url": "http://arxiv.org/abs/2508.02220v1",
        "published_date": "2025-08-04T09:11:51+00:00",
        "updated_date": "2025-08-04T09:11:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Doanh Cao Bui",
            "Jin Tae Kwak"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "COSFormer is a Transformer-based continual learning framework for multi-task WSI analysis, demonstrating superior generalizability and effectiveness in clinical applications.",
        "tldr_zh": "COSFormer是一个基于Transformer的持续学习框架，用于多任务WSI分析，在临床应用中展现出卓越的泛化能力和有效性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "REACT-KD: Region-Aware Cross-modal Topological Knowledge Distillation for Interpretable Medical Image Classification",
        "summary": "Reliable and interpretable tumor classification from clinical imaging remains\na core challenge due to heterogeneous modality quality, limited annotations,\nand the lack of structured anatomical guidance. We introduce REACT-KD, a\nRegion-Aware Cross-modal Topological Knowledge Distillation framework that\ntransfers rich supervision from high-fidelity multi-modal sources into a\nlightweight CT-based student model. The framework uses a dual teacher design:\none branch captures structure-function relationships using dual-tracer PET/CT,\nand the other models dose-aware features through synthetically degraded\nlow-dose CT data. These branches jointly guide the student model through two\ncomplementary objectives. The first focuses on semantic alignment via logits\ndistillation, while the second models anatomical topology using region graph\ndistillation. A shared CBAM-3D module is employed to maintain consistent\nattention across modalities. To improve reliability for deployment, REACT-KD\nintroduces modality dropout during training, allowing inference under partial\nor noisy inputs. The staging task for hepatocellular carcinoma (HCC) is\nconducted as a case study. REACT-KD achieves an average AUC of 93.4% on an\ninternal PET/CT cohort and maintains 76.6% to 81.5% AUC across varying dose\nlevels in external CT testing. Decision curve analysis shows that REACT-KD\nconsistently provides the highest clinical benefit across decision thresholds,\nsupporting its potential in real-world diagnostics. Code is available at\nhttps://github.com/Kinetics-JOJO/REACT-KD.",
        "url": "http://arxiv.org/abs/2508.02104v1",
        "published_date": "2025-08-04T06:29:34+00:00",
        "updated_date": "2025-08-04T06:29:34+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Hongzhao Chen",
            "Hexiao Ding",
            "Yufeng Jiang",
            "Jing Lan",
            "Ka Chun Li",
            "Gerald W. Y. Cheng",
            "Sam Ng",
            "Chi Lai Ho",
            "Jing Cai",
            "Liang-ting Lin",
            "Jung Sun Yoo"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "REACT-KD is a framework for tumor classification using cross-modal knowledge distillation, achieving high accuracy and interpretability in medical imaging.",
        "tldr_zh": "REACT-KD是一个利用跨模态知识蒸馏进行肿瘤分类的框架，在医学影像领域实现了高准确性和可解释性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Raw Data Matters: Enhancing Prompt Tuning by Internal Augmentation on Vision-Language Models",
        "summary": "For CLIP-based prompt tuning, introducing more data as additional knowledge\nfor enhancing fine-tuning process is proved to be an effective approach.\nExisting data amplification strategies for prompt tuning typically rely on\nexternal knowledge (e.g., large language models or pre-structured knowledge\nbases), resulting in higher costs for data collection and processing, while\ngenerally ignoring further utilization of features in image modality. To\naddress this, we propose Augmentation-driven Prompt Tuning (AugPT), a\nself-contained distillation-based prompt tuning approach using only internal\naugmentation on raw dataset to better exploit known features. Specifically,\nAugPT employs self-supervised augmentation on unlabeled images in the training\nset, and introduces a novel gating mechanism based on consensus test, reusing\nthe pre-trained prompt tuning backbone model to spontaneously filter noisy\nsamples, further enhancing the quality of augmented views. Extensive\nexperiments validate that AugPT simultaneously enhances model performance and\ngeneralization capability without using appended external knowledge. The code\nof AugPT is available at: https://github.com/JREion/AugPT .",
        "url": "http://arxiv.org/abs/2508.02671v1",
        "published_date": "2025-08-04T17:59:56+00:00",
        "updated_date": "2025-08-04T17:59:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoyang Li",
            "Liang Wang",
            "Chao Wang",
            "Siyu Zhou",
            "Jing Jiang",
            "Yan Peng",
            "Guodong Long"
        ],
        "ai_categories": [
            "Transformer",
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper presents an approach called AugPT for enhancing prompt tuning in vision-language models by using internal augmentation on raw data, which improves model performance without the need for external knowledge.",
        "tldr_zh": "本文提出了一种名为AugPT的方法，通过使用对原始数据的内部增强来增强视觉语言模型中的提示调整，从而提高模型性能，而无需外部知识。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PMGS: Reconstruction of Projectile Motion across Large Spatiotemporal Spans via 3D Gaussian Splatting",
        "summary": "Modeling complex rigid motion across large spatiotemporal spans remains an\nunresolved challenge in dynamic reconstruction. Existing paradigms are mainly\nconfined to short-term, small-scale deformation and offer limited consideration\nfor physical consistency. This study proposes PMGS, focusing on reconstructing\nProjectile Motion via 3D Gaussian Splatting. The workflow comprises two stages:\n1) Target Modeling: achieving object-centralized reconstruction through dynamic\nscene decomposition and an improved point density control; 2) Motion Recovery:\nrestoring full motion sequences by learning per-frame SE(3) poses. We introduce\nan acceleration consistency constraint to bridge Newtonian mechanics and pose\nestimation, and design a dynamic simulated annealing strategy that adaptively\nschedules learning rates based on motion states. Futhermore, we devise a Kalman\nfusion scheme to optimize error accumulation from multi-source observations to\nmitigate disturbances. Experiments show PMGS's superior performance in\nreconstructing high-speed nonlinear rigid motion compared to mainstream dynamic\nmethods.",
        "url": "http://arxiv.org/abs/2508.02660v1",
        "published_date": "2025-08-04T17:49:37+00:00",
        "updated_date": "2025-08-04T17:49:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yijun Xu",
            "Jingrui Zhang",
            "Yuhan Chen",
            "Dingwen Wang",
            "Lei Yu",
            "Chu He"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a method called PMGS for reconstructing high-speed nonlinear rigid motion through Projectile Motion via 3D Gaussian Splatting.",
        "tldr_zh": "本文介绍了一种名为PMGS的方法，通过三维高斯飞溅重建高速非线性刚性运动。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Evaluating Variance in Visual Question Answering Benchmarks",
        "summary": "Multimodal large language models (MLLMs) have emerged as powerful tools for\nvisual question answering (VQA), enabling reasoning and contextual\nunderstanding across visual and textual modalities. Despite their advancements,\nthe evaluation of MLLMs on VQA benchmarks often relies on point estimates,\noverlooking the significant variance in performance caused by factors such as\nstochastic model outputs, training seed sensitivity, and hyperparameter\nconfigurations. This paper critically examines these issues by analyzing\nvariance across 14 widely used VQA benchmarks, covering diverse tasks such as\nvisual reasoning, text understanding, and commonsense reasoning. We\nsystematically study the impact of training seed, framework non-determinism,\nmodel scale, and extended instruction finetuning on performance variability.\nAdditionally, we explore Cloze-style evaluation as an alternate assessment\nstrategy, studying its effectiveness in reducing stochasticity and improving\nreliability across benchmarks. Our findings highlight the limitations of\ncurrent evaluation practices and advocate for variance-aware methodologies to\nfoster more robust and reliable development of MLLMs.",
        "url": "http://arxiv.org/abs/2508.02645v1",
        "published_date": "2025-08-04T17:37:13+00:00",
        "updated_date": "2025-08-04T17:37:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nikitha SR"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper evaluates the variance in performance of multimodal large language models on visual question answering benchmarks, highlighting the need for more robust evaluation practices.",
        "tldr_zh": "本文评估了多模态大型语言模型在视觉问答基准上的性能变化，强调需要更强大的评估实践。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ReMoMask: Retrieval-Augmented Masked Motion Generation",
        "summary": "Text-to-Motion (T2M) generation aims to synthesize realistic and semantically\naligned human motion sequences from natural language descriptions. However,\ncurrent approaches face dual challenges: Generative models (e.g., diffusion\nmodels) suffer from limited diversity, error accumulation, and physical\nimplausibility, while Retrieval-Augmented Generation (RAG) methods exhibit\ndiffusion inertia, partial-mode collapse, and asynchronous artifacts. To\naddress these limitations, we propose ReMoMask, a unified framework integrating\nthree key innovations: 1) A Bidirectional Momentum Text-Motion Model decouples\nnegative sample scale from batch size via momentum queues, substantially\nimproving cross-modal retrieval precision; 2) A Semantic Spatio-temporal\nAttention mechanism enforces biomechanical constraints during part-level fusion\nto eliminate asynchronous artifacts; 3) RAG-Classier-Free Guidance incorporates\nminor unconditional generation to enhance generalization. Built upon MoMask's\nRVQ-VAE, ReMoMask efficiently generates temporally coherent motions in minimal\nsteps. Extensive experiments on standard benchmarks demonstrate the\nstate-of-the-art performance of ReMoMask, achieving a 3.88% and 10.97%\nimprovement in FID scores on HumanML3D and KIT-ML, respectively, compared to\nthe previous SOTA method RAG-T2M. Code:\nhttps://github.com/AIGeeksGroup/ReMoMask. Website:\nhttps://aigeeksgroup.github.io/ReMoMask.",
        "url": "http://arxiv.org/abs/2508.02605v1",
        "published_date": "2025-08-04T16:56:35+00:00",
        "updated_date": "2025-08-04T16:56:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhengdao Li",
            "Siheng Wang",
            "Zeyu Zhang",
            "Hao Tang"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "ReMoMask proposes a unified framework for generating human motion sequences from natural language descriptions, addressing limitations of current approaches through key innovations like bidirectional momentum model and semantic spatio-temporal attention.",
        "tldr_zh": "ReMoMask提出了一种统一框架，用于从自然语言描述中生成人体运动序列，通过双向动量模型和语义时空注意力等关键创新来解决当前方法的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Explainable AI Methods for Neuroimaging: Systematic Failures of Common Tools, the Need for Domain-Specific Validation, and a Proposal for Safe Application",
        "summary": "Trustworthy interpretation of deep learning models is critical for\nneuroimaging applications, yet commonly used Explainable AI (XAI) methods lack\nrigorous validation, risking misinterpretation. We performed the first\nlarge-scale, systematic comparison of XAI methods on ~45,000 structural brain\nMRIs using a novel XAI validation framework. This framework establishes\nverifiable ground truth by constructing prediction tasks with known signal\nsources - from localized anatomical features to subject-specific clinical\nlesions - without artificially altering input images. Our analysis reveals\nsystematic failures in two of the most widely used methods: GradCAM\nconsistently failed to localize predictive features, while Layer-wise Relevance\nPropagation generated extensive, artifactual explanations that suggest\nincompatibility with neuroimaging data characteristics. Our results indicate\nthat these failures stem from a domain mismatch, where methods with design\nprinciples tailored to natural images require substantial adaptation for\nneuroimaging data. In contrast, the simpler, gradient-based method SmoothGrad,\nwhich makes fewer assumptions about data structure, proved consistently\naccurate, suggesting its conceptual simplicity makes it more robust to this\ndomain shift. These findings highlight the need for domain-specific adaptation\nand validation of XAI methods, suggest that interpretations from prior\nneuroimaging studies using standard XAI methodology warrant re-evaluation, and\nprovide urgent guidance for practical application of XAI in neuroimaging.",
        "url": "http://arxiv.org/abs/2508.02560v1",
        "published_date": "2025-08-04T16:14:15+00:00",
        "updated_date": "2025-08-04T16:14:15+00:00",
        "categories": [
            "cs.LG",
            "cs.CV",
            "eess.IV",
            "q-bio.NC"
        ],
        "authors": [
            "Nys Tjade Siegel",
            "James H. Cole",
            "Mohamad Habes",
            "Stefan Haufe",
            "Kerstin Ritter",
            "Marc-André Schulz"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper evaluates Explainable AI methods for neuroimaging and finds that commonly used methods fail due to a domain mismatch, recommending domain-specific validation and adaptation for accurate interpretation.",
        "tldr_zh": "本文评估了神经影像学的可解释人工智能方法，发现常用方法由于领域不匹配而失败，建议进行领域特定的验证和调整以进行准确解释。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming",
        "summary": "Vision-Language Navigation (VLN) tasks often leverage panoramic RGB and depth\ninputs to provide rich spatial cues for action planning, but these sensors can\nbe costly or less accessible in real-world deployments. Recent approaches based\non Vision-Language Action (VLA) models achieve strong results with monocular\ninput, yet they still lag behind methods using panoramic RGB-D information. We\npresent MonoDream, a lightweight VLA framework that enables monocular agents to\nlearn a Unified Navigation Representation (UNR). This shared feature\nrepresentation jointly aligns navigation-relevant visual semantics (e.g.,\nglobal layout, depth, and future cues) and language-grounded action intent,\nenabling more reliable action prediction. MonoDream further introduces Latent\nPanoramic Dreaming (LPD) tasks to supervise the UNR, which train the model to\npredict latent features of panoramic RGB and depth observations at both current\nand future steps based on only monocular input. Experiments on multiple VLN\nbenchmarks show that MonoDream consistently improves monocular navigation\nperformance and significantly narrows the gap with panoramic-based agents.",
        "url": "http://arxiv.org/abs/2508.02549v1",
        "published_date": "2025-08-04T16:01:30+00:00",
        "updated_date": "2025-08-04T16:01:30+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Shuo Wang",
            "Yongcai Wang",
            "Wanting Li",
            "Yucheng Wang",
            "Maiyue Chen",
            "Kaihui Wang",
            "Zhizhong Su",
            "Xudong Cai",
            "Yeying Jin",
            "Deying Li",
            "Zhaoxin Fan"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "MonoDream is a lightweight VLA framework that improves monocular navigation performance through Unified Navigation Representation and Latent Panoramic Dreaming tasks.",
        "tldr_zh": "MonoDream是一种轻量级的VLA框架，通过统一导航表示和潜在全景梦想任务提高了单眼导航性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Precision-Aware Video Compression for Reducing Bandwidth Requirements in Video Communication for Vehicle Detection-Based Applications",
        "summary": "Computer vision has become a popular tool in intelligent transportation\nsystems (ITS), enabling various applications through roadside traffic cameras\nthat capture video and transmit it in real time to computing devices within the\nsame network. The efficiency of this video transmission largely depends on the\navailable bandwidth of the communication system. However, limited bandwidth can\nlead to communication bottlenecks, hindering the real-time performance of ITS\napplications. To mitigate this issue, lossy video compression techniques can be\nused to reduce bandwidth requirements, at the cost of degrading video quality.\nThis degradation can negatively impact the accuracy of applications that rely\non real-time vehicle detection. Additionally, vehicle detection accuracy is\ninfluenced by environmental factors such as weather and lighting conditions,\nsuggesting that compression levels should be dynamically adjusted in response\nto these variations. In this work, we utilize a framework called\nPrecision-Aware Video Compression (PAVC), where a roadside video camera\ncaptures footage of vehicles on roadways, compresses videos, and then transmits\nthem to a processing unit, running a vehicle detection algorithm for\nsafety-critical applications, such as real-time collision risk assessment. The\nsystem dynamically adjusts the video compression level based on current weather\nand lighting conditions to maintain vehicle detection accuracy while minimizing\nbandwidth usage. Our results demonstrate that PAVC improves vehicle detection\naccuracy by up to 13% and reduces communication bandwidth requirements by up to\n8.23x in areas with moderate bandwidth availability. Moreover, in locations\nwith severely limited bandwidth, PAVC reduces bandwidth requirements by up to\n72x while preserving vehicle detection performance.",
        "url": "http://arxiv.org/abs/2508.02533v1",
        "published_date": "2025-08-04T15:41:52+00:00",
        "updated_date": "2025-08-04T15:41:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Abyad Enan",
            "Jon C Calhoun",
            "Mashrur Chowdhury"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper presents a Precision-Aware Video Compression (PAVC) framework for improving vehicle detection accuracy in intelligent transportation systems by dynamically adjusting video compression levels based on weather and lighting conditions.",
        "tldr_zh": "该论文提出了一种精度感知视频压缩（PAVC）框架，通过根据天气和光照条件动态调整视频压缩级别，以提高智能交通系统中的车辆检测准确性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From Pixels to Pathology: Restoration Diffusion for Diagnostic-Consistent Virtual IHC",
        "summary": "Hematoxylin and eosin (H&E) staining is the clinical standard for assessing\ntissue morphology, but it lacks molecular-level diagnostic information. In\ncontrast, immunohistochemistry (IHC) provides crucial insights into biomarker\nexpression, such as HER2 status for breast cancer grading, but remains costly\nand time-consuming, limiting its use in time-sensitive clinical workflows. To\naddress this gap, virtual staining from H&E to IHC has emerged as a promising\nalternative, yet faces two core challenges: (1) Lack of fair evaluation of\nsynthetic images against misaligned IHC ground truths, and (2) preserving\nstructural integrity and biological variability during translation. To this\nend, we present an end-to-end framework encompassing both generation and\nevaluation in this work. We introduce Star-Diff, a structure-aware staining\nrestoration diffusion model that reformulates virtual staining as an image\nrestoration task. By combining residual and noise-based generation pathways,\nStar-Diff maintains tissue structure while modeling realistic biomarker\nvariability. To evaluate the diagnostic consistency of the generated IHC\npatches, we propose the Semantic Fidelity Score (SFS), a\nclinical-grading-task-driven metric that quantifies class-wise semantic\ndegradation based on biomarker classification accuracy. Unlike pixel-level\nmetrics such as SSIM and PSNR, SFS remains robust under spatial misalignment\nand classifier uncertainty. Experiments on the BCI dataset demonstrate that\nStar-Diff achieves state-of-the-art (SOTA) performance in both visual fidelity\nand diagnostic relevance. With rapid inference and strong clinical alignment,it\npresents a practical solution for applications such as intraoperative virtual\nIHC synthesis.",
        "url": "http://arxiv.org/abs/2508.02528v1",
        "published_date": "2025-08-04T15:36:58+00:00",
        "updated_date": "2025-08-04T15:36:58+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Jingsong Liu",
            "Xiaofeng Deng",
            "Han Li",
            "Azar Kazemi",
            "Christian Grashei",
            "Gesa Wilkens",
            "Xin You",
            "Tanja Groll",
            "Nassir Navab",
            "Carolin Mogler",
            "Peter J. Schüffler"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper presents a framework for generating virtual IHC images from H&E stained images, achieving state-of-the-art visual fidelity and diagnostic relevance.",
        "tldr_zh": "该论文提出了一个从H&E染色图像生成虚拟IHC图像的框架，实现了最先进的视觉保真度和诊断相关性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Engagement Prediction of Short Videos with Large Multimodal Models",
        "summary": "The rapid proliferation of user-generated content (UGC) on short-form video\nplatforms has made video engagement prediction increasingly important for\noptimizing recommendation systems and guiding content creation. However, this\ntask remains challenging due to the complex interplay of factors such as\nsemantic content, visual quality, audio characteristics, and user background.\nPrior studies have leveraged various types of features from different\nmodalities, such as visual quality, semantic content, background sound, etc.,\nbut often struggle to effectively model their cross-feature and cross-modality\ninteractions. In this work, we empirically investigate the potential of large\nmultimodal models (LMMs) for video engagement prediction. We adopt two\nrepresentative LMMs: VideoLLaMA2, which integrates audio, visual, and language\nmodalities, and Qwen2.5-VL, which models only visual and language modalities.\nSpecifically, VideoLLaMA2 jointly processes key video frames, text-based\nmetadata, and background sound, while Qwen2.5-VL utilizes only key video frames\nand text-based metadata. Trained on the SnapUGC dataset, both models\ndemonstrate competitive performance against state-of-the-art baselines,\nshowcasing the effectiveness of LMMs in engagement prediction. Notably,\nVideoLLaMA2 consistently outperforms Qwen2.5-VL, highlighting the importance of\naudio features in engagement prediction. By ensembling two types of models, our\nmethod achieves first place in the ICCV VQualA 2025 EVQA-SnapUGC Challenge on\nshort-form video engagement prediction. The code is available at\nhttps://github.com/sunwei925/LMM-EVQA.git.",
        "url": "http://arxiv.org/abs/2508.02516v1",
        "published_date": "2025-08-04T15:21:29+00:00",
        "updated_date": "2025-08-04T15:21:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wei Sun",
            "Linhan Cao",
            "Yuqin Cao",
            "Weixia Zhang",
            "Wen Wen",
            "Kaiwei Zhang",
            "Zijian Chen",
            "Fangfang Lu",
            "Xiongkuo Min",
            "Guangtao Zhai"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper explores the use of large multimodal models for predicting engagement with short videos, showing competitive performance and highlighting the importance of audio features.",
        "tldr_zh": "本文探讨了使用大型多模态模型预测观众与短视频的互动，在竞争性表现方面，强调了音频特征的重要性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots",
        "summary": "Panoramic cameras, capturing comprehensive 360-degree environmental data, are\nsuitable for quadruped robots in surrounding perception and interaction with\ncomplex environments. However, the scarcity of high-quality panoramic training\ndata-caused by inherent kinematic constraints and complex sensor calibration\nchallenges-fundamentally limits the development of robust perception systems\ntailored to these embodied platforms. To address this issue, we propose\nQuaDreamer-the first panoramic data generation engine specifically designed for\nquadruped robots. QuaDreamer focuses on mimicking the motion paradigm of\nquadruped robots to generate highly controllable, realistic panoramic videos,\nproviding a data source for downstream tasks. Specifically, to effectively\ncapture the unique vertical vibration characteristics exhibited during\nquadruped locomotion, we introduce Vertical Jitter Encoding (VJE). VJE extracts\ncontrollable vertical signals through frequency-domain feature filtering and\nprovides high-quality prompts. To facilitate high-quality panoramic video\ngeneration under jitter signal control, we propose a Scene-Object Controller\n(SOC) that effectively manages object motion and boosts background jitter\ncontrol through the attention mechanism. To address panoramic distortions in\nwide-FoV video generation, we propose the Panoramic Enhancer (PE)-a dual-stream\narchitecture that synergizes frequency-texture refinement for local detail\nenhancement with spatial-structure correction for global geometric consistency.\nWe further demonstrate that the generated video sequences can serve as training\ndata for the quadruped robot's panoramic visual perception model, enhancing the\nperformance of multi-object tracking in 360-degree scenes. The source code and\nmodel weights will be publicly available at\nhttps://github.com/losehu/QuaDreamer.",
        "url": "http://arxiv.org/abs/2508.02512v1",
        "published_date": "2025-08-04T15:18:01+00:00",
        "updated_date": "2025-08-04T15:18:01+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Sheng Wu",
            "Fei Teng",
            "Hao Shi",
            "Qi Jiang",
            "Kai Luo",
            "Kaiwei Wang",
            "Kailun Yang"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Transformer",
            "Other"
        ],
        "tldr": "QuaDreamer proposes a panoramic data generation engine for quadruped robots, focusing on mimicking their motion paradigm to generate controllable panoramic videos for perception tasks.",
        "tldr_zh": "QuaDreamer提出了一种专门为四足机器人设计的全景数据生成引擎，重点是模仿它们的运动范例，生成可控的全景视频用于感知任务。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Rethinking Transparent Object Grasping: Depth Completion with Monocular Depth Estimation and Instance Mask",
        "summary": "Due to the optical properties, transparent objects often lead depth cameras\nto generate incomplete or invalid depth data, which in turn reduces the\naccuracy and reliability of robotic grasping. Existing approaches typically\ninput the RGB-D image directly into the network to output the complete depth,\nexpecting the model to implicitly infer the reliability of depth values.\nHowever, while effective in training datasets, such methods often fail to\ngeneralize to real-world scenarios, where complex light interactions lead to\nhighly variable distributions of valid and invalid depth data. To address this,\nwe propose ReMake, a novel depth completion framework guided by an instance\nmask and monocular depth estimation. By explicitly distinguishing transparent\nregions from non-transparent ones, the mask enables the model to concentrate on\nlearning accurate depth estimation in these areas from RGB-D input during\ntraining. This targeted supervision reduces reliance on implicit reasoning and\nimproves generalization to real-world scenarios. Additionally, monocular depth\nestimation provides depth context between the transparent object and its\nsurroundings, enhancing depth prediction accuracy. Extensive experiments show\nthat our method outperforms existing approaches on both benchmark datasets and\nreal-world scenarios, demonstrating superior accuracy and generalization\ncapability. Code and videos are available at\nhttps://chengyaofeng.github.io/ReMake.github.io/.",
        "url": "http://arxiv.org/abs/2508.02507v1",
        "published_date": "2025-08-04T15:14:47+00:00",
        "updated_date": "2025-08-04T15:14:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yaofeng Cheng",
            "Xinkai Gao",
            "Sen Zhang",
            "Chao Zeng",
            "Fusheng Zha",
            "Lining Sun",
            "Chenguang Yang"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper proposes ReMake, a depth completion framework using instance mask and monocular depth estimation to improve accuracy in grasping transparent objects.",
        "tldr_zh": "本文提出了ReMake，一种深度完成框架，使用实例掩模和单眼深度估计，以提高透明对象抓取的准确性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Low-Frequency First: Eliminating Floating Artifacts in 3D Gaussian Splatting",
        "summary": "3D Gaussian Splatting (3DGS) is a powerful and computationally efficient\nrepresentation for 3D reconstruction. Despite its strengths, 3DGS often\nproduces floating artifacts, which are erroneous structures detached from the\nactual geometry and significantly degrade visual fidelity. The underlying\nmechanisms causing these artifacts, particularly in low-quality initialization\nscenarios, have not been fully explored. In this paper, we investigate the\norigins of floating artifacts from a frequency-domain perspective and identify\nunder-optimized Gaussians as the primary source. Based on our analysis, we\npropose \\textit{Eliminating-Floating-Artifacts} Gaussian Splatting (EFA-GS),\nwhich selectively expands under-optimized Gaussians to prioritize accurate\nlow-frequency learning. Additionally, we introduce complementary depth-based\nand scale-based strategies to dynamically refine Gaussian expansion,\neffectively mitigating detail erosion. Extensive experiments on both synthetic\nand real-world datasets demonstrate that EFA-GS substantially reduces floating\nartifacts while preserving high-frequency details, achieving an improvement of\n1.68 dB in PSNR over baseline method on our RWLQ dataset. Furthermore, we\nvalidate the effectiveness of our approach in downstream 3D editing tasks. Our\nimplementation will be released on GitHub.",
        "url": "http://arxiv.org/abs/2508.02493v1",
        "published_date": "2025-08-04T15:03:56+00:00",
        "updated_date": "2025-08-04T15:03:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jianchao Wang",
            "Peng Zhou",
            "Cen Li",
            "Rong Quan",
            "Jie Qin"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper addresses the issue of floating artifacts in 3D reconstruction caused by under-optimized Gaussians, proposing a method to reduce these artifacts while preserving high-frequency details.",
        "tldr_zh": "该论文解决了3D重建中由于优化不足的高斯函数导致的漂浮伪影问题，提出了一种方法来减少这些伪影同时保留高频细节。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MindShot: Multi-Shot Video Reconstruction from fMRI with LLM Decoding",
        "summary": "Reconstructing dynamic videos from fMRI is important for understanding visual\ncognition and enabling vivid brain-computer interfaces. However, current\nmethods are critically limited to single-shot clips, failing to address the\nmulti-shot nature of real-world experiences. Multi-shot reconstruction faces\nfundamental challenges: fMRI signal mixing across shots, the temporal\nresolution mismatch between fMRI and video obscuring rapid scene changes, and\nthe lack of dedicated multi-shot fMRI-video datasets. To overcome these\nlimitations, we propose a novel divide-and-decode framework for multi-shot fMRI\nvideo reconstruction. Our core innovations are: (1) A shot boundary predictor\nmodule explicitly decomposing mixed fMRI signals into shot-specific segments.\n(2) Generative keyframe captioning using LLMs, which decodes robust textual\ndescriptions from each segment, overcoming temporal blur by leveraging\nhigh-level semantics. (3) Novel large-scale data synthesis (20k samples) from\nexisting datasets. Experimental results demonstrate our framework outperforms\nstate-of-the-art methods in multi-shot reconstruction fidelity. Ablation\nstudies confirm the critical role of fMRI decomposition and semantic\ncaptioning, with decomposition significantly improving decoded caption CLIP\nsimilarity by 71.8%. This work establishes a new paradigm for multi-shot fMRI\nreconstruction, enabling accurate recovery of complex visual narratives through\nexplicit decomposition and semantic prompting.",
        "url": "http://arxiv.org/abs/2508.02480v1",
        "published_date": "2025-08-04T14:47:17+00:00",
        "updated_date": "2025-08-04T14:47:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenwen Zeng",
            "Yonghuang Wu",
            "Yifan Chen",
            "Xuan Xie",
            "Chengqian Zhao",
            "Feiyu Yin",
            "Guoqing Wu",
            "Jinhua Yu"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel framework for reconstructing multi-shot videos from fMRI data, outperforming current methods in fidelity.",
        "tldr_zh": "该论文提出了一种新颖的框架，用于从fMRI数据重建多次拍摄的视频，在保真度方面优于当前方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Fine-grained Multiple Supervisory Network for Multi-modal Manipulation Detecting and Grounding",
        "summary": "The task of Detecting and Grounding Multi-Modal Media Manipulation (DGM$^4$)\nis a branch of misinformation detection. Unlike traditional binary\nclassification, it includes complex subtasks such as forgery content\nlocalization and forgery method classification. Consider that existing methods\nare often limited in performance due to neglecting the erroneous interference\ncaused by unreliable unimodal data and failing to establish comprehensive\nforgery supervision for mining fine-grained tampering traces. In this paper, we\npresent a Fine-grained Multiple Supervisory (FMS) network, which incorporates\nmodality reliability supervision, unimodal internal supervision and cross-modal\nsupervision to provide comprehensive guidance for DGM$^4$ detection. For\nmodality reliability supervision, we propose the Multimodal Decision Supervised\nCorrection (MDSC) module. It leverages unimodal weak supervision to correct the\nmulti-modal decision-making process. For unimodal internal supervision, we\npropose the Unimodal Forgery Mining Reinforcement (UFMR) module. It amplifies\nthe disparity between real and fake information within unimodal modality from\nboth feature-level and sample-level perspectives. For cross-modal supervision,\nwe propose the Multimodal Forgery Alignment Reasoning (MFAR) module. It\nutilizes soft-attention interactions to achieve cross-modal feature perception\nfrom both consistency and inconsistency perspectives, where we also design the\ninteraction constraints to ensure the interaction quality. Extensive\nexperiments demonstrate the superior performance of our FMS compared to\nstate-of-the-art methods.",
        "url": "http://arxiv.org/abs/2508.02479v1",
        "published_date": "2025-08-04T14:46:59+00:00",
        "updated_date": "2025-08-04T14:46:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinquan Yu",
            "Wei Lu",
            "Xiangyang Luo"
        ],
        "ai_categories": [
            "Multimodality"
        ],
        "tldr": "The paper introduces a Fine-grained Multiple Supervisory Network for Detecting and Grounding Multi-Modal Media Manipulation, outperforming existing methods in extensive experiments.",
        "tldr_zh": "本文介绍了一种用于检测和定位多模态媒体操纵的精细多监督网络，在广泛实验中表现优异。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multi-class Image Anomaly Detection for Practical Applications: Requirements and Robust Solutions",
        "summary": "Recent advances in image anomaly detection have extended unsupervised\nlearning-based models from single-class settings to multi-class frameworks,\naiming to improve efficiency in training time and model storage. When a single\nmodel is trained to handle multiple classes, it often underperforms compared to\nclass-specific models in terms of per-class detection accuracy. Accordingly,\nprevious studies have primarily focused on narrowing this performance gap.\nHowever, the way class information is used, or not used, remains a relatively\nunderstudied factor that could influence how detection thresholds are defined\nin multi-class image anomaly detection. These thresholds, whether\nclass-specific or class-agnostic, significantly affect detection outcomes. In\nthis study, we identify and formalize the requirements that a multi-class image\nanomaly detection model must satisfy under different conditions, depending on\nwhether class labels are available during training and evaluation. We then\nre-examine existing methods under these criteria. To meet these challenges, we\npropose Hierarchical Coreset (HierCore), a novel framework designed to satisfy\nall defined requirements. HierCore operates effectively even without class\nlabels, leveraging a hierarchical memory bank to estimate class-wise decision\ncriteria for anomaly detection. We empirically validate the applicability and\nrobustness of existing methods and HierCore under four distinct scenarios,\ndetermined by the presence or absence of class labels in the training and\nevaluation phases. The experimental results demonstrate that HierCore\nconsistently meets all requirements and maintains strong, stable performance\nacross all settings, highlighting its practical potential for real-world\nmulti-class anomaly detection tasks.",
        "url": "http://arxiv.org/abs/2508.02477v1",
        "published_date": "2025-08-04T14:44:40+00:00",
        "updated_date": "2025-08-04T14:44:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jaehyuk Heo",
            "Pilsung Kang"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a new framework called Hierarchical Coreset (HierCore) for multi-class image anomaly detection, addressing the issue of underperformance in single model detection compared to class-specific models.",
        "tldr_zh": "该论文介绍了一种名为分层核心集（HierCore）的新框架，用于多类图像异常检测，解决了单一模型检测性能不及特定类别模型的问题。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SAMPO: Visual Preference Optimization for Intent-Aware Segmentation with Vision Foundation Models",
        "summary": "Foundation models like Segment Anything Model (SAM) excel in promptable\nsegmentation but suffer from an intent gap: they segment only explicitly\nprompted objects, failing to generalize to semantically related instances\nimplicitly desired by users. This limitation is critical in domains with dense\nhomogeneous objects (e.g., biomedical nuclei segmentation), where sparse visual\nprompts typically yield incomplete results, rendering dense annotations\nimpractical due to prohibitive cost. To bridge this gap, we introduce SAMPO\n(Segment Anything Model with Preference Optimization), a novel framework that\nteaches visual foundation models to infer high-level categorical intent from\nsparse visual interactions. Unlike conventional pixel-level fine-tuning, SAMPO\noptimizes models to implicitly capture target-class characteristics through\npreference optimization. This approach, which operates without dependency on\nlanguage models, enables robust multi-object segmentation even under sparse\nprompting and demonstrates superior data efficiency during fine-tuning.\nValidated on three medical segmentation tasks, SAMPO achieves state-of-the-art\nperformance: on challenging tasks like PanNuke-T2, our method, when fine-tuned\nwith only 10% of the training data, significantly outperforms all existing\nmethods trained on the full 100% dataset, achieving an improvement of over 9\npercentage points compared to the best baseline. Our work establishes a new\nparadigm for intent-aware alignment in visual foundation models, removing\ndependencies on auxiliary prompt generators or language-model-assisted\npreference learning.",
        "url": "http://arxiv.org/abs/2508.02464v1",
        "published_date": "2025-08-04T14:31:11+00:00",
        "updated_date": "2025-08-04T14:31:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yonghuang Wu",
            "Wenwen Zeng",
            "Xuan Xie",
            "Chengqian Zhao",
            "Guoqing Wu",
            "Jinhua Yu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "SAMPO introduces a framework to improve intent-aware segmentation by inferring high-level categorical intent from sparse visual interactions, achieving state-of-the-art performance on medical segmentation tasks.",
        "tldr_zh": "SAMPO引入了一个框架，通过从稀疏视觉交互中推断高级分类意图来改善意图感知分割，在医学分割任务上表现卓越。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Glioblastoma Overall Survival Prediction With Vision Transformers",
        "summary": "Glioblastoma is one of the most aggressive and common brain tumors, with a\nmedian survival of 10-15 months. Predicting Overall Survival (OS) is critical\nfor personalizing treatment strategies and aligning clinical decisions with\npatient outcomes. In this study, we propose a novel Artificial Intelligence\n(AI) approach for OS prediction using Magnetic Resonance Imaging (MRI) images,\nexploiting Vision Transformers (ViTs) to extract hidden features directly from\nMRI images, eliminating the need of tumor segmentation. Unlike traditional\napproaches, our method simplifies the workflow and reduces computational\nresource requirements.\n  The proposed model was evaluated on the BRATS dataset, reaching an accuracy\nof 62.5% on the test set, comparable to the top-performing methods.\nAdditionally, it demonstrated balanced performance across precision, recall,\nand F1 score, overcoming the best model in these metrics. The dataset size\nlimits the generalization of the ViT which typically requires larger datasets\ncompared to convolutional neural networks. This limitation in generalization is\nobserved across all the cited studies. This work highlights the applicability\nof ViTs for downsampled medical imaging tasks and establishes a foundation for\nOS prediction models that are computationally efficient and do not rely on\nsegmentation.",
        "url": "http://arxiv.org/abs/2508.02439v1",
        "published_date": "2025-08-04T13:59:57+00:00",
        "updated_date": "2025-08-04T13:59:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yin Lin",
            "iccardo Barbieri",
            "Domenico Aquino",
            "Giuseppe Lauria",
            "Marina Grisoli",
            "Elena De Momi",
            "Alberto Redaelli",
            "Simona Ferrante"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel AI approach using Vision Transformers to predict Overall Survival in Glioblastoma patients from MRI images without the need for tumor segmentation.",
        "tldr_zh": "该论文介绍了一种新的人工智能方法，使用ViTs从MRI图像中预测胶质母细胞瘤患者的总体生存，无需肿瘤分割。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination via Attention Lens",
        "summary": "Large vision-language models (LVLMs) have demonstrated remarkable multimodal\ncomprehension and reasoning capabilities, but they still suffer from severe\nobject hallucination. Previous studies primarily attribute the flaw to\nlinguistic prior caused by the scale mismatch between visual encoders and large\nlanguage models (LLMs) in LVLMs. Specifically, as current LVLMs are built upon\nLLMs, they tend to over-rely on textual prompts and internal knowledge of LLMs,\ngenerating descriptions inconsistent with visual cues. However, through an\nin-depth investigation of the hallucinated mechanisms, we empirically reveal a\npreviously overlooked phenomenon: LVLMs may ignore not only visual information\nbut also textual modality during hallucination, a behavior termed as modality\nbias, which indicates that LVLMs struggle to simultaneously attend to both\nvisual and textual modalities, leading to fragmented understanding of\nuser-provided instructions. Based on this observation, we propose a simple yet\neffective training-free method to mitigate object hallucination. Concretely, we\nintervene and adjust the attention weights of textual and visual tokens,\nbalancing cross-modal compatibility for better alignment with user intentions.\nFurthermore, we adopt a contrastive decoding strategy to reduce the LVLM's\noverreliance on its parametric knowledge, synergistically enhancing our\nattention manipulation. Extensive experiments confirm the widespread presence\nof modality bias in LVLMs. Notably, our method effectively mitigates\nhallucination across multiple open-source LVLMs and benchmarks, highlighting\nits generalizability and efficacy.",
        "url": "http://arxiv.org/abs/2508.02419v1",
        "published_date": "2025-08-04T13:40:59+00:00",
        "updated_date": "2025-08-04T13:40:59+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Haohan Zheng",
            "Zhenguo Zhang"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer"
        ],
        "tldr": "The paper addresses object hallucination in large vision-language models by identifying and mitigating modality bias through attention manipulation, leading to improved alignment with user intentions.",
        "tldr_zh": "本文通过注意力干预来识别和减轻大型视觉语言模型中的模态偏差，从而改善与用户意图的对齐。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Hydra: Accurate Multi-Modal Leaf Wetness Sensing with mm-Wave and Camera Fusion",
        "summary": "Leaf Wetness Duration (LWD), the time that water remains on leaf surfaces, is\ncrucial in the development of plant diseases. Existing LWD detection lacks\nstandardized measurement techniques, and variations across different plant\ncharacteristics limit its effectiveness. Prior research proposes diverse\napproaches, but they fail to measure real natural leaves directly and lack\nresilience in various environmental conditions. This reduces the precision and\nrobustness, revealing a notable practical application and effectiveness gap in\nreal-world agricultural settings. This paper presents Hydra, an innovative\napproach that integrates millimeter-wave (mm-Wave) radar with camera technology\nto detect leaf wetness by determining if there is water on the leaf. We can\nmeasure the time to determine the LWD based on this detection. Firstly, we\ndesign a Convolutional Neural Network (CNN) to selectively fuse multiple\nmm-Wave depth images with an RGB image to generate multiple feature images.\nThen, we develop a transformer-based encoder to capture the inherent connection\namong the multiple feature images to generate a feature map, which is further\nfed to a classifier for detection. Moreover, we augment the dataset during\ntraining to generalize our model. Implemented using a frequency-modulated\ncontinuous-wave (FMCW) radar within the 76 to 81 GHz band, Hydra's performance\nis meticulously evaluated on plants, demonstrating the potential to classify\nleaf wetness with up to 96% accuracy across varying scenarios. Deploying Hydra\nin the farm, including rainy, dawn, or poorly light nights, it still achieves\nan accuracy rate of around 90%.",
        "url": "http://arxiv.org/abs/2508.02409v1",
        "published_date": "2025-08-04T13:33:06+00:00",
        "updated_date": "2025-08-04T13:33:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yimeng Liu",
            "Maolin Gan",
            "Huaili Zeng",
            "Li Liu",
            "Younsuk Dong",
            "Zhichao Cao"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "Hydra presents a novel approach that combines mm-Wave radar and camera technology to detect leaf wetness duration on plants, showing high accuracy in classifying leaf wetness in various scenarios.",
        "tldr_zh": "Hydra提出了一种结合毫米波雷达和摄像技术的新方法，用于检测植物上的叶片湿润时间，展示在各种情景下对叶片湿度的分类准确率很高。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GR-Gaussian: Graph-Based Radiative Gaussian Splatting for Sparse-View CT Reconstruction",
        "summary": "3D Gaussian Splatting (3DGS) has emerged as a promising approach for CT\nreconstruction. However, existing methods rely on the average gradient\nmagnitude of points within the view, often leading to severe needle-like\nartifacts under sparse-view conditions. To address this challenge, we propose\nGR-Gaussian, a graph-based 3D Gaussian Splatting framework that suppresses\nneedle-like artifacts and improves reconstruction accuracy under sparse-view\nconditions. Our framework introduces two key innovations: (1) a Denoised Point\nCloud Initialization Strategy that reduces initialization errors and\naccelerates convergence; and (2) a Pixel-Graph-Aware Gradient Strategy that\nrefines gradient computation using graph-based density differences, improving\nsplitting accuracy and density representation. Experiments on X-3D and\nreal-world datasets validate the effectiveness of GR-Gaussian, achieving PSNR\nimprovements of 0.67 dB and 0.92 dB, and SSIM gains of 0.011 and 0.021. These\nresults highlight the applicability of GR-Gaussian for accurate CT\nreconstruction under challenging sparse-view conditions.",
        "url": "http://arxiv.org/abs/2508.02408v1",
        "published_date": "2025-08-04T13:31:42+00:00",
        "updated_date": "2025-08-04T13:31:42+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Yikuang Yuluo",
            "Yue Ma",
            "Kuan Shen",
            "Tongtong Jin",
            "Wang Liao",
            "Yangpu Ma",
            "Fuquan Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "GR-Gaussian is a graph-based 3D Gaussian Splatting framework that improves CT reconstruction accuracy under sparse-view conditions by suppressing needle-like artifacts and introducing key innovations.",
        "tldr_zh": "GR-Gaussian是一种基于图的3D高斯飞溅框架，通过抑制稀疏视图条件下的针状伪影并引入关键创新，提高CT重建精度。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Improving Generalization of Language-Conditioned Robot Manipulation",
        "summary": "The control of robots for manipulation tasks generally relies on visual\ninput. Recent advances in vision-language models (VLMs) enable the use of\nnatural language instructions to condition visual input and control robots in a\nwider range of environments. However, existing methods require a large amount\nof data to fine-tune VLMs for operating in unseen environments. In this paper,\nwe present a framework that learns object-arrangement tasks from just a few\ndemonstrations. We propose a two-stage framework that divides\nobject-arrangement tasks into a target localization stage, for picking the\nobject, and a region determination stage for placing the object. We present an\ninstance-level semantic fusion module that aligns the instance-level image\ncrops with the text embedding, enabling the model to identify the target\nobjects defined by the natural language instructions. We validate our method on\nboth simulation and real-world robotic environments. Our method, fine-tuned\nwith a few demonstrations, improves generalization capability and demonstrates\nzero-shot ability in real-robot manipulation scenarios.",
        "url": "http://arxiv.org/abs/2508.02405v1",
        "published_date": "2025-08-04T13:29:26+00:00",
        "updated_date": "2025-08-04T13:29:26+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Chenglin Cui",
            "Chaoran Zhu",
            "Changjae Oh",
            "Andrea Cavallaro"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper presents a framework that improves the generalization of language-conditioned robot manipulation tasks with limited demonstrations, demonstrating zero-shot ability in real-robot scenarios.",
        "tldr_zh": "本文提出了一个框架，通过有限的示范来提高语言条件的机器人操纵任务的泛化性，在真实机器人场景中展示了零-shot能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SMART-Ship: A Comprehensive Synchronized Multi-modal Aligned Remote Sensing Targets Dataset and Benchmark for Berthed Ships Analysis",
        "summary": "Given the limitations of satellite orbits and imaging conditions, multi-modal\nremote sensing (RS) data is crucial in enabling long-term earth observation.\nHowever, maritime surveillance remains challenging due to the complexity of\nmulti-scale targets and the dynamic environments. To bridge this critical gap,\nwe propose a Synchronized Multi-modal Aligned Remote sensing Targets dataset\nfor berthed ships analysis (SMART-Ship), containing spatiotemporal registered\nimages with fine-grained annotation for maritime targets from five modalities:\nvisible-light, synthetic aperture radar (SAR), panchromatic, multi-spectral,\nand near-infrared. Specifically, our dataset consists of 1092 multi-modal image\nsets, covering 38,838 ships. Each image set is acquired within one week and\nregistered to ensure spatiotemporal consistency. Ship instances in each set are\nannotated with polygonal location information, fine-grained categories,\ninstance-level identifiers, and change region masks, organized hierarchically\nto support diverse multi-modal RS tasks. Furthermore, we define standardized\nbenchmarks on five fundamental tasks and comprehensively compare representative\nmethods across the dataset. Thorough experiment evaluations validate that the\nproposed SMART-Ship dataset could support various multi-modal RS interpretation\ntasks and reveal the promising directions for further exploration.",
        "url": "http://arxiv.org/abs/2508.02384v1",
        "published_date": "2025-08-04T13:09:58+00:00",
        "updated_date": "2025-08-04T13:09:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chen-Chen Fan",
            "Peiyao Guo",
            "Linping Zhang",
            "Kehan Qi",
            "Haolin Huang",
            "Yong-Qiang Mao",
            "Yuxi Suo",
            "Zhizhuo Jiang",
            "Yu Liu",
            "You He"
        ],
        "ai_categories": [
            "Dataset",
            "Multimodality"
        ],
        "tldr": "The paper introduces SMART-Ship, a dataset for maritime targets analysis using multi-modal remote sensing data, supporting various RS tasks.",
        "tldr_zh": "本文介绍了SMART-Ship，这是一个使用多模态遥感数据进行海洋目标分析的数据集，支持各种RS任务。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Uni-Layout: Integrating Human Feedback in Unified Layout Generation and Evaluation",
        "summary": "Layout generation plays a crucial role in enhancing both user experience and\ndesign efficiency. However, current approaches suffer from task-specific\ngeneration capabilities and perceptually misaligned evaluation metrics, leading\nto limited applicability and ineffective measurement. In this paper, we propose\n\\textit{Uni-Layout}, a novel framework that achieves unified generation,\nhuman-mimicking evaluation and alignment between the two. For universal\ngeneration, we incorporate various layout tasks into a single taxonomy and\ndevelop a unified generator that handles background or element contents\nconstrained tasks via natural language prompts. To introduce human feedback for\nthe effective evaluation of layouts, we build \\textit{Layout-HF100k}, the first\nlarge-scale human feedback dataset with 100,000 expertly annotated layouts.\nBased on \\textit{Layout-HF100k}, we introduce a human-mimicking evaluator that\nintegrates visual and geometric information, employing a Chain-of-Thought\nmechanism to conduct qualitative assessments alongside a confidence estimation\nmodule to yield quantitative measurements. For better alignment between the\ngenerator and the evaluator, we integrate them into a cohesive system by\nadopting Dynamic-Margin Preference Optimization (DMPO), which dynamically\nadjusts margins based on preference strength to better align with human\njudgments. Extensive experiments show that \\textit{Uni-Layout} significantly\noutperforms both task-specific and general-purpose methods. Our code is\npublicly available at https://github.com/JD-GenX/Uni-Layout.",
        "url": "http://arxiv.org/abs/2508.02374v1",
        "published_date": "2025-08-04T13:02:23+00:00",
        "updated_date": "2025-08-04T13:02:23+00:00",
        "categories": [
            "cs.CV",
            "cs.IR",
            "cs.LG"
        ],
        "authors": [
            "Shuo Lu",
            "Yanyin Chen",
            "Wei Feng",
            "Jiahao Fan",
            "Fengheng Li",
            "Zheng Zhang",
            "Jingjing Lv",
            "Junjie Shen",
            "Ching Law",
            "Jian Liang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "Uni-Layout proposes a framework for unified layout generation and evaluation, incorporating human feedback for effective assessment. It outperforms task-specific and general-purpose methods.",
        "tldr_zh": "Uni-Layout提出了一个统一的布局生成和评估框架，结合人类反馈进行有效评估。它优于特定任务和通用方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Transport-Guided Rectified Flow Inversion: Improved Image Editing Using Optimal Transport Theory",
        "summary": "Effective image inversion in rectified flow models - mapping real images to\neditable latent representations - is crucial for practical image editing\napplications; however, achieving optimal balance between reconstruction\nfidelity and editing flexibility remains a fundamental challenge. In this work,\nwe introduce the Optimal Transport Inversion Pipeline (OTIP), a zero-shot\nframework that leverages optimal transport theory to guide the inversion\nprocess in rectified flow models. Our underlying hypothesis is that\nincorporating transport-based guidance during the reverse diffusion process can\neffectively balance reconstruction accuracy and editing controllability through\nprincipled trajectory optimization. The method computes optimal transport paths\nbetween image and noise distributions while maintaining computational\nefficiency. Our approach achieves high-fidelity reconstruction with LPIPS\nscores of 0.001 and SSIM of 0.992 on face editing benchmarks, demonstrating\nsuperior preservation of fine-grained details compared to existing methods. We\nevaluate the framework across multiple editing tasks, observing 7.8% to 12.9%\nimprovements in reconstruction loss over RF-Inversion on the LSUN-Bedroom and\nLSUN-Church datasets, respectively. For semantic face editing, our method\nachieves an 11.2% improvement in identity preservation and a 1.6% enhancement\nin perceptual quality, while maintaining computational efficiency comparable to\nbaseline approaches. Qualitatively, our method produces visually compelling\nedits with superior semantic consistency and fine-grained detail preservation\nacross diverse editing scenarios. Code is available at:\nhttps://github.com/marianlupascu/OT-Inversion",
        "url": "http://arxiv.org/abs/2508.02363v1",
        "published_date": "2025-08-04T12:50:58+00:00",
        "updated_date": "2025-08-04T12:50:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Marian Lupascu",
            "Mihai-Sorin Stupariu"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper introduces a framework using optimal transport theory to improve image editing in rectified flow models, achieving high-fidelity reconstruction and better editing controllability.",
        "tldr_zh": "本文引入了一个框架，利用最优输运理论改进了矫正流模型中的图像编辑，实现了高保真度的重建和更好的编辑可控性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "mmWave Radar-Based Non-Line-of-Sight Pedestrian Localization at T-Junctions Utilizing Road Layout Extraction via Camera",
        "summary": "Pedestrians Localization in Non-Line-of-Sight (NLoS) regions within urban\nenvironments poses a significant challenge for autonomous driving systems.\nWhile mmWave radar has demonstrated potential for detecting objects in such\nscenarios, the 2D radar point cloud (PCD) data is susceptible to distortions\ncaused by multipath reflections, making accurate spatial inference difficult.\nAdditionally, although camera images provide high-resolution visual\ninformation, they lack depth perception and cannot directly observe objects in\nNLoS regions. In this paper, we propose a novel framework that interprets radar\nPCD through road layout inferred from camera for localization of NLoS\npedestrians. The proposed method leverages visual information from the camera\nto interpret 2D radar PCD, enabling spatial scene reconstruction. The\neffectiveness of the proposed approach is validated through experiments\nconducted using a radar-camera system mounted on a real vehicle. The\nlocalization performance is evaluated using a dataset collected in outdoor NLoS\ndriving environments, demonstrating the practical applicability of the method.",
        "url": "http://arxiv.org/abs/2508.02348v1",
        "published_date": "2025-08-04T12:31:11+00:00",
        "updated_date": "2025-08-04T12:31:11+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Byeonggyu Park",
            "Hee-Yeun Kim",
            "Byonghyok Choi",
            "Hansang Cho",
            "Byungkwan Kim",
            "Soomok Lee",
            "Mingu Jeon",
            "Seong-Woo Kim"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "Proposes a novel framework that combines radar data and camera images to localize pedestrians in Non-Line-of-Sight regions at T-Junctions for autonomous driving systems.",
        "tldr_zh": "提出了一种新颖的框架，结合雷达数据和摄像头图像，在T型路口的非直线视野区域定位行人，用于自主驾驶系统。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Learning Partially-Decorrelated Common Spaces for Ad-hoc Video Search",
        "summary": "Ad-hoc Video Search (AVS) involves using a textual query to search for\nmultiple relevant videos in a large collection of unlabeled short videos. The\nmain challenge of AVS is the visual diversity of relevant videos. A simple\nquery such as \"Find shots of a man and a woman dancing together indoors\" can\nspan a multitude of environments, from brightly lit halls and shadowy bars to\ndance scenes in black-and-white animations. It is therefore essential to\nretrieve relevant videos as comprehensively as possible. Current solutions for\nthe AVS task primarily fuse multiple features into one or more common spaces,\nyet overlook the need for diverse spaces. To fully exploit the expressive\ncapability of individual features, we propose LPD, short for Learning Partially\nDecorrelated common spaces. LPD incorporates two key innovations:\nfeature-specific common space construction and the de-correlation loss.\nSpecifically, LPD learns a separate common space for each video and text\nfeature, and employs de-correlation loss to diversify the ordering of negative\nsamples across different spaces. To enhance the consistency of multi-space\nconvergence, we designed an entropy-based fair multi-space triplet ranking\nloss. Extensive experiments on the TRECVID AVS benchmarks (2016-2023) justify\nthe effectiveness of LPD. Moreover, diversity visualizations of LPD's spaces\nhighlight its ability to enhance result diversity.",
        "url": "http://arxiv.org/abs/2508.02340v1",
        "published_date": "2025-08-04T12:21:16+00:00",
        "updated_date": "2025-08-04T12:21:16+00:00",
        "categories": [
            "cs.CV",
            "cs.IR",
            "cs.MM"
        ],
        "authors": [
            "Fan Hu",
            "Zijie Xin",
            "Xirong Li"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces LPD, a method for ad-hoc video search that learns partially decorrelated common spaces to enhance result diversity.",
        "tldr_zh": "该论文介绍了LPD，一种用于临时视频搜索的方法，学习部分去相关化的共同空间以增强结果的多样性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Qwen-Image Technical Report",
        "summary": "We present Qwen-Image, an image generation foundation model in the Qwen\nseries that achieves significant advances in complex text rendering and precise\nimage editing. To address the challenges of complex text rendering, we design a\ncomprehensive data pipeline that includes large-scale data collection,\nfiltering, annotation, synthesis, and balancing. Moreover, we adopt a\nprogressive training strategy that starts with non-text-to-text rendering,\nevolves from simple to complex textual inputs, and gradually scales up to\nparagraph-level descriptions. This curriculum learning approach substantially\nenhances the model's native text rendering capabilities. As a result,\nQwen-Image not only performs exceptionally well in alphabetic languages such as\nEnglish, but also achieves remarkable progress on more challenging logographic\nlanguages like Chinese. To enhance image editing consistency, we introduce an\nimproved multi-task training paradigm that incorporates not only traditional\ntext-to-image (T2I) and text-image-to-image (TI2I) tasks but also\nimage-to-image (I2I) reconstruction, effectively aligning the latent\nrepresentations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed\nthe original image into Qwen2.5-VL and the VAE encoder to obtain semantic and\nreconstructive representations, respectively. This dual-encoding mechanism\nenables the editing module to strike a balance between preserving semantic\nconsistency and maintaining visual fidelity. Qwen-Image achieves\nstate-of-the-art performance, demonstrating its strong capabilities in both\nimage generation and editing across multiple benchmarks.",
        "url": "http://arxiv.org/abs/2508.02324v1",
        "published_date": "2025-08-04T11:49:20+00:00",
        "updated_date": "2025-08-04T11:49:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chenfei Wu",
            "Jiahao Li",
            "Jingren Zhou",
            "Junyang Lin",
            "Kaiyuan Gao",
            "Kun Yan",
            "Sheng-ming Yin",
            "Shuai Bai",
            "Xiao Xu",
            "Yilei Chen",
            "Yuxiang Chen",
            "Zecheng Tang",
            "Zekai Zhang",
            "Zhengyi Wang",
            "An Yang",
            "Bowen Yu",
            "Chen Cheng",
            "Dayiheng Liu",
            "Deqing Li",
            "Hang Zhang",
            "Hao Meng",
            "Hu Wei",
            "Jingyuan Ni",
            "Kai Chen",
            "Kuan Cao",
            "Liang Peng",
            "Lin Qu",
            "Minggang Wu",
            "Peng Wang",
            "Shuting Yu",
            "Tingkun Wen",
            "Wensen Feng",
            "Xiaoxiao Xu",
            "Yi Wang",
            "Yichang Zhang",
            "Yongqiang Zhu",
            "Yujia Wu",
            "Yuxuan Cai",
            "Zenan Liu"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Dataset"
        ],
        "tldr": "Qwen-Image is an image generation model that excels in complex text rendering and image editing, achieving state-of-the-art performance on multiple benchmarks.",
        "tldr_zh": "Qwen-Image是一个在复杂文字渲染和图像编辑方面表现出色的图像生成模型，对多个基准测试取得了最新成果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dream-to-Recon: Monocular 3D Reconstruction with Diffusion-Depth Distillation from Single Images",
        "summary": "Volumetric scene reconstruction from a single image is crucial for a broad\nrange of applications like autonomous driving and robotics. Recent volumetric\nreconstruction methods achieve impressive results, but generally require\nexpensive 3D ground truth or multi-view supervision. We propose to leverage\npre-trained 2D diffusion models and depth prediction models to generate\nsynthetic scene geometry from a single image. This can then be used to distill\na feed-forward scene reconstruction model. Our experiments on the challenging\nKITTI-360 and Waymo datasets demonstrate that our method matches or outperforms\nstate-of-the-art baselines that use multi-view supervision, and offers unique\nadvantages, for example regarding dynamic scenes.",
        "url": "http://arxiv.org/abs/2508.02323v1",
        "published_date": "2025-08-04T11:43:12+00:00",
        "updated_date": "2025-08-04T11:43:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Philipp Wulff",
            "Felix Wimbauer",
            "Dominik Muhle",
            "Daniel Cremers"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper proposes a method for 3D reconstruction from a single image using pre-trained 2D diffusion and depth prediction models, showing promising results on challenging datasets without the need for multi-view supervision.",
        "tldr_zh": "该论文提出了一种使用预训练的2D扩散和深度预测模型进行单图像的3D重建方法，展示了在具有挑战性的数据集上表现出色且无需多视角监督的潜力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Zero-shot Compositional Action Recognition with Neural Logic Constraints",
        "summary": "Zero-shot compositional action recognition (ZS-CAR) aims to identify unseen\nverb-object compositions in the videos by exploiting the learned knowledge of\nverb and object primitives during training. Despite compositional learning's\nprogress in ZS-CAR, two critical challenges persist: 1) Missing compositional\nstructure constraint, leading to spurious correlations between primitives; 2)\nNeglecting semantic hierarchy constraint, leading to semantic ambiguity and\nimpairing the training process. In this paper, we argue that human-like\nsymbolic reasoning offers a principled solution to these challenges by\nexplicitly modeling compositional and hierarchical structured abstraction. To\nthis end, we propose a logic-driven ZS-CAR framework LogicCAR that integrates\ndual symbolic constraints: Explicit Compositional Logic and Hierarchical\nPrimitive Logic. Specifically, the former models the restrictions within the\ncompositions, enhancing the compositional reasoning ability of our model. The\nlatter investigates the semantical dependencies among different primitives,\nempowering the models with fine-to-coarse reasoning capacity. By formalizing\nthese constraints in first-order logic and embedding them into neural network\narchitectures, LogicCAR systematically bridges the gap between symbolic\nabstraction and existing models. Extensive experiments on the Sth-com dataset\ndemonstrate that our LogicCAR outperforms existing baseline methods, proving\nthe effectiveness of our logic-driven constraints.",
        "url": "http://arxiv.org/abs/2508.02320v1",
        "published_date": "2025-08-04T11:40:42+00:00",
        "updated_date": "2025-08-04T11:40:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Gefan Ye",
            "Lin Li",
            "Kexin Li",
            "Jun Xiao",
            "Long chen"
        ],
        "ai_categories": [
            "AIGC"
        ],
        "tldr": "The paper proposes a LogicCAR framework for zero-shot compositional action recognition by integrating symbolic reasoning constraints, outperforming existing methods on the Sth-com dataset.",
        "tldr_zh": "本文提出了一个LogicCAR框架，通过集成符号推理约束，实现了零样本组合动作识别，在Sth-com数据集上超过了现有方法。",
        "relevance_score": 1,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Real Unsupervised Anomaly Detection Via Confident Meta-Learning",
        "summary": "So-called unsupervised anomaly detection is better described as\nsemi-supervised, as it assumes all training data are nominal. This assumption\nsimplifies training but requires manual data curation, introducing bias and\nlimiting adaptability. We propose Confident Meta-learning (CoMet), a novel\ntraining strategy that enables deep anomaly detection models to learn from\nuncurated datasets where nominal and anomalous samples coexist, eliminating the\nneed for explicit filtering. Our approach integrates Soft Confident Learning,\nwhich assigns lower weights to low-confidence samples, and Meta-Learning, which\nstabilizes training by regularizing updates based on training validation loss\ncovariance. This prevents overfitting and enhances robustness to noisy data.\nCoMet is model-agnostic and can be applied to any anomaly detection method\ntrainable via gradient descent. Experiments on MVTec-AD, VIADUCT, and KSDD2\nwith two state-of-the-art models demonstrate the effectiveness of our approach,\nconsistently improving over the baseline methods, remaining insensitive to\nanomalies in the training set, and setting a new state-of-the-art across all\ndatasets.",
        "url": "http://arxiv.org/abs/2508.02293v1",
        "published_date": "2025-08-04T11:03:12+00:00",
        "updated_date": "2025-08-04T11:03:12+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Muhammad Aqeel",
            "Shakiba Sharifi",
            "Marco Cristani",
            "Francesco Setti"
        ],
        "ai_categories": [
            "AIGC",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel training strategy called CoMet for unsupervised anomaly detection without manual data curation, showing improved performance over existing methods.",
        "tldr_zh": "本文提出了一种名为CoMet的新训练策略，用于无监督异常检测，无需手动数据整理，表现优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SplatSSC: Decoupled Depth-Guided Gaussian Splatting for Semantic Scene Completion",
        "summary": "Monocular 3D Semantic Scene Completion (SSC) is a challenging yet promising\ntask that aims to infer dense geometric and semantic descriptions of a scene\nfrom a single image. While recent object-centric paradigms significantly\nimprove efficiency by leveraging flexible 3D Gaussian primitives, they still\nrely heavily on a large number of randomly initialized primitives, which\ninevitably leads to 1) inefficient primitive initialization and 2) outlier\nprimitives that introduce erroneous artifacts. In this paper, we propose\nSplatSSC, a novel framework that resolves these limitations with a depth-guided\ninitialization strategy and a principled Gaussian aggregator. Instead of random\ninitialization, SplatSSC utilizes a dedicated depth branch composed of a\nGroup-wise Multi-scale Fusion (GMF) module, which integrates multi-scale image\nand depth features to generate a sparse yet representative set of initial\nGaussian primitives. To mitigate noise from outlier primitives, we develop the\nDecoupled Gaussian Aggregator (DGA), which enhances robustness by decomposing\ngeometric and semantic predictions during the Gaussian-to-voxel splatting\nprocess. Complemented with a specialized Probability Scale Loss, our method\nachieves state-of-the-art performance on the Occ-ScanNet dataset, outperforming\nprior approaches by over 6.3% in IoU and 4.1% in mIoU, while reducing both\nlatency and memory consumption by more than 9.3%. The code will be released\nupon acceptance.",
        "url": "http://arxiv.org/abs/2508.02261v1",
        "published_date": "2025-08-04T10:09:31+00:00",
        "updated_date": "2025-08-04T10:09:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rui Qian",
            "Haozhi Cao",
            "Tianchen Deng",
            "Shenghai Yuan",
            "Lihua Xie"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes SplatSSC, a framework for monocular 3D semantic scene completion, achieving state-of-the-art performance with improved efficiency and accuracy.",
        "tldr_zh": "本文提出了SplatSSC，一个用于单目3D语义场景完成的框架，通过改进效率和准确性实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Patho-AgenticRAG: Towards Multimodal Agentic Retrieval-Augmented Generation for Pathology VLMs via Reinforcement Learning",
        "summary": "Although Vision Language Models (VLMs) have shown strong generalization in\nmedical imaging, pathology presents unique challenges due to ultra-high\nresolution, complex tissue structures, and nuanced clinical semantics. These\nfactors make pathology VLMs prone to hallucinations, i.e., generating outputs\ninconsistent with visual evidence, which undermines clinical trust. Existing\nRAG approaches in this domain largely depend on text-based knowledge bases,\nlimiting their ability to leverage diagnostic visual cues. To address this, we\npropose Patho-AgenticRAG, a multimodal RAG framework with a database built on\npage-level embeddings from authoritative pathology textbooks. Unlike\ntraditional text-only retrieval systems, it supports joint text-image search,\nenabling direct retrieval of textbook pages that contain both the queried text\nand relevant visual cues, thus avoiding the loss of critical image-based\ninformation. Patho-AgenticRAG also supports reasoning, task decomposition, and\nmulti-turn search interactions, improving accuracy in complex diagnostic\nscenarios. Experiments show that Patho-AgenticRAG significantly outperforms\nexisting multimodal models in complex pathology tasks like multiple-choice\ndiagnosis and visual question answering. Our project is available at the\nPatho-AgenticRAG repository:\nhttps://github.com/Wenchuan-Zhang/Patho-AgenticRAG.",
        "url": "http://arxiv.org/abs/2508.02258v1",
        "published_date": "2025-08-04T10:03:08+00:00",
        "updated_date": "2025-08-04T10:03:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenchuan Zhang",
            "Jingru Guo",
            "Hengzhe Zhang",
            "Penghao Zhang",
            "Jie Chen",
            "Shuwan Zhang",
            "Zhang Zhang",
            "Yuhao Yi",
            "Hong Bu"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces Patho-AgenticRAG, a multimodal retrieval-augmented generation framework for pathology VLMs that outperforms existing models in complex pathology tasks.",
        "tldr_zh": "本文介绍了Patho-AgenticRAG，一种用于病理VLM的多模态检索增强生成框架，在复杂的病理任务中优于现有模型。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal Entity Linking",
        "summary": "Multimodal entity linking plays a crucial role in a wide range of\napplications. Recent advances in large language model-based methods have become\nthe dominant paradigm for this task, effectively leveraging both textual and\nvisual modalities to enhance performance. Despite their success, these methods\nstill face two challenges, including unnecessary incorporation of image data in\ncertain scenarios and the reliance only on a one-time extraction of visual\nfeatures, which can undermine their effectiveness and accuracy. To address\nthese challenges, we propose a novel LLM-based framework for the multimodal\nentity linking task, called Intra- and Inter-modal Collaborative Reflections.\nThis framework prioritizes leveraging text information to address the task.\nWhen text alone is insufficient to link the correct entity through intra- and\ninter-modality evaluations, it employs a multi-round iterative strategy that\nintegrates key visual clues from various aspects of the image to support\nreasoning and enhance matching accuracy. Extensive experiments on three widely\nused public datasets demonstrate that our framework consistently outperforms\ncurrent state-of-the-art methods in the task, achieving improvements of 3.2%,\n5.1%, and 1.6%, respectively. Our code is available at\nhttps://github.com/ziyan-xiaoyu/I2CR/.",
        "url": "http://arxiv.org/abs/2508.02243v1",
        "published_date": "2025-08-04T09:43:54+00:00",
        "updated_date": "2025-08-04T09:43:54+00:00",
        "categories": [
            "cs.CV",
            "cs.IR"
        ],
        "authors": [
            "Ziyan Liu",
            "Junwen Li",
            "Kaiwen Li",
            "Tong Ruan",
            "Chao Wang",
            "Xinyan He",
            "Zongyu Wang",
            "Xuezhi Cao",
            "Jingping Liu"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel framework, I2CR, for multimodal entity linking that leverages text information and visual clues to outperform state-of-the-art methods.",
        "tldr_zh": "本文提出了一种新的框架I2CR，用于多模态实体链接，利用文本信息和视觉线索胜过了现有最先进的方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Forecasting When to Forecast: Accelerating Diffusion Models with Confidence-Gated Taylor",
        "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in\nvisual generation tasks. However, their low inference speed limits their\ndeployment in low-resource applications. Recent training-free approaches\nexploit the redundancy of features across timesteps by caching and reusing past\nrepresentations to accelerate inference. Building on this idea, TaylorSeer\ninstead uses cached features to predict future ones via Taylor expansion.\nHowever, its module-level prediction across all transformer blocks (e.g.,\nattention or feedforward modules) requires storing fine-grained intermediate\nfeatures, leading to notable memory and computation overhead. Moreover, it\nadopts a fixed caching schedule without considering the varying accuracy of\npredictions across timesteps, which can lead to degraded outputs when\nprediction fails. To address these limitations, we propose a novel approach to\nbetter leverage Taylor-based acceleration. First, we shift the Taylor\nprediction target from the module level to the last block level, significantly\nreducing the number of cached features. Furthermore, observing strong\nsequential dependencies among Transformer blocks, we propose to use the error\nbetween the Taylor-estimated and actual outputs of the first block as an\nindicator of prediction reliability. If the error is small, we trust the Taylor\nprediction for the last block; otherwise, we fall back to full computation,\nthereby enabling a dynamic caching mechanism. Empirical results show that our\nmethod achieves a better balance between speed and quality, achieving a 3.17x\nacceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible\nquality drop. The Project Page is\n\\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}",
        "url": "http://arxiv.org/abs/2508.02240v1",
        "published_date": "2025-08-04T09:39:31+00:00",
        "updated_date": "2025-08-04T09:39:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xiaoliu Guan",
            "Lielin Jiang",
            "Hanqi Chen",
            "Xu Zhang",
            "Jiaxing Yan",
            "Guanzhong Wang",
            "Yi Liu",
            "Zetao Zhang",
            "Yu Wu"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer"
        ],
        "tldr": "The paper proposes a method to accelerate Diffusion Transformers for visual generation tasks by predicting future features using cached features and dynamic caching mechanisms.",
        "tldr_zh": "本文提出了一种方法，通过使用缓存的特征和动态缓存机制来加速扩散变换器，用于视觉生成任务。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "An Event-based Fast Intensity Reconstruction Scheme for UAV Real-time Perception",
        "summary": "Event cameras offer significant advantages, including a wide dynamic range,\nhigh temporal resolution, and immunity to motion blur, making them highly\npromising for addressing challenging visual conditions. Extracting and\nutilizing effective information from asynchronous event streams is essential\nfor the onboard implementation of event cameras. In this paper, we propose a\nstreamlined event-based intensity reconstruction scheme, event-based single\nintegration (ESI), to address such implementation challenges. This method\nguarantees the portability of conventional frame-based vision methods to\nevent-based scenarios and maintains the intrinsic advantages of event cameras.\nThe ESI approach reconstructs intensity images by performing a single\nintegration of the event streams combined with an enhanced decay algorithm.\nSuch a method enables real-time intensity reconstruction at a high frame rate,\ntypically 100 FPS. Furthermore, the relatively low computation load of ESI fits\nonboard implementation suitably, such as in UAV-based visual tracking\nscenarios. Extensive experiments have been conducted to evaluate the\nperformance comparison of ESI and state-of-the-art algorithms. Compared to\nstate-of-the-art algorithms, ESI demonstrates remarkable runtime efficiency\nimprovements, superior reconstruction quality, and a high frame rate. As a\nresult, ESI enhances UAV onboard perception significantly under visual\nadversary surroundings. In-flight tests, ESI demonstrates effective performance\nfor UAV onboard visual tracking under extremely low illumination\nconditions(2-10lux), whereas other comparative algorithms fail due to\ninsufficient frame rate, poor image quality, or limited real-time performance.",
        "url": "http://arxiv.org/abs/2508.02238v1",
        "published_date": "2025-08-04T09:37:00+00:00",
        "updated_date": "2025-08-04T09:37:00+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Xin Dong",
            "Yiwei Zhang",
            "Yangjie Cui",
            "Jinwu Xiang",
            "Daochun Li",
            "Zhan Tu"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper proposes a streamlined event-based intensity reconstruction scheme for UAV real-time perception using event cameras, showing significant improvements in runtime efficiency and quality compared to state-of-the-art algorithms under challenging visual conditions.",
        "tldr_zh": "该论文提出了一种用于无人机实时感知的简化事件驱动强度重建方案，利用事件相机在挑战性视觉环境下与最先进算法相比，显著提高了运行效率和质量。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CMIC: Content-Adaptive Mamba for Learned Image Compression",
        "summary": "Recent Learned image compression (LIC) leverages Mamba-style state-space\nmodels (SSMs) for global receptive fields with linear complexity. However,\nvanilla Mamba is content-agnostic, relying on fixed and predefined selective\nscans, which restricts its ability to dynamically and fully exploit content\ndependencies. We introduce Content-Adaptive Mamba (CAM), a dynamic SSM that\naddresses two critical limitations. First, it employs content-aware token\nreorganization, clustering and reordering tokens based on content similarity to\nprioritize proximity in feature space over Euclidean space. Second, it\nintegrates global priors into SSM via a prompt dictionary, effectively\nmitigating the strict causality and long-range decay in the token interactions\nof Mamba. These innovations enable CAM to better capture global dependencies\nwhile preserving computational efficiency. Leveraging CAM, our Content-Adaptive\nMamba-based LIC model (CMIC) achieves state-of-the-art rate-distortion\nperformance, surpassing VTM-21.0 by -15.91\\%, -21.34\\%, and -17.58\\% BD-rate on\nKodak, Tecnick, and CLIC benchmarks, respectively.",
        "url": "http://arxiv.org/abs/2508.02192v1",
        "published_date": "2025-08-04T08:42:23+00:00",
        "updated_date": "2025-08-04T08:42:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yunuo Chen",
            "Zezheng Lyu",
            "Bing He",
            "Hongwei Hu",
            "Qi Wang",
            "Yuan Tian",
            "Li Song",
            "Wenjun Zhang",
            "Guo Lu"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces Content-Adaptive Mamba (CAM) for image compression, achieving state-of-the-art performance by capturing global dependencies efficiently.",
        "tldr_zh": "本文介绍了适应内容的Mamba（CAM）用于图像压缩，在捕获全局依赖关系方面效率高，达到了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Weakly Supervised Multimodal Temporal Forgery Localization via Multitask Learning",
        "summary": "The spread of Deepfake videos has caused a trust crisis and impaired social\nstability. Although numerous approaches have been proposed to address the\nchallenges of Deepfake detection and localization, there is still a lack of\nsystematic research on the weakly supervised multimodal fine-grained temporal\nforgery localization (WS-MTFL). In this paper, we propose a novel weakly\nsupervised multimodal temporal forgery localization via multitask learning\n(WMMT), which addresses the WS-MTFL under the multitask learning paradigm. WMMT\nachieves multimodal fine-grained Deepfake detection and temporal partial\nforgery localization using merely video-level annotations. Specifically, visual\nand audio modality detection are formulated as two binary classification tasks.\nThe multitask learning paradigm is introduced to integrate these tasks into a\nmultimodal task. Furthermore, WMMT utilizes a Mixture-of-Experts structure to\nadaptively select appropriate features and localization head, achieving\nexcellent flexibility and localization precision in WS-MTFL. A feature\nenhancement module with temporal property preserving attention mechanism is\nproposed to identify the intra- and inter-modality feature deviation and\nconstruct comprehensive video features. To further explore the temporal\ninformation for weakly supervised learning, an extensible deviation perceiving\nloss has been proposed, which aims to enlarge the deviation of adjacent\nsegments of the forged samples and reduce the deviation of genuine samples.\nExtensive experiments demonstrate the effectiveness of multitask learning for\nWS-MTFL, and the WMMT achieves comparable results to fully supervised\napproaches in several evaluation metrics.",
        "url": "http://arxiv.org/abs/2508.02179v1",
        "published_date": "2025-08-04T08:22:39+00:00",
        "updated_date": "2025-08-04T08:22:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenbo Xu",
            "Wei Lu",
            "Xiangyang Luo"
        ],
        "ai_categories": [
            "Multimodality",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel method for weakly supervised multimodal temporal forgery detection and localization using multitask learning, achieving comparable results to fully supervised approaches.",
        "tldr_zh": "本文提出了一种新颖方法，利用多任务学习进行弱监督多模态时间伪造检测和定位，实现了与全监督方法相媲美的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GaussianCross: Cross-modal Self-supervised 3D Representation Learning via Gaussian Splatting",
        "summary": "The significance of informative and robust point representations has been\nwidely acknowledged for 3D scene understanding. Despite existing\nself-supervised pre-training counterparts demonstrating promising performance,\nthe model collapse and structural information deficiency remain prevalent due\nto insufficient point discrimination difficulty, yielding unreliable\nexpressions and suboptimal performance. In this paper, we present\nGaussianCross, a novel cross-modal self-supervised 3D representation learning\narchitecture integrating feed-forward 3D Gaussian Splatting (3DGS) techniques\nto address current challenges. GaussianCross seamlessly converts\nscale-inconsistent 3D point clouds into a unified cuboid-normalized Gaussian\nrepresentation without missing details, enabling stable and generalizable\npre-training. Subsequently, a tri-attribute adaptive distillation splatting\nmodule is incorporated to construct a 3D feature field, facilitating synergetic\nfeature capturing of appearance, geometry, and semantic cues to maintain\ncross-modal consistency. To validate GaussianCross, we perform extensive\nevaluations on various benchmarks, including ScanNet, ScanNet200, and S3DIS. In\nparticular, GaussianCross shows a prominent parameter and data efficiency,\nachieving superior performance through linear probing (<0.1% parameters) and\nlimited data training (1% of scenes) compared to state-of-the-art methods.\nFurthermore, GaussianCross demonstrates strong generalization capabilities,\nimproving the full fine-tuning accuracy by 9.3% mIoU and 6.1% AP$_{50}$ on\nScanNet200 semantic and instance segmentation tasks, respectively, supporting\nthe effectiveness of our approach. The code, weights, and visualizations are\npublicly available at\n\\href{https://rayyoh.github.io/GaussianCross/}{https://rayyoh.github.io/GaussianCross/}.",
        "url": "http://arxiv.org/abs/2508.02172v1",
        "published_date": "2025-08-04T08:12:44+00:00",
        "updated_date": "2025-08-04T08:12:44+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "authors": [
            "Lei Yao",
            "Yi Wang",
            "Yi Zhang",
            "Moyun Liu",
            "Lap-Pui Chau"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces GaussianCross, a novel self-supervised 3D representation learning method that uses Gaussian splatting to address point cloud representation challenges and achieve superior performance on various benchmarks.",
        "tldr_zh": "本文介绍了GaussianCross，一种新颖的自监督3D表示学习方法，利用高斯分布来解决点云表示挑战，并在各种基准测试中取得优越表现。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "After the Party: Navigating the Mapping From Color to Ambient Lighting",
        "summary": "Illumination in practical scenarios is inherently complex, involving colored\nlight sources, occlusions, and diverse material interactions that produce\nintricate reflectance and shading effects. However, existing methods often\noversimplify this challenge by assuming a single light source or uniform,\nwhite-balanced lighting, leaving many of these complexities unaddressed.In this\npaper, we introduce CL3AN, the first large-scale, high-resolution dataset of\nits kind designed to facilitate the restoration of images captured under\nmultiple Colored Light sources to their Ambient-Normalized counterparts.\nThrough benchmarking, we find that leading approaches often produce artifacts,\nsuch as illumination inconsistencies, texture leakage, and color distortion,\nprimarily due to their limited ability to precisely disentangle illumination\nfrom reflectance. Motivated by this insight, we achieve such a desired\ndecomposition through a novel learning framework that leverages explicit\nchromaticity and luminance components guidance, drawing inspiration from the\nprinciples of the Retinex model. Extensive evaluations on existing benchmarks\nand our dataset demonstrate the effectiveness of our approach, showcasing\nenhanced robustness under non-homogeneous color lighting and material-specific\nreflectance variations, all while maintaining a highly competitive\ncomputational cost. The benchmark, codes, and models are available at\nwww.github.com/fvasluianu97/RLN2.",
        "url": "http://arxiv.org/abs/2508.02168v1",
        "published_date": "2025-08-04T08:07:03+00:00",
        "updated_date": "2025-08-04T08:07:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Florin-Alexandru Vasluianu",
            "Tim Seizinger",
            "Zongwei Wu",
            "Radu Timofte"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces CL3AN, a dataset and method for restoring images captured under multiple colored light sources to ambient-normalized counterparts, addressing challenges like illumination inconsistencies and color distortion.",
        "tldr_zh": "本文介绍了CL3AN，这是一个用于将图像从多个有色光源下恢复到环境标准化的数据集和方法，解决了照明不一致和颜色失真等挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Subject or Style: Adaptive and Training-Free Mixture of LoRAs",
        "summary": "Fine-tuning models via Low-Rank Adaptation (LoRA) demonstrates remarkable\nperformance in subject-driven or style-driven generation tasks. Studies have\nexplored combinations of different LoRAs to jointly generate learned styles and\ncontent. However, current methods struggle to balance the original subject and\nstyle, and often require additional training. Recently, K-LoRA proposed a\ntraining-free LoRA fusion method. But it involves multiple hyperparameters,\nmaking it difficult to adapt to all styles and subjects. In this paper, we\npropose EST-LoRA, a training-free adaptive LoRA fusion method. It\ncomprehensively considers three critical factors: \\underline{E}nergy of matrix,\n\\underline{S}tyle discrepancy scores and \\underline{T}ime steps. Analogous to\nthe Mixture of Experts (MoE) architecture, the model adaptively selects between\nsubject LoRA and style LoRA within each attention layer. This integrated\nselection mechanism ensures balanced contributions from both components during\nthe generation process. Experimental results show that EST-LoRA outperforms\nstate-of-the-art methods in both qualitative and quantitative evaluations and\nachieves faster generation speed compared to other efficient fusion approaches.\nOur code is publicly available at:\nhttps://anonymous.4open.science/r/EST-LoRA-F318.",
        "url": "http://arxiv.org/abs/2508.02165v1",
        "published_date": "2025-08-04T08:05:18+00:00",
        "updated_date": "2025-08-04T08:05:18+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Jia-Chen Zhang",
            "Yu-Jie Xiong"
        ],
        "ai_categories": [
            "LoRA"
        ],
        "tldr": "The paper introduces EST-LoRA, a training-free adaptive method for fusing Low-Rank Adaptation techniques in style-driven generation tasks, outperforming state-of-the-art methods in quality and speed.",
        "tldr_zh": "本文介绍了EST-LoRA，一种无需训练的自适应方法，用于在风格驱动生成任务中融合低秩适应技术，在质量和速度上优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Efficient Chambolle-Pock based algorithms for Convoltional sparse representation",
        "summary": "Recently convolutional sparse representation (CSR), as a sparse\nrepresentation technique, has attracted increasing attention in the field of\nimage processing, due to its good characteristic of translate-invariance. The\ncontent of CSR usually consists of convolutional sparse coding (CSC) and\nconvolutional dictionary learning (CDL), and many studies focus on how to solve\nthe corresponding optimization problems. At present, the most efficient\noptimization scheme for CSC is based on the alternating direction method of\nmultipliers (ADMM). However, the ADMM-based approach involves a penalty\nparameter that needs to be carefully selected, and improper parameter selection\nmay result in either no convergence or very slow convergence. In this paper, a\nnovel fast and efficient method using Chambolle-Pock(CP) framework is proposed,\nwhich does not require extra manual selection parameters in solving processing,\nand has faster convergence speed. Furthermore, we propose an anisotropic total\nvariation penalty of the coefficient maps for CSC and apply the CP algorithm to\nsolve it. In addition, we also apply the CP framework to solve the\ncorresponding CDL problem. Experiments show that for noise-free image the\nproposed CSC algorithms can achieve rival results of the latest ADMM-based\napproach, while outperforms in removing noise from Gaussian noise pollution\nimage.",
        "url": "http://arxiv.org/abs/2508.02152v1",
        "published_date": "2025-08-04T07:49:59+00:00",
        "updated_date": "2025-08-04T07:49:59+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Yi Liu",
            "Junjing Li",
            "Yang Chen",
            "Haowei Tang",
            "Pengcheng Zhang",
            "Tianling Lyu",
            "Zhiguo Gui"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper presents a novel fast and efficient method using the Chambolle-Pock framework for solving convolutional sparse representation problems, showing promising results in noise removal from images.",
        "tldr_zh": "本文提出了一种使用Chambolle-Pock框架解决卷积稀疏表示问题的新方法，显示出在图像中去除噪声方面具有很好的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AttriCtrl: Fine-Grained Control of Aesthetic Attribute Intensity in Diffusion Models",
        "summary": "Recent breakthroughs in text-to-image diffusion models have significantly\nenhanced both the visual fidelity and semantic controllability of generated\nimages. However, fine-grained control over aesthetic attributes remains\nchallenging, especially when users require continuous and intensity-specific\nadjustments. Existing approaches often rely on vague textual prompts, which are\ninherently ambiguous in expressing both the aesthetic semantics and the desired\nintensity, or depend on costly human preference data for alignment, limiting\ntheir scalability and practicality. To address these limitations, we propose\nAttriCtrl, a plug-and-play framework for precise and continuous control of\naesthetic attributes. Specifically, we quantify abstract aesthetics by\nleveraging semantic similarity from pre-trained vision-language models, and\nemploy a lightweight value encoder that maps scalar intensities in $[0,1]$ to\nlearnable embeddings within diffusion-based generation. This design enables\nintuitive and customizable aesthetic manipulation, with minimal training\noverhead and seamless integration into existing generation pipelines. Extensive\nexperiments demonstrate that AttriCtrl achieves accurate control over\nindividual attributes as well as flexible multi-attribute composition.\nMoreover, it is fully compatible with popular open-source controllable\ngeneration frameworks, showcasing strong integration capability and practical\nutility across diverse generation scenarios.",
        "url": "http://arxiv.org/abs/2508.02151v1",
        "published_date": "2025-08-04T07:49:40+00:00",
        "updated_date": "2025-08-04T07:49:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Die Chen",
            "Zhongjie Duan",
            "Zhiwen Li",
            "Cen Chen",
            "Daoyuan Chen",
            "Yaliang Li",
            "Yinda Chen"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer"
        ],
        "tldr": "The paper introduces AttriCtrl, a framework for precise and continuous control of aesthetic attributes in image generation models.",
        "tldr_zh": "这篇论文介绍了AttriCtrl，这是一个用于图像生成模型中对审美属性进行精确连续控制的框架。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AID4AD: Aerial Image Data for Automated Driving Perception",
        "summary": "This work investigates the integration of spatially aligned aerial imagery\ninto perception tasks for automated vehicles (AVs). As a central contribution,\nwe present AID4AD, a publicly available dataset that augments the nuScenes\ndataset with high-resolution aerial imagery precisely aligned to its local\ncoordinate system. The alignment is performed using SLAM-based point cloud maps\nprovided by nuScenes, establishing a direct link between aerial data and\nnuScenes local coordinate system. To ensure spatial fidelity, we propose an\nalignment workflow that corrects for localization and projection distortions. A\nmanual quality control process further refines the dataset by identifying a set\nof high-quality alignments, which we publish as ground truth to support future\nresearch on automated registration. We demonstrate the practical value of\nAID4AD in two representative tasks: in online map construction, aerial imagery\nserves as a complementary input that improves the mapping process; in motion\nprediction, it functions as a structured environmental representation that\nreplaces high-definition maps. Experiments show that aerial imagery leads to a\n15-23% improvement in map construction accuracy and a 2% gain in trajectory\nprediction performance. These results highlight the potential of aerial imagery\nas a scalable and adaptable source of environmental context in automated\nvehicle systems, particularly in scenarios where high-definition maps are\nunavailable, outdated, or costly to maintain. AID4AD, along with evaluation\ncode and pretrained models, is publicly released to foster further research in\nthis direction: https://github.com/DriverlessMobility/AID4AD.",
        "url": "http://arxiv.org/abs/2508.02140v1",
        "published_date": "2025-08-04T07:38:18+00:00",
        "updated_date": "2025-08-04T07:38:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Daniel Lengerer",
            "Mathias Pechinger",
            "Klaus Bogenberger",
            "Carsten Markgraf"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper introduces AID4AD, a dataset combining aerial imagery with automated driving perception tasks, showing improved map construction accuracy and trajectory prediction performance.",
        "tldr_zh": "本文介绍了AID4AD数据集，将航拍图像与自动驾驶感知任务结合，显示出改善地图构建准确性和轨迹预测性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VDEGaussian: Video Diffusion Enhanced 4D Gaussian Splatting for Dynamic Urban Scenes Modeling",
        "summary": "Dynamic urban scene modeling is a rapidly evolving area with broad\napplications. While current approaches leveraging neural radiance fields or\nGaussian Splatting have achieved fine-grained reconstruction and high-fidelity\nnovel view synthesis, they still face significant limitations. These often stem\nfrom a dependence on pre-calibrated object tracks or difficulties in accurately\nmodeling fast-moving objects from undersampled capture, particularly due to\nchallenges in handling temporal discontinuities. To overcome these issues, we\npropose a novel video diffusion-enhanced 4D Gaussian Splatting framework. Our\nkey insight is to distill robust, temporally consistent priors from a test-time\nadapted video diffusion model. To ensure precise pose alignment and effective\nintegration of this denoised content, we introduce two core innovations: a\njoint timestamp optimization strategy that refines interpolated frame poses,\nand an uncertainty distillation method that adaptively extracts target content\nwhile preserving well-reconstructed regions. Extensive experiments demonstrate\nthat our method significantly enhances dynamic modeling, especially for\nfast-moving objects, achieving an approximate PSNR gain of 2 dB for novel view\nsynthesis over baseline approaches.",
        "url": "http://arxiv.org/abs/2508.02129v1",
        "published_date": "2025-08-04T07:24:05+00:00",
        "updated_date": "2025-08-04T07:24:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuru Xiao",
            "Zihan Lin",
            "Chao Lu",
            "Deming Zhai",
            "Kui Jiang",
            "Wenbo Zhao",
            "Wei Zhang",
            "Junjun Jiang",
            "Huanran Wang",
            "Xianming Liu"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper proposes a novel video diffusion-enhanced 4D Gaussian Splatting framework for dynamic urban scene modeling, improving reconstruction and view synthesis, especially for fast-moving objects.",
        "tldr_zh": "该论文提出了一种新的视频扩散增强的4D高斯喷洒框架，用于动态城市场景建模，特别适用于快速移动物体。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Beyond RGB and Events: Enhancing Object Detection under Adverse Lighting with Monocular Normal Maps",
        "summary": "Accurate object detection under adverse lighting conditions is critical for\nreal-world applications such as autonomous driving. Although neuromorphic event\ncameras have been introduced to handle these scenarios, adverse lighting often\ninduces distracting reflections from tunnel walls or road surfaces, which\nfrequently lead to false obstacle detections. However, neither RGB nor event\ndata alone is robust enough to address these complexities, and mitigating these\nissues without additional sensors remains underexplored. To overcome these\nchallenges, we propose leveraging normal maps, directly predicted from\nmonocular RGB images, as robust geometric cues to suppress false positives and\nenhance detection accuracy. We introduce NRE-Net, a novel multi-modal detection\nframework that effectively fuses three complementary modalities: monocularly\npredicted surface normal maps, RGB images, and event streams. To optimize the\nfusion process, our framework incorporates two key modules: the Adaptive\nDual-stream Fusion Module (ADFM), which integrates RGB and normal map features,\nand the Event-modality Aware Fusion Module (EAFM), which adapts to the high\ndynamic range characteristics of event data. Extensive evaluations on the\nDSEC-Det-sub and PKU-DAVIS-SOD datasets demonstrate that NRE-Net significantly\noutperforms state-of-the-art methods. Our approach achieves mAP50 improvements\nof 7.9% and 6.1% over frame-based approaches (e.g., YOLOX), while surpassing\nthe fusion-based SFNet by 2.7% on the DSEC-Det-sub dataset and SODFormer by\n7.1% on the PKU-DAVIS-SOD dataset.",
        "url": "http://arxiv.org/abs/2508.02127v1",
        "published_date": "2025-08-04T07:19:20+00:00",
        "updated_date": "2025-08-04T07:19:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mingjie Liu",
            "Hanqing Liu",
            "Chuang Zhu"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel multi-modal detection framework, NRE-Net, that leverages normal maps derived from monocular RGB images to improve object detection under adverse lighting conditions, outperforming existing methods on relevant datasets.",
        "tldr_zh": "本文提出了一种新的多模态检测框架NRE-Net，利用从单眼RGB图像推导的法线图来改善在恶劣光照条件下的目标检测，优于现有方法在相关数据集上的表现。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Tackling Ill-posedness of Reversible Image Conversion with Well-posed Invertible Network",
        "summary": "Reversible image conversion (RIC) suffers from ill-posedness issues due to\nits forward conversion process being considered an underdetermined system.\nDespite employing invertible neural networks (INN), existing RIC methods\nintrinsically remain ill-posed as inevitably introducing uncertainty by\nincorporating randomly sampled variables. To tackle the ill-posedness dilemma,\nwe focus on developing a reliable approximate left inverse for the\nunderdetermined system by constructing an overdetermined system with a non-zero\nGram determinant, thus ensuring a well-posed solution. Based on this principle,\nwe propose a well-posed invertible $1\\times1$ convolution (WIC), which\neliminates the reliance on random variable sampling and enables the development\nof well-posed invertible networks. Furthermore, we design two innovative\nnetworks, WIN-Na\\\"ive and WIN, with the latter incorporating advanced\nskip-connections to enhance long-term memory. Our methods are evaluated across\ndiverse RIC tasks, including reversible image hiding, image rescaling, and\nimage decolorization, consistently achieving state-of-the-art performance.\nExtensive experiments validate the effectiveness of our approach, demonstrating\nits ability to overcome the bottlenecks of existing RIC solutions and setting a\nnew benchmark in the field. Codes are available in\nhttps://github.com/BNU-ERC-ITEA/WIN.",
        "url": "http://arxiv.org/abs/2508.02111v1",
        "published_date": "2025-08-04T06:40:01+00:00",
        "updated_date": "2025-08-04T06:40:01+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Yuanfei Huang",
            "Hua Huang"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper proposes a new method, Well-posed Invertible Network, to address the ill-posedness issue in reversible image conversion by constructing a reliable approximate left inverse for the underdetermined system.",
        "tldr_zh": "本文提出了一种新方法，井定反射网络(WIN)，以解决可逆图像转换中的不适定性问题，通过构建可靠的近似左逆解决欠定系统。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AutoLoRA: Automatic LoRA Retrieval and Fine-Grained Gated Fusion for Text-to-Image Generation",
        "summary": "Despite recent advances in photorealistic image generation through\nlarge-scale models like FLUX and Stable Diffusion v3, the practical deployment\nof these architectures remains constrained by their inherent intractability to\nparameter fine-tuning. While low-rank adaptation (LoRA) have demonstrated\nefficacy in enabling model customization with minimal parameter overhead, the\neffective utilization of distributed open-source LoRA modules faces three\ncritical challenges: sparse metadata annotation, the requirement for zero-shot\nadaptation capabilities, and suboptimal fusion strategies for multi-LoRA fusion\nstrategies. To address these limitations, we introduce a novel framework that\nenables semantic-driven LoRA retrieval and dynamic aggregation through two key\ncomponents: (1) weight encoding-base LoRA retriever that establishes a shared\nsemantic space between LoRA parameter matrices and text prompts, eliminating\ndependence on original training data, and (2) fine-grained gated fusion\nmechanism that computes context-specific fusion weights across network layers\nand diffusion timesteps to optimally integrate multiple LoRA modules during\ngeneration. Our approach achieves significant improvement in image generation\nperfermance, thereby facilitating scalable and data-efficient enhancement of\nfoundational models. This work establishes a critical bridge between the\nfragmented landscape of community-developed LoRAs and practical deployment\nrequirements, enabling collaborative model evolution through standardized\nadapter integration.",
        "url": "http://arxiv.org/abs/2508.02107v1",
        "published_date": "2025-08-04T06:36:00+00:00",
        "updated_date": "2025-08-04T06:36:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhiwen Li",
            "Zhongjie Duan",
            "Die Chen",
            "Cen Chen",
            "Daoyuan Chen",
            "Yaliang Li",
            "Yingda Chen"
        ],
        "ai_categories": [
            "LoRA",
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper introduces AutoLoRA, a framework for text-to-image generation that utilizes LoRA retrieval and fine-grained gated fusion to improve image generation performance.",
        "tldr_zh": "该论文介绍了AutoLoRA，这是一个利用LoRA检索和精细门控融合的框架，用于改进图像生成性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Immersive Human-X Interaction: A Real-Time Framework for Physically Plausible Motion Synthesis",
        "summary": "Real-time synthesis of physically plausible human interactions remains a\ncritical challenge for immersive VR/AR systems and humanoid robotics. While\nexisting methods demonstrate progress in kinematic motion generation, they\noften fail to address the fundamental tension between real-time responsiveness,\nphysical feasibility, and safety requirements in dynamic human-machine\ninteractions. We introduce Human-X, a novel framework designed to enable\nimmersive and physically plausible human interactions across diverse entities,\nincluding human-avatar, human-humanoid, and human-robot systems. Unlike\nexisting approaches that focus on post-hoc alignment or simplified physics, our\nmethod jointly predicts actions and reactions in real-time using an\nauto-regressive reaction diffusion planner, ensuring seamless synchronization\nand context-aware responses. To enhance physical realism and safety, we\nintegrate an actor-aware motion tracking policy trained with reinforcement\nlearning, which dynamically adapts to interaction partners' movements while\navoiding artifacts like foot sliding and penetration. Extensive experiments on\nthe Inter-X and InterHuman datasets demonstrate significant improvements in\nmotion quality, interaction continuity, and physical plausibility over\nstate-of-the-art methods. Our framework is validated in real-world\napplications, including virtual reality interface for human-robot interaction,\nshowcasing its potential for advancing human-robot collaboration.",
        "url": "http://arxiv.org/abs/2508.02106v1",
        "published_date": "2025-08-04T06:35:48+00:00",
        "updated_date": "2025-08-04T06:35:48+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Kaiyang Ji",
            "Ye Shi",
            "Zichen Jin",
            "Kangyi Chen",
            "Lan Xu",
            "Yuexin Ma",
            "Jingyi Yu",
            "Jingya Wang"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel framework called Human-X for real-time, physically plausible human interactions in VR/AR and robotics, showcasing significant improvements over existing methods.",
        "tldr_zh": "本文介绍了一个名为Human-X的新框架，用于实时、物理合理的人际互动在VR/AR和机器人技术中，表现出明显的改进。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "VLM4D: Towards Spatiotemporal Awareness in Vision Language Models",
        "summary": "Vision language models (VLMs) have shown remarkable capabilities in\nintegrating linguistic and visual reasoning but remain fundamentally limited in\nunderstanding dynamic spatiotemporal interactions. Humans effortlessly track\nand reason about object movements, rotations, and perspective shifts-abilities\nessential for robust dynamic real-world understanding yet notably lacking in\ncurrent VLMs. In this paper, we introduce VLM4D, the first benchmark\nspecifically designed to evaluate the spatiotemporal reasoning capabilities of\nVLMs. Our benchmark comprises diverse real-world and synthetic videos\naccompanied by carefully curated question-answer pairs emphasizing\ntranslational and rotational motions, perspective awareness, and motion\ncontinuity. Through comprehensive evaluations of state-of-the-art open and\nclosed-source VLMs, we identify significant performance gaps compared to human\nbaselines, highlighting fundamental deficiencies in existing models. Extensive\nanalysis reveals that VLMs struggle particularly with integrating multiple\nvisual cues and maintaining temporal coherence. We further explore promising\ndirections, such as leveraging 4D feature field reconstruction and targeted\nspatiotemporal supervised fine-tuning, demonstrating their effectiveness in\nenhancing spatiotemporal comprehension. Our work aims to encourage deeper\nexploration into improving VLMs' spatial and temporal grounding, paving the way\ntowards more capable and reliable visual intelligence for dynamic environments.",
        "url": "http://arxiv.org/abs/2508.02095v1",
        "published_date": "2025-08-04T06:06:06+00:00",
        "updated_date": "2025-08-04T06:06:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shijie Zhou",
            "Alexander Vilesov",
            "Xuehai He",
            "Ziyu Wan",
            "Shuwang Zhang",
            "Aditya Nagachandra",
            "Di Chang",
            "Dongdong Chen",
            "Xin Eric Wang",
            "Achuta Kadambi"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces VLM4D, a benchmark to evaluate spatiotemporal reasoning in Vision Language Models, highlighting deficiencies in current models and proposing improvements.",
        "tldr_zh": "本文介绍了VLM4D，这是一个用于评估视觉语言模型中时空推理能力的基准，突出了当前模型的不足之处并提出了改进方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "YOLOv1 to YOLOv11: A Comprehensive Survey of Real-Time Object Detection Innovations and Challenges",
        "summary": "Over the past decade, object detection has advanced significantly, with the\nYOLO (You Only Look Once) family of models transforming the landscape of\nreal-time vision applications through unified, end-to-end detection frameworks.\nFrom YOLOv1's pioneering regression-based detection to the latest YOLOv9, each\nversion has systematically enhanced the balance between speed, accuracy, and\ndeployment efficiency through continuous architectural and algorithmic\nadvancements.. Beyond core object detection, modern YOLO architectures have\nexpanded to support tasks such as instance segmentation, pose estimation,\nobject tracking, and domain-specific applications including medical imaging and\nindustrial automation. This paper offers a comprehensive review of the YOLO\nfamily, highlighting architectural innovations, performance benchmarks,\nextended capabilities, and real-world use cases. We critically analyze the\nevolution of YOLO models and discuss emerging research directions that extend\ntheir impact across diverse computer vision domains.",
        "url": "http://arxiv.org/abs/2508.02067v1",
        "published_date": "2025-08-04T05:13:51+00:00",
        "updated_date": "2025-08-04T05:13:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Manikanta Kotthapalli",
            "Deepika Ravipati",
            "Reshma Bhatia"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper provides a comprehensive survey of the YOLO family of models, highlighting their evolution, innovations, performance benchmarks, and real-world applications in object detection and related tasks.",
        "tldr_zh": "本文全面调研了YOLO系列模型，重点介绍了它们在目标检测和相关任务中的演变、创新、性能基准和实际应用。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "StarPose: 3D Human Pose Estimation via Spatial-Temporal Autoregressive Diffusion",
        "summary": "Monocular 3D human pose estimation remains a challenging task due to inherent\ndepth ambiguities and occlusions. Compared to traditional methods based on\nTransformers or Convolutional Neural Networks (CNNs), recent diffusion-based\napproaches have shown superior performance, leveraging their probabilistic\nnature and high-fidelity generation capabilities. However, these methods often\nfail to account for the spatial and temporal correlations across predicted\nframes, resulting in limited temporal consistency and inferior accuracy in\npredicted 3D pose sequences. To address these shortcomings, this paper proposes\nStarPose, an autoregressive diffusion framework that effectively incorporates\nhistorical 3D pose predictions and spatial-temporal physical guidance to\nsignificantly enhance both the accuracy and temporal coherence of pose\npredictions. Unlike existing approaches, StarPose models the 2D-to-3D pose\nmapping as an autoregressive diffusion process. By synergically integrating\npreviously predicted 3D poses with 2D pose inputs via a Historical Pose\nIntegration Module (HPIM), the framework generates rich and informative\nhistorical pose embeddings that guide subsequent denoising steps, ensuring\ntemporally consistent predictions. In addition, a fully plug-and-play\nSpatial-Temporal Physical Guidance (STPG) mechanism is tailored to refine the\ndenoising process in an iterative manner, which further enforces spatial\nanatomical plausibility and temporal motion dynamics, rendering robust and\nrealistic pose estimates. Extensive experiments on benchmark datasets\ndemonstrate that StarPose outperforms state-of-the-art methods, achieving\nsuperior accuracy and temporal consistency in 3D human pose estimation. Code is\navailable at https://github.com/wileychan/StarPose.",
        "url": "http://arxiv.org/abs/2508.02056v1",
        "published_date": "2025-08-04T04:50:05+00:00",
        "updated_date": "2025-08-04T04:50:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoxin Yang",
            "Weihong Chen",
            "Xuemiao Xu",
            "Cheng Xu",
            "Peng Xiao",
            "Cuifeng Sun",
            "Shaoyu Huang",
            "Shengfeng He"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "StarPose is a 3D human pose estimation method that utilizes autoregressive diffusion to improve accuracy and temporal consistency in pose predictions.",
        "tldr_zh": "StarPose是一种3D人体姿势估计方法，利用自回归扩散来提高姿势预测的准确性和时间一致性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HCF: Hierarchical Cascade Framework for Distributed Multi-Stage Image Compression",
        "summary": "Distributed multi-stage image compression -- where visual content traverses\nmultiple processing nodes under varying quality requirements -- poses\nchallenges. Progressive methods enable bitstream truncation but underutilize\navailable compute resources; successive compression repeats costly pixel-domain\noperations and suffers cumulative quality loss and inefficiency;\nfixed-parameter models lack post-encoding flexibility. In this work, we\ndeveloped the Hierarchical Cascade Framework (HCF) that achieves high\nrate-distortion performance and better computational efficiency through direct\nlatent-space transformations across network nodes in distributed multi-stage\nimage compression system. Under HCF, we introduced policy-driven quantization\ncontrol to optimize rate-distortion trade-offs, and established the edge\nquantization principle through differential entropy analysis. The configuration\nbased on this principle demonstrates up to 0.6dB PSNR gains over other\nconfigurations. When comprehensively evaluated on the Kodak, CLIC, and\nCLIC2020-mobile datasets, HCF outperforms successive-compression methods by up\nto 5.56% BD-Rate in PSNR on CLIC, while saving up to 97.8% FLOPs, 96.5% GPU\nmemory, and 90.0% execution time. It also outperforms state-of-the-art\nprogressive compression methods by up to 12.64% BD-Rate on Kodak and enables\nretraining-free cross-quality adaptation with 7.13-10.87% BD-Rate reductions on\nCLIC2020-mobile.",
        "url": "http://arxiv.org/abs/2508.02051v1",
        "published_date": "2025-08-04T04:37:56+00:00",
        "updated_date": "2025-08-04T04:37:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junhao Cai",
            "Taegun An",
            "Chengjun Jin",
            "Sung Il Choi",
            "JuHyun Park",
            "Changhee Joo"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces HCF, a framework for distributed multi-stage image compression that achieves high rate-distortion performance and computational efficiency.",
        "tldr_zh": "本文介绍了HCF，这是一个用于分布式多阶段图像压缩的框架，实现了高码率失真性能和计算效率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Bench2ADVLM: A Closed-Loop Benchmark for Vision-language Models in Autonomous Driving",
        "summary": "Vision-Language Models (VLMs) have recently emerged as a promising paradigm\nin autonomous driving (AD). However, current performance evaluation protocols\nfor VLM-based AD systems (ADVLMs) are predominantly confined to open-loop\nsettings with static inputs, neglecting the more realistic and informative\nclosed-loop setting that captures interactive behavior, feedback resilience,\nand real-world safety. To address this, we introduce Bench2ADVLM, a unified\nhierarchical closed-loop evaluation framework for real-time, interactive\nassessment of ADVLMs across both simulation and physical platforms. Inspired by\ndual-process theories of cognition, we first adapt diverse ADVLMs to simulation\nenvironments via a dual-system adaptation architecture. In this design,\nheterogeneous high-level driving commands generated by target ADVLMs (fast\nsystem) are interpreted by a general-purpose VLM (slow system) into\nstandardized mid-level control actions suitable for execution in simulation. To\nbridge the gap between simulation and reality, we design a physical control\nabstraction layer that translates these mid-level actions into low-level\nactuation signals, enabling, for the first time, closed-loop testing of ADVLMs\non physical vehicles. To enable more comprehensive evaluation, Bench2ADVLM\nintroduces a self-reflective scenario generation module that automatically\nexplores model behavior and uncovers potential failure modes for\nsafety-critical scenario generation. Overall, Bench2ADVLM establishes a\nhierarchical evaluation pipeline that seamlessly integrates high-level abstract\nreasoning, mid-level simulation actions, and low-level real-world execution.\nExperiments on diverse scenarios across multiple state-of-the-art ADVLMs and\nphysical platforms validate the diagnostic strength of our framework, revealing\nthat existing ADVLMs still exhibit limited performance under closed-loop\nconditions.",
        "url": "http://arxiv.org/abs/2508.02028v1",
        "published_date": "2025-08-04T03:43:23+00:00",
        "updated_date": "2025-08-04T03:43:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianyuan Zhang",
            "Ting Jin",
            "Lu Wang",
            "Jiangfan Liu",
            "Siyuan Liang",
            "Mingchuan Zhang",
            "Aishan Liu",
            "Xianglong Liu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "Bench2ADVLM introduces a closed-loop evaluation framework for Vision-Language Models in Autonomous Driving, bridging simulation and real-world testing with a self-reflective scenario generation module.",
        "tldr_zh": "Bench2ADVLM引入了一个闭环评估框架，用于自动驾驶中的视觉语言模型，在模拟和实际测试中搭建桥梁，并配备了自反思场景生成模块。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Devil is in the Detail: Towards Injecting Fine Details of Image Prompt in Image Generation via Conflict-free Guidance and Stratified Attention",
        "summary": "While large-scale text-to-image diffusion models enable the generation of\nhigh-quality, diverse images from text prompts, these prompts struggle to\ncapture intricate details, such as textures, preventing the user intent from\nbeing reflected. This limitation has led to efforts to generate images\nconditioned on user-provided images, referred to as image prompts. Recent work\nmodifies the self-attention mechanism to impose image conditions in generated\nimages by replacing or concatenating the keys and values from the image prompt.\nThis enables the self-attention layer to work like a cross-attention layer,\ngenerally used to incorporate text prompts. In this paper, we identify two\ncommon issues in existing methods of modifying self-attention to generate\nimages that reflect the details of image prompts. First, existing approaches\nneglect the importance of image prompts in classifier-free guidance.\nSpecifically, current methods use image prompts as both desired and undesired\nconditions in classifier-free guidance, causing conflicting signals. To resolve\nthis, we propose conflict-free guidance by using image prompts only as desired\nconditions, ensuring that the generated image faithfully reflects the image\nprompt. In addition, we observe that the two most common self-attention\nmodifications involve a trade-off between the realism of the generated image\nand alignment with the image prompt. Specifically, selecting more keys and\nvalues from the image prompt improves alignment, while selecting more from the\ngenerated image enhances realism. To balance both, we propose an new\nself-attention modification method, Stratified Attention to jointly use keys\nand values from both images rather than selecting between them. Through\nextensive experiments across three image generation tasks, we show that the\nproposed method outperforms existing image-prompting models in faithfully\nreflecting the image prompt.",
        "url": "http://arxiv.org/abs/2508.02004v1",
        "published_date": "2025-08-04T02:48:06+00:00",
        "updated_date": "2025-08-04T02:48:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kyungmin Jo",
            "Jooyeol Yun",
            "Jaegul Choo"
        ],
        "ai_categories": [
            "GAN",
            "Transformer"
        ],
        "tldr": "The paper proposes a new method called Conflict-free Guidance and Stratified Attention to improve image generation from prompts by addressing issues related to detailed image prompts.",
        "tldr_zh": "本文提出了一种新的方法，称为无冲突引导和分层注意，通过解决与详细图像提示相关的问题，改善图像生成。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Fast and Memory-efficient Non-line-of-sight Imaging with Quasi-Fresnel Transform",
        "summary": "Non-line-of-sight (NLOS) imaging seeks to reconstruct hidden objects by\nanalyzing reflections from intermediary surfaces. Existing methods typically\nmodel both the measurement data and the hidden scene in three dimensions,\noverlooking the inherently two-dimensional nature of most hidden objects. This\noversight leads to high computational costs and substantial memory consumption,\nlimiting practical applications and making real-time, high-resolution NLOS\nimaging on lightweight devices challenging. In this paper, we introduce a novel\napproach that represents the hidden scene using two-dimensional functions and\nemploys a Quasi-Fresnel transform to establish a direct inversion formula\nbetween the measurement data and the hidden scene. This transformation\nleverages the two-dimensional characteristics of the problem to significantly\nreduce computational complexity and memory requirements. Our algorithm\nefficiently performs fast transformations between these two-dimensional\naggregated data, enabling rapid reconstruction of hidden objects with minimal\nmemory usage. Compared to existing methods, our approach reduces runtime and\nmemory demands by several orders of magnitude while maintaining imaging\nquality. The substantial reduction in memory usage not only enhances\ncomputational efficiency but also enables NLOS imaging on lightweight devices\nsuch as mobile and embedded systems. We anticipate that this method will\nfacilitate real-time, high-resolution NLOS imaging and broaden its\napplicability across a wider range of platforms.",
        "url": "http://arxiv.org/abs/2508.02003v1",
        "published_date": "2025-08-04T02:46:56+00:00",
        "updated_date": "2025-08-04T02:46:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yijun Wei",
            "Jianyu Wang",
            "Leping Xiao",
            "Zuoqiang Shi",
            "Xing Fu",
            "Lingyun Qiu"
        ],
        "ai_categories": [
            "Transformer",
            "Other"
        ],
        "tldr": "The paper introduces a new method for Non-line-of-sight (NLOS) imaging that significantly reduces computational complexity and memory requirements by representing hidden scenes using two-dimensional functions and employing the Quasi-Fresnel transform.",
        "tldr_zh": "该论文介绍了一种新的非直射视图成像方法，通过使用二维函数表示隐藏场景并利用准-菲涅尔变换，显著降低了计算复杂性和内存需求。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "IMoRe: Implicit Program-Guided Reasoning for Human Motion Q&A",
        "summary": "Existing human motion Q\\&A methods rely on explicit program execution, where\nthe requirement for manually defined functional modules may limit the\nscalability and adaptability. To overcome this, we propose an implicit\nprogram-guided motion reasoning (IMoRe) framework that unifies reasoning across\nmultiple query types without manually designed modules. Unlike existing\nimplicit reasoning approaches that infer reasoning operations from question\nwords, our model directly conditions on structured program functions, ensuring\na more precise execution of reasoning steps. Additionally, we introduce a\nprogram-guided reading mechanism, which dynamically selects multi-level motion\nrepresentations from a pretrained motion Vision Transformer (ViT), capturing\nboth high-level semantics and fine-grained motion cues. The reasoning module\niteratively refines memory representations, leveraging structured program\nfunctions to extract relevant information for different query types. Our model\nachieves state-of-the-art performance on Babel-QA and generalizes to a newly\nconstructed motion Q\\&A dataset based on HuMMan, demonstrating its adaptability\nacross different motion reasoning datasets. Code and dataset are available at:\nhttps://github.com/LUNAProject22/IMoRe.",
        "url": "http://arxiv.org/abs/2508.01984v1",
        "published_date": "2025-08-04T01:44:41+00:00",
        "updated_date": "2025-08-04T01:44:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chen Li",
            "Chinthani Sugandhika",
            "Yeo Keat Ee",
            "Eric Peh",
            "Hao Zhang",
            "Hong Yang",
            "Deepu Rajan",
            "Basura Fernando"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes IMoRe, a framework for human motion Q&A that utilizes implicit program-guided reasoning to achieve state-of-the-art performance and adaptability across different datasets.",
        "tldr_zh": "本文提出了IMoRe框架，用于人体运动问答，利用隐式程序引导推理实现了跨不同数据集的最先进性能和适应性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Self-Supervised YOLO: Leveraging Contrastive Learning for Label-Efficient Object Detection",
        "summary": "One-stage object detectors such as the YOLO family achieve state-of-the-art\nperformance in real-time vision applications but remain heavily reliant on\nlarge-scale labeled datasets for training. In this work, we present a\nsystematic study of contrastive self-supervised learning (SSL) as a means to\nreduce this dependency by pretraining YOLOv5 and YOLOv8 backbones on unlabeled\nimages using the SimCLR framework. Our approach introduces a simple yet\neffective pipeline that adapts YOLO's convolutional backbones as encoders,\nemploys global pooling and projection heads, and optimizes a contrastive loss\nusing augmentations of the COCO unlabeled dataset (120k images). The pretrained\nbackbones are then fine-tuned on a cyclist detection task with limited labeled\ndata. Experimental results show that SSL pretraining leads to consistently\nhigher mAP, faster convergence, and improved precision-recall performance,\nespecially in low-label regimes. For example, our SimCLR-pretrained YOLOv8\nachieves a mAP@50:95 of 0.7663, outperforming its supervised counterpart\ndespite using no annotations during pretraining. These findings establish a\nstrong baseline for applying contrastive SSL to one-stage detectors and\nhighlight the potential of unlabeled data as a scalable resource for\nlabel-efficient object detection.",
        "url": "http://arxiv.org/abs/2508.01966v1",
        "published_date": "2025-08-04T00:27:12+00:00",
        "updated_date": "2025-08-04T00:27:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Manikanta Kotthapalli",
            "Reshma Bhatia",
            "Nainsi Jain"
        ],
        "ai_categories": [
            "Dataset",
            "GAN",
            "AIGC"
        ],
        "tldr": "The paper introduces a system using self-supervised learning to reduce the dependency on labeled data for object detection, achieving improved performance.",
        "tldr_zh": "该论文介绍了一种使用自监督学习的系统，以减少目标检测对标注数据的依赖，实现了性能的提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks",
        "summary": "Vision-language models (VLMs) have exhibited impressive capabilities across\ndiverse image understanding tasks, but still struggle in settings that require\nreasoning over extended sequences of camera frames from a video. This limits\ntheir utility in embodied settings, which require reasoning over long frame\nsequences from a continuous stream of visual input at each moment of a task\nattempt. To address this limitation, we propose ROVER (Reasoning Over VidEo\nRecursively), a framework that enables the model to recursively decompose\nlong-horizon video trajectories into segments corresponding to shorter subtasks\nwithin the trajectory. In doing so, ROVER facilitates more focused and accurate\nreasoning over temporally localized frame sequences without losing global\ncontext. We evaluate ROVER, implemented using an in-context learning approach,\non diverse OpenX Embodiment videos and on a new dataset derived from RoboCasa\nthat consists of 543 videos showing both expert and perturbed non-expert\ntrajectories across 27 robotic manipulation tasks. ROVER outperforms strong\nbaselines across three video reasoning tasks: task progress estimation,\nframe-level natural language reasoning, and video question answering. We\nobserve that, by reducing the number of frames the model reasons over at each\ntimestep, ROVER mitigates hallucinations, especially during unexpected or\nnon-optimal moments of a trajectory. In addition, by enabling the\nimplementation of a subtask-specific sliding context window, ROVER's time\ncomplexity scales linearly with video length, an asymptotic improvement over\nbaselines. Demos, code, and data available at: https://rover-vlm.github.io",
        "url": "http://arxiv.org/abs/2508.01943v1",
        "published_date": "2025-08-03T22:33:43+00:00",
        "updated_date": "2025-08-03T22:33:43+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Philip Schroeder",
            "Ondrej Biza",
            "Thomas Weng",
            "Hongyin Luo",
            "James Glass"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "ROVER is a framework that enables vision-language models to reason over long video sequences by decomposing them into subtasks, outperforming baselines in various video reasoning tasks.",
        "tldr_zh": "ROVER是一个框架，通过将视频序列分解为子任务，使视觉语言模型能够对长视频序列进行推理，在各种视频推理任务中胜过基线模型。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Less is More: AMBER-AFNO -- a New Benchmark for Lightweight 3D Medical Image Segmentation",
        "summary": "This work presents the results of a methodological transfer from remote\nsensing to healthcare, adapting AMBER -- a transformer-based model originally\ndesigned for multiband images, such as hyperspectral data -- to the task of 3D\nmedical datacube segmentation. In this study, we use the AMBER architecture\nwith Adaptive Fourier Neural Operators (AFNO) in place of the multi-head\nself-attention mechanism. While existing models rely on various forms of\nattention to capture global context, AMBER-AFNO achieves this through\nfrequency-domain mixing, enabling a drastic reduction in model complexity. This\ndesign reduces the number of trainable parameters by over 80% compared to\nUNETR++, while maintaining a FLOPs count comparable to other state-of-the-art\narchitectures. Model performance is evaluated on two benchmark 3D medical\ndatasets -- ACDC and Synapse -- using standard metrics such as Dice Similarity\nCoefficient (DSC) and Hausdorff Distance (HD), demonstrating that AMBER-AFNO\nachieves competitive or superior accuracy with significant gains in training\nefficiency, inference speed, and memory usage.",
        "url": "http://arxiv.org/abs/2508.01941v1",
        "published_date": "2025-08-03T22:31:00+00:00",
        "updated_date": "2025-08-03T22:31:00+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Andrea Dosi",
            "Semanto Mondal",
            "Rajib Chandra Ghosh",
            "Massimo Brescia",
            "Giuseppe Longo"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "AMBER-AFNO is a new lightweight 3D medical image segmentation model that achieves competitive accuracy with significant gains in efficiency and performance.",
        "tldr_zh": "AMBER-AFNO 是一种新型轻量级的3D医学图像分割模型，以较高的效率和性能获得了竞争性准确性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Proactive Disentangled Modeling of Trigger-Object Pairings for Backdoor Defense",
        "summary": "Deep neural networks (DNNs) and generative AI (GenAI) are increasingly\nvulnerable to backdoor attacks, where adversaries embed triggers into inputs to\ncause models to misclassify or misinterpret target labels. Beyond traditional\nsingle-trigger scenarios, attackers may inject multiple triggers across various\nobject classes, forming unseen backdoor-object configurations that evade\nstandard detection pipelines. In this paper, we introduce DBOM (Disentangled\nBackdoor-Object Modeling), a proactive framework that leverages structured\ndisentanglement to identify and neutralize both seen and unseen backdoor\nthreats at the dataset level. Specifically, DBOM factorizes input image\nrepresentations by modeling triggers and objects as independent primitives in\nthe embedding space through the use of Vision-Language Models (VLMs). By\nleveraging the frozen, pre-trained encoders of VLMs, our approach decomposes\nthe latent representations into distinct components through a learnable visual\nprompt repository and prompt prefix tuning, ensuring that the relationships\nbetween triggers and objects are explicitly captured. To separate trigger and\nobject representations in the visual prompt repository, we introduce the\ntrigger-object separation and diversity losses that aids in disentangling\ntrigger and object visual features. Next, by aligning image features with\nfeature decomposition and fusion, as well as learned contextual prompt tokens\nin a shared multimodal space, DBOM enables zero-shot generalization to novel\ntrigger-object pairings that were unseen during training, thereby offering\ndeeper insights into adversarial attack patterns. Experimental results on\nCIFAR-10 and GTSRB demonstrate that DBOM robustly detects poisoned images prior\nto downstream training, significantly enhancing the security of DNN training\npipelines.",
        "url": "http://arxiv.org/abs/2508.01932v1",
        "published_date": "2025-08-03T21:58:15+00:00",
        "updated_date": "2025-08-03T21:58:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kyle Stein",
            "Andrew A. Mahyari",
            "Guillermo Francia III",
            "Eman El-Sheikh"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a framework called DBOM for detecting and neutralizing backdoor attacks in deep neural networks by disentangling trigger-object pairings at the dataset level using Vision-Language Models.",
        "tldr_zh": "本文引入了一种名为DBOM的框架，通过使用视觉-语言模型在数据集级别上解开触发-对象配对来检测和中和深度神经网络中的后门攻击。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "IAUNet: Instance-Aware U-Net",
        "summary": "Instance segmentation is critical in biomedical imaging to accurately\ndistinguish individual objects like cells, which often overlap and vary in\nsize. Recent query-based methods, where object queries guide segmentation, have\nshown strong performance. While U-Net has been a go-to architecture in medical\nimage segmentation, its potential in query-based approaches remains largely\nunexplored. In this work, we present IAUNet, a novel query-based U-Net\narchitecture. The core design features a full U-Net architecture, enhanced by a\nnovel lightweight convolutional Pixel decoder, making the model more efficient\nand reducing the number of parameters. Additionally, we propose a Transformer\ndecoder that refines object-specific features across multiple scales. Finally,\nwe introduce the 2025 Revvity Full Cell Segmentation Dataset, a unique resource\nwith detailed annotations of overlapping cell cytoplasm in brightfield images,\nsetting a new benchmark for biomedical instance segmentation. Experiments on\nmultiple public datasets and our own show that IAUNet outperforms most\nstate-of-the-art fully convolutional, transformer-based, and query-based models\nand cell segmentation-specific models, setting a strong baseline for cell\ninstance segmentation tasks. Code is available at\nhttps://github.com/SlavkoPrytula/IAUNet",
        "url": "http://arxiv.org/abs/2508.01928v1",
        "published_date": "2025-08-03T21:36:20+00:00",
        "updated_date": "2025-08-03T21:36:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "68T45, 62H35",
            "I.4.8; I.2.10; I.5.1"
        ],
        "authors": [
            "Yaroslav Prytula",
            "Illia Tsiporenko",
            "Ali Zeynalli",
            "Dmytro Fishman"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "IAUNet is a query-based U-Net architecture designed for instance segmentation in biomedical imaging, outperforming state-of-the-art models and setting a strong baseline for cell instance segmentation tasks.",
        "tldr_zh": "IAUNet是一种用于生物医学成像的基于查询的U-Net架构，优于现有模型，为细胞实例分割任务设定了强有力的基准线。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EgoTrigger: Toward Audio-Driven Image Capture for Human Memory Enhancement in All-Day Energy-Efficient Smart Glasses",
        "summary": "All-day smart glasses are likely to emerge as platforms capable of continuous\ncontextual sensing, uniquely positioning them for unprecedented assistance in\nour daily lives. Integrating the multi-modal AI agents required for human\nmemory enhancement while performing continuous sensing, however, presents a\nmajor energy efficiency challenge for all-day usage. Achieving this balance\nrequires intelligent, context-aware sensor management. Our approach,\nEgoTrigger, leverages audio cues from the microphone to selectively activate\npower-intensive cameras, enabling efficient sensing while preserving\nsubstantial utility for human memory enhancement. EgoTrigger uses a lightweight\naudio model (YAMNet) and a custom classification head to trigger image capture\nfrom hand-object interaction (HOI) audio cues, such as the sound of a drawer\nopening or a medication bottle being opened. In addition to evaluating on the\nQA-Ego4D dataset, we introduce and evaluate on the Human Memory Enhancement\nQuestion-Answer (HME-QA) dataset. Our dataset contains 340 human-annotated\nfirst-person QA pairs from full-length Ego4D videos that were curated to ensure\nthat they contained audio, focusing on HOI moments critical for contextual\nunderstanding and memory. Our results show EgoTrigger can use 54% fewer frames\non average, significantly saving energy in both power-hungry sensing components\n(e.g., cameras) and downstream operations (e.g., wireless transmission), while\nachieving comparable performance on datasets for an episodic memory task. We\nbelieve this context-aware triggering strategy represents a promising direction\nfor enabling energy-efficient, functional smart glasses capable of all-day use\n-- supporting applications like helping users recall where they placed their\nkeys or information about their routine activities (e.g., taking medications).",
        "url": "http://arxiv.org/abs/2508.01915v1",
        "published_date": "2025-08-03T20:51:23+00:00",
        "updated_date": "2025-08-03T20:51:23+00:00",
        "categories": [
            "cs.CV",
            "cs.ET",
            "cs.HC",
            "cs.LG",
            "cs.SD",
            "eess.AS"
        ],
        "authors": [
            "Akshay Paruchuri",
            "Sinan Hersek",
            "Lavisha Aggarwal",
            "Qiao Yang",
            "Xin Liu",
            "Achin Kulshrestha",
            "Andrea Colaco",
            "Henry Fuchs",
            "Ishan Chatterjee"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes a context-aware triggering strategy called EgoTrigger for energy-efficient smart glasses, enabling human memory enhancement through audio-driven image capture during daily activities.",
        "tldr_zh": "该论文提出了一种面向能源高效智能眼镜的上下文感知触发策略EgoTrigger，通过音频驱动的图像捕获在日常活动中增强人类记忆。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Medical Image De-Identification Resources: Synthetic DICOM Data and Tools for Validation",
        "summary": "Medical imaging research increasingly depends on large-scale data sharing to\npromote reproducibility and train Artificial Intelligence (AI) models. Ensuring\npatient privacy remains a significant challenge for open-access data sharing.\nDigital Imaging and Communications in Medicine (DICOM), the global standard\ndata format for medical imaging, encodes both essential clinical metadata and\nextensive protected health information (PHI) and personally identifiable\ninformation (PII). Effective de-identification must remove identifiers,\npreserve scientific utility, and maintain DICOM validity. Tools exist to\nperform de-identification, but few assess its effectiveness, and most rely on\nsubjective reviews, limiting reproducibility and regulatory confidence. To\naddress this gap, we developed an openly accessible DICOM dataset infused with\nsynthetic PHI/PII and an evaluation framework for benchmarking image\nde-identification workflows. The Medical Image de-identification (MIDI) dataset\nwas built using publicly available de-identified data from The Cancer Imaging\nArchive (TCIA). It includes 538 subjects (216 for validation, 322 for testing),\n605 studies, 708 series, and 53,581 DICOM image instances. These span multiple\nvendors, imaging modalities, and cancer types. Synthetic PHI and PII were\nembedded into structured data elements, plain text data elements, and pixel\ndata to simulate real-world identity leaks encountered by TCIA curation teams.\nAccompanying evaluation tools include a Python script, answer keys (known\ntruth), and mapping files that enable automated comparison of curated data\nagainst expected transformations. The framework is aligned with the HIPAA\nPrivacy Rule \"Safe Harbor\" method, DICOM PS3.15 Confidentiality Profiles, and\nTCIA best practices. It supports objective, standards-driven evaluation of\nde-identification workflows, promoting safer and more consistent medical image\nsharing.",
        "url": "http://arxiv.org/abs/2508.01889v1",
        "published_date": "2025-08-03T18:48:28+00:00",
        "updated_date": "2025-08-03T18:48:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Michael W. Rutherford",
            "Tracy Nolan",
            "Linmin Pei",
            "Ulrike Wagner",
            "Qinyan Pan",
            "Phillip Farmer",
            "Kirk Smith",
            "Benjamin Kopchick",
            "Laura Opsahl-Ong",
            "Granger Sutton",
            "David Clunie",
            "Keyvan Farahani",
            "Fred Prior"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper introduces a dataset and tools for evaluating the effectiveness of de-identification of medical images to protect patient privacy in data sharing for AI model training.",
        "tldr_zh": "该论文介绍了一个数据集和工具，用于评估医学图像去识别的有效性，以保护患者隐私在为AI模型训练的数据共享中。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "DiffusionFF: Face Forgery Detection via Diffusion-based Artifact Localization",
        "summary": "The rapid evolution of deepfake generation techniques demands robust and\naccurate face forgery detection algorithms. While determining whether an image\nhas been manipulated remains essential, the ability to precisely localize\nforgery artifacts has become increasingly important for improving model\nexplainability and fostering user trust. To address this challenge, we propose\nDiffusionFF, a novel framework that enhances face forgery detection through\ndiffusion-based artifact localization. Our method utilizes a denoising\ndiffusion model to generate high-quality Structural Dissimilarity (DSSIM) maps,\nwhich effectively capture subtle traces of manipulation. These DSSIM maps are\nthen fused with high-level semantic features extracted by a pretrained forgery\ndetector, leading to significant improvements in detection accuracy. Extensive\nexperiments on both cross-dataset and intra-dataset benchmarks demonstrate that\nDiffusionFF not only achieves superior detection performance but also offers\nprecise and fine-grained artifact localization, highlighting its overall\neffectiveness.",
        "url": "http://arxiv.org/abs/2508.01873v1",
        "published_date": "2025-08-03T18:06:04+00:00",
        "updated_date": "2025-08-03T18:06:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Siran Peng",
            "Haoyuan Zhang",
            "Li Gao",
            "Tianshuo Zhang",
            "Bao Li",
            "Zhen Lei"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "DiffusionFF is a new framework for detecting face forgery that focuses on precise artifact localization using diffusion models and high-level semantic features.",
        "tldr_zh": "DiffusionFF是一个新的人脸伪造检测框架，利用扩散模型和高级语义特征实现精确的伪造物件定位。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Conditional Diffusion Model with Anatomical-Dose Dual Constraints for End-to-End Multi-Tumor Dose Prediction",
        "summary": "Radiotherapy treatment planning often relies on time-consuming,\ntrial-and-error adjustments that heavily depend on the expertise of\nspecialists, while existing deep learning methods face limitations in\ngeneralization, prediction accuracy, and clinical applicability. To tackle\nthese challenges, we propose ADDiff-Dose, an Anatomical-Dose Dual Constraints\nConditional Diffusion Model for end-to-end multi-tumor dose prediction. The\nmodel employs LightweightVAE3D to compress high-dimensional CT data and\nintegrates multimodal inputs, including target and organ-at-risk (OAR) masks\nand beam parameters, within a progressive noise addition and denoising\nframework. It incorporates conditional features via a multi-head attention\nmechanism and utilizes a composite loss function combining MSE, conditional\nterms, and KL divergence to ensure both dosimetric accuracy and compliance with\nclinical constraints. Evaluation on a large-scale public dataset (2,877 cases)\nand three external institutional cohorts (450 cases in total) demonstrates that\nADDiff-Dose significantly outperforms traditional baselines, achieving an MAE\nof 0.101-0.154 (compared to 0.316 for UNet and 0.169 for GAN models), a DICE\ncoefficient of 0.927 (a 6.8% improvement), and limiting spinal cord maximum\ndose error to within 0.1 Gy. The average plan generation time per case is\nreduced to 22 seconds. Ablation studies confirm that the structural encoder\nenhances compliance with clinical dose constraints by 28.5%. To our knowledge,\nthis is the first study to introduce a conditional diffusion model framework\nfor radiotherapy dose prediction, offering a generalizable and efficient\nsolution for automated treatment planning across diverse tumor sites, with the\npotential to substantially reduce planning time and improve clinical workflow\nefficiency.",
        "url": "http://arxiv.org/abs/2508.02043v1",
        "published_date": "2025-08-04T04:25:47+00:00",
        "updated_date": "2025-08-04T04:25:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hui Xie",
            "Haiqin Hu",
            "Lijuan Ding",
            "Qing Li",
            "Yue Sun",
            "Tao Tan"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces ADDiff-Dose, a model for predicting radiation doses in cancer treatment, outperforming traditional methods and offering a faster and more efficient solution.",
        "tldr_zh": "该论文介绍了ADDiff-Dose模型，用于预测癌症治疗中的放射剂量，超越传统方法，提供更快速和高效的解决方案。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7.75
    },
    {
        "title": "Towards Reliable Audio Deepfake Attribution and Model Recognition: A Multi-Level Autoencoder-Based Framework",
        "summary": "The proliferation of audio deepfakes poses a growing threat to trust in\ndigital communications. While detection methods have advanced, attributing\naudio deepfakes to their source models remains an underexplored yet crucial\nchallenge. In this paper we introduce LAVA (Layered Architecture for Voice\nAttribution), a hierarchical framework for audio deepfake detection and model\nrecognition that leverages attention-enhanced latent representations extracted\nby a convolutional autoencoder trained solely on fake audio. Two specialized\nclassifiers operate on these features: Audio Deepfake Attribution (ADA), which\nidentifies the generation technology, and Audio Deepfake Model Recognition\n(ADMR), which recognize the specific generative model instance. To improve\nrobustness under open-set conditions, we incorporate confidence-based rejection\nthresholds. Experiments on ASVspoof2021, FakeOrReal, and CodecFake show strong\nperformance: the ADA classifier achieves F1-scores over 95% across all\ndatasets, and the ADMR module reaches 96.31% macro F1 across six classes.\nAdditional tests on unseen attacks from ASVpoof2019 LA and error propagation\nanalysis confirm LAVA's robustness and reliability. The framework advances the\nfield by introducing a supervised approach to deepfake attribution and model\nrecognition under open-set conditions, validated on public benchmarks and\naccompanied by publicly released models and code. Models and code are available\nat https://www.github.com/adipiz99/lava-framework.",
        "url": "http://arxiv.org/abs/2508.02521v1",
        "published_date": "2025-08-04T15:31:13+00:00",
        "updated_date": "2025-08-04T15:31:13+00:00",
        "categories": [
            "cs.SD",
            "cs.CV",
            "eess.AS"
        ],
        "authors": [
            "Andrea Di Pierno",
            "Luca Guarnera",
            "Dario Allegra",
            "Sebastiano Battiato"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a framework called LAVA for reliable audio deepfake attribution and model recognition using a hierarchical autoencoder-based approach.",
        "tldr_zh": "本文介绍了一种名为LAVA的框架，用于使用分层自动编码器的方法进行可靠的音频深假归因和模型识别。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "$ε$-Softmax: Approximating One-Hot Vectors for Mitigating Label Noise",
        "summary": "Noisy labels pose a common challenge for training accurate deep neural\nnetworks. To mitigate label noise, prior studies have proposed various robust\nloss functions to achieve noise tolerance in the presence of label noise,\nparticularly symmetric losses. However, they usually suffer from the\nunderfitting issue due to the overly strict symmetric condition. In this work,\nwe propose a simple yet effective approach for relaxing the symmetric\ncondition, namely $\\epsilon$-softmax, which simply modifies the outputs of the\nsoftmax layer to approximate one-hot vectors with a controllable error\n$\\epsilon$. Essentially, $\\epsilon$-softmax not only acts as an alternative for\nthe softmax layer, but also implicitly plays the crucial role in modifying the\nloss function. We prove theoretically that $\\epsilon$-softmax can achieve\nnoise-tolerant learning with controllable excess risk bound for almost any loss\nfunction. Recognizing that $\\epsilon$-softmax-enhanced losses may slightly\nreduce fitting ability on clean datasets, we further incorporate them with one\nsymmetric loss, thereby achieving a better trade-off between robustness and\neffective learning. Extensive experiments demonstrate the superiority of our\nmethod in mitigating synthetic and real-world label noise. The code is\navailable at https://github.com/cswjl/eps-softmax.",
        "url": "http://arxiv.org/abs/2508.02387v1",
        "published_date": "2025-08-04T13:10:48+00:00",
        "updated_date": "2025-08-04T13:10:48+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Jialiang Wang",
            "Xiong Zhou",
            "Deming Zhai",
            "Junjun Jiang",
            "Xiangyang Ji",
            "Xianming Liu"
        ],
        "ai_categories": [
            "GAN",
            "Other"
        ],
        "tldr": "The paper proposes a new approach called ε-softmax to mitigate label noise in deep neural networks, achieving noise-tolerant learning by approximating one-hot vectors with controlled error ε.",
        "tldr_zh": "本文提出了一种新方法，称为ε-softmax，用于减轻深度神经网络中的标签噪声，通过控制误差ε逼近one-hot向量，实现了对噪声的容忍学习。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Enhancing Object Discovery for Unsupervised Instance Segmentation and Object Detection",
        "summary": "We propose Cut-Once-and-LEaRn (COLER), a simple approach for unsupervised\ninstance segmentation and object detection. COLER first uses our developed\nCutOnce to generate coarse pseudo labels, then enables the detector to learn\nfrom these masks. CutOnce applies Normalized Cut only once and does not rely on\nany clustering methods, but it can generate multiple object masks in an image.\nWe have designed several novel yet simple modules that not only allow CutOnce\nto fully leverage the object discovery capabilities of self-supervised models,\nbut also free it from reliance on mask post-processing. During training, COLER\nachieves strong performance without requiring specially designed loss functions\nfor pseudo labels, and its performance is further improved through\nself-training. COLER is a zero-shot unsupervised model that outperforms\nprevious state-of-the-art methods on multiple benchmarks.We believe our method\ncan help advance the field of unsupervised object localization.",
        "url": "http://arxiv.org/abs/2508.02386v1",
        "published_date": "2025-08-04T13:10:39+00:00",
        "updated_date": "2025-08-04T13:10:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xingyu Feng",
            "Hebei Gao",
            "Hong Li"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Object"
        ],
        "tldr": "The paper introduces COLER, a simple approach for unsupervised instance segmentation and object detection, outperforming previous methods on multiple benchmarks.",
        "tldr_zh": "本文介绍了COLER，一种简单的方法，用于无监督实例分割和目标检测，在多个基准测试中表现优异。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Is Uncertainty Quantification a Viable Alternative to Learned Deferral?",
        "summary": "Artificial Intelligence (AI) holds the potential to dramatically improve\npatient care. However, it is not infallible, necessitating\nhuman-AI-collaboration to ensure safe implementation. One aspect of AI safety\nis the models' ability to defer decisions to a human expert when they are\nlikely to misclassify autonomously. Recent research has focused on methods that\nlearn to defer by optimising a surrogate loss function that finds the optimal\ntrade-off between predicting a class label or deferring. However, during\nclinical translation, models often face challenges such as data shift.\nUncertainty quantification methods aim to estimate a model's confidence in its\npredictions. However, they may also be used as a deferral strategy which does\nnot rely on learning from specific training distribution. We hypothesise that\nmodels developed to quantify uncertainty are more robust to out-of-distribution\n(OOD) input than learned deferral models that have been trained in a supervised\nfashion. To investigate this hypothesis, we constructed an extensive evaluation\nstudy on a large ophthalmology dataset, examining both learned deferral models\nand established uncertainty quantification methods, assessing their performance\nin- and out-of-distribution. Specifically, we evaluate their ability to\naccurately classify glaucoma from fundus images while deferring cases with a\nhigh likelihood of error. We find that uncertainty quantification methods may\nbe a promising choice for AI deferral.",
        "url": "http://arxiv.org/abs/2508.02319v1",
        "published_date": "2025-08-04T11:37:59+00:00",
        "updated_date": "2025-08-04T11:37:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Anna M. Wundram",
            "Christian F. Baumgartner"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper compares uncertainty quantification methods with learned deferral models for AI safety in the medical field, finding that uncertainties methods may be more robust.",
        "tldr_zh": "本文比较了不确定性量化方法与学习推迟模型在医学领域的AI安全性，发现不确定性方法可能更加稳健。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "Whole-body Representation Learning For Competing Preclinical Disease Risk Assessment",
        "summary": "Reliable preclinical disease risk assessment is essential to move public\nhealthcare from reactive treatment to proactive identification and prevention.\nHowever, image-based risk prediction algorithms often consider one condition at\na time and depend on hand-crafted features obtained through segmentation tools.\nWe propose a whole-body self-supervised representation learning method for the\npreclinical disease risk assessment under a competing risk modeling. This\napproach outperforms whole-body radiomics in multiple diseases, including\ncardiovascular disease (CVD), type 2 diabetes (T2D), chronic obstructive\npulmonary disease (COPD), and chronic kidney disease (CKD). Simulating a\npreclinical screening scenario and subsequently combining with cardiac MRI, it\nsharpens further the prediction for CVD subgroups: ischemic heart disease\n(IHD), hypertensive diseases (HD), and stroke. The results indicate the\ntranslational potential of whole-body representations as a standalone screening\nmodality and as part of a multi-modal framework within clinical workflows for\nearly personalized risk stratification. The code is available at\nhttps://github.com/yayapa/WBRLforCR/",
        "url": "http://arxiv.org/abs/2508.02307v1",
        "published_date": "2025-08-04T11:20:31+00:00",
        "updated_date": "2025-08-04T11:20:31+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Dmitrii Seletkov",
            "Sophie Starck",
            "Ayhan Can Erdur",
            "Yundi Zhang",
            "Daniel Rueckert",
            "Rickmer Braren"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper introduces a whole-body representation learning method for preclinical disease risk assessment, outperforming traditional radiomics in multiple diseases.",
        "tldr_zh": "该论文提出了一种全身表征学习方法，用于预测多种疾病的风险，优于传统的放射学方法。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "Unleashing the Temporal Potential of Stereo Event Cameras for Continuous-Time 3D Object Detection",
        "summary": "3D object detection is essential for autonomous systems, enabling precise\nlocalization and dimension estimation. While LiDAR and RGB cameras are widely\nused, their fixed frame rates create perception gaps in high-speed scenarios.\nEvent cameras, with their asynchronous nature and high temporal resolution,\noffer a solution by capturing motion continuously. The recent approach, which\nintegrates event cameras with conventional sensors for continuous-time\ndetection, struggles in fast-motion scenarios due to its dependency on\nsynchronized sensors. We propose a novel stereo 3D object detection framework\nthat relies solely on event cameras, eliminating the need for conventional 3D\nsensors. To compensate for the lack of semantic and geometric information in\nevent data, we introduce a dual filter mechanism that extracts both.\nAdditionally, we enhance regression by aligning bounding boxes with\nobject-centric information. Experiments show that our method outperforms prior\napproaches in dynamic environments, demonstrating the potential of event\ncameras for robust, continuous-time 3D perception. The code is available at\nhttps://github.com/mickeykang16/Ev-Stereo3D.",
        "url": "http://arxiv.org/abs/2508.02288v1",
        "published_date": "2025-08-04T10:57:03+00:00",
        "updated_date": "2025-08-04T10:57:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jae-Young Kang",
            "Hoonhee Cho",
            "Kuk-Jin Yoon"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel stereo 3D object detection framework that solely relies on event cameras, outperforming previous approaches in dynamic environments.",
        "tldr_zh": "本文介绍了一种新颖的立体 3D 物体检测框架，完全依赖事件相机，在动态环境中表现优于先前方法。",
        "relevance_score": 2,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7.5
    },
    {
        "title": "Test-Time Model Adaptation for Quantized Neural Networks",
        "summary": "Quantizing deep models prior to deployment is a widely adopted technique to\nspeed up inference for various real-time applications, such as autonomous\ndriving. However, quantized models often suffer from severe performance\ndegradation in dynamic environments with potential domain shifts and this\ndegradation is significantly more pronounced compared with their full-precision\ncounterparts, as shown by our theoretical and empirical illustrations. To\naddress the domain shift problem, test-time adaptation (TTA) has emerged as an\neffective solution by enabling models to learn adaptively from test data.\nUnfortunately, existing TTA methods are often impractical for quantized models\nas they typically rely on gradient backpropagation--an operation that is\nunsupported on quantized models due to vanishing gradients, as well as memory\nand latency constraints. In this paper, we focus on TTA for quantized models to\nimprove their robustness and generalization ability efficiently. We propose a\ncontinual zeroth-order adaptation (ZOA) framework that enables efficient model\nadaptation using only two forward passes, eliminating the computational burden\nof existing methods. Moreover, we propose a domain knowledge management scheme\nto store and reuse different domain knowledge with negligible memory\nconsumption, reducing the interference of different domain knowledge and\nfostering the knowledge accumulation during long-term adaptation. Experimental\nresults on three classical architectures, including quantized transformer-based\nand CNN-based models, demonstrate the superiority of our methods for quantized\nmodel adaptation. On the quantized W6A6 ViT-B model, our ZOA is able to achieve\na 5.0\\% improvement over the state-of-the-art FOA on ImageNet-C dataset. The\nsource code is available at https://github.com/DengZeshuai/ZOA.",
        "url": "http://arxiv.org/abs/2508.02180v1",
        "published_date": "2025-08-04T08:24:19+00:00",
        "updated_date": "2025-08-04T08:24:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zeshuai Deng",
            "Guohao Chen",
            "Shuaicheng Niu",
            "Hui Luo",
            "Shuhai Zhang",
            "Yifan Yang",
            "Renjie Chen",
            "Wei Luo",
            "Mingkui Tan"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a Test-Time Adaptation framework for quantized neural networks to improve their robustness and generalization ability efficiently.",
        "tldr_zh": "本文介绍了一种针对量化神经网络的测试时间模型自适应框架，以有效提高其鲁棒性和泛化能力。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "DeflareMamba: Hierarchical Vision Mamba for Contextually Consistent Lens Flare Removal",
        "summary": "Lens flare removal remains an information confusion challenge in the\nunderlying image background and the optical flares, due to the complex optical\ninteractions between light sources and camera lens. While recent solutions have\nshown promise in decoupling the flare corruption from image, they often fail to\nmaintain contextual consistency, leading to incomplete and inconsistent flare\nremoval. To eliminate this limitation, we propose DeflareMamba, which leverages\nthe efficient sequence modeling capabilities of state space models while\nmaintains the ability to capture local-global dependencies. Particularly, we\ndesign a hierarchical framework that establishes long-range pixel correlations\nthrough varied stride sampling patterns, and utilize local-enhanced state space\nmodels that simultaneously preserves local details. To the best of our\nknowledge, this is the first work that introduces state space models to the\nflare removal task. Extensive experiments demonstrate that our method\neffectively removes various types of flare artifacts, including scattering and\nreflective flares, while maintaining the natural appearance of non-flare\nregions. Further downstream applications demonstrate the capacity of our method\nto improve visual object recognition and cross-modal semantic understanding.\nCode is available at https://github.com/BNU-ERC-ITEA/DeflareMamba.",
        "url": "http://arxiv.org/abs/2508.02113v1",
        "published_date": "2025-08-04T06:49:48+00:00",
        "updated_date": "2025-08-04T06:49:48+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Yihang Huang",
            "Yuanfei Huang",
            "Junhui Lin",
            "Hua Huang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "DeflareMamba proposes a novel hierarchical vision model for removing lens flares with contextual consistency, using state space models and long-range pixel correlations.",
        "tldr_zh": "DeflareMamba提出了一种新颖的分层视觉模型，用于以上下文一致性删除镜头闪光，利用状态空间模型和长距离像素相关性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Mapillary Vistas Validation for Fine-Grained Traffic Signs: A Benchmark Revealing Vision-Language Model Limitations",
        "summary": "Obtaining high-quality fine-grained annotations for traffic signs is critical\nfor accurate and safe decision-making in autonomous driving. Widely used\ndatasets, such as Mapillary, often provide only coarse-grained labels - without\ndistinguishing semantically important types such as stop signs or speed limit\nsigns. To this end, we present a new validation set for traffic signs derived\nfrom the Mapillary dataset called Mapillary Vistas Validation for Traffic Signs\n(MVV), where we decompose composite traffic signs into granular, semantically\nmeaningful categories. The dataset includes pixel-level instance masks and has\nbeen manually annotated by expert annotators to ensure label fidelity. Further,\nwe benchmark several state-of-the-art VLMs against the self-supervised DINOv2\nmodel on this dataset and show that DINOv2 consistently outperforms all VLM\nbaselines-not only on traffic sign recognition, but also on heavily represented\ncategories like vehicles and humans. Our analysis reveals significant\nlimitations in current vision-language models for fine-grained visual\nunderstanding and establishes DINOv2 as a strong baseline for dense semantic\nmatching in autonomous driving scenarios. This dataset and evaluation framework\npave the way for more reliable, interpretable, and scalable perception systems.\n  Code and data are available at: https://github.com/nec-labs-ma/relabeling",
        "url": "http://arxiv.org/abs/2508.02047v1",
        "published_date": "2025-08-04T04:29:06+00:00",
        "updated_date": "2025-08-04T04:29:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sparsh Garg",
            "Abhishek Aich"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a new dataset for fine-grained traffic sign recognition in autonomous driving, highlighting limitations in current vision-language models and proposing a new model DINOv2 as a strong baseline.",
        "tldr_zh": "本文介绍了一个新的数据集用于自动驾驶中的细粒度交通标志识别，重点强调了当前视觉语言模型的局限性，并提出了一个新模型DINOv2作为一个强大的基准。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "From Photons to Physics: Autonomous Indoor Drones and the Future of Objective Property Assessment",
        "summary": "The convergence of autonomous indoor drones with physics-aware sensing\ntechnologies promises to transform property assessment from subjective visual\ninspection to objective, quantitative measurement. This comprehensive review\nexamines the technical foundations enabling this paradigm shift across four\ncritical domains: (1) platform architectures optimized for indoor navigation,\nwhere weight constraints drive innovations in heterogeneous computing,\ncollision-tolerant design, and hierarchical control systems; (2) advanced\nsensing modalities that extend perception beyond human vision, including\nhyperspectral imaging for material identification, polarimetric sensing for\nsurface characterization, and computational imaging with metaphotonics enabling\nradical miniaturization; (3) intelligent autonomy through active reconstruction\nalgorithms, where drones equipped with 3D Gaussian Splatting make strategic\ndecisions about viewpoint selection to maximize information gain within battery\nconstraints; and (4) integration pathways with existing property workflows,\nincluding Building Information Modeling (BIM) systems and industry standards\nlike Uniform Appraisal Dataset (UAD) 3.6.",
        "url": "http://arxiv.org/abs/2508.01965v1",
        "published_date": "2025-08-04T00:20:25+00:00",
        "updated_date": "2025-08-04T00:20:25+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "I.2.9; I.4.8; I.2.10; C.3; J.2"
        ],
        "authors": [
            "Petteri Teikari",
            "Mike Jarrell",
            "Irene Bandera Moreno",
            "Harri Pesola"
        ],
        "ai_categories": [
            "Multimodality",
            "AIGC",
            "Dataset",
            "LoRA"
        ],
        "tldr": "The paper explores how autonomous indoor drones coupled with advanced sensing technologies can revolutionize property assessment from subjective to objective measurements.",
        "tldr_zh": "本文探讨了自主室内无人机与先进传感技术如何将物业评估从主观转变为客观测量。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "Clinical Expert Uncertainty Guided Generalized Label Smoothing for Medical Noisy Label Learning",
        "summary": "Many previous studies have proposed extracting image labels from clinical\nnotes to create large-scale medical image datasets at a low cost. However,\nthese approaches inherently suffer from label noise due to uncertainty from the\nclinical experts. When radiologists and physicians analyze medical images to\nmake diagnoses, they often include uncertainty-aware notes such as ``maybe'' or\n``not excluded''. Unfortunately, current text-mining methods overlook these\nnuances, resulting in the creation of noisy labels. Existing methods for\nhandling noisy labels in medical image analysis, which typically address the\nproblem through post-processing techniques, have largely ignored the important\nissue of expert-driven uncertainty contributing to label noise. To better\nincorporate the expert-written uncertainty in clinical notes into medical image\nanalysis and address the label noise issue, we first examine the impact of\nclinical expert uncertainty on label noise. We then propose a clinical expert\nuncertainty-aware benchmark, along with a label smoothing method, which\nsignificantly improves performance compared to current state-of-the-art\napproaches.",
        "url": "http://arxiv.org/abs/2508.02495v1",
        "published_date": "2025-08-04T15:05:27+00:00",
        "updated_date": "2025-08-04T15:05:27+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Kunyu Zhang",
            "Lin Gu",
            "Liangchen Liu",
            "Yingke Chen",
            "Bingyang Wang",
            "Jin Yan",
            "Yingying Zhu"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper investigates the impact of clinical expert uncertainty on label noise in medical image analysis and proposes a labeling method that significantly improves performance compared to current approaches.",
        "tldr_zh": "本文研究了临床专家不确定性对医学图像分析中标签噪声的影响，并提出了一种标签方法，与当前方法相比显著提高性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "InfoSyncNet: Information Synchronization Temporal Convolutional Network for Visual Speech Recognition",
        "summary": "Estimating spoken content from silent videos is crucial for applications in\nAssistive Technology (AT) and Augmented Reality (AR). However, accurately\nmapping lip movement sequences in videos to words poses significant challenges\ndue to variability across sequences and the uneven distribution of information\nwithin each sequence. To tackle this, we introduce InfoSyncNet, a non-uniform\nsequence modeling network enhanced by tailored data augmentation techniques.\nCentral to InfoSyncNet is a non-uniform quantization module positioned between\nthe encoder and decoder, enabling dynamic adjustment to the network's focus and\neffectively handling the natural inconsistencies in visual speech data.\nAdditionally, multiple training strategies are incorporated to enhance the\nmodel's capability to handle variations in lighting and the speaker's\norientation. Comprehensive experiments on the LRW and LRW1000 datasets confirm\nthe superiority of InfoSyncNet, achieving new state-of-the-art accuracies of\n92.0% and 60.7% Top-1 ACC. The code is available for download (see comments).",
        "url": "http://arxiv.org/abs/2508.02460v1",
        "published_date": "2025-08-04T14:27:01+00:00",
        "updated_date": "2025-08-04T14:27:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junxiao Xue",
            "Xiaozhen Liu",
            "Xuecheng Wu",
            "Fei Yu",
            "Jun Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces InfoSyncNet, a network for visual speech recognition from silent videos using non-uniform sequence modeling and tailored data augmentation. It achieves state-of-the-art results on LRW and LRW1000 datasets.",
        "tldr_zh": "该论文介绍了InfoSyncNet，使用非均匀序列建模和定制数据增强来识别无声视频中的视觉语音。在LRW和LRW1000数据集上取得了最新的结果。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Uncertainty Estimation for Novel Views in Gaussian Splatting from Primitive-Based Representations of Error and Visibility",
        "summary": "In this work, we present a novel method for uncertainty estimation (UE) in\nGaussian Splatting. UE is crucial for using Gaussian Splatting in critical\napplications such as robotics and medicine. Previous methods typically estimate\nthe variance of Gaussian primitives and use the rendering process to obtain\npixel-wise uncertainties. Our method establishes primitive representations of\nerror and visibility of trainings views, which carries meaningful uncertainty\ninformation. This representation is obtained by projection of training error\nand visibility onto the primitives. Uncertainties of novel views are obtained\nby rendering the primitive representations of uncertainty for those novel\nviews, yielding uncertainty feature maps. To aggregate these uncertainty\nfeature maps of novel views, we perform a pixel-wise regression on holdout\ndata. In our experiments, we analyze the different components of our method,\ninvestigating various combinations of uncertainty feature maps and regression\nmodels. Furthermore, we considered the effect of separating splatting into\nforeground and background. Our UEs show high correlations to true errors,\noutperforming state-of-the-art methods, especially on foreground objects. The\ntrained regression models show generalization capabilities to new scenes,\nallowing uncertainty estimation without the need for holdout data.",
        "url": "http://arxiv.org/abs/2508.02443v1",
        "published_date": "2025-08-04T14:02:20+00:00",
        "updated_date": "2025-08-04T14:02:20+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Thomas Gottwald",
            "Edgar Heinert",
            "Matthias Rottmann"
        ],
        "ai_categories": [
            "Multimodality",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a new method for uncertainty estimation in Gaussian Splatting, outperforming previous methods and showing generalization capabilities to new scenes.",
        "tldr_zh": "本文介绍了一种在高斯喷溅中进行不确定性估计的新方法，优于先前的方法，并展现了对新场景的泛化能力。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 7.25
    },
    {
        "title": "Identifying actionable driver mutations in lung cancer using an efficient Asymmetric Transformer Decoder",
        "summary": "Identifying actionable driver mutations in non-small cell lung cancer (NSCLC)\ncan impact treatment decisions and significantly improve patient outcomes.\nDespite guideline recommendations, broader adoption of genetic testing remains\nchallenging due to limited availability and lengthy turnaround times. Machine\nLearning (ML) methods for Computational Pathology (CPath) offer a potential\nsolution; however, research often focuses on only one or two common mutations,\nlimiting the clinical value of these tools and the pool of patients who can\nbenefit from them. This study evaluates various Multiple Instance Learning\n(MIL) techniques to detect six key actionable NSCLC driver mutations: ALK,\nBRAF, EGFR, ERBB2, KRAS, and MET ex14. Additionally, we introduce an Asymmetric\nTransformer Decoder model that employs queries and key-values of varying\ndimensions to maintain a low query dimensionality. This approach efficiently\nextracts information from patch embeddings and minimizes overfitting risks,\nproving highly adaptable to the MIL setting. Moreover, we present a method to\ndirectly utilize tissue type in the model, addressing a typical MIL limitation\nwhere either all regions or only some specific regions are analyzed, neglecting\nbiological relevance. Our method outperforms top MIL models by an average of\n3%, and over 4% when predicting rare mutations such as ERBB2 and BRAF, moving\nML-based tests closer to being practical alternatives to standard genetic\ntesting.",
        "url": "http://arxiv.org/abs/2508.02431v1",
        "published_date": "2025-08-04T13:50:00+00:00",
        "updated_date": "2025-08-04T13:50:00+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Biagio Brattoli",
            "Jack Shi",
            "Jongchan Park",
            "Taebum Lee",
            "Donggeun Yoo",
            "Sergio Pereira"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper presents an efficient model for identifying actionable driver mutations in lung cancer using multiple instance learning techniques and an Asymmetric Transformer Decoder.",
        "tldr_zh": "该论文提出了一种高效的模型，利用多实例学习技术和不对称变压器解码器来识别肺癌中的可操作驱动突变。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Do Edges Matter? Investigating Edge-Enhanced Pre-Training for Medical Image Segmentation",
        "summary": "Medical image segmentation is crucial for disease diagnosis and treatment\nplanning, yet developing robust segmentation models often requires substantial\ncomputational resources and large datasets. Existing research shows that\npre-trained and finetuned foundation models can boost segmentation performance.\nHowever, questions remain about how particular image preprocessing steps may\ninfluence segmentation performance across different medical imaging modalities.\nIn particular, edges-abrupt transitions in pixel intensity-are widely\nacknowledged as vital cues for object boundaries but have not been\nsystematically examined in the pre-training of foundation models. We address\nthis gap by investigating to which extend pre-training with data processed\nusing computationally efficient edge kernels, such as kirsch, can improve\ncross-modality segmentation capabilities of a foundation model. Two versions of\na foundation model are first trained on either raw or edge-enhanced data across\nmultiple medical imaging modalities, then finetuned on selected raw subsets\ntailored to specific medical modalities. After systematic investigation using\nthe medical domains Dermoscopy, Fundus, Mammography, Microscopy, OCT, US, and\nXRay, we discover both increased and reduced segmentation performance across\nmodalities using edge-focused pre-training, indicating the need for a selective\napplication of this approach. To guide such selective applications, we propose\na meta-learning strategy. It uses standard deviation and image entropy of the\nraw image to choose between a model pre-trained on edge-enhanced or on raw data\nfor optimal performance. Our experiments show that integrating this\nmeta-learning layer yields an overall segmentation performance improvement\nacross diverse medical imaging tasks by 16.42% compared to models pre-trained\non edge-enhanced data only and 19.30% compared to models pre-trained on raw\ndata only.",
        "url": "http://arxiv.org/abs/2508.02281v1",
        "published_date": "2025-08-04T10:52:42+00:00",
        "updated_date": "2025-08-04T10:52:42+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "H.0"
        ],
        "authors": [
            "Paul Zaha",
            "Lars Böcking",
            "Simeon Allmendinger",
            "Leopold Müller",
            "Niklas Kühl"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer",
            "AIGC"
        ],
        "tldr": "The paper investigates the impact of using edge-enhanced pre-training on medical image segmentation, showing both increased and reduced performance across modalities. A meta-learning strategy is proposed to guide the selective application of this approach, leading to an overall segmentation performance improvement.",
        "tldr_zh": "本文研究了在医学图像分割中使用边缘增强预训练的影响，显示在模态之间既有增加也有降低的性能。提出了一种元学习策略，指导选择性应用该方法，从而提高整体分割性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Semi-Supervised Dual-Threshold Contrastive Learning for Ultrasound Image Classification and Segmentation",
        "summary": "Confidence-based pseudo-label selection usually generates overly confident\nyet incorrect predictions, due to the early misleadingness of model and\noverfitting inaccurate pseudo-labels in the learning process, which heavily\ndegrades the performance of semi-supervised contrastive learning. Moreover,\nsegmentation and classification tasks are treated independently and the\naffinity fails to be fully explored. To address these issues, we propose a\nnovel semi-supervised dual-threshold contrastive learning strategy for\nultrasound image classification and segmentation, named Hermes. This strategy\ncombines the strengths of contrastive learning with semi-supervised learning,\nwhere the pseudo-labels assist contrastive learning by providing additional\nguidance. Specifically, an inter-task attention and saliency module is also\ndeveloped to facilitate information sharing between the segmentation and\nclassification tasks. Furthermore, an inter-task consistency learning strategy\nis designed to align tumor features across both tasks, avoiding negative\ntransfer for reducing features discrepancy. To solve the lack of publicly\navailable ultrasound datasets, we have collected the SZ-TUS dataset, a thyroid\nultrasound image dataset. Extensive experiments on two public ultrasound\ndatasets and one private dataset demonstrate that Hermes consistently\noutperforms several state-of-the-art methods across various semi-supervised\nsettings.",
        "url": "http://arxiv.org/abs/2508.02265v1",
        "published_date": "2025-08-04T10:15:53+00:00",
        "updated_date": "2025-08-04T10:15:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Peng Zhang",
            "Zhihui Lai",
            "Heng Kong"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel semi-supervised dual-threshold contrastive learning strategy named Hermes for ultrasound image classification and segmentation, which outperforms state-of-the-art methods across various settings.",
        "tldr_zh": "本文提出了一种名为Hermes的新型半监督双阈值对比学习策略，用于超声图像分类和分割，在各种设置下均胜过现有方法。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Failure Cases Are Better Learned But Boundary Says Sorry: Facilitating Smooth Perception Change for Accuracy-Robustness Trade-Off in Adversarial Training",
        "summary": "Adversarial Training (AT) is one of the most effective methods to train\nrobust Deep Neural Networks (DNNs). However, AT creates an inherent trade-off\nbetween clean accuracy and adversarial robustness, which is commonly attributed\nto the more complicated decision boundary caused by the insufficient learning\nof hard adversarial samples. In this work, we reveal a counterintuitive fact\nfor the first time: From the perspective of perception consistency, hard\nadversarial samples that can still attack the robust model after AT are already\nlearned better than those successfully defended. Thus, different from previous\nviews, we argue that it is rather the over-sufficient learning of hard\nadversarial samples that degrades the decision boundary and contributes to the\ntrade-off problem. Specifically, the excessive pursuit of perception\nconsistency would force the model to view the perturbations as noise and ignore\nthe information within them, which should have been utilized to induce a\nsmoother perception transition towards the decision boundary to support its\nestablishment to an appropriate location. In response, we define a new AT\nobjective named Robust Perception, encouraging the model perception to change\nsmoothly with input perturbations, based on which we propose a novel Robust\nPerception Adversarial Training (RPAT) method, effectively mitigating the\ncurrent accuracy-robustness trade-off. Experiments on CIFAR-10, CIFAR-100, and\nTiny-ImageNet with ResNet-18, PreActResNet-18, and WideResNet-34-10 demonstrate\nthe effectiveness of our method beyond four common baselines and 12\nstate-of-the-art (SOTA) works. The code is available at\nhttps://github.com/FlaAI/RPAT.",
        "url": "http://arxiv.org/abs/2508.02186v1",
        "published_date": "2025-08-04T08:31:45+00:00",
        "updated_date": "2025-08-04T08:31:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanyun Wang",
            "Li Liu"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a new method named Robust Perception Adversarial Training (RPAT) to improve the trade-off between clean accuracy and adversarial robustness in Deep Neural Networks by encouraging smoother perception transitions.",
        "tldr_zh": "本文提出了一种名为Robust Perception Adversarial Training (RPAT)的新方法，通过促进更平滑的知觉转变来改善深度神经网络的干净准确性和对抗性鲁棒性之间的权衡。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Unified Category-Level Object Detection and Pose Estimation from RGB Images using 3D Prototypes",
        "summary": "Recognizing objects in images is a fundamental problem in computer vision.\nAlthough detecting objects in 2D images is common, many applications require\ndetermining their pose in 3D space. Traditional category-level methods rely on\nRGB-D inputs, which may not always be available, or employ two-stage approaches\nthat use separate models and representations for detection and pose estimation.\nFor the first time, we introduce a unified model that integrates detection and\npose estimation into a single framework for RGB images by leveraging neural\nmesh models with learned features and multi-model RANSAC. Our approach achieves\nstate-of-the-art results for RGB category-level pose estimation on REAL275,\nimproving on the current state-of-the-art by 22.9% averaged across all\nscale-agnostic metrics. Finally, we demonstrate that our unified method\nexhibits greater robustness compared to single-stage baselines. Our code and\nmodels are available at\nhttps://github.com/Fischer-Tom/unified-detection-and-pose-estimation.",
        "url": "http://arxiv.org/abs/2508.02157v1",
        "published_date": "2025-08-04T07:57:39+00:00",
        "updated_date": "2025-08-04T07:57:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tom Fischer",
            "Xiaojie Zhang",
            "Eddy Ilg"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a unified model for object detection and pose estimation in 3D space from RGB images, achieving state-of-the-art results and greater robustness compared to existing methods.",
        "tldr_zh": "本文介绍了一种从RGB图像中检测物体并估计其在3D空间中姿态的统一模型，取得了当前最先进的结果，并展示了与现有方法相比更大的鲁棒性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "AURORA: Augmented Understanding via Structured Reasoning and Reinforcement Learning for Reference Audio-Visual Segmentation",
        "summary": "Reference Audio-Visual Segmentation (Ref-AVS) tasks challenge models to\nprecisely locate sounding objects by integrating visual, auditory, and textual\ncues. Existing methods often lack genuine semantic understanding, tending to\nmemorize fixed reasoning patterns. Furthermore, jointly training for reasoning\nand segmentation can compromise pixel-level precision. To address these issues,\nwe introduce AURORA, a novel framework designed to enhance genuine reasoning\nand language comprehension in reference audio-visual segmentation. We employ a\nstructured Chain-of-Thought (CoT) prompting mechanism to guide the model\nthrough a step-by-step reasoning process and introduce a novel segmentation\nfeature distillation loss to effectively integrate these reasoning abilities\nwithout sacrificing segmentation performance. To further cultivate the model's\ngenuine reasoning capabilities, we devise a further two-stage training\nstrategy: first, a ``corrective reflective-style training\" stage utilizes\nself-correction to enhance the quality of reasoning paths, followed by\nreinforcement learning via Group Reward Policy Optimization (GRPO) to bolster\nrobustness in challenging scenarios. Experiments demonstrate that AURORA\nachieves state-of-the-art performance on Ref-AVS benchmarks and generalizes\neffectively to unreferenced segmentation.",
        "url": "http://arxiv.org/abs/2508.02149v1",
        "published_date": "2025-08-04T07:47:38+00:00",
        "updated_date": "2025-08-04T07:47:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziyang Luo",
            "Nian Liu",
            "Fahad Shahbaz Khan",
            "Junwei Han"
        ],
        "ai_categories": [
            "AIGC",
            "GAN"
        ],
        "tldr": "The paper introduces AURORA, a framework for reference audio-visual segmentation that enhances genuine reasoning and language comprehension. It achieves state-of-the-art performance on benchmarks and generalizes effectively to unreferenced segmentation.",
        "tldr_zh": "该论文介绍了AURORA，这是一个用于参考音频-视觉分割的框架，可以增强真实的推理和语言理解。它在基准测试中取得了最先进的性能，并且可以有效地推广到非参考分割。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "ScrewSplat: An End-to-End Method for Articulated Object Recognition",
        "summary": "Articulated object recognition -- the task of identifying both the geometry\nand kinematic joints of objects with movable parts -- is essential for enabling\nrobots to interact with everyday objects such as doors and laptops. However,\nexisting approaches often rely on strong assumptions, such as a known number of\narticulated parts; require additional inputs, such as depth images; or involve\ncomplex intermediate steps that can introduce potential errors -- limiting\ntheir practicality in real-world settings. In this paper, we introduce\nScrewSplat, a simple end-to-end method that operates solely on RGB\nobservations. Our approach begins by randomly initializing screw axes, which\nare then iteratively optimized to recover the object's underlying kinematic\nstructure. By integrating with Gaussian Splatting, we simultaneously\nreconstruct the 3D geometry and segment the object into rigid, movable parts.\nWe demonstrate that our method achieves state-of-the-art recognition accuracy\nacross a diverse set of articulated objects, and further enables zero-shot,\ntext-guided manipulation using the recovered kinematic model.",
        "url": "http://arxiv.org/abs/2508.02146v1",
        "published_date": "2025-08-04T07:45:31+00:00",
        "updated_date": "2025-08-04T07:45:31+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Seungyeon Kim",
            "Junsu Ha",
            "Young Hun Kim",
            "Yonghyeon Lee",
            "Frank C. Park"
        ],
        "ai_categories": [
            "AIGC"
        ],
        "tldr": "The paper introduces ScrewSplat, an end-to-end method for articulated object recognition using only RGB observations, achieving state-of-the-art accuracy and enabling text-guided manipulation.",
        "tldr_zh": "本文介绍了ScrewSplat，一种利用RGB观测进行关节对象识别的端到端方法，实现了最先进的准确性，并实现了文本引导操作。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "TrackletGait: A Robust Framework for Gait Recognition in the Wild",
        "summary": "Gait recognition aims to identify individuals based on their body shape and\nwalking patterns. Though much progress has been achieved driven by deep\nlearning, gait recognition in real-world surveillance scenarios remains quite\nchallenging to current methods. Conventional approaches, which rely on periodic\ngait cycles and controlled environments, struggle with the non-periodic and\noccluded silhouette sequences encountered in the wild. In this paper, we\npropose a novel framework, TrackletGait, designed to address these challenges\nin the wild. We propose Random Tracklet Sampling, a generalization of existing\nsampling methods, which strikes a balance between robustness and representation\nin capturing diverse walking patterns. Next, we introduce Haar Wavelet-based\nDownsampling to preserve information during spatial downsampling. Finally, we\npresent a Hardness Exclusion Triplet Loss, designed to exclude low-quality\nsilhouettes by discarding hard triplet samples. TrackletGait achieves\nstate-of-the-art results, with 77.8 and 80.4 rank-1 accuracy on the Gait3D and\nGREW datasets, respectively, while using only 10.3M backbone parameters.\nExtensive experiments are also conducted to further investigate the factors\naffecting gait recognition in the wild.",
        "url": "http://arxiv.org/abs/2508.02143v1",
        "published_date": "2025-08-04T07:43:04+00:00",
        "updated_date": "2025-08-04T07:43:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shaoxiong Zhang",
            "Jinkai Zheng",
            "Shangdong Zhu",
            "Chenggang Yan"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "TrackletGait is a framework for gait recognition in real-world scenarios, achieving state-of-the-art results with innovative sampling and loss methods.",
        "tldr_zh": "TrackletGait是一个用于在真实场景下进行步态识别的框架，通过创新的采样和损失方法取得了最先进的结果。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "A Neural Quality Metric for BRDF Models",
        "summary": "Accurately evaluating the quality of bidirectional reflectance distribution\nfunction (BRDF) models is essential for photo-realistic rendering. Traditional\nBRDF-space metrics often employ numerical error measures that fail to capture\nperceptual differences evident in rendered images. In this paper, we introduce\nthe first perceptually informed neural quality metric for BRDF evaluation that\noperates directly in BRDF space, eliminating the need for rendering during\nquality assessment. Our metric is implemented as a compact multi-layer\nperceptron (MLP), trained on a dataset of measured BRDFs supplemented with\nsynthetically generated data and labelled using a perceptually validated\nimage-space metric. The network takes as input paired samples of reference and\napproximated BRDFs and predicts their perceptual quality in terms of\njust-objectionable-difference (JOD) scores. We show that our neural metric\nachieves significantly higher correlation with human judgments than existing\nBRDF-space metrics. While its performance as a loss function for BRDF fitting\nremains limited, the proposed metric offers a perceptually grounded alternative\nfor evaluating BRDF models.",
        "url": "http://arxiv.org/abs/2508.02131v1",
        "published_date": "2025-08-04T07:24:46+00:00",
        "updated_date": "2025-08-04T07:24:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Behnaz Kavoosighafi",
            "Rafal K. Mantiuk",
            "Saghi Hajisharif",
            "Ehsan Miandji",
            "Jonas Unger"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a neural quality metric for evaluating BRDF models directly in BRDF space, without the need for rendering, showing higher correlation with human judgments than existing metrics.",
        "tldr_zh": "本文介绍了一种神经质量度量，用于直接在BRDF空间评估模型，无需渲染，显示比现有指标更高的与人类判断的相关性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 7.25
    },
    {
        "title": "S-RRG-Bench: Structured Radiology Report Generation with Fine-Grained Evaluation Framework",
        "summary": "Radiology report generation (RRG) for diagnostic images, such as chest\nX-rays, plays a pivotal role in both clinical practice and AI. Traditional\nfree-text reports suffer from redundancy and inconsistent language,\ncomplicating the extraction of critical clinical details. Structured radiology\nreport generation (S-RRG) offers a promising solution by organizing information\ninto standardized, concise formats. However, existing approaches often rely on\nclassification or visual question answering (VQA) pipelines that require\npredefined label sets and produce only fragmented outputs. Template-based\napproaches, which generate reports by replacing keywords within fixed sentence\npatterns, further compromise expressiveness and often omit clinically important\ndetails. In this work, we present a novel approach to S-RRG that includes\ndataset construction, model training, and the introduction of a new evaluation\nframework. We first create a robust chest X-ray dataset (MIMIC-STRUC) that\nincludes disease names, severity levels, probabilities, and anatomical\nlocations, ensuring that the dataset is both clinically relevant and\nwell-structured. We train an LLM-based model to generate standardized,\nhigh-quality reports. To assess the generated reports, we propose a specialized\nevaluation metric (S-Score) that not only measures disease prediction accuracy\nbut also evaluates the precision of disease-specific details, thus offering a\nclinically meaningful metric for report quality that focuses on elements\ncritical to clinical decision-making and demonstrates a stronger alignment with\nhuman assessments. Our approach highlights the effectiveness of structured\nreports and the importance of a tailored evaluation metric for S-RRG, providing\na more clinically relevant measure of report quality.",
        "url": "http://arxiv.org/abs/2508.02082v1",
        "published_date": "2025-08-04T05:49:41+00:00",
        "updated_date": "2025-08-04T05:49:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yingshu Li",
            "Yunyi Liu",
            "Zhanyu Wang",
            "Xinyu Liang",
            "Lingqiao Liu",
            "Lei Wang",
            "Luping Zhou"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset",
            "LoRA"
        ],
        "tldr": "The paper introduces a structured approach to generating radiology reports with a new evaluation framework, focusing on accuracy and clinical relevance.",
        "tldr_zh": "该论文引入了一种结构化的方法，生成放射学报告，并提出了一个新的评估框架，侧重于准确性和临床相关性。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "CVD-SfM: A Cross-View Deep Front-end Structure-from-Motion System for Sparse Localization in Multi-Altitude Scenes",
        "summary": "We present a novel multi-altitude camera pose estimation system, addressing\nthe challenges of robust and accurate localization across varied altitudes when\nonly considering sparse image input. The system effectively handles diverse\nenvironmental conditions and viewpoint variations by integrating the cross-view\ntransformer, deep features, and structure-from-motion into a unified framework.\nTo benchmark our method and foster further research, we introduce two newly\ncollected datasets specifically tailored for multi-altitude camera pose\nestimation; datasets of this nature remain rare in the current literature. The\nproposed framework has been validated through extensive comparative analyses on\nthese datasets, demonstrating that our system achieves superior performance in\nboth accuracy and robustness for multi-altitude sparse pose estimation tasks\ncompared to existing solutions, making it well suited for real-world robotic\napplications such as aerial navigation, search and rescue, and automated\ninspection.",
        "url": "http://arxiv.org/abs/2508.01936v1",
        "published_date": "2025-08-03T22:11:48+00:00",
        "updated_date": "2025-08-03T22:11:48+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Yaxuan Li",
            "Yewei Huang",
            "Bijay Gaudel",
            "Hamidreza Jafarnejadsani",
            "Brendan Englot"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a system for accurate localization in multi-altitude scenes using sparse image input, achieving superior performance compared to existing solutions.",
        "tldr_zh": "本文介绍了一种利用稀疏图像输入实现多高度场景准确定位的系统，相较于现有解决方案，其性能更优。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Understanding the Risks of Asphalt Art on the Reliability of Surveillance Perception Systems",
        "summary": "Artistic crosswalks featuring asphalt art, introduced by different\norganizations in recent years, aim to enhance the visibility and safety of\npedestrians. However, their visual complexity may interfere with surveillance\nsystems that rely on vision-based object detection models. In this study, we\ninvestigate the impact of asphalt art on pedestrian detection performance of a\npretrained vision-based object detection model. We construct realistic\ncrosswalk scenarios by compositing various street art patterns into a fixed\nsurveillance scene and evaluate the model's performance in detecting\npedestrians on asphalt-arted crosswalks under both benign and adversarial\nconditions. A benign case refers to pedestrian crosswalks painted with existing\nnormal asphalt art, whereas an adversarial case involves digitally crafted or\naltered asphalt art perpetrated by an attacker. Our results show that while\nsimple, color-based designs have minimal effect, complex artistic patterns,\nparticularly those with high visual salience, can significantly degrade\npedestrian detection performance. Furthermore, we demonstrate that\nadversarially crafted asphalt art can be exploited to deliberately obscure real\npedestrians or generate non-existent pedestrian detections. These findings\nhighlight a potential vulnerability in urban vision-based pedestrian\nsurveillance systems and underscore the importance of accounting for\nenvironmental visual variations when designing robust pedestrian perception\nmodels.",
        "url": "http://arxiv.org/abs/2508.02530v1",
        "published_date": "2025-08-04T15:40:03+00:00",
        "updated_date": "2025-08-04T15:40:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jin Ma",
            "Abyad Enan",
            "Long Cheng",
            "Mashrur Chowdhury"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "The paper investigates how artistic crosswalks can impact the performance of surveillance systems in detecting pedestrians, highlighting potential vulnerabilities and the importance of considering environmental visual variations in pedestrian perception models.",
        "tldr_zh": "本文研究了艺术过街的如何影响监控系统在检测行人时的表现，突出了潜在的脆弱性，并强调了在行人感知模型中考虑环境视觉变化的重要性。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "TRUDI and TITUS: A Multi-Perspective Dataset and A Three-Stage Recognition System for Transportation Unit Identification",
        "summary": "Identifying transportation units (TUs) is essential for improving the\nefficiency of port logistics. However, progress in this field has been hindered\nby the lack of publicly available benchmark datasets that capture the diversity\nand dynamics of real-world port environments. To address this gap, we present\nthe TRUDI dataset-a comprehensive collection comprising 35,034 annotated\ninstances across five categories: container, tank container, trailer, ID text,\nand logo. The images were captured at operational ports using both ground-based\nand aerial cameras, under a wide variety of lighting and weather conditions.\nFor the identification of TUs-which involves reading the 11-digit alphanumeric\nID typically painted on each unit-we introduce TITUS, a dedicated pipeline that\noperates in three stages: (1) segmenting the TU instances, (2) detecting the\nlocation of the ID text, and (3) recognising and validating the extracted ID.\nUnlike alternative systems, which often require similar scenes, specific camera\nangles or gate setups, our evaluation demonstrates that TITUS reliably\nidentifies TUs from a range of camera perspectives and in varying lighting and\nweather conditions. By making the TRUDI dataset publicly available, we provide\na robust benchmark that enables the development and comparison of new\napproaches. This contribution supports digital transformation efforts in\nmultipurpose ports and helps to increase the efficiency of entire logistics\nchains.",
        "url": "http://arxiv.org/abs/2508.02372v1",
        "published_date": "2025-08-04T13:02:04+00:00",
        "updated_date": "2025-08-04T13:02:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Emre Gülsoylu",
            "André Kelm",
            "Lennart Bengtson",
            "Matthias Hirsch",
            "Christian Wilms",
            "Tim Rolff",
            "Janick Edinger",
            "Simone Frintrop"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper introduces TRUDI dataset and TITUS recognition system for identifying transportation units in port logistics, aiming to improve efficiency.",
        "tldr_zh": "该论文介绍了TRUDI数据集和TITUS识别系统，用于在港口物流中识别运输单位，旨在提高效率。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Toward a reliable PWM-based light-emitting diode visual stimulus for improved SSVEP response with minimal visual fatigue",
        "summary": "Steady state visual evoked response (SSVEP) is widely used in visual-based\ndiagnosis and applications such as brain computer interfacing due to its high\ninformation transfer rate and the capability to activate commands through\nsimple gaze control. However, one major impediment in using flashing visual\nstimulus to obtain SSVEP is eye fatigue that prevents continued long term use\npreventing practical deployment. This combined with the difficulty in\nestablishing precise pulse-width modulation (PWM) that results in poorer\naccuracy warrants the development of appropriate approach to solve these\nissues. Various studies have suggested the usage of high frequencies of visual\nstimulus to reduce the visual fatigue for the user but this results in poor\nresponse performance. Here, the authors study the use of extremely high\nduty-cycles in the stimulus in the hope of solving these constraints.\nElectroencephalogram data was recorded with PWM duty-cycles of 50 to 95%\ngenerated by a precise custom-made light-emitting diode hardware and tested ten\nsubjects responded that increasing duty-cycles had less visual strain for all\nthe frequency values and the SSVEP exhibited a subject-independent peak\nresponse for duty-cycle of 85%. This could pave the way for increased usage of\nSSVEP for practical applications.",
        "url": "http://arxiv.org/abs/2508.02359v1",
        "published_date": "2025-08-04T12:48:59+00:00",
        "updated_date": "2025-08-04T12:48:59+00:00",
        "categories": [
            "eess.SP",
            "cs.CV",
            "cs.SE"
        ],
        "authors": [
            "Surej Mouli",
            "Ramaswamy Palaniappan"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper explores using high duty-cycles in light stimuli to improve SSVEP response and reduce visual fatigue, showing promising results for practical applications.",
        "tldr_zh": "本文探讨了使用高占空比的光刺激来改善SSVEP反应并减少视觉疲劳，显示出对实际应用具有前景的结果。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Correspondence-Free Fast and Robust Spherical Point Pattern Registration",
        "summary": "Existing methods for rotation estimation between two spherical\n($\\mathbb{S}^2$) patterns typically rely on spherical cross-correlation\nmaximization between two spherical function. However, these approaches exhibit\ncomputational complexities greater than cubic $O(n^3)$ with respect to rotation\nspace discretization and lack extensive evaluation under significant outlier\ncontamination. To this end, we propose a rotation estimation algorithm between\ntwo spherical patterns with linear time complexity $O(n)$. Unlike existing\nspherical-function-based methods, we explicitly represent spherical patterns as\ndiscrete 3D point sets on the unit sphere, reformulating rotation estimation as\na spherical point-set alignment (i.e., Wahba problem for 3D unit vectors).\nGiven the geometric nature of our formulation, our spherical pattern alignment\nalgorithm naturally aligns with the Wahba problem framework for 3D unit\nvectors. Specifically, we introduce three novel algorithms: (1) SPMC (Spherical\nPattern Matching by Correlation), (2) FRS (Fast Rotation Search), and (3) a\nhybrid approach (SPMC+FRS) that combines the advantages of the previous two\nmethods. Our experiments demonstrate that in the $\\mathbb{S}^2$ domain and in\ncorrespondence-free settings, our algorithms are over 10x faster and over 10x\nmore accurate than current state-of-the-art methods for the Wahba problem with\noutliers. We validate our approach through extensive simulations on a new\ndataset of spherical patterns, the ``Robust Vector Alignment Dataset.\n\"Furthermore, we adapt our methods to two real-world tasks: (i) Point Cloud\nRegistration (PCR) and (ii) rotation estimation for spherical images.",
        "url": "http://arxiv.org/abs/2508.02339v1",
        "published_date": "2025-08-04T12:21:05+00:00",
        "updated_date": "2025-08-04T12:21:05+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Anik Sarker",
            "Alan T. Asbeck"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper proposes a fast and robust algorithm for aligning spherical patterns using 3D point sets, demonstrating significant speed and accuracy improvements over existing methods.",
        "tldr_zh": "本文提出了一种快速而鲁棒的算法，用于对齐球面模式，通过使用3D点集，展示了相对于现有方法的显着速度和准确性改进。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SGAD: Semantic and Geometric-aware Descriptor for Local Feature Matching",
        "summary": "Local feature matching remains a fundamental challenge in computer vision.\nRecent Area to Point Matching (A2PM) methods have improved matching accuracy.\nHowever, existing research based on this framework relies on inefficient\npixel-level comparisons and complex graph matching that limit scalability. In\nthis work, we introduce the Semantic and Geometric-aware Descriptor Network\n(SGAD), which fundamentally rethinks area-based matching by generating highly\ndiscriminative area descriptors that enable direct matching without complex\ngraph optimization. This approach significantly improves both accuracy and\nefficiency of area matching. We further improve the performance of area\nmatching through a novel supervision strategy that decomposes the area matching\ntask into classification and ranking subtasks. Finally, we introduce the\nHierarchical Containment Redundancy Filter (HCRF) to eliminate overlapping\nareas by analyzing containment graphs. SGAD demonstrates remarkable performance\ngains, reducing runtime by 60x (0.82s vs. 60.23s) compared to MESA. Extensive\nevaluations show consistent improvements across multiple point matchers:\nSGAD+LoFTR reduces runtime compared to DKM, while achieving higher accuracy\n(0.82s vs. 1.51s, 65.98 vs. 61.11) in outdoor pose estimation, and SGAD+ROMA\ndelivers +7.39% AUC@5{\\deg} in indoor pose estimation, establishing a new\nstate-of-the-art.",
        "url": "http://arxiv.org/abs/2508.02278v1",
        "published_date": "2025-08-04T10:46:53+00:00",
        "updated_date": "2025-08-04T10:46:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiangzeng Liu",
            "Chi Wang",
            "Guanglu Shi",
            "Xiaodong Zhang",
            "Qiguang Miao",
            "Miao Fan"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "SGAD introduces a new approach for local feature matching in computer vision, improving accuracy and efficiency through area descriptors and novel supervision strategies.",
        "tldr_zh": "SGAD 引入了一种新的方法，通过区域描述符和新的监督策略改进了计算机视觉中的局部特征匹配。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Semi-Supervised Semantic Segmentation via Derivative Label Propagation",
        "summary": "Semi-supervised semantic segmentation, which leverages a limited set of\nlabeled images, helps to relieve the heavy annotation burden. While\npseudo-labeling strategies yield promising results, there is still room for\nenhancing the reliability of pseudo-labels. Hence, we develop a semi-supervised\nframework, namely DerProp, equipped with a novel derivative label propagation\nto rectify imperfect pseudo-labels. Our label propagation method imposes\ndiscrete derivative operations on pixel-wise feature vectors as additional\nregularization, thereby generating strictly regularized similarity metrics.\nDoing so effectively alleviates the ill-posed problem that identical\nsimilarities correspond to different features, through constraining the\nsolution space. Extensive experiments are conducted to verify the rationality\nof our design, and demonstrate our superiority over other methods. Codes are\navailable at https://github.com/ForawardStar/DerProp/.",
        "url": "http://arxiv.org/abs/2508.02254v1",
        "published_date": "2025-08-04T10:01:12+00:00",
        "updated_date": "2025-08-04T10:01:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuanbin Fu",
            "Xiaojie Guo"
        ],
        "ai_categories": [
            "LoRA",
            "Dataset"
        ],
        "tldr": "The paper introduces a semi-supervised framework called DerProp for semantic segmentation, improving pseudo-label reliability through derivative label propagation.",
        "tldr_zh": "本文介绍了一种名为DerProp的半监督框架，通过导数标签传播改进伪标签的可靠性，用于语义分割。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    },
    {
        "title": "A Moment Matching-Based Method for Sparse and Noisy Point Cloud Registration",
        "summary": "Point cloud registration is a key step in robotic perception tasks, such as\nSimultaneous Localization and Mapping (SLAM). It is especially challenging in\nconditions with sparse points and heavy noise. Traditional registration\nmethods, such as Iterative Closest Point (ICP) and Normal Distributions\nTransform (NDT), often have difficulties in achieving a robust and accurate\nalignment under these conditions. In this paper, we propose a registration\nframework based on moment matching. In particular, the point clouds are\nregarded as i.i.d. samples drawn from the same distribution observed in the\nsource and target frames. We then match the generalized Gaussian Radial Basis\nmoments calculated from the point clouds to estimate the rigid transformation\nbetween two frames. Moreover, such method does not require explicit\npoint-to-point correspondences among the point clouds. We further show the\nconsistency of the proposed method. Experiments on synthetic and real-world\ndatasets show that our approach achieves higher accuracy and robustness than\nexisting methods. In addition, we integrate our framework into a 4D Radar SLAM\nsystem. The proposed method significantly improves the localization performance\nand achieves results comparable to LiDAR-based systems. These findings\ndemonstrate the potential of moment matching technique for robust point cloud\nregistration in sparse and noisy scenarios.",
        "url": "http://arxiv.org/abs/2508.02187v1",
        "published_date": "2025-08-04T08:31:53+00:00",
        "updated_date": "2025-08-04T08:31:53+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Xingyi Li",
            "Han Zhang",
            "Ziliang Wang",
            "Yukai Yang",
            "Weidong Chen"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper proposes a moment matching-based method for robust point cloud registration in sparse and noisy scenarios, showing better accuracy and robustness compared to existing methods.",
        "tldr_zh": "本文提出了一种基于矩匹配的方法，用于在稀疏且嘈杂的情况下实现点云配准，显示出比现有方法更好的准确性和鲁棒性。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Deep classification algorithm for De-identification of DICOM medical images",
        "summary": "Background : De-identification of DICOM (Digital Imaging and Communi-cations\nin Medicine) files is an essential component of medical image research.\nPersonal Identifiable Information (PII) and/or Personal Health Identifying\nInformation (PHI) need to be hidden or removed due to legal reasons. According\nto the Health Insurance Portability and Accountability Act (HIPAA) and privacy\nrules, also full-face photographic images and any compa-rable images are direct\nidentifiers and are considered protected health information that also need to\nbe de-identified. Objective : The study aimed to implement a method that permit\nto de-identify the PII and PHI information present in the header and burned on\nthe pixel data of DICOM. Methods : To execute the de-identification, we\nimplemented an algorithm based on the safe harbor method, defined by HIPAA. Our\nalgorithm uses input customizable parameter to classify and then possibly\nde-identify individual DICOM tags. Results : The most sensible information,\nlike names, history, personal data and institution were successfully\nrecognized. Conclusions : We developed a python algorithm that is able to\nclassify infor-mation present in a DICOM file. The flexibility provided by the\nuse of customi-zable input parameters, which allow the user to customize the\nentire process de-pending on the case (e.g., the language), makes the entire\nprogram very promis-ing for both everyday use and research purposes. Our code\nis available at https://github.com/rtdicomexplorer/deep_deidentification.",
        "url": "http://arxiv.org/abs/2508.02177v1",
        "published_date": "2025-08-04T08:21:18+00:00",
        "updated_date": "2025-08-04T08:21:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bufano Michele",
            "Kotter Elmar"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper presents a deep classification algorithm for de-identification of DICOM medical images, successfully recognizing sensitive information like names, history, personal data, and institution.",
        "tldr_zh": "该论文提出了一种用于DICOM医学图像去识别的深度分类算法，成功识别出姓名、历史、个人数据和机构等敏感信息。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Deeply Dual Supervised learning for melanoma recognition",
        "summary": "As the application of deep learning in dermatology continues to grow, the\nrecognition of melanoma has garnered significant attention, demonstrating\npotential for improving diagnostic accuracy. Despite advancements in image\nclassification techniques, existing models still face challenges in identifying\nsubtle visual cues that differentiate melanoma from benign lesions. This paper\npresents a novel Deeply Dual Supervised Learning framework that integrates\nlocal and global feature extraction to enhance melanoma recognition. By\nemploying a dual-pathway structure, the model focuses on both fine-grained\nlocal features and broader contextual information, ensuring a comprehensive\nunderstanding of the image content. The framework utilizes a dual attention\nmechanism that dynamically emphasizes critical features, thereby reducing the\nrisk of overlooking subtle characteristics of melanoma. Additionally, we\nintroduce a multi-scale feature aggregation strategy to ensure robust\nperformance across varying image resolutions. Extensive experiments on\nbenchmark datasets demonstrate that our framework significantly outperforms\nstate-of-the-art methods in melanoma detection, achieving higher accuracy and\nbetter resilience against false positives. This work lays the foundation for\nfuture research in automated skin cancer recognition and highlights the\neffectiveness of dual supervised learning in medical image analysis.",
        "url": "http://arxiv.org/abs/2508.01994v1",
        "published_date": "2025-08-04T02:22:26+00:00",
        "updated_date": "2025-08-04T02:22:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rujosh Polma",
            "Krishnan Menon Iyer"
        ],
        "ai_categories": [
            "AIGC",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a Deeply Dual Supervised Learning framework for melanoma recognition, outperforming existing methods and laying a foundation for automated skin cancer recognition.",
        "tldr_zh": "该论文引入了一种深度双监督学习框架用于黑色素瘤识别，优于现有方法，并为自动皮肤癌识别奠定了基础。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.0
    },
    {
        "title": "On-the-Fly Object-aware Representative Point Selection in Point Cloud",
        "summary": "Point clouds are essential for object modeling and play a critical role in\nassisting driving tasks for autonomous vehicles (AVs). However, the significant\nvolume of data generated by AVs creates challenges for storage, bandwidth, and\nprocessing cost. To tackle these challenges, we propose a representative point\nselection framework for point cloud downsampling, which preserves critical\nobject-related information while effectively filtering out irrelevant\nbackground points. Our method involves two steps: (1) Object Presence\nDetection, where we introduce an unsupervised density peak-based classifier and\na supervised Na\\\"ive Bayes classifier to handle diverse scenarios, and (2)\nSampling Budget Allocation, where we propose a strategy that selects\nobject-relevant points while maintaining a high retention rate of object\ninformation. Extensive experiments on the KITTI and nuScenes datasets\ndemonstrate that our method consistently outperforms state-of-the-art baselines\nin both efficiency and effectiveness across varying sampling rates. As a\nmodel-agnostic solution, our approach integrates seamlessly with diverse\ndownstream models, making it a valuable and scalable addition to the 3D point\ncloud downsampling toolkit for AV applications.",
        "url": "http://arxiv.org/abs/2508.01980v1",
        "published_date": "2025-08-04T01:39:09+00:00",
        "updated_date": "2025-08-04T01:39:09+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Xiaoyu Zhang",
            "Ziwei Wang",
            "Hai Dong",
            "Zhifeng Bao",
            "Jiajun Liu"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes a representative point selection framework for downsampling point clouds in autonomous vehicles, which outperforms existing methods in efficiency and effectiveness.",
        "tldr_zh": "本文提出了一种代表性点选择框架，用于在自动驾驶汽车中对点云进行降采样，在效率和有效性方面优于现有方法。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video Understanding",
        "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.",
        "url": "http://arxiv.org/abs/2508.01875v1",
        "published_date": "2025-08-03T18:15:42+00:00",
        "updated_date": "2025-08-03T18:15:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haolin Yang",
            "Feilong Tang",
            "Linxiao Zhao",
            "Xiang An",
            "Ming Hu",
            "Huifa Li",
            "Xinlin Zhuang",
            "Boqian Wang",
            "Yifan Lu",
            "Xiaofeng Zhang",
            "Abdalla Swikir",
            "Junjun He",
            "Zongyuan Ge",
            "Imran Razzak"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes an anticipatory agent for real-time streaming video understanding that outperforms existing methods in response accuracy and real-time efficiency.",
        "tldr_zh": "本文提出了一种预测性代理，用于实时流视频理解，在响应准确性和实时效率方面优于现有方法。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "HGTS-Former: Hierarchical HyperGraph Transformer for Multivariate Time Series Analysis",
        "summary": "Multivariate time series analysis has long been one of the key research\ntopics in the field of artificial intelligence. However, analyzing complex time\nseries data remains a challenging and unresolved problem due to its high\ndimensionality, dynamic nature, and complex interactions among variables.\nInspired by the strong structural modeling capability of hypergraphs, this\npaper proposes a novel hypergraph-based time series transformer backbone\nnetwork, termed HGTS-Former, to address the multivariate coupling in time\nseries data. Specifically, given the multivariate time series signal, we first\nnormalize and embed each patch into tokens. Then, we adopt the multi-head\nself-attention to enhance the temporal representation of each patch. The\nhierarchical hypergraphs are constructed to aggregate the temporal patterns\nwithin each channel and fine-grained relations between different variables.\nAfter that, we convert the hyperedge into node features through the EdgeToNode\nmodule and adopt the feed-forward network to further enhance the output\nfeatures. Extensive experiments conducted on two multivariate time series tasks\nand eight datasets fully validated the effectiveness of our proposed\nHGTS-Former. The source code will be released on\nhttps://github.com/Event-AHU/Time_Series_Analysis.",
        "url": "http://arxiv.org/abs/2508.02411v1",
        "published_date": "2025-08-04T13:33:28+00:00",
        "updated_date": "2025-08-04T13:33:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Xiao Wang",
            "Hao Si",
            "Fan Zhang",
            "Xiaoya Zhou",
            "Dengdi Sun",
            "Wanli Lyu",
            "Qingquan Yang",
            "Jin Tang"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces HGTS-Former, a hypergraph-based time series transformer for multivariate time series analysis, showing promising results on various datasets.",
        "tldr_zh": "本文介绍了HGTS-Former，一种用于多元时间序列分析的超图转换器，对各种数据集展示了有希望的结果。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "InspectVLM: Unified in Theory, Unreliable in Practice",
        "summary": "Unified vision-language models (VLMs) promise to streamline computer vision\npipelines by reframing multiple visual tasks such as classification, detection,\nand keypoint localization within a single language-driven interface. This\narchitecture is particularly appealing in industrial inspection, where managing\ndisjoint task-specific models introduces complexity, inefficiency, and\nmaintenance overhead. In this paper, we critically evaluate the viability of\nthis unified paradigm using InspectVLM, a Florence-2-based VLM trained on\nInspectMM, our new large-scale multimodal, multitask inspection dataset. While\nInspectVLM performs competitively on image-level classification and structured\nkeypoint tasks, we find that it fails to match traditional ResNet-based models\nin core inspection metrics. Notably, the model exhibits brittle behavior under\nlow prompt variability, produces degenerate outputs for fine-grained object\ndetection, and frequently defaults to memorized language responses regardless\nof visual input. Our findings suggest that while language-driven unification\noffers conceptual elegance, current VLMs lack the visual grounding and\nrobustness necessary for deployment in precision critical industrial\ninspections.",
        "url": "http://arxiv.org/abs/2508.01921v1",
        "published_date": "2025-08-03T21:09:35+00:00",
        "updated_date": "2025-08-03T21:09:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Conor Wallace",
            "Isaac Corley",
            "Jonathan Lwowski"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper evaluates the performance of unified vision-language models in industrial inspection tasks and finds shortcomings in visual grounding and robustness.",
        "tldr_zh": "该论文评估了统一的视觉-语言模型在工业检验任务中的表现，并发现在视觉基础和稳健性方面存在不足。",
        "relevance_score": 4,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6.25
    },
    {
        "title": "Localizing Audio-Visual Deepfakes via Hierarchical Boundary Modeling",
        "summary": "Audio-visual temporal deepfake localization under the content-driven partial\nmanipulation remains a highly challenging task. In this scenario, the deepfake\nregions are usually only spanning a few frames, with the majority of the rest\nremaining identical to the original. To tackle this, we propose a Hierarchical\nBoundary Modeling Network (HBMNet), which includes three modules: an\nAudio-Visual Feature Encoder that extracts discriminative frame-level\nrepresentations, a Coarse Proposal Generator that predicts candidate boundary\nregions, and a Fine-grained Probabilities Generator that refines these\nproposals using bidirectional boundary-content probabilities. From the modality\nperspective, we enhance audio-visual learning through dedicated encoding and\nfusion, reinforced by frame-level supervision to boost discriminability. From\nthe temporal perspective, HBMNet integrates multi-scale cues and bidirectional\nboundary-content relationships. Experiments show that encoding and fusion\nprimarily improve precision, while frame-level supervision boosts recall. Each\nmodule (audio-visual fusion, temporal scales, bi-directionality) contributes\ncomplementary benefits, collectively enhancing localization performance. HBMNet\noutperforms BA-TFD and UMMAFormer and shows improved potential scalability with\nmore training data.",
        "url": "http://arxiv.org/abs/2508.02000v1",
        "published_date": "2025-08-04T02:41:09+00:00",
        "updated_date": "2025-08-04T02:41:09+00:00",
        "categories": [
            "cs.SD",
            "cs.CV",
            "eess.AS",
            "eess.IV"
        ],
        "authors": [
            "Xuanjun Chen",
            "Shih-Peng Cheng",
            "Jiawei Du",
            "Lin Zhang",
            "Xiaoxiao Miao",
            "Chung-Che Wang",
            "Haibin Wu",
            "Hung-yi Lee",
            "Jyh-Shing Roger Jang"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Multimodality"
        ],
        "tldr": "The paper proposes a Hierarchical Boundary Modeling Network (HBMNet) to localize audio-visual deepfakes, achieving high precision and recall through multi-scale cues and bidirectional relationships.",
        "tldr_zh": "本文提出了一种分层边界建模网络（HBMNet），用于定位音视频深度伪造，通过多尺度线索和双向关系实现高精确度和召回率。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]