[
    {
        "title": "Precise Action-to-Video Generation Through Visual Action Prompts",
        "summary": "We present visual action prompts, a unified action representation for\naction-to-video generation of complex high-DoF interactions while maintaining\ntransferable visual dynamics across domains. Action-driven video generation\nfaces a precision-generality trade-off: existing methods using text, primitive\nactions, or coarse masks offer generality but lack precision, while\nagent-centric action signals provide precision at the cost of cross-domain\ntransferability. To balance action precision and dynamic transferability, we\npropose to \"render\" actions into precise visual prompts as domain-agnostic\nrepresentations that preserve both geometric precision and cross-domain\nadaptability for complex actions; specifically, we choose visual skeletons for\ntheir generality and accessibility. We propose robust pipelines to construct\nskeletons from two interaction-rich data sources - human-object interactions\n(HOI) and dexterous robotic manipulation - enabling cross-domain training of\naction-driven generative models. By integrating visual skeletons into\npretrained video generation models via lightweight fine-tuning, we enable\nprecise action control of complex interaction while preserving the learning of\ncross-domain dynamics. Experiments on EgoVid, RT-1 and DROID demonstrate the\neffectiveness of our proposed approach. Project page:\nhttps://zju3dv.github.io/VAP/.",
        "url": "http://arxiv.org/abs/2508.13104v1",
        "published_date": "2025-08-18T17:12:28+00:00",
        "updated_date": "2025-08-18T17:12:28+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Yuang Wang",
            "Chao Wen",
            "Haoyu Guo",
            "Sida Peng",
            "Minghan Qin",
            "Hujun Bao",
            "Xiaowei Zhou",
            "Ruizhen Hu"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces visual action prompts for generating videos of complex actions, balancing precision and transferability.",
        "tldr_zh": "本文引入视觉动作提示，用于生成复杂动作的视频，平衡精确性和可传递性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "X-Ray-CoT: Interpretable Chest X-ray Diagnosis with Vision-Language Models via Chain-of-Thought Reasoning",
        "summary": "Chest X-ray imaging is crucial for diagnosing pulmonary and cardiac diseases,\nyet its interpretation demands extensive clinical experience and suffers from\ninter-observer variability. While deep learning models offer high diagnostic\naccuracy, their black-box nature hinders clinical adoption in high-stakes\nmedical settings. To address this, we propose X-Ray-CoT (Chest X-Ray\nChain-of-Thought), a novel framework leveraging Vision-Language Large Models\n(LVLMs) for intelligent chest X-ray diagnosis and interpretable report\ngeneration. X-Ray-CoT simulates human radiologists' \"chain-of-thought\" by first\nextracting multi-modal features and visual concepts, then employing an\nLLM-based component with a structured Chain-of-Thought prompting strategy to\nreason and produce detailed natural language diagnostic reports. Evaluated on\nthe CORDA dataset, X-Ray-CoT achieves competitive quantitative performance,\nwith a Balanced Accuracy of 80.52% and F1 score of 78.65% for disease\ndiagnosis, slightly surpassing existing black-box models. Crucially, it\nuniquely generates high-quality, explainable reports, as validated by\npreliminary human evaluations. Our ablation studies confirm the integral role\nof each proposed component, highlighting the necessity of multi-modal fusion\nand CoT reasoning for robust and transparent medical AI. This work represents a\nsignificant step towards trustworthy and clinically actionable AI systems in\nmedical imaging.",
        "url": "http://arxiv.org/abs/2508.12455v1",
        "published_date": "2025-08-17T18:00:41+00:00",
        "updated_date": "2025-08-17T18:00:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chee Ng",
            "Liliang Sun",
            "Shaoqing Tang"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "X-Ray-CoT is a novel framework that uses Vision-Language Large Models for interpretable chest X-ray diagnosis, achieving competitive performance and generating high-quality, transparent diagnostic reports.",
        "tldr_zh": "X-Ray-CoT是一个新颖的框架，利用视觉-语言大模型进行可解释的胸部X射线诊断，取得了竞争性表现，并生成高质量、透明的诊断报告。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "Lumen: Consistent Video Relighting and Harmonious Background Replacement with Video Generative Models",
        "summary": "Video relighting is a challenging yet valuable task, aiming to replace the\nbackground in videos while correspondingly adjusting the lighting in the\nforeground with harmonious blending. During translation, it is essential to\npreserve the original properties of the foreground, e.g., albedo, and propagate\nconsistent relighting among temporal frames. In this paper, we propose Lumen,\nan end-to-end video relighting framework developed on large-scale video\ngenerative models, receiving flexible textual description for instructing the\ncontrol of lighting and background. Considering the scarcity of high-qualified\npaired videos with the same foreground in various lighting conditions, we\nconstruct a large-scale dataset with a mixture of realistic and synthetic\nvideos. For the synthetic domain, benefiting from the abundant 3D assets in the\ncommunity, we leverage advanced 3D rendering engine to curate video pairs in\ndiverse environments. For the realistic domain, we adapt a HDR-based lighting\nsimulation to complement the lack of paired in-the-wild videos. Powered by the\naforementioned dataset, we design a joint training curriculum to effectively\nunleash the strengths of each domain, i.e., the physical consistency in\nsynthetic videos, and the generalized domain distribution in realistic videos.\nTo implement this, we inject a domain-aware adapter into the model to decouple\nthe learning of relighting and domain appearance distribution. We construct a\ncomprehensive benchmark to evaluate Lumen together with existing methods, from\nthe perspectives of foreground preservation and video consistency assessment.\nExperimental results demonstrate that Lumen effectively edit the input into\ncinematic relighted videos with consistent lighting and strict foreground\npreservation. Our project page: https://lumen-relight.github.io/",
        "url": "http://arxiv.org/abs/2508.12945v1",
        "published_date": "2025-08-18T14:21:22+00:00",
        "updated_date": "2025-08-18T14:21:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jianshu Zeng",
            "Yuxuan Liu",
            "Yutong Feng",
            "Chenxuan Miao",
            "Zixiang Gao",
            "Jiwang Qu",
            "Jianzhang Zhang",
            "Bin Wang",
            "Kun Yuan"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces Lumen, a video relighting framework, that replaces background and adjusts lighting while preserving foreground details and consistency across frames.",
        "tldr_zh": "本文介绍了Lumen，一个视频重光框架，可以替换背景，调整光线，并保持前景细节和帧间一致性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Multi-source Multimodal Progressive Domain Adaption for Audio-Visual Deception Detection",
        "summary": "This paper presents the winning approach for the 1st MultiModal Deception\nDetection (MMDD) Challenge at the 1st Workshop on Subtle Visual Computing\n(SVC). Aiming at the domain shift issue across source and target domains, we\npropose a Multi-source Multimodal Progressive Domain Adaptation (MMPDA)\nframework that transfers the audio-visual knowledge from diverse source domains\nto the target domain. By gradually aligning source and the target domain at\nboth feature and decision levels, our method bridges domain shifts across\ndiverse multimodal datasets. Extensive experiments demonstrate the\neffectiveness of our approach securing Top-2 place. Our approach reaches 60.43%\non accuracy and 56.99\\% on F1-score on competition stage 2, surpassing the 1st\nplace team by 5.59% on F1-score and the 3rd place teams by 6.75% on accuracy.\nOur code is available at https://github.com/RH-Lin/MMPDA.",
        "url": "http://arxiv.org/abs/2508.12842v1",
        "published_date": "2025-08-18T11:30:08+00:00",
        "updated_date": "2025-08-18T11:30:08+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Ronghao Lin",
            "Sijie Mai",
            "Ying Zeng",
            "Qiaolin He",
            "Aolin Xiong",
            "Haifeng Hu"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a Multi-source Multimodal Progressive Domain Adaptation framework that outperformed in a deception detection challenge by aligning source and target domains in audio-visual data.",
        "tldr_zh": "本文介绍了一个多源多模态渐进领域自适应框架，通过在音频-视觉数据中对齐源领域和目标领域，在欺骗检测挑战中表现优异。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "4DNeX: Feed-Forward 4D Generative Modeling Made Easy",
        "summary": "We present 4DNeX, the first feed-forward framework for generating 4D (i.e.,\ndynamic 3D) scene representations from a single image. In contrast to existing\nmethods that rely on computationally intensive optimization or require\nmulti-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D\ngeneration by fine-tuning a pretrained video diffusion model. Specifically, 1)\nto alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale\ndataset with high-quality 4D annotations generated using advanced\nreconstruction approaches. 2) we introduce a unified 6D video representation\nthat jointly models RGB and XYZ sequences, facilitating structured learning of\nboth appearance and geometry. 3) we propose a set of simple yet effective\nadaptation strategies to repurpose pretrained video diffusion models for 4D\nmodeling. 4DNeX produces high-quality dynamic point clouds that enable\nnovel-view video synthesis. Extensive experiments demonstrate that 4DNeX\noutperforms existing 4D generation methods in efficiency and generalizability,\noffering a scalable solution for image-to-4D modeling and laying the foundation\nfor generative 4D world models that simulate dynamic scene evolution.",
        "url": "http://arxiv.org/abs/2508.13154v1",
        "published_date": "2025-08-18T17:59:55+00:00",
        "updated_date": "2025-08-18T17:59:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhaoxi Chen",
            "Tianqi Liu",
            "Long Zhuo",
            "Jiawei Ren",
            "Zeng Tao",
            "He Zhu",
            "Fangzhou Hong",
            "Liang Pan",
            "Ziwei Liu"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "4DNeX is a feed-forward framework for generating dynamic 3D scene representations from a single image, outperforming existing methods in efficiency and generalizability.",
        "tldr_zh": "4DNeX是一种从单个图像生成动态3D场景表示的前馈框架，在效率和泛化能力方面优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model",
        "summary": "Recent advances in interactive video generations have demonstrated diffusion\nmodel's potential as world models by capturing complex physical dynamics and\ninteractive behaviors. However, existing interactive world models depend on\nbidirectional attention and lengthy inference steps, severely limiting\nreal-time performance. Consequently, they are hard to simulate real-world\ndynamics, where outcomes must update instantaneously based on historical\ncontext and current actions. To address this, we present Matrix-Game 2.0, an\ninteractive world model generates long videos on-the-fly via few-step\nauto-regressive diffusion. Our framework consists of three key components: (1)\nA scalable data production pipeline for Unreal Engine and GTA5 environments to\neffectively produce massive amounts (about 1200 hours) of video data with\ndiverse interaction annotations; (2) An action injection module that enables\nframe-level mouse and keyboard inputs as interactive conditions; (3) A few-step\ndistillation based on the casual architecture for real-time and streaming video\ngeneration. Matrix Game 2.0 can generate high-quality minute-level videos\nacross diverse scenes at an ultra-fast speed of 25 FPS. We open-source our\nmodel weights and codebase to advance research in interactive world modeling.",
        "url": "http://arxiv.org/abs/2508.13009v1",
        "published_date": "2025-08-18T15:28:53+00:00",
        "updated_date": "2025-08-18T15:28:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xianglong He",
            "Chunli Peng",
            "Zexiang Liu",
            "Boyang Wang",
            "Yifan Zhang",
            "Qi Cui",
            "Fei Kang",
            "Biao Jiang",
            "Mengyin An",
            "Yangyang Ren",
            "Baixin Xu",
            "Hao-Xiang Guo",
            "Kaixiong Gong",
            "Cyrus Wu",
            "Wei Li",
            "Xuchen Song",
            "Yang Liu",
            "Eric Li",
            "Yahui Zhou"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "Matrix-Game 2.0 presents an interactive world model that can generate high-quality videos in real-time, enabling diverse interactive scenes at a fast speed.",
        "tldr_zh": "Matrix-Game 2.0提出了一种实时生成高质量视频的交互式世界模型，能够在快速速度下实现多样化的交互场景。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model",
        "summary": "Multimodal Empathetic Response Generation (MERG) is crucial for building\nemotionally intelligent human-computer interactions. Although large language\nmodels (LLMs) have improved text-based ERG, challenges remain in handling\nmultimodal emotional content and maintaining identity consistency. Thus, we\npropose E3RG, an Explicit Emotion-driven Empathetic Response Generation System\nbased on multimodal LLMs which decomposes MERG task into three parts:\nmultimodal empathy understanding, empathy memory retrieval, and multimodal\nresponse generation. By integrating advanced expressive speech and video\ngenerative models, E3RG delivers natural, emotionally rich, and\nidentity-consistent responses without extra training. Experiments validate the\nsuperiority of our system on both zero-shot and few-shot settings, securing\nTop-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.\nOur code is available at https://github.com/RH-Lin/E3RG.",
        "url": "http://arxiv.org/abs/2508.12854v1",
        "published_date": "2025-08-18T11:47:02+00:00",
        "updated_date": "2025-08-18T11:47:02+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.HC",
            "cs.MM"
        ],
        "authors": [
            "Ronghao Lin",
            "Shuai Shen",
            "Weipeng Hu",
            "Qiaolin He",
            "Aolin Xiong",
            "Li Huang",
            "Haifeng Hu",
            "Yap-peng Tan"
        ],
        "ai_categories": [
            "Multimodality",
            "AIGC"
        ],
        "tldr": "The paper introduces E3RG, a system for generating empathetic responses that combines large language models with expressive speech and video generative models. It outperformed other systems in a challenge and is available for use.",
        "tldr_zh": "本文介绍了E3RG，这是一个将大型语言模型与表达性语音和视频生成模型相结合的系统，用于生成富有共鸣的回应。它在挑战赛中表现优秀，并可供使用。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "DyCrowd: Towards Dynamic Crowd Reconstruction from a Large-scene Video",
        "summary": "3D reconstruction of dynamic crowds in large scenes has become increasingly\nimportant for applications such as city surveillance and crowd analysis.\nHowever, current works attempt to reconstruct 3D crowds from a static image,\ncausing a lack of temporal consistency and inability to alleviate the typical\nimpact caused by occlusions. In this paper, we propose DyCrowd, the first\nframework for spatio-temporally consistent 3D reconstruction of hundreds of\nindividuals' poses, positions and shapes from a large-scene video. We design a\ncoarse-to-fine group-guided motion optimization strategy for occlusion-robust\ncrowd reconstruction in large scenes. To address temporal instability and\nsevere occlusions, we further incorporate a VAE (Variational Autoencoder)-based\nhuman motion prior along with a segment-level group-guided optimization. The\ncore of our strategy leverages collective crowd behavior to address long-term\ndynamic occlusions. By jointly optimizing the motion sequences of individuals\nwith similar motion segments and combining this with the proposed Asynchronous\nMotion Consistency (AMC) loss, we enable high-quality unoccluded motion\nsegments to guide the motion recovery of occluded ones, ensuring robust and\nplausible motion recovery even in the presence of temporal desynchronization\nand rhythmic inconsistencies. Additionally, in order to fill the gap of no\nexisting well-annotated large-scene video dataset, we contribute a virtual\nbenchmark dataset, VirtualCrowd, for evaluating dynamic crowd reconstruction\nfrom large-scene videos. Experimental results demonstrate that the proposed\nmethod achieves state-of-the-art performance in the large-scene dynamic crowd\nreconstruction task. The code and dataset will be available for research\npurposes.",
        "url": "http://arxiv.org/abs/2508.12644v1",
        "published_date": "2025-08-18T06:09:38+00:00",
        "updated_date": "2025-08-18T06:09:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hao Wen",
            "Hongbo Kang",
            "Jian Ma",
            "Jing Huang",
            "Yuanwang Yang",
            "Haozhe Lin",
            "Yu-Kun Lai",
            "Kun Li"
        ],
        "ai_categories": [
            "Dataset",
            "GAN"
        ],
        "tldr": "The paper presents DyCrowd, a framework for consistent 3D reconstruction of dynamic crowds in large scenes from videos, achieving state-of-the-art performance.",
        "tldr_zh": "该论文提出了DyCrowd，一个从视频中对大场景中的动态人群进行一致的3D重建的框架，取得了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "ViDA-UGC: Detailed Image Quality Analysis via Visual Distortion Assessment for UGC Images",
        "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have introduced a\nparadigm shift for Image Quality Assessment (IQA) from unexplainable image\nquality scoring to explainable IQA, demonstrating practical applications like\nquality control and optimization guidance. However, current explainable IQA\nmethods not only inadequately use the same distortion criteria to evaluate both\nUser-Generated Content (UGC) and AI-Generated Content (AIGC) images, but also\nlack detailed quality analysis for monitoring image quality and guiding image\nrestoration. In this study, we establish the first large-scale Visual\nDistortion Assessment Instruction Tuning Dataset for UGC images, termed\nViDA-UGC, which comprises 11K images with fine-grained quality grounding,\ndetailed quality perception, and reasoning quality description data. This\ndataset is constructed through a distortion-oriented pipeline, which involves\nhuman subject annotation and a Chain-of-Thought (CoT) assessment framework.\nThis framework guides GPT-4o to generate quality descriptions by identifying\nand analyzing UGC distortions, which helps capturing rich low-level visual\nfeatures that inherently correlate with distortion patterns. Moreover, we\ncarefully select 476 images with corresponding 6,149 question answer pairs from\nViDA-UGC and invite a professional team to ensure the accuracy and quality of\nGPT-generated information. The selected and revised data further contribute to\nthe first UGC distortion assessment benchmark, termed ViDA-UGC-Bench.\nExperimental results demonstrate the effectiveness of the ViDA-UGC and CoT\nframework for consistently enhancing various image quality analysis abilities\nacross multiple base MLLMs on ViDA-UGC-Bench and Q-Bench, even surpassing\nGPT-4o.",
        "url": "http://arxiv.org/abs/2508.12605v1",
        "published_date": "2025-08-18T04:02:58+00:00",
        "updated_date": "2025-08-18T04:02:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenjie Liao",
            "Jieyu Yuan",
            "Yifang Xu",
            "Chunle Guo",
            "Zilong Zhang",
            "Jihong Li",
            "Jiachen Fu",
            "Haotian Fan",
            "Tao Li",
            "Junhui Cui",
            "Chongyi Li"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces ViDA-UGC, a dataset for detailed quality analysis of User-Generated Content images, using a distortion assessment framework to enhance image quality analysis abilities.",
        "tldr_zh": "本文介绍了ViDA-UGC，这是一个用于对用户生成内容图像进行详细质量分析的数据集，利用了失真评估框架来增强图像质量分析能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "IGFuse: Interactive 3D Gaussian Scene Reconstruction via Multi-Scans Fusion",
        "summary": "Reconstructing complete and interactive 3D scenes remains a fundamental\nchallenge in computer vision and robotics, particularly due to persistent\nobject occlusions and limited sensor coverage. Multiview observations from a\nsingle scene scan often fail to capture the full structural details. Existing\napproaches typically rely on multi stage pipelines, such as segmentation,\nbackground completion, and inpainting or require per-object dense scanning,\nboth of which are error-prone, and not easily scalable. We propose IGFuse, a\nnovel framework that reconstructs interactive Gaussian scene by fusing\nobservations from multiple scans, where natural object rearrangement between\ncaptures reveal previously occluded regions. Our method constructs segmentation\naware Gaussian fields and enforces bi-directional photometric and semantic\nconsistency across scans. To handle spatial misalignments, we introduce a\npseudo-intermediate scene state for unified alignment, alongside collaborative\nco-pruning strategies to refine geometry. IGFuse enables high fidelity\nrendering and object level scene manipulation without dense observations or\ncomplex pipelines. Extensive experiments validate the framework's strong\ngeneralization to novel scene configurations, demonstrating its effectiveness\nfor real world 3D reconstruction and real-to-simulation transfer. Our project\npage is available online.",
        "url": "http://arxiv.org/abs/2508.13153v1",
        "published_date": "2025-08-18T17:59:47+00:00",
        "updated_date": "2025-08-18T17:59:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenhao Hu",
            "Zesheng Li",
            "Haonan Zhou",
            "Liu Liu",
            "Xuexiang Wen",
            "Zhizhong Su",
            "Xi Li",
            "Gaoang Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "IGFuse is a novel framework for reconstructing interactive 3D scenes by fusing observations from multiple scans, enabling high fidelity rendering and object level scene manipulation without complex pipelines.",
        "tldr_zh": "IGFuse是一个新颖的框架，通过融合来自多个扫描的观察结果，实现了对互动3D场景的重建，实现高保真渲染和物体级场景操作而不需要复杂的流程。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence",
        "summary": "This work studies the challenge of transfer animations between characters\nwhose skeletal topologies differ substantially. While many techniques have\nadvanced retargeting techniques in decades, transfer motions across diverse\ntopologies remains less-explored. The primary obstacle lies in the inherent\ntopological inconsistency between source and target skeletons, which restricts\nthe establishment of straightforward one-to-one bone correspondences. Besides,\nthe current lack of large-scale paired motion datasets spanning different\ntopological structures severely constrains the development of data-driven\napproaches. To address these limitations, we introduce Motion2Motion, a novel,\ntraining-free framework. Simply yet effectively, Motion2Motion works with only\none or a few example motions on the target skeleton, by accessing a sparse set\nof bone correspondences between the source and target skeletons. Through\ncomprehensive qualitative and quantitative evaluations, we demonstrate that\nMotion2Motion achieves efficient and reliable performance in both\nsimilar-skeleton and cross-species skeleton transfer scenarios. The practical\nutility of our approach is further evidenced by its successful integration in\ndownstream applications and user interfaces, highlighting its potential for\nindustrial applications. Code and data are available at\nhttps://lhchen.top/Motion2Motion.",
        "url": "http://arxiv.org/abs/2508.13139v1",
        "published_date": "2025-08-18T17:50:31+00:00",
        "updated_date": "2025-08-18T17:50:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ling-Hao Chen",
            "Yuhong Zhang",
            "Zixin Yin",
            "Zhiyang Dou",
            "Xin Chen",
            "Jingbo Wang",
            "Taku Komura",
            "Lei Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces Motion2Motion, a training-free framework for transferring animations between characters with different skeletal topologies, achieving efficient performance in similar-skeleton and cross-species scenarios.",
        "tldr_zh": "本文介绍了Motion2Motion，这是一个无需训练的框架，用于在具有不同骨骼拓扑结构的角色之间传输动画，实现了在相似骨架和跨物种情景中的高效性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DMS:Diffusion-Based Multi-Baseline Stereo Generation for Improving Self-Supervised Depth Estimation",
        "summary": "While supervised stereo matching and monocular depth estimation have advanced\nsignificantly with learning-based algorithms, self-supervised methods using\nstereo images as supervision signals have received relatively less focus and\nrequire further investigation. A primary challenge arises from ambiguity\nintroduced during photometric reconstruction, particularly due to missing\ncorresponding pixels in ill-posed regions of the target view, such as\nocclusions and out-of-frame areas. To address this and establish explicit\nphotometric correspondences, we propose DMS, a model-agnostic approach that\nutilizes geometric priors from diffusion models to synthesize novel views along\nthe epipolar direction, guided by directional prompts. Specifically, we\nfinetune a Stable Diffusion model to simulate perspectives at key positions:\nleft-left view shifted from the left camera, right-right view shifted from the\nright camera, along with an additional novel view between the left and right\ncameras. These synthesized views supplement occluded pixels, enabling explicit\nphotometric reconstruction. Our proposed DMS is a cost-free, ''plug-and-play''\nmethod that seamlessly enhances self-supervised stereo matching and monocular\ndepth estimation, and relies solely on unlabeled stereo image pairs for both\ntraining and synthesizing. Extensive experiments demonstrate the effectiveness\nof our approach, with up to 35% outlier reduction and state-of-the-art\nperformance across multiple benchmark datasets.",
        "url": "http://arxiv.org/abs/2508.13091v1",
        "published_date": "2025-08-18T17:05:15+00:00",
        "updated_date": "2025-08-18T17:05:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zihua Liu",
            "Yizhou Li",
            "Songyan Zhang",
            "Masatoshi Okutomi"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a diffusion-based multi-baseline stereo generation method to improve self-supervised depth estimation, achieving state-of-the-art results on benchmark datasets.",
        "tldr_zh": "本文介绍了一种基于扩散的多基线立体生成方法，用于改进自监督深度估计，在基准数据集上取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ID-Card Synthetic Generation: Toward a Simulated Bona fide Dataset",
        "summary": "Nowadays, the development of a Presentation Attack Detection (PAD) system for\nID cards presents a challenge due to the lack of images available to train a\nrobust PAD system and the increase in diversity of possible attack instrument\nspecies. Today, most algorithms focus on generating attack samples and do not\ntake into account the limited number of bona fide images. This work is one of\nthe first to propose a method for mimicking bona fide images by generating\nsynthetic versions of them using Stable Diffusion, which may help improve the\ngeneralisation capabilities of the detector. Furthermore, the new images\ngenerated are evaluated in a system trained from scratch and in a commercial\nsolution. The PAD system yields an interesting result, as it identifies our\nimages as bona fide, which has a positive impact on detection performance and\ndata restrictions.",
        "url": "http://arxiv.org/abs/2508.13078v1",
        "published_date": "2025-08-18T16:48:57+00:00",
        "updated_date": "2025-08-18T16:48:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qingwen Zeng",
            "Juan E. Tapia",
            "Izan Garcia",
            "Juan M. Espin",
            "Christoph Busch"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper proposes a method for generating synthetic bona fide ID card images to improve Presentation Attack Detection systems.",
        "tldr_zh": "本文提出了一种生成合成真实身份证图像的方法，以提升展示攻击检测系统的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Eyes on the Image: Gaze Supervised Multimodal Learning for Chest X-ray Diagnosis and Report Generation",
        "summary": "We propose a two-stage multimodal framework that enhances disease\nclassification and region-aware radiology report generation from chest X-rays,\nleveraging the MIMIC-Eye dataset. In the first stage, we introduce a\ngaze-guided contrastive learning architecture for disease classification. It\nintegrates visual features, clinical labels, bounding boxes, and radiologist\neye-tracking signals and is equipped with a novel multi-term gaze-attention\nloss combining MSE, KL divergence, correlation, and center-of-mass alignment.\nIncorporating fixations improves F1 score from 0.597 to 0.631 (+5.70%) and AUC\nfrom 0.821 to 0.849 (+3.41%), while also improving precision and recall,\nhighlighting the effectiveness of gaze-informed attention supervision. In the\nsecond stage, we present a modular report generation pipeline that extracts\nconfidence-weighted diagnostic keywords, maps them to anatomical regions using\na curated dictionary constructed from domain-specific priors, and generates\nregion-aligned sentences via structured prompts. This pipeline improves report\nquality as measured by clinical keyword recall and ROUGE overlap. Our results\ndemonstrate that integrating gaze data improves both classification performance\nand the interpretability of generated medical reports.",
        "url": "http://arxiv.org/abs/2508.13068v1",
        "published_date": "2025-08-18T16:42:29+00:00",
        "updated_date": "2025-08-18T16:42:29+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Tanjim Islam Riju",
            "Shuchismita Anwar",
            "Saman Sarker Joy",
            "Farig Sadeque",
            "Swakkhar Shatabda"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes a two-stage multimodal framework that uses eye-tracking data to improve disease classification and radiology report generation from chest X-rays.",
        "tldr_zh": "本文提出了一个两阶段的多模态框架，利用眼动数据改进了从胸部X光片中的疾病分类和放射学报告生成。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Odo: Depth-Guided Diffusion for Identity-Preserving Body Reshaping",
        "summary": "Human shape editing enables controllable transformation of a person's body\nshape, such as thin, muscular, or overweight, while preserving pose, identity,\nclothing, and background. Unlike human pose editing, which has advanced\nrapidly, shape editing remains relatively underexplored. Current approaches\ntypically rely on 3D morphable models or image warping, often introducing\nunrealistic body proportions, texture distortions, and background\ninconsistencies due to alignment errors and deformations. A key limitation is\nthe lack of large-scale, publicly available datasets for training and\nevaluating body shape manipulation methods. In this work, we introduce the\nfirst large-scale dataset of 18,573 images across 1523 subjects, specifically\ndesigned for controlled human shape editing. It features diverse variations in\nbody shape, including fat, muscular and thin, captured under consistent\nidentity, clothing, and background conditions. Using this dataset, we propose\nOdo, an end-to-end diffusion-based method that enables realistic and intuitive\nbody reshaping guided by simple semantic attributes. Our approach combines a\nfrozen UNet that preserves fine-grained appearance and background details from\nthe input image with a ControlNet that guides shape transformation using target\nSMPL depth maps. Extensive experiments demonstrate that our method outperforms\nprior approaches, achieving per-vertex reconstruction errors as low as 7.5mm,\nsignificantly lower than the 13.6mm observed in baseline methods, while\nproducing realistic results that accurately match the desired target shapes.",
        "url": "http://arxiv.org/abs/2508.13065v1",
        "published_date": "2025-08-18T16:37:29+00:00",
        "updated_date": "2025-08-18T16:37:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Siddharth Khandelwal",
            "Sridhar Kamath",
            "Arjun Jain"
        ],
        "ai_categories": [
            "Dataset",
            "Diffusion"
        ],
        "tldr": "The paper introduces a large-scale dataset and a diffusion-based method for realistic body reshaping guided by semantic attributes.",
        "tldr_zh": "本文介绍了一个大型数据集和一种以语义属性为指导的现实身体重塑扩散方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "IntelliCap: Intelligent Guidance for Consistent View Sampling",
        "summary": "Novel view synthesis from images, for example, with 3D Gaussian splatting,\nhas made great progress. Rendering fidelity and speed are now ready even for\ndemanding virtual reality applications. However, the problem of assisting\nhumans in collecting the input images for these rendering algorithms has\nreceived much less attention. High-quality view synthesis requires uniform and\ndense view sampling. Unfortunately, these requirements are not easily addressed\nby human camera operators, who are in a hurry, impatient, or lack understanding\nof the scene structure and the photographic process. Existing approaches to\nguide humans during image acquisition concentrate on single objects or neglect\nview-dependent material characteristics. We propose a novel situated\nvisualization technique for scanning at multiple scales. During the scanning of\na scene, our method identifies important objects that need extended image\ncoverage to properly represent view-dependent appearance. To this end, we\nleverage semantic segmentation and category identification, ranked by a\nvision-language model. Spherical proxies are generated around highly ranked\nobjects to guide the user during scanning. Our results show superior\nperformance in real scenes compared to conventional view sampling strategies.",
        "url": "http://arxiv.org/abs/2508.13043v1",
        "published_date": "2025-08-18T16:00:31+00:00",
        "updated_date": "2025-08-18T16:00:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ayaka Yasunaga",
            "Hideo Saito",
            "Dieter Schmalstieg",
            "Shohei Mori"
        ],
        "ai_categories": [
            "Multimodality",
            "AIGC",
            "Transformer"
        ],
        "tldr": "The paper proposes an intelligent guidance system for assisting humans in collecting input images for high-quality view synthesis, leveraging semantic segmentation and category identification.",
        "tldr_zh": "本文提出了一种智能引导系统，用于帮助人类收集高质量视图合成的输入图像，利用语义分割和类别识别。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HierAdaptMR: Cross-Center Cardiac MRI Reconstruction with Hierarchical Feature Adapters",
        "summary": "Deep learning-based cardiac MRI reconstruction faces significant domain shift\nchallenges when deployed across multiple clinical centers with heterogeneous\nscanner configurations and imaging protocols. We propose HierAdaptMR, a\nhierarchical feature adaptation framework that addresses multi-level domain\nvariations through parameter-efficient adapters. Our method employs\nProtocol-Level Adapters for sequence-specific characteristics and Center-Level\nAdapters for scanner-dependent variations, built upon a variational unrolling\nbackbone. A Universal Adapter enables generalization to entirely unseen centers\nthrough stochastic training that learns center-invariant adaptations. The\nframework utilizes multi-scale SSIM loss with frequency domain enhancement and\ncontrast-adaptive weighting for robust optimization. Comprehensive evaluation\non the CMRxRecon2025 dataset spanning 5+ centers, 10+ scanners, and 9\nmodalities demonstrates superior cross-center generalization while maintaining\nreconstruction quality. code: https://github.com/Ruru-Xu/HierAdaptMR",
        "url": "http://arxiv.org/abs/2508.13026v1",
        "published_date": "2025-08-18T15:43:19+00:00",
        "updated_date": "2025-08-18T15:43:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruru Xu",
            "Ilkay Oksuz"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "HierAdaptMR proposes a hierarchical feature adaptation framework for deep learning-based cardiac MRI reconstruction across multiple clinical centers with heterogeneous scanner configurations, showing superior cross-center generalization.",
        "tldr_zh": "HierAdaptMR提出了一种层次特征适应框架，用于深度学习的心脏MRI重建，跨越了具有异构扫描仪配置的多个临床中心，展示出优越的跨中心泛化能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EgoTwin: Dreaming Body and View in First Person",
        "summary": "While exocentric video synthesis has achieved great progress, egocentric\nvideo generation remains largely underexplored, which requires modeling\nfirst-person view content along with camera motion patterns induced by the\nwearer's body movements. To bridge this gap, we introduce a novel task of joint\negocentric video and human motion generation, characterized by two key\nchallenges: 1) Viewpoint Alignment: the camera trajectory in the generated\nvideo must accurately align with the head trajectory derived from human motion;\n2) Causal Interplay: the synthesized human motion must causally align with the\nobserved visual dynamics across adjacent video frames. To address these\nchallenges, we propose EgoTwin, a joint video-motion generation framework built\non the diffusion transformer architecture. Specifically, EgoTwin introduces a\nhead-centric motion representation that anchors the human motion to the head\njoint and incorporates a cybernetics-inspired interaction mechanism that\nexplicitly captures the causal interplay between video and motion within\nattention operations. For comprehensive evaluation, we curate a large-scale\nreal-world dataset of synchronized text-video-motion triplets and design novel\nmetrics to assess video-motion consistency. Extensive experiments demonstrate\nthe effectiveness of the EgoTwin framework.",
        "url": "http://arxiv.org/abs/2508.13013v1",
        "published_date": "2025-08-18T15:33:09+00:00",
        "updated_date": "2025-08-18T15:33:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingqiao Xiu",
            "Fangzhou Hong",
            "Yicong Li",
            "Mengze Li",
            "Wentao Wang",
            "Sirui Han",
            "Liang Pan",
            "Ziwei Liu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces EgoTwin, a framework for generating egocentric videos and human motion by aligning camera trajectory with head motion and capturing causal interplay within attention operations.",
        "tldr_zh": "本文介绍了EgoTwin，一个通过将摄像机轨迹与头部运动对齐并在注意力操作中捕获因果相互作用来生成自我中心视频和人体运动的框架。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Omni Survey for Multimodality Analysis in Visual Object Tracking",
        "summary": "The development of smart cities has led to the generation of massive amounts\nof multi-modal data in the context of a range of tasks that enable a\ncomprehensive monitoring of the smart city infrastructure and services. This\npaper surveys one of the most critical tasks, multi-modal visual object\ntracking (MMVOT), from the perspective of multimodality analysis. Generally,\nMMVOT differs from single-modal tracking in four key aspects, data collection,\nmodality alignment and annotation, model designing, and evaluation.\nAccordingly, we begin with an introduction to the relevant data modalities,\nlaying the groundwork for their integration. This naturally leads to a\ndiscussion of challenges of multi-modal data collection, alignment, and\nannotation. Subsequently, existing MMVOT methods are categorised, based on\ndifferent ways to deal with visible (RGB) and X modalities: programming the\nauxiliary X branch with replicated or non-replicated experimental\nconfigurations from the RGB branch. Here X can be thermal infrared (T), depth\n(D), event (E), near infrared (NIR), language (L), or sonar (S). The final part\nof the paper addresses evaluation and benchmarking. In summary, we undertake an\nomni survey of all aspects of multi-modal visual object tracking (VOT),\ncovering six MMVOT tasks and featuring 338 references in total. In addition, we\ndiscuss the fundamental rhetorical question: Is multi-modal tracking always\nguaranteed to provide a superior solution to unimodal tracking with the help of\ninformation fusion, and if not, in what circumstances its application is\nbeneficial. Furthermore, for the first time in this field, we analyse the\ndistributions of the object categories in the existing MMVOT datasets,\nrevealing their pronounced long-tail nature and a noticeable lack of animal\ncategories when compared with RGB datasets.",
        "url": "http://arxiv.org/abs/2508.13000v1",
        "published_date": "2025-08-18T15:18:59+00:00",
        "updated_date": "2025-08-18T15:18:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhangyong Tang",
            "Tianyang Xu",
            "Xuefeng Zhu",
            "Hui Li",
            "Shaochuan Zhao",
            "Tao Zhou",
            "Chunyang Cheng",
            "Xiaojun Wu",
            "Josef Kittler"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper surveys the field of multi-modal visual object tracking in the context of smart cities, discussing challenges and methods for integrating different data modalities.",
        "tldr_zh": "本文调查了智慧城市环境下多模态视觉对象跟踪领域，讨论了整合不同数据模态所面临的挑战和方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dextr: Zero-Shot Neural Architecture Search with Singular Value Decomposition and Extrinsic Curvature",
        "summary": "Zero-shot Neural Architecture Search (NAS) typically optimises the\narchitecture search process by exploiting the network or gradient properties at\ninitialisation through zero-cost proxies. The existing proxies often rely on\nlabelled data, which is usually unavailable in real-world settings.\nFurthermore, the majority of the current methods focus either on optimising the\nconvergence and generalisation attributes or solely on the expressivity of the\nnetwork architectures. To address both limitations, we first demonstrate how\nchannel collinearity affects the convergence and generalisation properties of a\nneural network. Then, by incorporating the convergence, generalisation and\nexpressivity in one approach, we propose a zero-cost proxy that omits the\nrequirement of labelled data for its computation. In particular, we leverage\nthe Singular Value Decomposition (SVD) of the neural network layer features and\nthe extrinsic curvature of the network output to design our proxy. %As a\nresult, the proposed proxy is formulated as the simplified harmonic mean of the\nlogarithms of two key components: the sum of the inverse of the feature\ncondition number and the extrinsic curvature of the network output. Our\napproach enables accurate prediction of network performance on test data using\nonly a single label-free data sample. Our extensive evaluation includes a total\nof six experiments, including the Convolutional Neural Network (CNN) search\nspace, i.e. DARTS and the Transformer search space, i.e. AutoFormer. The\nproposed proxy demonstrates a superior performance on multiple correlation\nbenchmarks, including NAS-Bench-101, NAS-Bench-201, and\nTransNAS-Bench-101-micro; as well as on the NAS task within the DARTS and the\nAutoFormer search space, all while being notably efficient. The code is\navailable at https://github.com/rohanasthana/Dextr.",
        "url": "http://arxiv.org/abs/2508.12977v1",
        "published_date": "2025-08-18T14:52:14+00:00",
        "updated_date": "2025-08-18T14:52:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rohan Asthana",
            "Joschua Conrad",
            "Maurits Ortmanns",
            "Vasileios Belagiannis"
        ],
        "ai_categories": [
            "Transformer",
            "AIGC"
        ],
        "tldr": "The paper proposes a zero-shot Neural Architecture Search method using Singular Value Decomposition and Extrinsic Curvature, achieving accurate prediction of network performance with only one label-free data sample.",
        "tldr_zh": "本文提出了一种使用奇异值分解和外曲率的零检索神经架构搜索方法，仅使用一个无标签数据样本就能准确预测网络性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Compact Attention: Exploiting Structured Spatio-Temporal Sparsity for Fast Video Generation",
        "summary": "The computational demands of self-attention mechanisms pose a critical\nchallenge for transformer-based video generation, particularly in synthesizing\nultra-long sequences. Current approaches, such as factorized attention and\nfixed sparse patterns, fail to fully exploit the inherent spatio-temporal\nredundancies in video data. Through systematic analysis of video diffusion\ntransformers (DiT), we uncover a key insight: Attention matrices exhibit\nstructured, yet heterogeneous sparsity patterns, where specialized heads\ndynamically attend to distinct spatiotemporal regions (e.g., local pattern,\ncross-shaped pattern, or global pattern). Existing sparse attention methods\neither impose rigid constraints or introduce significant overhead, limiting\ntheir effectiveness. To address this, we propose Compact Attention, a\nhardware-aware acceleration framework featuring three innovations: 1) Adaptive\ntiling strategies that approximate diverse spatial interaction patterns via\ndynamic tile grouping, 2) Temporally varying windows that adjust sparsity\nlevels based on frame proximity, and 3) An automated configuration search\nalgorithm that optimizes sparse patterns while preserving critical attention\npathways. Our method achieves 1.6~2.5x acceleration in attention computation on\nsingle-GPU setups while maintaining comparable visual quality with\nfull-attention baselines. This work provides a principled approach to unlocking\nefficient long-form video generation through structured sparsity exploitation.\nProject Page: https://yo-ava.github.io/Compact-Attention.github.io/",
        "url": "http://arxiv.org/abs/2508.12969v1",
        "published_date": "2025-08-18T14:45:42+00:00",
        "updated_date": "2025-08-18T14:45:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qirui Li",
            "Guangcong Zheng",
            "Qi Zhao",
            "Jie Li",
            "Bin Dong",
            "Yiwu Yao",
            "Xi Li"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper proposes Compact Attention, a method to accelerate attention computation in transformer-based video generation by exploiting structured spatio-temporal sparsity, achieving 1.6~2.5x speedup on single-GPU setups while maintaining visual quality.",
        "tldr_zh": "本文提出了压缩注意力，这是一种利用结构空间-时间稀疏性加速变压器视频生成的方法，在单GPU设置上实现了1.6~2.5倍的加速，同时保持了视觉质量。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GazeDETR: Gaze Detection using Disentangled Head and Gaze Representations",
        "summary": "Gaze communication plays a crucial role in daily social interactions.\nQuantifying this behavior can help in human-computer interaction and digital\nphenotyping. While end-to-end models exist for gaze target detection, they only\nutilize a single decoder to simultaneously localize human heads and predict\ntheir corresponding gaze (e.g., 2D points or heatmap) in a scene. This\nmultitask learning approach generates a unified and entangled representation\nfor human head localization and gaze location prediction. Herein, we propose\nGazeDETR, a novel end-to-end architecture with two disentangled decoders that\nindividually learn unique representations and effectively utilize coherent\nattentive fields for each subtask. More specifically, we demonstrate that its\nhuman head predictor utilizes local information, while its gaze decoder\nincorporates both local and global information. Our proposed architecture\nachieves state-of-the-art results on the GazeFollow, VideoAttentionTarget and\nChildPlay datasets. It outperforms existing end-to-end models with a notable\nmargin.",
        "url": "http://arxiv.org/abs/2508.12966v1",
        "published_date": "2025-08-18T14:41:18+00:00",
        "updated_date": "2025-08-18T14:41:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ryan Anthony Jalova de Belen",
            "Gelareh Mohammadi",
            "Arcot Sowmya"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "GazeDETR is a novel architecture for gaze detection that uses disentangled decoders to improve accuracy and outperforms existing models on various datasets.",
        "tldr_zh": "GazeDETR是一种新颖的用于注视检测的架构，通过使用解耦解码器来提高准确性，并在各种数据集上表现优于现有模型。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Breaking Reward Collapse: Adaptive Reinforcement for Open-ended Medical Reasoning with Enhanced Semantic Discrimination",
        "summary": "Reinforcement learning (RL) with rule-based rewards has demonstrated strong\npotential in enhancing the reasoning and generalization capabilities of\nvision-language models (VLMs) and large language models (LLMs), while reducing\ncomputational overhead. However, its application in medical imaging remains\nunderexplored. Existing reinforcement fine-tuning (RFT) approaches in this\ndomain primarily target closed-ended visual question answering (VQA), limiting\ntheir applicability to real-world clinical reasoning. In contrast, open-ended\nmedical VQA better reflects clinical practice but has received limited\nattention. While some efforts have sought to unify both formats via\nsemantically guided RL, we observe that model-based semantic rewards often\nsuffer from reward collapse, where responses with significant semantic\ndifferences receive similar scores. To address this, we propose ARMed (Adaptive\nReinforcement for Medical Reasoning), a novel RL framework for open-ended\nmedical VQA. ARMed first incorporates domain knowledge through supervised\nfine-tuning (SFT) on chain-of-thought data, then applies reinforcement learning\nwith textual correctness and adaptive semantic rewards to enhance reasoning\nquality. We evaluate ARMed on six challenging medical VQA benchmarks. Results\nshow that ARMed consistently boosts both accuracy and generalization, achieving\na 32.64% improvement on in-domain tasks and an 11.65% gain on out-of-domain\nbenchmarks. These results highlight the critical role of reward\ndiscriminability in medical RL and the promise of semantically guided rewards\nfor enabling robust and clinically meaningful multimodal reasoning.",
        "url": "http://arxiv.org/abs/2508.12957v1",
        "published_date": "2025-08-18T14:31:26+00:00",
        "updated_date": "2025-08-18T14:31:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yizhou Liu",
            "Jingwei Wei",
            "Zizhi Chen",
            "Minghao Han",
            "Xukun Zhang",
            "Keliang Liu",
            "Lihua Zhang"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality"
        ],
        "tldr": "The paper introduces ARMed, a novel reinforcement learning framework for open-ended medical reasoning, improving reasoning quality and generalization in medical VQA tasks.",
        "tldr_zh": "本文介绍了ARMed，一个用于医学开放式推理的新型强化学习框架，提高了医学VQA任务中的推理质量和泛化性能。",
        "relevance_score": 1,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Fully Automated Segmentation of Fiber Bundles in Anatomic Tracing Data",
        "summary": "Anatomic tracer studies are critical for validating and improving diffusion\nMRI (dMRI) tractography. However, large-scale analysis of data from such\nstudies is hampered by the labor-intensive process of annotating fiber bundles\nmanually on histological slides. Existing automated methods often miss sparse\nbundles or require complex post-processing across consecutive sections,\nlimiting their flexibility and generalizability. We present a streamlined,\nfully automated framework for fiber bundle segmentation in macaque tracer data,\nbased on a U-Net architecture with large patch sizes, foreground aware\nsampling, and semisupervised pre-training. Our approach eliminates common\nerrors such as mislabeling terminals as bundles, improves detection of sparse\nbundles by over 20% and reduces the False Discovery Rate (FDR) by 40% compared\nto the state-of-the-art, all while enabling analysis of standalone slices. This\nnew framework will facilitate the automated analysis of anatomic tracing data\nat a large scale, generating more ground-truth data that can be used to\nvalidate and optimize dMRI tractography methods.",
        "url": "http://arxiv.org/abs/2508.12942v1",
        "published_date": "2025-08-18T14:17:24+00:00",
        "updated_date": "2025-08-18T14:17:24+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Kyriaki-Margarita Bintsi",
            "Yaël Balbastre",
            "Jingjing Wu",
            "Julia F. Lehman",
            "Suzanne N. Haber",
            "Anastasia Yendiki"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper presents a fully automated framework for segmenting fiber bundles in anatomic tracer data, improving accuracy and reducing errors compared to existing methods.",
        "tldr_zh": "该论文提出了一个完全自动化的框架，用于在解剖示踪数据中分割纤维束，相较于现有方法，提高了准确性并减少了错误。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SEDEG:Sequential Enhancement of Decoder and Encoder's Generality for Class Incremental Learning with Small Memory",
        "summary": "In incremental learning, enhancing the generality of knowledge is crucial for\nadapting to dynamic data inputs. It can develop generalized representations or\nmore balanced decision boundaries, preventing the degradation of long-term\nknowledge over time and thus mitigating catastrophic forgetting. Some emerging\nincremental learning methods adopt an encoder-decoder architecture and have\nachieved promising results. In the encoder-decoder achitecture, improving the\ngeneralization capabilities of both the encoder and decoder is critical, as it\nhelps preserve previously learned knowledge while ensuring adaptability and\nrobustness to new, diverse data inputs. However, many existing continual\nmethods focus solely on enhancing one of the two components, which limits their\neffectiveness in mitigating catastrophic forgetting. And these methods perform\neven worse in small-memory scenarios, where only a limited number of historical\nsamples can be stored. To mitigate this limitation, we introduces SEDEG, a\ntwo-stage training framework for vision transformers (ViT), focusing on\nsequentially improving the generality of both Decoder and Encoder. Initially,\nSEDEG trains an ensembled encoder through feature boosting to learn generalized\nrepresentations, which subsequently enhance the decoder's generality and\nbalance the classifier. The next stage involves using knowledge distillation\n(KD) strategies to compress the ensembled encoder and develop a new, more\ngeneralized encoder. This involves using a balanced KD approach and feature KD\nfor effective knowledge transfer. Extensive experiments on three benchmark\ndatasets show SEDEG's superior performance, and ablation studies confirm the\nefficacy of its components. The code is available at\nhttps://github.com/ShaolingPu/CIL.",
        "url": "http://arxiv.org/abs/2508.12932v1",
        "published_date": "2025-08-18T13:55:59+00:00",
        "updated_date": "2025-08-18T13:55:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hongyang Chen",
            "Shaoling Pu",
            "Lingyu Zheng",
            "Zhongwu Sun"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper presents SEDEG, a two-stage training framework for vision transformers, aiming to enhance the generality of both Encoder and Decoder to improve class incremental learning with small memory.",
        "tldr_zh": "本文提出了SEDEG，这是一个为视觉转换器设计的两阶段训练框架，旨在增强编码器和解码器的通用性，以改善具有小内存的类增量学习。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "7Bench: a Comprehensive Benchmark for Layout-guided Text-to-image Models",
        "summary": "Layout-guided text-to-image models offer greater control over the generation\nprocess by explicitly conditioning image synthesis on the spatial arrangement\nof elements. As a result, their adoption has increased in many computer vision\napplications, ranging from content creation to synthetic data generation. A\ncritical challenge is achieving precise alignment between the image, textual\nprompt, and layout, ensuring semantic fidelity and spatial accuracy. Although\nrecent benchmarks assess text alignment, layout alignment remains overlooked,\nand no existing benchmark jointly evaluates both. This gap limits the ability\nto evaluate a model's spatial fidelity, which is crucial when using\nlayout-guided generation for synthetic data, as errors can introduce noise and\ndegrade data quality. In this work, we introduce 7Bench, the first benchmark to\nassess both semantic and spatial alignment in layout-guided text-to-image\ngeneration. It features text-and-layout pairs spanning seven challenging\nscenarios, investigating object generation, color fidelity, attribute\nrecognition, inter-object relationships, and spatial control. We propose an\nevaluation protocol that builds on existing frameworks by incorporating the\nlayout alignment score to assess spatial accuracy. Using 7Bench, we evaluate\nseveral state-of-the-art diffusion models, uncovering their respective\nstrengths and limitations across diverse alignment tasks. The benchmark is\navailable at https://github.com/Elizzo/7Bench.",
        "url": "http://arxiv.org/abs/2508.12919v1",
        "published_date": "2025-08-18T13:37:51+00:00",
        "updated_date": "2025-08-18T13:37:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Elena Izzo",
            "Luca Parolari",
            "Davide Vezzaro",
            "Lamberto Ballan"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper introduces 7Bench, a benchmark for layout-guided text-to-image models that assesses both semantic and spatial alignment, helping evaluate model's spatial fidelity in image generation.",
        "tldr_zh": "这篇论文介绍了7Bench，这是一个为布局引导的文本到图像模型提供综合评估的基准，评估语义和空间对齐，帮助评估模型在图像生成中的空间保真度。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CMF-IoU: Multi-Stage Cross-Modal Fusion 3D Object Detection with IoU Joint Prediction",
        "summary": "Multi-modal methods based on camera and LiDAR sensors have garnered\nsignificant attention in the field of 3D detection. However, many prevalent\nworks focus on single or partial stage fusion, leading to insufficient feature\nextraction and suboptimal performance. In this paper, we introduce a\nmulti-stage cross-modal fusion 3D detection framework, termed CMF-IOU, to\neffectively address the challenge of aligning 3D spatial and 2D semantic\ninformation. Specifically, we first project the pixel information into 3D space\nvia a depth completion network to get the pseudo points, which unifies the\nrepresentation of the LiDAR and camera information. Then, a bilateral\ncross-view enhancement 3D backbone is designed to encode LiDAR points and\npseudo points. The first sparse-to-distant (S2D) branch utilizes an\nencoder-decoder structure to reinforce the representation of sparse LiDAR\npoints. The second residual view consistency (ResVC) branch is proposed to\nmitigate the influence of inaccurate pseudo points via both the 3D and 2D\nconvolution processes. Subsequently, we introduce an iterative voxel-point\naware fine grained pooling module, which captures the spatial information from\nLiDAR points and textural information from pseudo points in the proposal\nrefinement stage. To achieve more precise refinement during iteration, an\nintersection over union (IoU) joint prediction branch integrated with a novel\nproposals generation technique is designed to preserve the bounding boxes with\nboth high IoU and classification scores. Extensive experiments show the\nsuperior performance of our method on the KITTI, nuScenes and Waymo datasets.",
        "url": "http://arxiv.org/abs/2508.12917v1",
        "published_date": "2025-08-18T13:32:07+00:00",
        "updated_date": "2025-08-18T13:32:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhiwei Ning",
            "Zhaojiang Liu",
            "Xuanang Gao",
            "Yifan Zuo",
            "Jie Yang",
            "Yuming Fang",
            "Wei Liu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a multi-stage cross-modal fusion 3D detection framework, CMF-IoU, which aligns 3D spatial and 2D semantic information for improved detection performance.",
        "tldr_zh": "本文介绍了一种多阶段跨模态融合3D检测框架CMF-IoU，可以对齐3D空间和2D语义信息，从而提高检测性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CTFlow: Video-Inspired Latent Flow Matching for 3D CT Synthesis",
        "summary": "Generative modelling of entire CT volumes conditioned on clinical reports has\nthe potential to accelerate research through data augmentation,\nprivacy-preserving synthesis and reducing regulator-constraints on patient data\nwhile preserving diagnostic signals. With the recent release of CT-RATE, a\nlarge-scale collection of 3D CT volumes paired with their respective clinical\nreports, training large text-conditioned CT volume generation models has become\nachievable. In this work, we introduce CTFlow, a 0.5B latent flow matching\ntransformer model, conditioned on clinical reports. We leverage the A-VAE from\nFLUX to define our latent space, and rely on the CT-Clip text encoder to encode\nthe clinical reports. To generate consistent whole CT volumes while keeping the\nmemory constraints tractable, we rely on a custom autoregressive approach,\nwhere the model predicts the first sequence of slices of the volume from\ntext-only, and then relies on the previously generated sequence of slices and\nthe text, to predict the following sequence. We evaluate our results against\nstate-of-the-art generative CT model, and demonstrate the superiority of our\napproach in terms of temporal coherence, image diversity and text-image\nalignment, with FID, FVD, IS scores and CLIP score.",
        "url": "http://arxiv.org/abs/2508.12900v1",
        "published_date": "2025-08-18T12:58:21+00:00",
        "updated_date": "2025-08-18T12:58:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jiayi Wang",
            "Hadrien Reynaud",
            "Franciskus Xaverius Erick",
            "Bernhard Kainz"
        ],
        "ai_categories": [
            "Transformer",
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces CTFlow, a model for generating 3D CT volumes conditioned on clinical reports, demonstrating superior performance compared to state-of-the-art models.",
        "tldr_zh": "本文介绍了CTFlow模型，用于生成受临床报告调控的3D CT体积，并表现出比最先进模型更优越的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "ONG: One-Shot NMF-based Gradient Masking for Efficient Model Sparsification",
        "summary": "Deep Neural Networks (DNNs) have achieved remarkable success but their large\nsize poses deployment challenges. While various pruning techniques exist, many\ninvolve complex iterative processes, specialized criteria, or struggle to\nmaintain sparsity effectively during training. We introduce ONG (One-shot\nNMF-based Gradient Masking), a novel sparsification strategy that identifies\nsalient weight structures using Non-negative Matrix Factorization (NMF) for\none-shot pruning at the outset of training. Subsequently, ONG employs a precise\ngradient masking mechanism to ensure that only unpruned weights are updated,\nstrictly preserving the target sparsity throughout the training phase. We\nintegrate ONG into the BIMP comparative framework and evaluate it on CIFAR-10\nand CIFAR-100 with ResNet56, ResNet34, and ResNet18 against established stable\nsparsification methods. Our experiments demonstrate ONG's ability to achieve\ncomparable or superior performance at various sparsity levels while maintaining\nstructural integrity post-pruning and offering a clear mechanism for targeting\ndesired sparsities.",
        "url": "http://arxiv.org/abs/2508.12891v1",
        "published_date": "2025-08-18T12:46:05+00:00",
        "updated_date": "2025-08-18T12:46:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sankar Behera",
            "Yamuna Prasad"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces ONG, a one-shot sparsification strategy using NMF and gradient masking to efficiently sparsify neural networks while maintaining performance.",
        "tldr_zh": "本文介绍了ONG，一种利用NMF和梯度屏蔽的一次性稀疏化策略，可以高效稀疏化神经网络同时保持性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models",
        "summary": "Classifier-free Guidance (CFG) is a widely used technique in modern diffusion\nmodels for enhancing sample quality and prompt adherence. However, through an\nempirical analysis on Gaussian mixture modeling with a closed-form solution, we\nobserve a discrepancy between the suboptimal results produced by CFG and the\nground truth. The model's excessive reliance on these suboptimal predictions\noften leads to semantic incoherence and low-quality outputs. To address this\nissue, we first empirically demonstrate that the model's suboptimal predictions\ncan be effectively refined using sub-networks of the model itself. Building on\nthis insight, we propose S^2-Guidance, a novel method that leverages stochastic\nblock-dropping during the forward process to construct stochastic sub-networks,\neffectively guiding the model away from potential low-quality predictions and\ntoward high-quality outputs. Extensive qualitative and quantitative experiments\non text-to-image and text-to-video generation tasks demonstrate that\nS^2-Guidance delivers superior performance, consistently surpassing CFG and\nother advanced guidance strategies. Our code will be released.",
        "url": "http://arxiv.org/abs/2508.12880v1",
        "published_date": "2025-08-18T12:31:20+00:00",
        "updated_date": "2025-08-18T12:31:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chubin Chen",
            "Jiashu Zhu",
            "Xiaokun Feng",
            "Nisha Huang",
            "Meiqi Wu",
            "Fangyuan Mao",
            "Jiahong Wu",
            "Xiangxiang Chu",
            "Xiu Li"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion"
        ],
        "tldr": "The paper introduces a new method called S^2-Guidance for improving diffusion models by leveraging stochastic sub-networks to avoid low-quality predictions and produce high-quality outputs.",
        "tldr_zh": "这篇论文介绍了一种新方法，称为S^2-Guidance，通过利用随机子网络来改进扩散模型，避免低质量的预测，产生高质量的输出。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Preserve and Sculpt: Manifold-Aligned Fine-tuning of Vision-Language Models for Few-Shot Learning",
        "summary": "Pretrained vision-language models (VLMs), such as CLIP, have shown remarkable\npotential in few-shot image classification and led to numerous effective\ntransfer learning strategies. These methods leverage the pretrained knowledge\nof VLMs to enable effective domain adaptation while mitigating overfitting\nthrough parameter-efficient tuning or instance-based consistency constraints.\nHowever, such regularizations often neglect the geometric structure of data\ndistribution, which may lead to distortion of the overall semantic\nrepresentation. To overcome this limitation, we propose a novel fine-tuning\nmethod, Manifold-Preserving and Sculpting Tuning (MPS-Tuning). Regarding the\ndata distribution in feature space as a semantic manifold, MPS-Tuning\nexplicitly constrains the intrinsic geometry of this manifold while further\nsculpting it to enhance class separability. Specifically, MPS-Tuning preserves\nboth macroscopic and microscopic topological structures of the original\nmanifold by aligning Gram matrices of features before and after fine-tuning.\nTheoretically, this constraint is shown to approximate an upper bound of the\nGromov-Wasserstein distance. Furthermore, features from the image and text\nmodalities are paired, and pairwise similarities are optimized to enhance the\nmanifold's class discriminability. Extensive experiments demonstrate that\nMPS-Tuning significantly improves model performance while effectively\npreserving the structure of the semantic manifold. The code will be released.",
        "url": "http://arxiv.org/abs/2508.12877v1",
        "published_date": "2025-08-18T12:28:43+00:00",
        "updated_date": "2025-08-18T12:28:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dexia Chen",
            "Qianjie Zhu",
            "Weibing Li",
            "Yue Yu",
            "Tong Zhang",
            "Ruixuan Wang"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper proposes a novel fine-tuning method for vision-language models that aims to preserve the data distribution structure while enhancing class separability, leading to improved model performance.",
        "tldr_zh": "本文提出了一种新颖的微调方法，用于视觉-语言模型，旨在保持数据分布结构并增强类别可分性，从而提高模型性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Cross-Domain Few-Shot Learning via Multi-View Collaborative Optimization with Vision-Language Models",
        "summary": "Vision-language models (VLMs) pre-trained on natural image and language data,\nsuch as CLIP, have exhibited significant potential in few-shot image\nrecognition tasks, leading to development of various efficient transfer\nlearning methods. These methods exploit inherent pre-learned knowledge in VLMs\nand have achieved strong performance on standard image datasets. However, their\neffectiveness is often limited when confronted with cross-domain tasks where\nimaging domains differ from natural images. To address this limitation, we\npropose Consistency-guided Multi-view Collaborative Optimization (CoMuCo), a\nnovel fine-tuning strategy for VLMs. This strategy employs two functionally\ncomplementary expert modules to extract multi-view features, while\nincorporating prior knowledge-based consistency constraints and information\ngeometry-based consensus mechanisms to enhance the robustness of feature\nlearning. Additionally, a new cross-domain few-shot benchmark is established to\nhelp comprehensively evaluate methods on imaging domains distinct from natural\nimages. Extensive empirical evaluations on both existing and newly proposed\nbenchmarks suggest CoMuCo consistently outperforms current methods in few-shot\ntasks. The code and benchmark will be released.",
        "url": "http://arxiv.org/abs/2508.12861v1",
        "published_date": "2025-08-18T12:00:09+00:00",
        "updated_date": "2025-08-18T12:00:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dexia Chen",
            "Wentao Zhang",
            "Qianjie Zhu",
            "Ping Hu",
            "Weibing Li",
            "Tong Zhang",
            "Ruixuan Wang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "Proposes a novel fine-tuning strategy called CoMuCo for Vision-language models to improve few-shot learning in cross-domain tasks, outperforming current methods.",
        "tldr_zh": "提出一种名为CoMuCo的新颖微调策略，用于改进视觉语言模型在跨领域任务中的少样本学习，优于当前方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DEEP-SEA: Deep-Learning Enhancement for Environmental Perception in Submerged Aquatics",
        "summary": "Continuous and reliable underwater monitoring is essential for assessing\nmarine biodiversity, detecting ecological changes and supporting autonomous\nexploration in aquatic environments. Underwater monitoring platforms rely on\nmainly visual data for marine biodiversity analysis, ecological assessment and\nautonomous exploration. However, underwater environments present significant\nchallenges due to light scattering, absorption and turbidity, which degrade\nimage clarity and distort colour information, which makes accurate observation\ndifficult. To address these challenges, we propose DEEP-SEA, a novel deep\nlearning-based underwater image restoration model to enhance both low- and\nhigh-frequency information while preserving spatial structures. The proposed\nDual-Frequency Enhanced Self-Attention Spatial and Frequency Modulator aims to\nadaptively refine feature representations in frequency domains and\nsimultaneously spatial information for better structural preservation. Our\ncomprehensive experiments on EUVP and LSUI datasets demonstrate the superiority\nover the state of the art in restoring fine-grained image detail and structural\nconsistency. By effectively mitigating underwater visual degradation, DEEP-SEA\nhas the potential to improve the reliability of underwater monitoring platforms\nfor more accurate ecological observation, species identification and autonomous\nnavigation.",
        "url": "http://arxiv.org/abs/2508.12824v1",
        "published_date": "2025-08-18T11:07:26+00:00",
        "updated_date": "2025-08-18T11:07:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuang Chen",
            "Ronald Thenius",
            "Farshad Arvin",
            "Amir Atapour-Abarghouei"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "DEEP-SEA proposes a deep learning model to enhance underwater image quality for better ecological observation and autonomous navigation in aquatic environments.",
        "tldr_zh": "DEEP-SEA提出了一种深度学习模型，以增强水下图像质量，为水生环境下更好的生态观察和自主导航提供支持。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Learning to Steer: Input-dependent Steering for Multimodal LLMs",
        "summary": "Steering has emerged as a practical approach to enable post-hoc guidance of\nLLMs towards enforcing a specific behavior. However, it remains largely\nunderexplored for multimodal LLMs (MLLMs); furthermore, existing steering\ntechniques, such as mean steering, rely on a single steering vector, applied\nindependently of the input query. This paradigm faces limitations when the\ndesired behavior is dependent on the example at hand. For example, a safe\nanswer may consist in abstaining from answering when asked for an illegal\nactivity, or may point to external resources or consultation with an expert\nwhen asked about medical advice. In this paper, we investigate a fine-grained\nsteering that uses an input-specific linear shift. This shift is computed using\ncontrastive input-specific prompting. However, the input-specific prompts\nrequired for this approach are not known at test time. Therefore, we propose to\ntrain a small auxiliary module to predict the input-specific steering vector.\nOur approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces\nhallucinations and enforces safety in MLLMs, outperforming other static\nbaselines.",
        "url": "http://arxiv.org/abs/2508.12815v1",
        "published_date": "2025-08-18T10:53:20+00:00",
        "updated_date": "2025-08-18T10:53:20+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Jayneel Parekh",
            "Pegah Khayatan",
            "Mustafa Shukor",
            "Arnaud Dapogny",
            "Alasdair Newson",
            "Matthieu Cord"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality"
        ],
        "tldr": "The paper introduces a new approach called Learn-to-Steer (L2S) for fine-grained steering in multimodal Language Models (LLMs), improving safety and reducing hallucinations.",
        "tldr_zh": "本文介绍了一种名为Learn-to-Steer (L2S)的新方法，用于多模态语言模型（LLMs）的精细化引导，提高安全性并减少幻觉。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Next Visual Granularity Generation",
        "summary": "We propose a novel approach to image generation by decomposing an image into\na structured sequence, where each element in the sequence shares the same\nspatial resolution but differs in the number of unique tokens used, capturing\ndifferent level of visual granularity. Image generation is carried out through\nour newly introduced Next Visual Granularity (NVG) generation framework, which\ngenerates a visual granularity sequence beginning from an empty image and\nprogressively refines it, from global layout to fine details, in a structured\nmanner. This iterative process encodes a hierarchical, layered representation\nthat offers fine-grained control over the generation process across multiple\ngranularity levels. We train a series of NVG models for class-conditional image\ngeneration on the ImageNet dataset and observe clear scaling behavior. Compared\nto the VAR series, NVG consistently outperforms it in terms of FID scores (3.30\n-> 3.03, 2.57 ->2.44, 2.09 -> 2.06). We also conduct extensive analysis to\nshowcase the capability and potential of the NVG framework. Our code and models\nwill be released.",
        "url": "http://arxiv.org/abs/2508.12811v1",
        "published_date": "2025-08-18T10:47:37+00:00",
        "updated_date": "2025-08-18T10:47:37+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Yikai Wang",
            "Zhouxia Wang",
            "Zhonghua Wu",
            "Qingyi Tao",
            "Kang Liao",
            "Chen Change Loy"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a Next Visual Granularity (NVG) generation framework for image generation, offering fine-grained control over multiple granularity levels and outperforming existing models on the ImageNet dataset.",
        "tldr_zh": "本文引入了一种新的图像生成框架Next Visual Granularity（NVG），在ImageNet数据集上表现出色，提供对多个粒度级别的精细控制。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Vehicle detection from GSV imagery: Predicting travel behaviour for cycling and motorcycling using Computer Vision",
        "summary": "Transportation influence health by shaping exposure to physical activity, air\npollution and injury risk.Comparative data on cycling and motorcycling\nbehaviours is scarce, particularly at a global scale.Street view imagery, such\nas Google Street View (GSV), combined with computer vision, is a valuable\nresource for efficiently capturing travel behaviour data.This study\ndemonstrates a novel approach using deep learning on street view images to\nestimate cycling and motorcycling levels across diverse cities worldwide.We\nutilized data from 185 global cities.The data on mode shares of cycling and\nmotorcycling estimated using travel surveys or censuses.We used GSV images to\ndetect cycles and motorcycles in sampled locations, using 8000 images per\ncity.The YOLOv4 model, fine-tuned using images from six cities, achieved a mean\naverage precision of 89% for detecting cycles and motorcycles in GSV images.A\nglobal prediction model was developed using beta regression with city-level\nmode shares as outcome, with log transformed explanatory variables of counts of\nGSV-detected images with cycles and motorcycles, while controlling for\npopulation density.We found strong correlations between GSV motorcycle counts\nand motorcycle mode share (0.78) and moderate correlations between GSV cycle\ncounts and cycling mode share (0.51).Beta regression models predicted mode\nshares with $R^2$ values of 0.614 for cycling and 0.612 for motorcycling,\nachieving median absolute errors (MDAE) of 1.3% and 1.4%,\nrespectively.Scatterplots demonstrated consistent prediction accuracy, though\ncities like Utrecht and Cali were outliers.The model was applied to 60 cities\nglobally for which we didn't have recent mode share data.We provided estimates\nfor some cities in the Middle East, Latin America and East Asia.With computer\nvision, GSV images capture travel modes and activity, providing insights\nalongside traditional data sources.",
        "url": "http://arxiv.org/abs/2508.12794v1",
        "published_date": "2025-08-18T10:17:30+00:00",
        "updated_date": "2025-08-18T10:17:30+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kyriaki",
            "Kokka",
            "Rahul Goel",
            "Ali Abbas",
            "Kerry A. Nice",
            "Luca Martial",
            "SM Labib",
            "Rihuan Ke",
            "Carola Bibiane Schönlieb",
            "James Woodcock"
        ],
        "ai_categories": [
            "Dataset",
            "Multimodality"
        ],
        "tldr": "This paper introduces a novel approach using deep learning on street view images to estimate cycling and motorcycling levels across global cities, achieving high accuracy in predicting mode shares.",
        "tldr_zh": "本文提出了一种新颖的方法，利用街景图像上的深度学习来估计全球城市的骑车和骑摩托车水平，实现了高精度预测模态份额。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Leveraging Diffusion Models for Stylization using Multiple Style Images",
        "summary": "Recent advances in latent diffusion models have enabled exciting progress in\nimage style transfer. However, several key issues remain. For example, existing\nmethods still struggle to accurately match styles. They are often limited in\nthe number of style images that can be used. Furthermore, they tend to entangle\ncontent and style in undesired ways. To address this, we propose leveraging\nmultiple style images which helps better represent style features and prevent\ncontent leaking from the style images. We design a method that leverages both\nimage prompt adapters and statistical alignment of the features during the\ndenoising process. With this, our approach is designed such that it can\nintervene both at the cross-attention and the self-attention layers of the\ndenoising UNet. For the statistical alignment, we employ clustering to distill\na small representative set of attention features from the large number of\nattention values extracted from the style samples. As demonstrated in our\nexperimental section, the resulting method achieves state-of-the-art results\nfor stylization.",
        "url": "http://arxiv.org/abs/2508.12784v1",
        "published_date": "2025-08-18T10:00:41+00:00",
        "updated_date": "2025-08-18T10:00:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dan Ruta",
            "Abdelaziz Djelouah",
            "Raphael Ortiz",
            "Christopher Schroers"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN"
        ],
        "tldr": "The paper proposes a method using multiple style images to improve image stylization by preventing content leaking and achieving state-of-the-art results.",
        "tldr_zh": "本文提出了一种使用多个风格图像改进图像样式化的方法，以防止内容泄漏并实现最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CLAIRE-DSA: Fluoroscopic Image Classification for Quality Assurance of Computer Vision Pipelines in Acute Ischemic Stroke",
        "summary": "Computer vision models can be used to assist during mechanical thrombectomy\n(MT) for acute ischemic stroke (AIS), but poor image quality often degrades\nperformance. This work presents CLAIRE-DSA, a deep learning--based framework\ndesigned to categorize key image properties in minimum intensity projections\n(MinIPs) acquired during MT for AIS, supporting downstream quality control and\nworkflow optimization. CLAIRE-DSA uses pre-trained ResNet backbone models,\nfine-tuned to predict nine image properties (e.g., presence of contrast,\nprojection angle, motion artefact severity). Separate classifiers were trained\non an annotated dataset containing $1,758$ fluoroscopic MinIPs. The model\nachieved excellent performance on all labels, with ROC-AUC ranging from $0.91$\nto $0.98$, and precision ranging from $0.70$ to $1.00$. The ability of\nCLAIRE-DSA to identify suitable images was evaluated on a segmentation task by\nfiltering poor quality images and comparing segmentation performance on\nfiltered and unfiltered datasets. Segmentation success rate increased from\n$42%$ to $69%$, $p < 0.001$. CLAIRE-DSA demonstrates strong potential as an\nautomated tool for accurately classifying image properties in DSA series of\nacute ischemic stroke patients, supporting image annotation and quality control\nin clinical and research applications. Source code is available at\nhttps://gitlab.com/icai-stroke-lab/wp3_neurointerventional_ai/claire-dsa.",
        "url": "http://arxiv.org/abs/2508.12755v1",
        "published_date": "2025-08-18T09:28:58+00:00",
        "updated_date": "2025-08-18T09:28:58+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Cristo J. van den Berg",
            "Frank G. te Nijenhuis",
            "Mirre J. Blaauboer",
            "Daan T. W. van Erp",
            "Carlijn M. Keppels",
            "Matthijs van der Sluijs",
            "Bob Roozenbeek",
            "Wim van Zwam",
            "Sandra Cornelissen",
            "Danny Ruijters",
            "Ruisheng Su",
            "Theo van Walsum"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "CLAIRE-DSA is a deep learning framework for classifying key image properties in fluoroscopic images during mechanical thrombectomy for acute ischemic stroke, showing strong potential for image annotation and quality control in clinical and research applications.",
        "tldr_zh": "CLAIRE-DSA是一个深度学习框架，用于分类急性缺血性卒中患者的数字减影血管造影(DSA)序列中的关键图像属性，对临床和研究应用中的图像标注和质量控制具有强大潜力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DCSCR: A Class-Specific Collaborative Representation based Network for Image Set Classification",
        "summary": "Image set classification (ISC), which can be viewed as a task of comparing\nsimilarities between sets consisting of unordered heterogeneous images with\nvariable quantities and qualities, has attracted growing research attention in\nrecent years. How to learn effective feature representations and how to explore\nthe similarities between different image sets are two key yet challenging\nissues in this field. However, existing traditional ISC methods classify image\nsets based on raw pixel features, ignoring the importance of feature learning.\nExisting deep ISC methods can learn deep features, but they fail to adaptively\nadjust the features when measuring set distances, resulting in limited\nperformance in few-shot ISC. To address the above issues, this paper combines\ntraditional ISC methods with deep models and proposes a novel few-shot ISC\napproach called Deep Class-specific Collaborative Representation (DCSCR)\nnetwork to simultaneously learn the frame- and concept-level feature\nrepresentations of each image set and the distance similarities between\ndifferent sets. Specifically, DCSCR consists of a fully convolutional deep\nfeature extractor module, a global feature learning module, and a\nclass-specific collaborative representation-based metric learning module. The\ndeep feature extractor and global feature learning modules are used to learn\n(local and global) frame-level feature representations, while the\nclass-specific collaborative representation-based metric learning module is\nexploit to adaptively learn the concept-level feature representation of each\nimage set and thus obtain the distance similarities between different sets by\ndeveloping a new CSCR-based contrastive loss function. Extensive experiments on\nseveral well-known few-shot ISC datasets demonstrate the effectiveness of the\nproposed method compared with some state-of-the-art image set classification\nalgorithms.",
        "url": "http://arxiv.org/abs/2508.12745v1",
        "published_date": "2025-08-18T09:09:55+00:00",
        "updated_date": "2025-08-18T09:09:55+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xizhan Gao",
            "Wei Hu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel few-shot image set classification method combining traditional and deep learning approaches to learn frame- and concept-level feature representations for improved similarity comparisons between sets.",
        "tldr_zh": "本文提出了一种新颖的少样本图像集分类方法，结合了传统和深度学习方法，学习每个图像集的帧级和概念级特征表示，以改进集合之间的相似性比较。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Frequency-Driven Inverse Kernel Prediction for Single Image Defocus Deblurring",
        "summary": "Single image defocus deblurring aims to recover an all-in-focus image from a\ndefocus counterpart, where accurately modeling spatially varying blur kernels\nremains a key challenge. Most existing methods rely on spatial features for\nkernel estimation, but their performance degrades in severely blurry regions\nwhere local high-frequency details are missing. To address this, we propose a\nFrequency-Driven Inverse Kernel Prediction network (FDIKP) that incorporates\nfrequency-domain representations to enhance structural identifiability in\nkernel modeling. Given the superior discriminative capability of the frequency\ndomain for blur modeling, we design a Dual-Branch Inverse Kernel Prediction\n(DIKP) strategy that improves the accuracy of kernel estimation while\nmaintaining stability. Moreover, considering the limited number of predicted\ninverse kernels, we introduce a Position Adaptive Convolution (PAC) to enhance\nthe adaptability of the deconvolution process. Finally, we propose a\nDual-Domain Scale Recurrent Module (DSRM) to fuse deconvolution results and\nprogressively improve deblurring quality from coarse to fine. Extensive\nexperiments demonstrate that our method outperforms existing approaches. Code\nwill be made publicly available.",
        "url": "http://arxiv.org/abs/2508.12736v1",
        "published_date": "2025-08-18T09:01:13+00:00",
        "updated_date": "2025-08-18T09:01:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ying Zhang",
            "Xiongxin Tang",
            "Chongyi Li",
            "Qiao Chen",
            "Yuquan Wu"
        ],
        "ai_categories": [
            "GAN",
            "Transformer"
        ],
        "tldr": "The paper proposes a Frequency-Driven Inverse Kernel Prediction network for single image defocus deblurring, which outperforms existing methods in accurately modeling spatially varying blur kernels.",
        "tldr_zh": "该论文提出了一种针对单幅图像虚焦去模糊的频率驱动逆核预测网络，其在准确建模空间变化模糊核方面优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian Splatting",
        "summary": "3D Gaussian Splatting (3DGS) has demonstrated impressive performance in novel\nview synthesis under dense-view settings. However, in sparse-view scenarios,\ndespite the realistic renderings in training views, 3DGS occasionally manifests\nappearance artifacts in novel views. This paper investigates the appearance\nartifacts in sparse-view 3DGS and uncovers a core limitation of current\napproaches: the optimized Gaussians are overly-entangled with one another to\naggressively fit the training views, which leads to a neglect of the real\nappearance distribution of the underlying scene and results in appearance\nartifacts in novel views. The analysis is based on a proposed metric, termed\nCo-Adaptation Score (CA), which quantifies the entanglement among Gaussians,\ni.e., co-adaptation, by computing the pixel-wise variance across multiple\nrenderings of the same viewpoint, with different random subsets of Gaussians.\nThe analysis reveals that the degree of co-adaptation is naturally alleviated\nas the number of training views increases. Based on the analysis, we propose\ntwo lightweight strategies to explicitly mitigate the co-adaptation in\nsparse-view 3DGS: (1) random gaussian dropout; (2) multiplicative noise\ninjection to the opacity. Both strategies are designed to be plug-and-play, and\ntheir effectiveness is validated across various methods and benchmarks. We hope\nthat our insights into the co-adaptation effect will inspire the community to\nachieve a more comprehensive understanding of sparse-view 3DGS.",
        "url": "http://arxiv.org/abs/2508.12720v1",
        "published_date": "2025-08-18T08:34:49+00:00",
        "updated_date": "2025-08-18T08:34:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kangjie Chen",
            "Yingji Zhong",
            "Zhihao Li",
            "Jiaqi Lin",
            "Youyu Chen",
            "Minghan Qin",
            "Haoqian Wang"
        ],
        "ai_categories": [
            "GAN",
            "Other"
        ],
        "tldr": "The paper investigates appearance artifacts in sparse-view 3D Gaussian Splatting and proposes strategies to mitigate co-adaptation effects.",
        "tldr_zh": "本文研究了稀疏视图3D高斯喷溅中的外观伪像，并提出了缓解共适应效应的策略。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Single-Reference Text-to-Image Manipulation with Dual Contrastive Denoising Score",
        "summary": "Large-scale text-to-image generative models have shown remarkable ability to\nsynthesize diverse and high-quality images. However, it is still challenging to\ndirectly apply these models for editing real images for two reasons. First, it\nis difficult for users to come up with a perfect text prompt that accurately\ndescribes every visual detail in the input image. Second, while existing models\ncan introduce desirable changes in certain regions, they often dramatically\nalter the input content and introduce unexpected changes in unwanted regions.\nTo address these challenges, we present Dual Contrastive Denoising Score, a\nsimple yet powerful framework that leverages the rich generative prior of\ntext-to-image diffusion models. Inspired by contrastive learning approaches for\nunpaired image-to-image translation, we introduce a straightforward dual\ncontrastive loss within the proposed framework. Our approach utilizes the\nextensive spatial information from the intermediate representations of the\nself-attention layers in latent diffusion models without depending on auxiliary\nnetworks. Our method achieves both flexible content modification and structure\npreservation between input and output images, as well as zero-shot\nimage-to-image translation. Through extensive experiments, we show that our\napproach outperforms existing methods in real image editing while maintaining\nthe capability to directly utilize pretrained text-to-image diffusion models\nwithout further training.",
        "url": "http://arxiv.org/abs/2508.12718v1",
        "published_date": "2025-08-18T08:30:07+00:00",
        "updated_date": "2025-08-18T08:30:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Syed Muhmmad Israr",
            "Feng Zhao"
        ],
        "ai_categories": [
            "GAN",
            "Transformer"
        ],
        "tldr": "The paper introduces a framework called Dual Contrastive Denoising Score to improve text-to-image editing by balancing content modification and structure preservation.",
        "tldr_zh": "本文提出了一种名为Dual Contrastive Denoising Score的框架，通过平衡内容修改和结构保存来改进文本到图像编辑。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Real-Time Sign Language Gestures to Speech Transcription using Deep Learning",
        "summary": "Communication barriers pose significant challenges for individuals with\nhearing and speech impairments, often limiting their ability to effectively\ninteract in everyday environments. This project introduces a real-time\nassistive technology solution that leverages advanced deep learning techniques\nto translate sign language gestures into textual and audible speech. By\nemploying convolution neural networks (CNN) trained on the Sign Language MNIST\ndataset, the system accurately classifies hand gestures captured live via\nwebcam. Detected gestures are instantaneously translated into their\ncorresponding meanings and transcribed into spoken language using\ntext-to-speech synthesis, thus facilitating seamless communication.\nComprehensive experiments demonstrate high model accuracy and robust real-time\nperformance with some latency, highlighting the system's practical\napplicability as an accessible, reliable, and user-friendly tool for enhancing\nthe autonomy and integration of sign language users in diverse social settings.",
        "url": "http://arxiv.org/abs/2508.12713v1",
        "published_date": "2025-08-18T08:25:18+00:00",
        "updated_date": "2025-08-18T08:25:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Brandone Fonya"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "GAN"
        ],
        "tldr": "The paper introduces a real-time assistive technology using deep learning to translate sign language gestures into spoken language, enhancing communication for individuals with hearing and speech impairments.",
        "tldr_zh": "本文介绍了一种利用深度学习将手语手势翻译为口头语言的实时辅助技术，提高了听力和言语障碍者的交流能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Argos: A Decentralized Federated System for Detection of Traffic Signs in CAVs",
        "summary": "Connected and automated vehicles generate vast amounts of sensor data daily,\nraising significant privacy and communication challenges for centralized\nmachine learning approaches in perception tasks. This study presents a\ndecentralized, federated learning framework tailored for traffic sign detection\nin vehicular networks to enable collaborative model training without sharing\nraw data. The framework partitioned traffic sign classes across vehicles for\nspecialized local training using lightweight object detectors, aggregated model\nparameters via algorithms like FedProx, FedAdam and FedAVG in a simulated\nenvironment with the Flower framework, and evaluated multiple configurations\nincluding varying server rounds, local epochs, client participation fractions,\nand data distributions. Experiments demonstrated that increasing server rounds\nfrom 2 to 20 boosted accuracy from below 0.1 to over 0.8, moderate local epochs\n(8-10) provided optimal efficiency with accuracies around 0.67, higher client\nparticipation fractions enhanced generalization up to 0.83, FedProx\noutperformed other aggregators in handling heterogeneity, non-IID data\ndistributions reduced performance compared to IID, and training duration\nprimarily scaled with the number of rounds rather than aggregation strategy. We\nconclude that this federated approach may offer a scalable, privacy-preserving\nsolution for real-world vehicular deployments, potentially guiding future\nintegrations of robust aggregation and communication optimizations to advance\nintelligent transportation systems.",
        "url": "http://arxiv.org/abs/2508.12712v1",
        "published_date": "2025-08-18T08:22:57+00:00",
        "updated_date": "2025-08-18T08:22:57+00:00",
        "categories": [
            "cs.LG",
            "cs.CV",
            "I.2.6; I.4.8"
        ],
        "authors": [
            "Seyed Mahdi Haji Seyed Hossein",
            "Alireza Hosseini",
            "Soheil Hajian Manesh",
            "Amirali Shahriary"
        ],
        "ai_categories": [
            "LoRA",
            "Dataset"
        ],
        "tldr": "The paper presents a decentralized federated learning framework for traffic sign detection in connected and automated vehicles without sharing raw data, showing promising results for real-world vehicular deployments.",
        "tldr_zh": "本文提出了一个去中心化的联邦学习框架，用于联网和自动驾驶车辆的交通标志检测，展示了在真实世界车辆部署中的良好结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection",
        "summary": "The proliferation of multimodal misinformation poses growing threats to\npublic discourse and societal trust. While Large Vision-Language Models (LVLMs)\nhave enabled recent progress in multimodal misinformation detection (MMD), the\nrise of generative AI (GenAI) tools introduces a new challenge: GenAI-driven\nnews diversity, characterized by highly varied and complex content. We show\nthat this diversity induces multi-level drift, comprising (1) model-level\nmisperception drift, where stylistic variations disrupt a model's internal\nreasoning, and (2) evidence-level drift, where expression diversity degrades\nthe quality or relevance of retrieved external evidence. These drifts\nsignificantly degrade the robustness of current LVLM-based MMD systems. To\nsystematically study this problem, we introduce DriftBench, a large-scale\nbenchmark comprising 16,000 news instances across six categories of\ndiversification. We design three evaluation tasks: (1) robustness of truth\nverification under multi-level drift; (2) susceptibility to adversarial\nevidence contamination generated by GenAI; and (3) analysis of reasoning\nconsistency across diverse inputs. Experiments with six state-of-the-art\nLVLM-based detectors show substantial performance drops (average F1 -14.8%) and\nincreasingly unstable reasoning traces, with even more severe failures under\nadversarial evidence injection. Our findings uncover fundamental\nvulnerabilities in existing MMD systems and suggest an urgent need for more\nresilient approaches in the GenAI era.",
        "url": "http://arxiv.org/abs/2508.12711v1",
        "published_date": "2025-08-18T08:19:43+00:00",
        "updated_date": "2025-08-18T08:19:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fanxiao Li",
            "Jiaying Wu",
            "Tingchao Fu",
            "Yunyun Dong",
            "Bingbing Song",
            "Wei Zhou"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper discusses the challenges of detecting misinformation in news articles due to the diversity introduced by generative AI tools, which significantly degrades the performance of existing models.",
        "tldr_zh": "本文讨论了由生成AI工具引入的多样性所导致的新闻文章中误信息检测的挑战，这显著降低了现有模型的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Neural Rendering for Sensor Adaptation in 3D Object Detection",
        "summary": "Autonomous vehicles often have varying camera sensor setups, which is\ninevitable due to restricted placement options for different vehicle types.\nTraining a perception model on one particular setup and evaluating it on a new,\ndifferent sensor setup reveals the so-called cross-sensor domain gap, typically\nleading to a degradation in accuracy. In this paper, we investigate the impact\nof the cross-sensor domain gap on state-of-the-art 3D object detectors. To this\nend, we introduce CamShift, a dataset inspired by nuScenes and created in CARLA\nto specifically simulate the domain gap between subcompact vehicles and sport\nutility vehicles (SUVs). Using CamShift, we demonstrate significant\ncross-sensor performance degradation, identify robustness dependencies on model\narchitecture, and propose a data-driven solution to mitigate the effect. On the\none hand, we show that model architectures based on a dense Bird's Eye View\n(BEV) representation with backward projection, such as BEVFormer, are the most\nrobust against varying sensor configurations. On the other hand, we propose a\nnovel data-driven sensor adaptation pipeline based on neural rendering, which\ncan transform entire datasets to match different camera sensor setups. Applying\nthis approach improves performance across all investigated 3D object detectors,\nmitigating the cross-sensor domain gap by a large margin and reducing the need\nfor new data collection by enabling efficient data reusability across vehicles\nwith different sensor setups. The CamShift dataset and the sensor adaptation\nbenchmark are available at https://dmholtz.github.io/camshift/.",
        "url": "http://arxiv.org/abs/2508.12695v1",
        "published_date": "2025-08-18T07:53:45+00:00",
        "updated_date": "2025-08-18T07:53:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Felix Embacher",
            "David Holtz",
            "Jonas Uhrig",
            "Marius Cordts",
            "Markus Enzweiler"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper investigates the impact of different camera sensor setups on 3D object detection in autonomous vehicles and proposes a data-driven solution using neural rendering to adapt to varying sensor configurations.",
        "tldr_zh": "本文研究了不同相机传感器设置对自动驾驶车辆中的3D目标检测的影响，并提出了使用神经渲染的数据驱动解决方案以适应不同的传感器配置。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration",
        "summary": "Leveraging the Transformer architecture and the diffusion process, video DiT\nmodels have emerged as a dominant approach for high-quality video generation.\nHowever, their multi-step iterative denoising process incurs high computational\ncost and inference latency. Caching, a widely adopted optimization method in\nDiT models, leverages the redundancy in the diffusion process to skip\ncomputations in different granularities (e.g., step, cfg, block). Nevertheless,\nexisting caching methods are limited to single-granularity strategies,\nstruggling to balance generation quality and inference speed in a flexible\nmanner. In this work, we propose MixCache, a training-free caching-based\nframework for efficient video DiT inference. It first distinguishes the\ninterference and boundary between different caching strategies, and then\nintroduces a context-aware cache triggering strategy to determine when caching\nshould be enabled, along with an adaptive hybrid cache decision strategy for\ndynamically selecting the optimal caching granularity. Extensive experiments on\ndiverse models demonstrate that, MixCache can significantly accelerate video\ngeneration (e.g., 1.94$\\times$ speedup on Wan 14B, 1.97$\\times$ speedup on\nHunyuanVideo) while delivering both superior generation quality and inference\nefficiency compared to baseline methods.",
        "url": "http://arxiv.org/abs/2508.12691v1",
        "published_date": "2025-08-18T07:49:33+00:00",
        "updated_date": "2025-08-18T07:49:33+00:00",
        "categories": [
            "cs.GR",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yuanxin Wei",
            "Lansong Diao",
            "Bujiao Chen",
            "Shenggan Cheng",
            "Zhengping Qian",
            "Wenyuan Yu",
            "Nong Xiao",
            "Wei Lin",
            "Jiangsu Du"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer",
            "GAN"
        ],
        "tldr": "MixCache proposes a caching-based framework to accelerate video DiT inference by dynamically selecting the optimal caching granularity, achieving significant speedups without compromising generation quality.",
        "tldr_zh": "MixCache 提出了一种基于缓存的框架，通过动态选择最佳缓存粒度来加速视频 DiT 推断，实现了显著的加速而不影响生成质量。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding",
        "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance in complex multimodal tasks. While MLLMs excel at visual perception\nand reasoning in third-person and egocentric videos, they are prone to\nhallucinations, generating coherent yet inaccurate responses. We present\nEgoIllusion, a first benchmark to evaluate MLLM hallucinations in egocentric\nvideos. EgoIllusion comprises 1,400 videos paired with 8,000 human-annotated\nopen and closed-ended questions designed to trigger hallucinations in both\nvisual and auditory cues in egocentric videos. Evaluations across ten MLLMs\nreveal significant challenges, including powerful models like GPT-4o and\nGemini, achieving only 59% accuracy. EgoIllusion lays the foundation in\ndeveloping robust benchmarks to evaluate the effectiveness of MLLMs and spurs\nthe development of better egocentric MLLMs with reduced hallucination rates.\nOur benchmark will be open-sourced for reproducibility.",
        "url": "http://arxiv.org/abs/2508.12687v1",
        "published_date": "2025-08-18T07:39:55+00:00",
        "updated_date": "2025-08-18T07:39:55+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Ashish Seth",
            "Utkarsh Tyagi",
            "Ramaneswaran Selvakumar",
            "Nishit Anand",
            "Sonal Kumar",
            "Sreyan Ghosh",
            "Ramani Duraiswami",
            "Chirag Agarwal",
            "Dinesh Manocha"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "EgoIllusion is a benchmark for evaluating hallucinations in egocentric videos by MLLMs. It highlights challenges and aims to improve MLLMs' accuracy.",
        "tldr_zh": "EgoIllusion是一个用于评估MLLMs在自我中心视频中幻觉的基准。它强调了挑战，并旨在提高MLLMs的准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Vision-G1: Towards General Vision Language Reasoning with Multi-Domain Data Curation",
        "summary": "Despite their success, current training pipelines for reasoning VLMs focus on\na limited range of tasks, such as mathematical and logical reasoning. As a\nresult, these models face difficulties in generalizing their reasoning\ncapabilities to a wide range of domains, primarily due to the scarcity of\nreadily available and verifiable reward data beyond these narrowly defined\nareas. Moreover, integrating data from multiple domains is challenging, as the\ncompatibility between domain-specific datasets remains uncertain. To address\nthese limitations, we build a comprehensive RL-ready visual reasoning dataset\nfrom 46 data sources across 8 dimensions, covering a wide range of tasks such\nas infographic, mathematical, spatial, cross-image, graphic user interface,\nmedical, common sense and general science. We propose an influence function\nbased data selection and difficulty based filtering strategy to identify\nhigh-quality training samples from this dataset. Subsequently, we train the\nVLM, referred to as Vision-G1, using multi-round RL with a data curriculum to\niteratively improve its visual reasoning capabilities. Our model achieves\nstate-of-the-art performance across various visual reasoning benchmarks,\noutperforming similar-sized VLMs and even proprietary models like GPT-4o and\nGemini-1.5 Flash. The model, code and dataset are publicly available at\nhttps://github.com/yuh-zha/Vision-G1.",
        "url": "http://arxiv.org/abs/2508.12680v1",
        "published_date": "2025-08-18T07:24:33+00:00",
        "updated_date": "2025-08-18T07:24:33+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Yuheng Zha",
            "Kun Zhou",
            "Yujia Wu",
            "Yushu Wang",
            "Jie Feng",
            "Zhi Xu",
            "Shibo Hao",
            "Zhengzhong Liu",
            "Eric P. Xing",
            "Zhiting Hu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces Vision-G1, a visual language model trained on a diverse dataset for general visual reasoning tasks, achieving state-of-the-art performance on various benchmarks.",
        "tldr_zh": "本文介绍了Vision-G1，一种在多样数据集上进行训练的视觉语言模型，实现了各种基准测试的最新性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Stable Diffusion-Based Approach for Human De-Occlusion",
        "summary": "Humans can infer the missing parts of an occluded object by leveraging prior\nknowledge and visible cues. However, enabling deep learning models to\naccurately predict such occluded regions remains a challenging task.\nDe-occlusion addresses this problem by reconstructing both the mask and RGB\nappearance. In this work, we focus on human de-occlusion, specifically\ntargeting the recovery of occluded body structures and appearances. Our\napproach decomposes the task into two stages: mask completion and RGB\ncompletion. The first stage leverages a diffusion-based human body prior to\nprovide a comprehensive representation of body structure, combined with\noccluded joint heatmaps that offer explicit spatial cues about missing regions.\nThe reconstructed amodal mask then serves as a conditioning input for the\nsecond stage, guiding the model on which areas require RGB reconstruction. To\nfurther enhance RGB generation, we incorporate human-specific textual features\nderived using a visual question answering (VQA) model and encoded via a CLIP\nencoder. RGB completion is performed using Stable Diffusion, with decoder\nfine-tuning applied to mitigate pixel-level degradation in visible regions -- a\nknown limitation of prior diffusion-based de-occlusion methods caused by latent\nspace transformations. Our method effectively reconstructs human appearances\neven under severe occlusions and consistently outperforms existing methods in\nboth mask and RGB completion. Moreover, the de-occluded images generated by our\napproach can improve the performance of downstream human-centric tasks, such as\n2D pose estimation and 3D human reconstruction. The code will be made publicly\navailable.",
        "url": "http://arxiv.org/abs/2508.12663v1",
        "published_date": "2025-08-18T06:53:29+00:00",
        "updated_date": "2025-08-18T06:53:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Seung Young Noh",
            "Ju Yong Chang"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper presents a stable diffusion-based approach for human de-occlusion, focusing on reconstructing both the mask and RGB appearance of occluded body structures. The method leverages diffusion-based prior, human-specific textual features, and Stable Diffusion for improved RGB generation.",
        "tldr_zh": "本文提出了一种稳定的基于扩散的方法用于人类去遮挡，重点是重建被遮挡的身体结构的掩模和RGB外观。该方法利用基于扩散的先验知识、人类特定的文本特征和稳定扩散来改进RGB生成。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Synthesizing Accurate and Realistic T1-weighted Contrast-Enhanced MR Images using Posterior-Mean Rectified Flow",
        "summary": "Contrast-enhanced (CE) T1-weighted MRI is central to neuro-oncologic\ndiagnosis but requires gadolinium-based agents, which add cost and scan time,\nraise environmental concerns, and may pose risks to patients. In this work, we\npropose a two-stage Posterior-Mean Rectified Flow (PMRF) pipeline for\nsynthesizing volumetric CE brain MRI from non-contrast inputs. First, a\npatch-based 3D U-Net predicts the voxel-wise posterior mean (minimizing MSE).\nThen, this initial estimate is refined by a time-conditioned 3D rectified flow\nto incorporate realistic textures without compromising structural fidelity. We\ntrain this model on a multi-institutional collection of paired pre- and\npost-contrast T1w volumes (BraTS 2023-2025). On a held-out test set of 360\ndiverse volumes, our best refined outputs achieve an axial FID of $12.46$ and\nKID of $0.007$ ($\\sim 68.7\\%$ lower FID than the posterior mean) while\nmaintaining low volumetric MSE of $0.057$ ($\\sim 27\\%$ higher than the\nposterior mean). Qualitative comparisons confirm that our method restores\nlesion margins and vascular details realistically, effectively navigating the\nperception-distortion trade-off for clinical deployment.",
        "url": "http://arxiv.org/abs/2508.12640v1",
        "published_date": "2025-08-18T05:55:57+00:00",
        "updated_date": "2025-08-18T05:55:57+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Bastian Brandstötter",
            "Erich Kobler"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a method for synthesizing realistic contrast-enhanced brain MRI images from non-contrast inputs using a two-stage pipeline, achieving high-quality results for clinical deployment.",
        "tldr_zh": "该论文提出了一种通过两阶段流程从非对比输入合成逼真的增强脑MRI图像的方法，实现了高质量结果，可用于临床部署。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SpotVLM: Cloud-edge Collaborative Real-time VLM based on Context Transfer",
        "summary": "Vision-Language Models (VLMs) are increasingly deployed in real-time\napplications such as autonomous driving and human-computer interaction, which\ndemand fast and reliable responses based on accurate perception. To meet these\nrequirements, existing systems commonly employ cloud-edge collaborative\narchitectures, such as partitioned Large Vision-Language Models (LVLMs) or task\noffloading strategies between Large and Small Vision-Language Models (SVLMs).\nHowever, these methods fail to accommodate cloud latency fluctuations and\noverlook the full potential of delayed but accurate LVLM responses. In this\nwork, we propose a novel cloud-edge collaborative paradigm for VLMs, termed\nContext Transfer, which treats the delayed outputs of LVLMs as historical\ncontext to provide real-time guidance for SVLMs inference. Based on this\nparadigm, we design SpotVLM, which incorporates both context replacement and\nvisual focus modules to refine historical textual input and enhance visual\ngrounding consistency. Extensive experiments on three real-time vision tasks\nacross four datasets demonstrate the effectiveness of the proposed framework.\nThe new paradigm lays the groundwork for more effective and latency-aware\ncollaboration strategies in future VLM systems.",
        "url": "http://arxiv.org/abs/2508.12638v1",
        "published_date": "2025-08-18T05:51:41+00:00",
        "updated_date": "2025-08-18T05:51:41+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Chen Qian",
            "Xinran Yu",
            "Zewen Huang",
            "Danyang Li",
            "Qiang Ma",
            "Fan Dang",
            "Xuan Ding",
            "Guangyong Shang",
            "Zheng Yang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a new cloud-edge collaborative paradigm for Vision-Language Models, called Context Transfer, improving real-time inference by using delayed responses as historical context.",
        "tldr_zh": "本文提出了一种新的云边协作范式，名为Context Transfer，通过将延迟响应作为历史上下文，提高了视觉语言模型的实时推理能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Creative4U: MLLMs-based Advertising Creative Image Selector with Comparative Reasoning",
        "summary": "Creative image in advertising is the heart and soul of e-commerce platform.\nAn eye-catching creative image can enhance the shopping experience for users,\nboosting income for advertisers and advertising revenue for platforms. With the\nadvent of AIGC technology, advertisers can produce large quantities of creative\nimages at minimal cost. However, they struggle to assess the creative quality\nto select. Existing methods primarily focus on creative ranking, which fails to\naddress the need for explainable creative selection.\n  In this work, we propose the first paradigm for explainable creative\nassessment and selection. Powered by multimodal large language models (MLLMs),\nour approach integrates the assessment and selection of creative images into a\nnatural language generation task. To facilitate this research, we construct\nCreativePair, the first comparative reasoning-induced creative dataset\nfeaturing 8k annotated image pairs, with each sample including a label\nindicating which image is superior. Additionally, we introduce Creative4U\n(pronounced Creative for You), a MLLMs-based creative selector that takes into\naccount users' interests. Through Reason-to-Select RFT, which includes\nsupervised fine-tuning with Chain-of-Thought (CoT-SFT) and Group Relative\nPolicy Optimization (GRPO) based reinforcement learning, Creative4U is able to\nevaluate and select creative images accurately. Both offline and online\nexperiments demonstrate the effectiveness of our approach. Our code and dataset\nwill be made public to advance research and industrial applications.",
        "url": "http://arxiv.org/abs/2508.12628v1",
        "published_date": "2025-08-18T05:11:30+00:00",
        "updated_date": "2025-08-18T05:11:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yukang Lin",
            "Xiang Zhang",
            "Shichang Jia",
            "Bowen Wan",
            "Chenghan Fu",
            "Xudong Ren",
            "Yueran Liu",
            "Wanxian Guan",
            "Pengji Wang",
            "Jian Xu",
            "Bo Zheng",
            "Baolin Liu"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces Creative4U, a system using MLLMs for selecting creative images in advertising based on comparative reasoning and user interests.",
        "tldr_zh": "本文介绍了Creative4U，一个利用MLLMs进行广告创意图像选择的系统，基于比较推理和用户兴趣。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "WIPES: Wavelet-based Visual Primitives",
        "summary": "Pursuing a continuous visual representation that offers flexible frequency\nmodulation and fast rendering speed has recently garnered increasing attention\nin the fields of 3D vision and graphics. However, existing representations\noften rely on frequency guidance or complex neural network decoding, leading to\nspectrum loss or slow rendering. To address these limitations, we propose\nWIPES, a universal Wavelet-based vIsual PrimitivES for representing\nmulti-dimensional visual signals. Building on the spatial-frequency\nlocalization advantages of wavelets, WIPES effectively captures both the\nlow-frequency \"forest\" and the high-frequency \"trees.\" Additionally, we develop\na wavelet-based differentiable rasterizer to achieve fast visual rendering.\nExperimental results on various visual tasks, including 2D image\nrepresentation, 5D static and 6D dynamic novel view synthesis, demonstrate that\nWIPES, as a visual primitive, offers higher rendering quality and faster\ninference than INR-based methods, and outperforms Gaussian-based\nrepresentations in rendering quality.",
        "url": "http://arxiv.org/abs/2508.12615v1",
        "published_date": "2025-08-18T04:24:01+00:00",
        "updated_date": "2025-08-18T04:24:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenhao Zhang",
            "Hao Zhu",
            "Delong Wu",
            "Di Kang",
            "Linchao Bao",
            "Zhan Ma",
            "Xun Cao"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces WIPES, a Wavelet-based Visual Primitives for multi-dimensional visual signal representation, offering high rendering quality and fast inference compared to existing methods.",
        "tldr_zh": "本文介绍了WIPES，一种基于小波的视觉原语，用于多维视觉信号表示，相较于现有方法，提供了更高的渲染质量和更快的推断速度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ViLaD: A Large Vision Language Diffusion Framework for End-to-End Autonomous Driving",
        "summary": "End-to-end autonomous driving systems built on Vision Language Models (VLMs)\nhave shown significant promise, yet their reliance on autoregressive\narchitectures introduces some limitations for real-world applications. The\nsequential, token-by-token generation process of these models results in high\ninference latency and cannot perform bidirectional reasoning, making them\nunsuitable for dynamic, safety-critical environments. To overcome these\nchallenges, we introduce ViLaD, a novel Large Vision Language Diffusion (LVLD)\nframework for end-to-end autonomous driving that represents a paradigm shift.\nViLaD leverages a masked diffusion model that enables parallel generation of\nentire driving decision sequences, significantly reducing computational\nlatency. Moreover, its architecture supports bidirectional reasoning, allowing\nthe model to consider both past and future simultaneously, and supports\nprogressive easy-first generation to iteratively improve decision quality. We\nconduct comprehensive experiments on the nuScenes dataset, where ViLaD\noutperforms state-of-the-art autoregressive VLM baselines in both planning\naccuracy and inference speed, while achieving a near-zero failure rate.\nFurthermore, we demonstrate the framework's practical viability through a\nreal-world deployment on an autonomous vehicle for an interactive parking task,\nconfirming its effectiveness and soundness for practical applications.",
        "url": "http://arxiv.org/abs/2508.12603v1",
        "published_date": "2025-08-18T04:01:56+00:00",
        "updated_date": "2025-08-18T04:01:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Can Cui",
            "Yupeng Zhou",
            "Juntong Peng",
            "Sung-Yeon Park",
            "Zichong Yang",
            "Prashanth Sankaranarayanan",
            "Jiaru Zhang",
            "Ruqi Zhang",
            "Ziran Wang"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "ViLaD is a novel framework for end-to-end autonomous driving that leverages a masked diffusion model to reduce computational latency and improve decision quality.",
        "tldr_zh": "ViLaD是一个用于端到端自动驾驶的新框架，利用掩蔽扩散模型来减少计算延迟并提高决策质量。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multimodal Chain of Continuous Thought for Latent-Space Reasoning in Vision-Language Models",
        "summary": "Many reasoning techniques for large multimodal models adapt language model\napproaches, such as Chain-of-Thought (CoT) prompting, which express reasoning\nas word sequences. While effective for text, these methods are suboptimal for\nmultimodal contexts, struggling to align audio, visual, and textual information\ndynamically. To explore an alternative paradigm, we propose the Multimodal\nChain of Continuous Thought (MCOUT), which enables reasoning directly in a\njoint latent space rather than in natural language. In MCOUT, the reasoning\nstate is represented as a continuous hidden vector, iteratively refined and\naligned with visual and textual embeddings, inspired by human reflective\ncognition. We develop two variants: MCOUT-Base, which reuses the language\nmodel`s last hidden state as the continuous thought for iterative reasoning,\nand MCOUT-Multi, which integrates multimodal latent attention to strengthen\ncross-modal alignment between visual and textual features. Experiments on\nbenchmarks including MMMU, ScienceQA, and MMStar show that MCOUT consistently\nimproves multimodal reasoning, yielding up to 8.23% accuracy gains over strong\nbaselines and improving BLEU scores up to 8.27% across multiple-choice and\nopen-ended tasks. These findings highlight latent continuous reasoning as a\npromising direction for advancing LMMs beyond language-bound CoT, offering a\nscalable framework for human-like reflective multimodal inference. Code is\navailable at https://github.com/Hanhpt23/OmniMod.",
        "url": "http://arxiv.org/abs/2508.12587v1",
        "published_date": "2025-08-18T02:50:20+00:00",
        "updated_date": "2025-08-18T02:50:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tan-Hanh Pham",
            "Chris Ngo"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes a new reasoning paradigm, Multimodal Chain of Continuous Thought (MCOUT), for latent-space reasoning in vision-language models, improving multimodal reasoning accuracy and alignment between visual and textual features.",
        "tldr_zh": "本文提出了一种新的推理范式，即多模态连续思考链（MCOUT），用于视觉-语言模型中的潜空间推理，提高了多模态推理准确性和视觉文本特征的对齐。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Structure-preserving Feature Alignment for Old Photo Colorization",
        "summary": "Deep learning techniques have made significant advancements in\nreference-based colorization by training on large-scale datasets. However,\ndirectly applying these methods to the task of colorizing old photos is\nchallenging due to the lack of ground truth and the notorious domain gap\nbetween natural gray images and old photos. To address this issue, we propose a\nnovel CNN-based algorithm called SFAC, i.e., Structure-preserving Feature\nAlignment Colorizer. SFAC is trained on only two images for old photo\ncolorization, eliminating the reliance on big data and allowing direct\nprocessing of the old photo itself to overcome the domain gap problem. Our\nprimary objective is to establish semantic correspondence between the two\nimages, ensuring that semantically related objects have similar colors. We\nachieve this through a feature distribution alignment loss that remains robust\nto different metric choices. However, utilizing robust semantic correspondence\nto transfer color from the reference to the old photo can result in inevitable\nstructure distortions. To mitigate this, we introduce a structure-preserving\nmechanism that incorporates a perceptual constraint at the feature level and a\nfrozen-updated pyramid at the pixel level. Extensive experiments demonstrate\nthe effectiveness of our method for old photo colorization, as confirmed by\nqualitative and quantitative metrics.",
        "url": "http://arxiv.org/abs/2508.12570v1",
        "published_date": "2025-08-18T02:10:40+00:00",
        "updated_date": "2025-08-18T02:10:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yingxue Pang",
            "Xin Jin",
            "Jun Fu",
            "Zhibo Chen"
        ],
        "ai_categories": [
            "GAN",
            "Other"
        ],
        "tldr": "The paper proposes a new algorithm called SFAC for colorizing old photos by establishing semantic correspondence between two images and preserving the structure of the old photos.",
        "tldr_zh": "该论文提出了一种名为SFAC的新算法，通过建立两幅图像之间的语义对应关系和保留旧照片的结构来为旧照片上色。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Anatomic Feature Fusion Model for Diagnosing Calcified Pulmonary Nodules on Chest X-Ray",
        "summary": "Accurate and timely identification of pulmonary nodules on chest X-rays can\ndifferentiate between life-saving early treatment and avoidable invasive\nprocedures. Calcification is a definitive indicator of benign nodules and is\nthe primary foundation for diagnosis. In actual practice, diagnosing pulmonary\nnodule calcification on chest X-rays predominantly depends on the physician's\nvisual assessment, resulting in significant diversity in interpretation.\nFurthermore, overlapping anatomical elements, such as ribs and spine,\ncomplicate the precise identification of calcification patterns. This study\npresents a calcification classification model that attains strong diagnostic\nperformance by utilizing fused features derived from raw images and their\nstructure-suppressed variants to reduce structural interference. We used 2,517\nlesion-free images and 656 nodule images (151 calcified nodules and 550\nnon-calcified nodules), all obtained from Ajou University Hospital. The\nsuggested model attained an accuracy of 86.52% and an AUC of 0.8889 in\ncalcification diagnosis, surpassing the model trained on raw images by 3.54%\nand 0.0385, respectively.",
        "url": "http://arxiv.org/abs/2508.12562v1",
        "published_date": "2025-08-18T01:49:46+00:00",
        "updated_date": "2025-08-18T01:49:46+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Hyeonjin Choi",
            "Yang-gon Kim",
            "Dong-yeon Yoo",
            "Ju-sung Sun",
            "Jung-won Lee"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper presents a model for diagnosing calcified pulmonary nodules on chest X-rays utilizing fused features from raw images and structure-suppressed variants, achieving high diagnostic performance.",
        "tldr_zh": "该论文提出了一种模型，用于诊断胸片上的钙化肺结节，利用原始图像和结构抑制变体的融合特征，实现了高诊断性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "REVEAL -- Reasoning and Evaluation of Visual Evidence through Aligned Language",
        "summary": "The rapid advancement of generative models has intensified the challenge of\ndetecting and interpreting visual forgeries, necessitating robust frameworks\nfor image forgery detection while providing reasoning as well as localization.\nWhile existing works approach this problem using supervised training for\nspecific manipulation or anomaly detection in the embedding space,\ngeneralization across domains remains a challenge. We frame this problem of\nforgery detection as a prompt-driven visual reasoning task, leveraging the\nsemantic alignment capabilities of large vision-language models. We propose a\nframework, `REVEAL` (Reasoning and Evaluation of Visual Evidence through\nAligned Language), that incorporates generalized guidelines. We propose two\ntangential approaches - (1) Holistic Scene-level Evaluation that relies on the\nphysics, semantics, perspective, and realism of the image as a whole and (2)\nRegion-wise anomaly detection that splits the image into multiple regions and\nanalyzes each of them. We conduct experiments over datasets from different\ndomains (Photoshop, DeepFake and AIGC editing). We compare the Vision Language\nModels against competitive baselines and analyze the reasoning provided by\nthem.",
        "url": "http://arxiv.org/abs/2508.12543v1",
        "published_date": "2025-08-18T00:42:02+00:00",
        "updated_date": "2025-08-18T00:42:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ipsita Praharaj",
            "Yukta Butala",
            "Yash Butala"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper presents a framework called REVEAL for detecting visual forgeries using vision-language models, with two approaches for evaluation. It compares these models against baselines on different datasets.",
        "tldr_zh": "本文提出了一种名为REVEAL的框架，用于使用视觉-语言模型检测视觉伪造，提出了两种评估方法，并在不同数据集上将这些模型与基线进行比较。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Toward Architecture-Agnostic Local Control of Posterior Collapse in VAEs",
        "summary": "Variational autoencoders (VAEs), one of the most widely used generative\nmodels, are known to suffer from posterior collapse, a phenomenon that reduces\nthe diversity of generated samples. To avoid posterior collapse, many prior\nworks have tried to control the influence of regularization loss. However, the\ntrade-off between reconstruction and regularization is not satisfactory. For\nthis reason, several methods have been proposed to guarantee latent\nidentifiability, which is the key to avoiding posterior collapse. However, they\nrequire structural constraints on the network architecture. For further\nclarification, we define local posterior collapse to reflect the importance of\nindividual sample points in the data space and to relax the network constraint.\nThen, we propose Latent Reconstruction(LR) loss, which is inspired by\nmathematical properties of injective and composite functions, to control\nposterior collapse without restriction to a specific architecture. We\nexperimentally evaluate our approach, which controls posterior collapse on\nvaried datasets such as MNIST, fashionMNIST, Omniglot, CelebA, and FFHQ.",
        "url": "http://arxiv.org/abs/2508.12530v1",
        "published_date": "2025-08-17T23:45:41+00:00",
        "updated_date": "2025-08-17T23:45:41+00:00",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML",
            "I.2.6"
        ],
        "authors": [
            "Hyunsoo Song",
            "Seungwhan Kim",
            "Seungkyu Lee"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a method to control posterior collapse in Variational Autoencoders without restricting the network architecture, achieving better diversity in generated samples.",
        "tldr_zh": "本文提出了一种方法，用以控制变分自动编码器中的后验坍缩，而无需限制网络架构，从而实现生成样本的更好多样性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MuSACo: Multimodal Subject-Specific Selection and Adaptation for Expression Recognition with Co-Training",
        "summary": "Personalized expression recognition (ER) involves adapting a machine learning\nmodel to subject-specific data for improved recognition of expressions with\nconsiderable interpersonal variability. Subject-specific ER can benefit\nsignificantly from multi-source domain adaptation (MSDA) methods, where each\ndomain corresponds to a specific subject, to improve model accuracy and\nrobustness. Despite promising results, state-of-the-art MSDA approaches often\noverlook multimodal information or blend sources into a single domain, limiting\nsubject diversity and failing to explicitly capture unique subject-specific\ncharacteristics. To address these limitations, we introduce MuSACo, a\nmulti-modal subject-specific selection and adaptation method for ER based on\nco-training. It leverages complementary information across multiple modalities\nand multiple source domains for subject-specific adaptation. This makes MuSACo\nparticularly relevant for affective computing applications in digital health,\nsuch as patient-specific assessment for stress or pain, where subject-level\nnuances are crucial. MuSACo selects source subjects relevant to the target and\ngenerates pseudo-labels using the dominant modality for class-aware learning,\nin conjunction with a class-agnostic loss to learn from less confident target\nsamples. Finally, source features from each modality are aligned, while only\nconfident target features are combined. Our experimental results on challenging\nmultimodal ER datasets: BioVid and StressID, show that MuSACo can outperform\nUDA (blending) and state-of-the-art MSDA methods.",
        "url": "http://arxiv.org/abs/2508.12522v1",
        "published_date": "2025-08-17T23:08:21+00:00",
        "updated_date": "2025-08-17T23:08:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Muhammad Osama Zeeshan",
            "Natacha Gillet",
            "Alessandro Lameiras Koerich",
            "Marco Pedersoli",
            "Francois Bremond",
            "Eric Granger"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Other"
        ],
        "tldr": "MuSACo is a multi-modal subject-specific selection and adaptation method for expression recognition using co-training, outperforming state-of-the-art methods on challenging multimodal datasets.",
        "tldr_zh": "MuSACo是一种多模态主题特定的选择和适应方法，利用协同训练用于表情识别，在具有挑战性的多模态数据集上表现优越。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "An Initial Study of Bird's-Eye View Generation for Autonomous Vehicles using Cross-View Transformers",
        "summary": "Bird's-Eye View (BEV) maps provide a structured, top-down abstraction that is\ncrucial for autonomous-driving perception. In this work, we employ Cross-View\nTransformers (CVT) for learning to map camera images to three BEV's channels -\nroad, lane markings, and planned trajectory - using a realistic simulator for\nurban driving. Our study examines generalization to unseen towns, the effect of\ndifferent camera layouts, and two loss formulations (focal and L1). Using\ntraining data from only a town, a four-camera CVT trained with the L1 loss\ndelivers the most robust test performance, evaluated in a new town. Overall,\nour results underscore CVT's promise for mapping camera inputs to reasonably\naccurate BEV maps.",
        "url": "http://arxiv.org/abs/2508.12520v1",
        "published_date": "2025-08-17T23:05:00+00:00",
        "updated_date": "2025-08-17T23:05:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Felipe Carlos dos Santos",
            "Eric Aislan Antonelo",
            "Gustavo Claudio Karl Couto"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper explores using Cross-View Transformers to generate Bird's-Eye View maps for autonomous vehicles, showing promising results in generalization and robustness.",
        "tldr_zh": "本文探讨了使用交叉视图变换器为自动驾驶车辆生成鸟瞰图，展示了在泛化和稳健性方面的良好结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LangVision-LoRA-NAS: Neural Architecture Search for Variable LoRA Rank in Vision Language Models",
        "summary": "Vision Language Models (VLMs) integrate visual and text modalities to enable\nmultimodal understanding and generation. These models typically combine a\nVision Transformer (ViT) as an image encoder and a Large Language Model (LLM)\nfor text generation. LoRA (Low-Rank Adaptation) is an efficient fine-tuning\nmethod to adapt pre-trained models to new tasks by introducing low-rank updates\nto their weights. While LoRA has emerged as a powerful technique for\nfine-tuning large models by introducing low-rank updates, current\nimplementations assume a fixed rank, potentially limiting flexibility and\nefficiency across diverse tasks. This paper introduces\n\\textit{LangVision-LoRA-NAS}, a novel framework that integrates Neural\nArchitecture Search (NAS) with LoRA to optimize VLMs for variable-rank\nadaptation. Our approach leverages NAS to dynamically search for the optimal\nLoRA rank configuration tailored to specific multimodal tasks, balancing\nperformance and computational efficiency. Through extensive experiments using\nthe LLaMA-3.2-11B model on several datasets, LangVision-LoRA-NAS demonstrates\nnotable improvement in model performance while reducing fine-tuning costs. Our\nBase and searched fine-tuned models on LLaMA-3.2-11B-Vision-Instruct can be\nfound\n\\href{https://huggingface.co/collections/krishnateja95/llama-32-11b-vision-instruct-langvision-lora-nas-6786cac480357a6a6fcc59ee}{\\textcolor{blue}{here}}\nand the code for LangVision-LoRA-NAS can be found\n\\href{https://github.com/krishnateja95/LangVision-NAS}{\\textcolor{blue}{here}}.",
        "url": "http://arxiv.org/abs/2508.12512v1",
        "published_date": "2025-08-17T22:19:02+00:00",
        "updated_date": "2025-08-17T22:19:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Krishna Teja Chitty-Venkata",
            "Murali Emani",
            "Venkatram Vishwanath"
        ],
        "ai_categories": [
            "LoRA",
            "Multimodality",
            "Transformer",
            "NAS",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces LangVision-LoRA-NAS, a framework combining Neural Architecture Search with LoRA for optimizing Vision Language Models by dynamically searching for the optimal LoRA rank configuration.",
        "tldr_zh": "该论文介绍了LangVision-LoRA-NAS，这是一个结合神经架构搜索和LoRA的框架，通过动态搜索最佳LoRA秩配置来优化视觉语言模型。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Design and Validation of a Responsible Artificial Intelligence-based System for the Referral of Diabetic Retinopathy Patients",
        "summary": "Diabetic Retinopathy (DR) is a leading cause of vision loss in working-age\nindividuals. Early detection of DR can reduce the risk of vision loss by up to\n95%, but a shortage of retinologists and challenges in timely examination\ncomplicate detection. Artificial Intelligence (AI) models using retinal fundus\nphotographs (RFPs) offer a promising solution. However, adoption in clinical\nsettings is hindered by low-quality data and biases that may lead AI systems to\nlearn unintended features. To address these challenges, we developed RAIS-DR, a\nResponsible AI System for DR screening that incorporates ethical principles\nacross the AI lifecycle. RAIS-DR integrates efficient convolutional models for\npreprocessing, quality assessment, and three specialized DR classification\nmodels. We evaluated RAIS-DR against the FDA-approved EyeArt system on a local\ndataset of 1,046 patients, unseen by both systems. RAIS-DR demonstrated\nsignificant improvements, with F1 scores increasing by 5-12%, accuracy by\n6-19%, and specificity by 10-20%. Additionally, fairness metrics such as\nDisparate Impact and Equal Opportunity Difference indicated equitable\nperformance across demographic subgroups, underscoring RAIS-DR's potential to\nreduce healthcare disparities. These results highlight RAIS-DR as a robust and\nethically aligned solution for DR screening in clinical settings. The code,\nweights of RAIS-DR are available at\nhttps://gitlab.com/inteligencia-gubernamental-jalisco/jalisco-retinopathy with\nRAIL.",
        "url": "http://arxiv.org/abs/2508.12506v1",
        "published_date": "2025-08-17T21:54:11+00:00",
        "updated_date": "2025-08-17T21:54:11+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "E. Ulises Moya-Sánchez",
            "Abraham Sánchez-Perez",
            "Raúl Nanclares Da Veiga",
            "Alejandro Zarate-Macías",
            "Edgar Villareal",
            "Alejandro Sánchez-Montes",
            "Edtna Jauregui-Ulloa",
            "Héctor Moreno",
            "Ulises Cortés"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper presents RAIS-DR, a Responsible AI System for DR screening, with significant improvements in performance and fairness metrics compared to existing systems.",
        "tldr_zh": "该论文介绍了RAIS-DR，一个负责任的人工智能系统，用于糖尿病视网膜病变筛查，相比现有系统在性能和公平性指标上有显著改善。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Skin Cancer Classification: Hybrid CNN-Transformer Models with KAN-Based Fusion",
        "summary": "Skin cancer classification is a crucial task in medical image analysis, where\nprecise differentiation between malignant and non-malignant lesions is\nessential for early diagnosis and treatment. In this study, we explore\nSequential and Parallel Hybrid CNN-Transformer models with Convolutional\nKolmogorov-Arnold Network (CKAN). Our approach integrates transfer learning and\nextensive data augmentation, where CNNs extract local spatial features,\nTransformers model global dependencies, and CKAN facilitates nonlinear feature\nfusion for improved representation learning. To assess generalization, we\nevaluate our models on multiple benchmark datasets (HAM10000,BCN20000 and\nPAD-UFES) under varying data distributions and class imbalances. Experimental\nresults demonstrate that hybrid CNN-Transformer architectures effectively\ncapture both spatial and contextual features, leading to improved\nclassification performance. Additionally, the integration of CKAN enhances\nfeature fusion through learnable activation functions, yielding more\ndiscriminative representations. Our proposed approach achieves competitive\nperformance in skin cancer classification, demonstrating 92.81% accuracy and\n92.47% F1-score on the HAM10000 dataset, 97.83% accuracy and 97.83% F1-score on\nthe PAD-UFES dataset, and 91.17% accuracy with 91.79% F1- score on the BCN20000\ndataset highlighting the effectiveness and generalizability of our model across\ndiverse datasets. This study highlights the significance of feature\nrepresentation and model design in advancing robust and accurate medical image\nclassification.",
        "url": "http://arxiv.org/abs/2508.12484v1",
        "published_date": "2025-08-17T19:57:34+00:00",
        "updated_date": "2025-08-17T19:57:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shubhi Agarwal",
            "Amulya Kumar Mahto"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes hybrid CNN-Transformer models with CKAN for skin cancer classification, achieving high accuracy across multiple datasets.",
        "tldr_zh": "本文提出了使用CKAN的混合CNN-Transformer模型用于皮肤癌分类，在多个数据集上实现了高准确率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision Mapping",
        "summary": "Traditional multimodal learning approaches require expensive alignment\npre-training to bridge vision and language modalities, typically projecting\nvisual features into discrete text token spaces. We challenge both fundamental\nassumptions underlying this paradigm by proposing Inverse-LLaVA, a novel\napproach that eliminates alignment pre-training entirely while inverting the\nconventional mapping direction. Rather than projecting visual features to text\nspace, our method maps text embeddings into continuous visual representation\nspace and performs fusion within transformer intermediate layers. Through\nselective additive components in attention mechanisms, we enable dynamic\nintegration of visual and textual representations without requiring massive\nimage-text alignment datasets. Comprehensive experiments across nine multimodal\nbenchmarks demonstrate nuanced performance trade-offs: Inverse-LLaVA achieves\nnotable improvements on reasoning-intensive and cognitive tasks (MM-VET: +0.2%,\nVizWiz: +1.8%, ScienceQA: +0.2%, cognitive reasoning: +27.2%), while showing\nexpected decreases in perception tasks requiring memorized visual-text\nassociations (celebrity recognition: -49.5%, OCR: -21.3%). These results\nprovide the first empirical evidence that alignment pre-training is not\nnecessary for effective multimodal learning, particularly for complex reasoning\ntasks. Our work establishes the feasibility of a new paradigm that reduces\ncomputational requirements by 45%, challenges conventional wisdom about\nmodality fusion, and opens new research directions for efficient multimodal\narchitectures that preserve modality-specific characteristics. Our project\nwebsite with code and additional resources is available at\nhttps://inverse-llava.github.io.",
        "url": "http://arxiv.org/abs/2508.12466v1",
        "published_date": "2025-08-17T18:36:04+00:00",
        "updated_date": "2025-08-17T18:36:04+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Xuhui Zhan",
            "Tyler Derr"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "Inverse-LLaVA proposes a novel approach to multimodal learning that eliminates alignment pre-training by mapping text embeddings into visual representation space, showing improved performance on reasoning tasks and reducing computational requirements.",
        "tldr_zh": "Inverse-LLaVA 提出了一种新的多模态学习方法，通过将文本嵌入映射到视觉表示空间，展示在推理任务上的性能提升，并降低计算要求。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Has GPT-5 Achieved Spatial Intelligence? An Empirical Study",
        "summary": "Multi-modal models have achieved remarkable progress in recent years.\nNevertheless, they continue to exhibit notable limitations in spatial\nunderstanding and reasoning, which are fundamental capabilities to achieving\nartificial general intelligence. With the recent release of GPT-5, allegedly\nthe most powerful AI model to date, it is timely to examine where the leading\nmodels stand on the path toward spatial intelligence. First, we propose a\ncomprehensive taxonomy of spatial tasks that unifies existing benchmarks and\ndiscuss the challenges in ensuring fair evaluation. We then evaluate\nstate-of-the-art proprietary and open-source models on eight key benchmarks, at\na cost exceeding one billion total tokens. Our empirical study reveals that (1)\nGPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)\nstill falls short of human performance across a broad spectrum of tasks.\nMoreover, we (3) identify the more challenging spatial intelligence problems\nfor multi-modal models, and (4) proprietary models do not exhibit a decisive\nadvantage when facing the most difficult problems. In addition, we conduct a\nqualitative evaluation across a diverse set of scenarios that are intuitive for\nhumans yet fail even the most advanced multi-modal models.",
        "url": "http://arxiv.org/abs/2508.13142v1",
        "published_date": "2025-08-18T17:55:17+00:00",
        "updated_date": "2025-08-18T17:55:17+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.LG",
            "cs.MM",
            "cs.RO"
        ],
        "authors": [
            "Zhongang Cai",
            "Yubo Wang",
            "Qingping Sun",
            "Ruisi Wang",
            "Chenyang Gu",
            "Wanqi Yin",
            "Zhiqian Lin",
            "Zhitao Yang",
            "Chen Wei",
            "Xuanke Shi",
            "Kewang Deng",
            "Xiaoyang Han",
            "Zukai Chen",
            "Jiaqi Li",
            "Xiangyu Fan",
            "Hanming Deng",
            "Lewei Lu",
            "Bo Li",
            "Ziwei Liu",
            "Quan Wang",
            "Dahua Lin",
            "Lei Yang"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality"
        ],
        "tldr": "The paper evaluates the spatial intelligence of GPT-5 and other models, finding that while GPT-5 shows strength, it still falls short of human performance on various tasks.",
        "tldr_zh": "本文评估了GPT-5和其他模型的空间智能，发现虽然GPT-5表现出色，但在各种任务上仍然不及人类表现。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 7.5
    },
    {
        "title": "Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy",
        "summary": "Vision-Language-Action (VLA) models frequently encounter challenges in\ngeneralizing to real-world environments due to inherent discrepancies between\nobservation and action spaces. Although training data are collected from\ndiverse camera perspectives, the models typically predict end-effector poses\nwithin the robot base coordinate frame, resulting in spatial inconsistencies.\nTo mitigate this limitation, we introduce the Observation-Centric VLA (OC-VLA)\nframework, which grounds action predictions directly in the camera observation\nspace. Leveraging the camera's extrinsic calibration matrix, OC-VLA transforms\nend-effector poses from the robot base coordinate system into the camera\ncoordinate system, thereby unifying prediction targets across heterogeneous\nviewpoints. This lightweight, plug-and-play strategy ensures robust alignment\nbetween perception and action, substantially improving model resilience to\ncamera viewpoint variations. The proposed approach is readily compatible with\nexisting VLA architectures, requiring no substantial modifications.\nComprehensive evaluations on both simulated and real-world robotic manipulation\ntasks demonstrate that OC-VLA accelerates convergence, enhances task success\nrates, and improves cross-view generalization. The code will be publicly\navailable.",
        "url": "http://arxiv.org/abs/2508.13103v1",
        "published_date": "2025-08-18T17:10:45+00:00",
        "updated_date": "2025-08-18T17:10:45+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Tianyi Zhang",
            "Haonan Duan",
            "Haoran Hao",
            "Yu Qiao",
            "Jifeng Dai",
            "Zhi Hou"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer"
        ],
        "tldr": "The paper introduces the Observation-Centric VLA framework to improve generalization in Vision-Language-Action models by grounding action predictions in the camera observation space.",
        "tldr_zh": "本文引入了观察中心VLA框架，通过在摄像机观察空间中根据地与动作预测来改善Vision-Language-Action模型的泛化能力。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads",
        "summary": "This work proposes XR-NPE, a high-throughput Mixed-precision SIMD Neural\nProcessing Engine, designed for extended reality (XR) perception workloads like\nvisual inertial odometry (VIO), object classification, and eye gaze extraction.\nXR-NPE is first to support FP4, Posit (4,1), Posit (8,0), and Posit (16,1)\nformats, with layer adaptive hybrid-algorithmic implementation supporting\nultra-low bit precision to significantly reduce memory bandwidth requirements,\nand accompanied by quantization-aware training for minimal accuracy loss. The\nproposed Reconfigurable Mantissa Multiplication and Exponent processing\nCircuitry (RMMEC) reduces dark silicon in the SIMD MAC compute engine, assisted\nby selective power gating to reduce energy consumption, providing 2.85x\nimproved arithmetic intensity. XR-NPE achieves a maximum operating frequency of\n1.72 GHz, area 0.016 mm2 , and arithmetic intensity 14 pJ at CMOS 28nm,\nreducing 42% area, 38% power compared to the best of state-of-the-art MAC\napproaches. The proposed XR-NPE based AXI-enabled Matrix-multiplication\nco-processor consumes 1.4x fewer LUTs, 1.77x fewer FFs, and provides 1.2x\nbetter energy efficiency compared to SoTA accelerators on VCU129. The proposed\nco-processor provides 23% better energy efficiency and 4% better compute\ndensity for VIO workloads. XR-NPE establishes itself as a scalable,\nprecision-adaptive compute engine for future resource-constrained XR devices.\nThe complete set for codes for results reproducibility are released publicly,\nenabling designers and researchers to readily adopt and build upon them.\nhttps://github.com/mukullokhande99/XR-NPE.",
        "url": "http://arxiv.org/abs/2508.13049v1",
        "published_date": "2025-08-18T16:13:00+00:00",
        "updated_date": "2025-08-18T16:13:00+00:00",
        "categories": [
            "cs.AR",
            "cs.AI",
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Tejas Chaudhari",
            "Akarsh J.",
            "Tanushree Dewangan",
            "Mukul Lokhande",
            "Santosh Kumar Vishvakarma"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "This paper introduces XR-NPE, a high-throughput Mixed-precision SIMD Neural Processing Engine for extended reality perception workloads, offering novel formats and adaptive algorithms to reduce memory bandwidth and energy consumption.",
        "tldr_zh": "本文介绍了XR-NPE，一种用于扩展现实感知工作负载的高吞吐量混合精度SIMD神经处理引擎，提供新颖格式和自适应算法，以减少内存带宽和能耗。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "Towards High-Resolution Industrial Image Anomaly Detection",
        "summary": "Current anomaly detection methods primarily focus on low-resolution\nscenarios. For high-resolution images, conventional downsampling often results\nin missed detections of subtle anomalous regions due to the loss of\nfine-grained discriminative information. Despite some progress, recent studies\nhave attempted to improve detection resolution by employing lightweight\nnetworks or using simple image tiling and ensemble methods. However, these\napproaches still struggle to meet the practical demands of industrial scenarios\nin terms of detection accuracy and efficiency. To address the above issues, we\npropose HiAD, a general framework for high-resolution anomaly detection. HiAD\nis capable of detecting anomalous regions of varying sizes in high-resolution\nimages under limited computational resources. Specifically, HiAD employs a\ndual-branch architecture that integrates anomaly cues across different scales\nto comprehensively capture both subtle and large-scale anomalies. Furthermore,\nit incorporates a multi-resolution feature fusion strategy to tackle the\nchallenges posed by fine-grained texture variations in high-resolution images.\nTo enhance both adaptability and efficiency, HiAD utilizes a detector pool in\nconjunction with various detector assignment strategies, enabling detectors to\nbe adaptively assigned based on patch features, ensuring detection performance\nwhile effectively controlling computational costs. We conduct extensive\nexperiments on our specifically constructed high-resolution anomaly detection\nbenchmarks, including MVTec-HD, VisA-HD, and the real-world benchmark\nRealIAD-HD, demonstrating the superior performance of HiAD. The code is\navailable at https://github.com/cnulab/HiAD.",
        "url": "http://arxiv.org/abs/2508.12931v1",
        "published_date": "2025-08-18T13:54:29+00:00",
        "updated_date": "2025-08-18T13:54:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ximiao Zhang",
            "Min Xu",
            "Xiuzhuang Zhou"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces HiAD, a framework for high-resolution anomaly detection in industrial images, addressing issues of missed detections and computational efficiency.",
        "tldr_zh": "本文介绍了HiAD，一个用于工业图像高分辨率异常检测的框架，解决了遗漏检测和计算效率的问题。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "Multi-Level Knowledge Distillation and Dynamic Self-Supervised Learning for Continual Learning",
        "summary": "Class-incremental with repetition (CIR), where previously trained classes\nrepeatedly introduced in future tasks, is a more realistic scenario than the\ntraditional class incremental setup, which assumes that each task contains\nunseen classes. CIR assumes that we can easily access abundant unlabeled data\nfrom external sources, such as the Internet. Therefore, we propose two\ncomponents that efficiently use the unlabeled data to ensure the high stability\nand the plasticity of models trained in CIR setup. First, we introduce\nmulti-level knowledge distillation (MLKD) that distills knowledge from multiple\nprevious models across multiple perspectives, including features and logits, so\nthe model can maintain much various previous knowledge. Moreover, we implement\ndynamic self-supervised loss (SSL) to utilize the unlabeled data that\naccelerates the learning of new classes, while dynamic weighting of SSL keeps\nthe focus of training to the primary task. Both of our proposed components\nsignificantly improve the performance in CIR setup, achieving 2nd place in the\nCVPR 5th CLVISION Challenge.",
        "url": "http://arxiv.org/abs/2508.12692v1",
        "published_date": "2025-08-18T07:50:20+00:00",
        "updated_date": "2025-08-18T07:50:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Taeheon Kim",
            "San Kim",
            "Minhyuk Seo",
            "Dongjae Jeon",
            "Wonje Jeong",
            "Jonghyun Choi"
        ],
        "ai_categories": [
            "AIGC"
        ],
        "tldr": "The paper proposes a method for continual learning by distilling knowledge from multiple previous models and utilizing unlabeled data through dynamic self-supervised learning, achieving significant performance improvement in a realistic scenario.",
        "tldr_zh": "本文提出了一种通过从多个先前模型中提炼知识并利用无标签数据进行动态自监督学习的方法，在一个更为现实的情景中取得了显著的性能提升。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Standardization of Neuromuscular Reflex Analysis -- Role of Fine-Tuned Vision-Language Model Consortium and OpenAI gpt-oss Reasoning LLM Enabled Decision Support System",
        "summary": "Accurate assessment of neuromuscular reflexes, such as the H-reflex, plays a\ncritical role in sports science, rehabilitation, and clinical neurology.\nTraditional analysis of H-reflex EMG waveforms is subject to variability and\ninterpretation bias among clinicians and researchers, limiting reliability and\nstandardization. To address these challenges, we propose a Fine-Tuned\nVision-Language Model (VLM) Consortium and a reasoning Large-Language Model\n(LLM)-enabled Decision Support System for automated H-reflex waveform\ninterpretation and diagnosis. Our approach leverages multiple VLMs, each\nfine-tuned on curated datasets of H-reflex EMG waveform images annotated with\nclinical observations, recovery timelines, and athlete metadata. These models\nare capable of extracting key electrophysiological features and predicting\nneuromuscular states, including fatigue, injury, and recovery, directly from\nEMG images and contextual metadata. Diagnostic outputs from the VLM consortium\nare aggregated using a consensus-based method and refined by a specialized\nreasoning LLM, which ensures robust, transparent, and explainable decision\nsupport for clinicians and sports scientists. The end-to-end platform\norchestrates seamless communication between the VLM ensemble and the reasoning\nLLM, integrating prompt engineering strategies and automated reasoning\nworkflows using LLM Agents. Experimental results demonstrate that this hybrid\nsystem delivers highly accurate, consistent, and interpretable H-reflex\nassessments, significantly advancing the automation and standardization of\nneuromuscular diagnostics. To our knowledge, this work represents the first\nintegration of a fine-tuned VLM consortium with a reasoning LLM for image-based\nH-reflex analysis, laying the foundation for next-generation AI-assisted\nneuromuscular assessment and athlete monitoring platforms.",
        "url": "http://arxiv.org/abs/2508.12473v1",
        "published_date": "2025-08-17T19:13:27+00:00",
        "updated_date": "2025-08-17T19:13:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Eranga Bandara",
            "Ross Gore",
            "Sachin Shetty",
            "Ravi Mukkamala",
            "Christopher Rhea",
            "Atmaram Yarlagadda",
            "Shaifali Kaushik",
            "L. H. M. P. De Silva",
            "Andriy Maznychenko",
            "Inna Sokolowska",
            "Amin Hass",
            "Kasun De Zoysa"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper proposes a system using Vision-Language Models and reasoning Large-Language Models for automated analysis of neuromuscular reflexes, improving standardization and reliability in diagnostics.",
        "tldr_zh": "本文提出了一种使用视觉语言模型和推理大语言模型的系统，用于自动分析神经肌肉反射，改善诊断的标准化和可靠性。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "D2-Mamba: Dual-Scale Fusion and Dual-Path Scanning with SSMs for Shadow Removal",
        "summary": "Shadow removal aims to restore images that are partially degraded by shadows,\nwhere the degradation is spatially localized and non-uniform. Unlike general\nrestoration tasks that assume global degradation, shadow removal can leverage\nabundant information from non-shadow regions for guidance. However, the\ntransformation required to correct shadowed areas often differs significantly\nfrom that of well-lit regions, making it challenging to apply uniform\ncorrection strategies. This necessitates the effective integration of non-local\ncontextual cues and adaptive modeling of region-specific transformations. To\nthis end, we propose a novel Mamba-based network featuring dual-scale fusion\nand dual-path scanning to selectively propagate contextual information based on\ntransformation similarity across regions. Specifically, the proposed Dual-Scale\nFusion Mamba Block (DFMB) enhances multi-scale feature representation by fusing\noriginal features with low-resolution features, effectively reducing boundary\nartifacts. The Dual-Path Mamba Group (DPMG) captures global features via\nhorizontal scanning and incorporates a mask-aware adaptive scanning strategy,\nwhich improves structural continuity and fine-grained region modeling.\nExperimental results demonstrate that our method significantly outperforms\nexisting state-of-the-art approaches on shadow removal benchmarks.",
        "url": "http://arxiv.org/abs/2508.12750v1",
        "published_date": "2025-08-18T09:20:21+00:00",
        "updated_date": "2025-08-18T09:20:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Linhao Li",
            "Boya Jin",
            "Zizhe Li",
            "Lanqing Guo",
            "Hao Cheng",
            "Bo Li",
            "Yongfeng Dong"
        ],
        "ai_categories": [
            "GAN",
            "Multimodality"
        ],
        "tldr": "The paper introduces a new network for shadow removal that utilizes dual-scale fusion and dual-path scanning to improve image restoration performance.",
        "tldr_zh": "本文介绍了一种利用双尺度融合和双路径扫描来改善图像修复性能的新网络。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "TTA-DAME: Test-Time Adaptation with Domain Augmentation and Model Ensemble for Dynamic Driving Conditions",
        "summary": "Test-time Adaptation (TTA) poses a challenge, requiring models to dynamically\nadapt and perform optimally on shifting target domains. This task is\nparticularly emphasized in real-world driving scenes, where weather domain\nshifts occur frequently. To address such dynamic changes, our proposed method,\nTTA-DAME, leverages source domain data augmentation into target domains.\nAdditionally, we introduce a domain discriminator and a specialized domain\ndetector to mitigate drastic domain shifts, especially from daytime to\nnighttime conditions. To further improve adaptability, we train multiple\ndetectors and consolidate their predictions through Non-Maximum Suppression\n(NMS). Our empirical validation demonstrates the effectiveness of our method,\nshowing significant performance enhancements on the SHIFT Benchmark.",
        "url": "http://arxiv.org/abs/2508.12690v1",
        "published_date": "2025-08-18T07:48:35+00:00",
        "updated_date": "2025-08-18T07:48:35+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Dongjae Jeon",
            "Taeheon Kim",
            "Seongwon Cho",
            "Minhyuk Seo",
            "Jonghyun Choi"
        ],
        "ai_categories": [
            "Multimodality",
            "GAN",
            "Dataset"
        ],
        "tldr": "TTA-DAME proposes a method for test-time adaptation in dynamic driving conditions using domain augmentation and model ensemble, showing significant performance improvements on the SHIFT Benchmark.",
        "tldr_zh": "TTA-DAME提出了一种用于动态驾驶条件下的测试时间适应的方法，使用了领域增强和模型集成，在SHIFT基准测试中表现出显著的性能提升。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "HOMI: Ultra-Fast EdgeAI platform for Event Cameras",
        "summary": "Event cameras offer significant advantages for edge robotics applications due\nto their asynchronous operation and sparse, event-driven output, making them\nwell-suited for tasks requiring fast and efficient closed-loop control, such as\ngesture-based human-robot interaction. Despite this potential, existing event\nprocessing solutions remain limited, often lacking complete end-to-end\nimplementations, exhibiting high latency, and insufficiently exploiting event\ndata sparsity. In this paper, we present HOMI, an ultra-low latency, end-to-end\nedge AI platform comprising a Prophesee IMX636 event sensor chip with an Xilinx\nZynq UltraScale+MPSoC FPGA chip, deploying an in-house developed AI\naccelerator. We have developed hardware-optimized pre-processing pipelines\nsupporting both constant-time and constant-event modes for histogram\naccumulation, linear and exponential time surfaces. Our general-purpose\nimplementation caters to both accuracy-driven and low-latency applications.\nHOMI achieves 94% accuracy on the DVS Gesture dataset as a use case when\nconfigured for high accuracy operation and provides a throughput of 1000 fps\nfor low-latency configuration. The hardware-optimised pipeline maintains a\ncompact memory footprint and utilises only 33% of the available LUT resources\non the FPGA, leaving ample headroom for further latency reduction, model\nparallelisation, multi-task deployments, or integration of more complex\narchitectures.",
        "url": "http://arxiv.org/abs/2508.12637v1",
        "published_date": "2025-08-18T05:47:48+00:00",
        "updated_date": "2025-08-18T05:47:48+00:00",
        "categories": [
            "cs.AR",
            "cs.CV",
            "cs.ET",
            "cs.NE"
        ],
        "authors": [
            "Shankaranarayanan H",
            "Satyapreet Singh Yadav",
            "Adithya Krishna",
            "Ajay Vikram P",
            "Mahesh Mehendale",
            "Chetan Singh Thakur"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces HOMI, an ultra-fast edge AI platform for event cameras, showcasing high accuracy and low latency performance for tasks like gesture-based human-robot interaction.",
        "tldr_zh": "本文介绍了HOMI，一种用于事件摄像头的超快速边缘人工智能平台，展示出在手势交互等任务中高准确性和低延迟性能。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "OpenMoCap: Rethinking Optical Motion Capture under Real-world Occlusion",
        "summary": "Optical motion capture is a foundational technology driving advancements in\ncutting-edge fields such as virtual reality and film production. However,\nsystem performance suffers severely under large-scale marker occlusions common\nin real-world applications. An in-depth analysis identifies two primary\nlimitations of current models: (i) the lack of training datasets accurately\nreflecting realistic marker occlusion patterns, and (ii) the absence of\ntraining strategies designed to capture long-range dependencies among markers.\nTo tackle these challenges, we introduce the CMU-Occlu dataset, which\nincorporates ray tracing techniques to realistically simulate practical marker\nocclusion patterns. Furthermore, we propose OpenMoCap, a novel motion-solving\nmodel designed specifically for robust motion capture in environments with\nsignificant occlusions. Leveraging a marker-joint chain inference mechanism,\nOpenMoCap enables simultaneous optimization and construction of deep\nconstraints between markers and joints. Extensive comparative experiments\ndemonstrate that OpenMoCap consistently outperforms competing methods across\ndiverse scenarios, while the CMU-Occlu dataset opens the door for future\nstudies in robust motion solving. The proposed OpenMoCap is integrated into the\nMoSen MoCap system for practical deployment. The code is released at:\nhttps://github.com/qianchen214/OpenMoCap.",
        "url": "http://arxiv.org/abs/2508.12610v1",
        "published_date": "2025-08-18T04:12:13+00:00",
        "updated_date": "2025-08-18T04:12:13+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Chen Qian",
            "Danyang Li",
            "Xinran Yu",
            "Zheng Yang",
            "Qiang Ma"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces OpenMoCap, a novel motion-solving model designed for robust motion capture in real-world environments with occlusions, along with the CMU-Occlu dataset to simulate realistic marker occlusion patterns.",
        "tldr_zh": "本文介绍了OpenMoCap，一种新颖的动作解决模型，旨在实现在具有遮挡的真实环境中进行稳健的动作捕捉，同时还提供了CMU-Occlu数据集来模拟真实的标记遮挡模式。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Real-Time Beach Litter Detection and Counting: A Comparative Analysis of RT-DETR Model Variants",
        "summary": "Coastal pollution is a pressing global environmental issue, necessitating\nscalable and automated solutions for monitoring and management. This study\ninvestigates the efficacy of the Real-Time Detection Transformer (RT-DETR), a\nstate-of-the-art, end-to-end object detection model, for the automated\ndetection and counting of beach litter. A rigorous comparative analysis is\nconducted between two model variants, RT-DETR-Large (RT-DETR-L) and\nRT-DETR-Extra-Large (RT-DETR-X), trained on a publicly available dataset of\ncoastal debris. The evaluation reveals that the RT-DETR-X model achieves\nmarginally superior accuracy, with a mean Average Precision at 50\\% IoU\n(mAP@50) of 0.816 and a mAP@50-95 of 0.612, compared to the RT-DETR-L model's\n0.810 and 0.606, respectively. However, this minor performance gain is realized\nat a significant computational cost; the RT-DETR-L model demonstrates a\nsubstantially faster inference time of 20.1 ms versus 34.5 ms for the\nRT-DETR-X. The findings suggest that the RT-DETR-L model offers a more\npractical and efficient solution for real-time, in-field deployment due to its\nsuperior balance of processing speed and detection accuracy. This research\nprovides valuable insights into the application of advanced Transformer-based\ndetectors for environmental conservation, highlighting the critical trade-offs\nbetween model complexity and operational viability.",
        "url": "http://arxiv.org/abs/2508.13101v1",
        "published_date": "2025-08-18T17:10:04+00:00",
        "updated_date": "2025-08-18T17:10:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Miftahul Huda",
            "Arsyiah Azahra",
            "Putri Maulida Chairani",
            "Dimas Rizky Ramadhani",
            "Nabila Azhari",
            "Ade Lailani"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "Study compares RT-DETR-L and RT-DETR-X models for beach litter detection, finding RT-DETR-L more efficient for real-time deployment.",
        "tldr_zh": "研究比较了RT-DETR-L和RT-DETR-X模型在海滩垃圾检测方面，发现RT-DETR-L更适合实时部署。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Checkmate: interpretable and explainable RSVQA is the endgame",
        "summary": "Remote Sensing Visual Question Answering (RSVQA) presents unique challenges\nin ensuring that model decisions are both understandable and grounded in visual\ncontent. Current models often suffer from a lack of interpretability and\nexplainability, as well as from biases in dataset distributions that lead to\nshortcut learning. In this work, we tackle these issues by introducing a novel\nRSVQA dataset, Chessboard, designed to minimize biases through 3'123'253\nquestions and a balanced answer distribution. Each answer is linked to one or\nmore cells within the image, enabling fine-grained visual reasoning.\n  Building on this dataset, we develop an explainable and interpretable model\ncalled Checkmate that identifies the image cells most relevant to its\ndecisions. Through extensive experiments across multiple model architectures,\nwe show that our approach improves transparency and supports more trustworthy\ndecision-making in RSVQA systems.",
        "url": "http://arxiv.org/abs/2508.13086v1",
        "published_date": "2025-08-18T16:59:43+00:00",
        "updated_date": "2025-08-18T16:59:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lucrezia Tosato",
            "Christel Tartini Chappuis",
            "Syrielle Montariol",
            "Flora Weissgerber",
            "Sylvain Lobry",
            "Devis Tuia"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper introduces a novel RSVQA dataset called Chessboard and a model Checkmate to improve transparency and decision-making in RSVQA systems.",
        "tldr_zh": "本文介绍了一个名为Chessboard的新型RSVQA数据集和一个名为Checkmate的模型，旨在提高RSVQA系统中的透明度和决策能力。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SlimComm: Doppler-Guided Sparse Queries for Bandwidth-Efficient Cooperative 3-D Perception",
        "summary": "Collaborative perception allows connected autonomous vehicles (CAVs) to\novercome occlusion and limited sensor range by sharing intermediate features.\nYet transmitting dense Bird's-Eye-View (BEV) feature maps can overwhelm the\nbandwidth available for inter-vehicle communication. We present SlimComm, a\ncommunication-efficient framework that integrates 4D radar Doppler with a\nquery-driven sparse scheme. SlimComm builds a motion-centric dynamic map to\ndistinguish moving from static objects and generates two query types: (i)\nreference queries on dynamic and high-confidence regions, and (ii) exploratory\nqueries probing occluded areas via a two-stage offset. Only query-specific BEV\nfeatures are exchanged and fused through multi-scale gated deformable\nattention, reducing payload while preserving accuracy. For evaluation, we\nrelease OPV2V-R and Adver-City-R, CARLA-based datasets with per-point Doppler\nradar. SlimComm achieves up to 90% lower bandwidth than full-map sharing while\nmatching or surpassing prior baselines across varied traffic densities and\nocclusions. Dataset and code will be available at: https://url.fzi.de/SlimComm.",
        "url": "http://arxiv.org/abs/2508.13007v1",
        "published_date": "2025-08-18T15:27:44+00:00",
        "updated_date": "2025-08-18T15:27:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Melih Yazgan",
            "Qiyuan Wu",
            "Iramm Hamdard",
            "Shiqi Li",
            "J. Marius Zoellner"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "SlimComm proposes a communication-efficient framework for collaborative perception in autonomous vehicles using Doppler radar and sparse queries to reduce bandwidth while maintaining accuracy.",
        "tldr_zh": "SlimComm提出了一种通信高效的框架，通过使用多普勒雷达和稀疏查询在自动驾驶车辆中进行协作感知，以减少带宽同时保持准确性。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Vitamin N: Benefits of Different Forms of Public Greenery for Urban Health",
        "summary": "Urban greenery is often linked to better health, yet findings from past\nresearch have been inconsistent. One reason is that official greenery metrics\nmeasure the amount or nearness of greenery but ignore how often people actually\nmay potentially see or use it in daily life. To address this gap, we introduced\na new classification that separates on-road greenery, which people see while\nwalking through streets, from off-road greenery, which requires planned visits.\nWe did so by combining aerial imagery of Greater London and greenery data from\nOpenStreetMap with quantified greenery from over 100,000 Google Street View\nimages and accessibility estimates based on 160,000 road segments. We linked\nthese measures to 7.45 billion medical prescriptions issued by the National\nHealth Service and processed through our methodology. These prescriptions cover\nfive conditions: diabetes, hypertension, asthma, depression, and anxiety, as\nwell as opioid use. As hypothesized, we found that green on-road was more\nstrongly linked to better health than four widely used official measures. For\nexample, hypertension prescriptions dropped by 3.68% in wards with on-road\ngreenery above the median citywide level compared to those below it. If all\nbelow-median wards reached the citywide median in on-road greenery,\nprescription costs could fall by up to {\\pounds}3.15 million each year. These\nresults suggest that greenery seen in daily life may be more relevant than\npublic yet secluded greenery, and that official metrics commonly used in the\nliterature have important limitations.",
        "url": "http://arxiv.org/abs/2508.12998v1",
        "published_date": "2025-08-18T15:17:33+00:00",
        "updated_date": "2025-08-18T15:17:33+00:00",
        "categories": [
            "cs.CY",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Sanja Šćepanović",
            "Sagar Joglekar",
            "Stephen Law",
            "Daniele Quercia",
            "Ke Zhou",
            "Alice Battiston",
            "Rossano Schifanella"
        ],
        "ai_categories": [
            "Dataset",
            "Diffusion"
        ],
        "tldr": "The paper explores the impact of different forms of urban greenery on public health, finding that greenery seen in daily life is more beneficial for health than secluded greenery. It also highlights limitations of commonly used greenery metrics.",
        "tldr_zh": "本文探讨了城市绿地对公共健康的影响，发现日常生活中可见的绿地对健康更有益，比偏僻的绿地更有效。同时突出了常用绿地度量指标的局限性。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Point upsampling networks for single-photon sensing",
        "summary": "Single-photon sensing has generated great interest as a prominent technique\nof long-distance and ultra-sensitive imaging, however, it tends to yield sparse\nand spatially biased point clouds, thus limiting its practical utility. In this\nwork, we propose using point upsampling networks to increase point density and\nreduce spatial distortion in single-photon point cloud. Particularly, our\nnetwork is built on the state space model which integrates a multi-path\nscanning mechanism to enrich spatial context, a bidirectional Mamba backbone to\ncapture global geometry and local details, and an adaptive upsample shift\nmodule to correct offset-induced distortions. Extensive experiments are\nimplemented on commonly-used datasets to confirm its high reconstruction\naccuracy and strong robustness to the distortion noise, and also on real-world\ndata to demonstrate that our model is able to generate visually consistent,\ndetail-preserving, and noise suppressed point clouds. Our work is the first to\nestablish the upsampling framework for single-photon sensing, and hence opens a\nnew avenue for single-photon sensing and its practical applications in the\ndownstreaming tasks.",
        "url": "http://arxiv.org/abs/2508.12986v1",
        "published_date": "2025-08-18T15:05:27+00:00",
        "updated_date": "2025-08-18T15:05:27+00:00",
        "categories": [
            "physics.optics",
            "cs.CV"
        ],
        "authors": [
            "Jinyi Liu",
            "Guoyang Zhao",
            "Lijun Liu",
            "Yiguang Hong",
            "Weiping Zhang",
            "Shuming Cheng"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces point upsampling networks to increase point density and reduce spatial distortion in single-photon point clouds for improved imaging. It demonstrates high reconstruction accuracy and robustness to noise, opening up new avenues for single-photon sensing applications.",
        "tldr_zh": "本文介绍了点上采样网络，以增加单光子点云的点密度，并减少空间失真，从而改善成像效果。它展示了高重建精度和对噪声的强鲁棒性，为单光子感测应用开辟了新途径。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MaskSem: Semantic-Guided Masking for Learning 3D Hybrid High-Order Motion Representation",
        "summary": "Human action recognition is a crucial task for intelligent robotics,\nparticularly within the context of human-robot collaboration research. In\nself-supervised skeleton-based action recognition, the mask-based\nreconstruction paradigm learns the spatial structure and motion patterns of the\nskeleton by masking joints and reconstructing the target from unlabeled data.\nHowever, existing methods focus on a limited set of joints and low-order motion\npatterns, limiting the model's ability to understand complex motion patterns.\nTo address this issue, we introduce MaskSem, a novel semantic-guided masking\nmethod for learning 3D hybrid high-order motion representations. This novel\nframework leverages Grad-CAM based on relative motion to guide the masking of\njoints, which can be represented as the most semantically rich temporal\norgions. The semantic-guided masking process can encourage the model to explore\nmore discriminative features. Furthermore, we propose using hybrid high-order\nmotion as the reconstruction target, enabling the model to learn multi-order\nmotion patterns. Specifically, low-order motion velocity and high-order motion\nacceleration are used together as the reconstruction target. This approach\noffers a more comprehensive description of the dynamic motion process,\nenhancing the model's understanding of motion patterns. Experiments on the\nNTU60, NTU120, and PKU-MMD datasets show that MaskSem, combined with a vanilla\ntransformer, improves skeleton-based action recognition, making it more\nsuitable for applications in human-robot interaction.",
        "url": "http://arxiv.org/abs/2508.12948v1",
        "published_date": "2025-08-18T14:24:04+00:00",
        "updated_date": "2025-08-18T14:24:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wei Wei",
            "Shaojie Zhang",
            "Yonghao Dang",
            "Jianqin Yin"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "MaskSem introduces a semantic-guided masking method for learning 3D hybrid high-order motion representations, improving skeleton-based action recognition for human-robot interaction applications.",
        "tldr_zh": "MaskSem 提出了一种语义引导的屏蔽方法，用于学习3D混合高阶运动表示，改进了基于骨架的动作识别，适用于人机交互应用。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SIS-Challenge: Event-based Spatio-temporal Instance Segmentation Challenge at the CVPR 2025 Event-based Vision Workshop",
        "summary": "We present an overview of the Spatio-temporal Instance Segmentation (SIS)\nchallenge held in conjunction with the CVPR 2025 Event-based Vision Workshop.\nThe task is to predict accurate pixel-level segmentation masks of defined\nobject classes from spatio-temporally aligned event camera and grayscale camera\ndata. We provide an overview of the task, dataset, challenge details and\nresults. Furthermore, we describe the methods used by the top-5 ranking teams\nin the challenge. More resources and code of the participants' methods are\navailable here:\nhttps://github.com/tub-rip/MouseSIS/blob/main/docs/challenge_results.md",
        "url": "http://arxiv.org/abs/2508.12813v1",
        "published_date": "2025-08-18T10:49:06+00:00",
        "updated_date": "2025-08-18T10:49:06+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Friedhelm Hamann",
            "Emil Mededovic",
            "Fabian Gülhan",
            "Yuli Wu",
            "Johannes Stegmaier",
            "Jing He",
            "Yiqing Wang",
            "Kexin Zhang",
            "Lingling Li",
            "Licheng Jiao",
            "Mengru Ma",
            "Hongxiang Huang",
            "Yuhao Yan",
            "Hongwei Ren",
            "Xiaopeng Lin",
            "Yulong Huang",
            "Bojun Cheng",
            "Se Hyun Lee",
            "Gyu Sung Ham",
            "Kanghan Oh",
            "Gi Hyun Lim",
            "Boxuan Yang",
            "Bowen Du",
            "Guillermo Gallego"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "Overview of the SIS challenge at CVPR 2025 Event-based Vision Workshop, focusing on spatio-temporal instance segmentation from event camera data.",
        "tldr_zh": "2025年CVPR活动视觉研讨会上SIS挑战的概述，重点关注来自事件相机数据的时空实例分割。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SocialTrack: Multi-Object Tracking in Complex Urban Traffic Scenes Inspired by Social Behavior",
        "summary": "As a key research direction in the field of multi-object tracking (MOT),\nUAV-based multi-object tracking has significant application value in the\nanalysis and understanding of urban intelligent transportation systems.\nHowever, in complex UAV perspectives, challenges such as small target scale\nvariations, occlusions, nonlinear crossing motions, and motion blur severely\nhinder the stability of multi-object tracking. To address these challenges,\nthis paper proposes a novel multi-object tracking framework, SocialTrack, aimed\nat enhancing the tracking accuracy and robustness of small targets in complex\nurban traffic environments. The specialized small-target detector enhances the\ndetection performance by employing a multi-scale feature enhancement mechanism.\nThe Velocity Adaptive Cubature Kalman Filter (VACKF) improves the accuracy of\ntrajectory prediction by incorporating a velocity dynamic modeling mechanism.\nThe Group Motion Compensation Strategy (GMCS) models social group motion priors\nto provide stable state update references for low-quality tracks, significantly\nimproving the target association accuracy in complex dynamic environments.\nFurthermore, the Spatio-Temporal Memory Prediction (STMP) leverages historical\ntrajectory information to predict the future state of low-quality tracks,\neffectively mitigating identity switching issues. Extensive experiments on the\nUAVDT and MOT17 datasets demonstrate that SocialTrack outperforms existing\nstate-of-the-art (SOTA) methods across several key metrics. Significant\nimprovements in MOTA and IDF1, among other core performance indicators,\nhighlight its superior robustness and adaptability. Additionally, SocialTrack\nis highly modular and compatible, allowing for seamless integration with\nexisting trackers to further enhance performance.",
        "url": "http://arxiv.org/abs/2508.12777v1",
        "published_date": "2025-08-18T09:53:32+00:00",
        "updated_date": "2025-08-18T09:53:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenguang Tao",
            "Xiaotian Wang",
            "Tian Yan",
            "Jie Yan",
            "Guodong Li",
            "Kun Bai"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces SocialTrack, a new multi-object tracking framework for urban traffic scenes using UAVs, which outperforms existing methods in accuracy and robustness.",
        "tldr_zh": "本文介绍了SocialTrack，一个新的多目标跟踪框架，用于城市交通场景中使用的无人机，在准确性和稳健性方面优于现有方法。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Harnessing Group-Oriented Consistency Constraints for Semi-Supervised Semantic Segmentation in CdZnTe Semiconductors",
        "summary": "Labeling Cadmium Zinc Telluride (CdZnTe) semiconductor images is challenging\ndue to the low-contrast defect boundaries, necessitating annotators to\ncross-reference multiple views. These views share a single ground truth (GT),\nforming a unique ``many-to-one'' relationship. This characteristic renders\nadvanced semi-supervised semantic segmentation (SSS) methods suboptimal, as\nthey are generally limited by a ``one-to-one'' relationship, where each image\nis independently associated with its GT. Such limitation may lead to error\naccumulation in low-contrast regions, further exacerbating confirmation bias.\nTo address this issue, we revisit the SSS pipeline from a group-oriented\nperspective and propose a human-inspired solution: the Intra-group Consistency\nAugmentation Framework (ICAF). First, we experimentally validate the inherent\nconsistency constraints within CdZnTe groups, establishing a group-oriented\nbaseline using the Intra-group View Sampling (IVS). Building on this insight,\nwe introduce the Pseudo-label Correction Network (PCN) to enhance consistency\nrepresentation, which consists of two key modules. The View Augmentation Module\n(VAM) improves boundary details by dynamically synthesizing a boundary-aware\nview through the aggregation of multiple views. In the View Correction Module\n(VCM), this synthesized view is paired with other views for information\ninteraction, effectively emphasizing salient regions while minimizing noise.\nExtensive experiments demonstrate the effectiveness of our solution for CdZnTe\nmaterials. Leveraging DeepLabV3+ with a ResNet-101 backbone as our segmentation\nmodel, we achieve a 70.6\\% mIoU on the CdZnTe dataset using only 2\ngroup-annotated data (5\\textperthousand). The code is available at\n\\href{https://github.com/pipixiapipi/ICAF}{https://github.com/pipixiapipi/ICAF}.",
        "url": "http://arxiv.org/abs/2508.12766v1",
        "published_date": "2025-08-18T09:40:36+00:00",
        "updated_date": "2025-08-18T09:40:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Peihao Li",
            "Yan Fang",
            "Man Liu",
            "Huihui Bai",
            "Anhong Wang",
            "Yunchao Wei",
            "Yao Zhao"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes the Intra-group Consistency Augmentation Framework (ICAF) to improve semi-supervised semantic segmentation in CdZnTe semiconductor images by leveraging group-oriented consistency constraints.",
        "tldr_zh": "本文提出了“组内一致性增强框架”（ICAF），通过利用面向组的一致性约束来改善CdZnTe半导体图像的半监督语义分割。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "WP-CLIP: Leveraging CLIP to Predict Wölfflin's Principles in Visual Art",
        "summary": "W\\\"olfflin's five principles offer a structured approach to analyzing\nstylistic variations for formal analysis. However, no existing metric\neffectively predicts all five principles in visual art. Computationally\nevaluating the visual aspects of a painting requires a metric that can\ninterpret key elements such as color, composition, and thematic choices. Recent\nadvancements in vision-language models (VLMs) have demonstrated their ability\nto evaluate abstract image attributes, making them promising candidates for\nthis task. In this work, we investigate whether CLIP, pre-trained on\nlarge-scale data, can understand and predict W\\\"olfflin's principles. Our\nfindings indicate that it does not inherently capture such nuanced stylistic\nelements. To address this, we fine-tune CLIP on annotated datasets of real art\nimages to predict a score for each principle. We evaluate our model, WP-CLIP,\non GAN-generated paintings and the Pandora-18K art dataset, demonstrating its\nability to generalize across diverse artistic styles. Our results highlight the\npotential of VLMs for automated art analysis.",
        "url": "http://arxiv.org/abs/2508.12668v1",
        "published_date": "2025-08-18T07:00:52+00:00",
        "updated_date": "2025-08-18T07:00:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Abhijay Ghildyal",
            "Li-Yun Wang",
            "Feng Liu"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces WP-CLIP, a model fine-tuned on art datasets to predict Wölfflin's principles in visual art using CLIP, showing potential for automated art analysis.",
        "tldr_zh": "本文介绍了WP-CLIP，一个在艺术数据集上微调的模型，利用CLIP来预测 Wölfflin 的原则，展示了自动艺术分析的潜力。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Learn Faster and Remember More: Balancing Exploration and Exploitation for Continual Test-time Adaptation",
        "summary": "Continual Test-Time Adaptation (CTTA) aims to adapt a source pre-trained\nmodel to continually changing target domains during inference. As a fundamental\nprinciple, an ideal CTTA method should rapidly adapt to new domains\n(exploration) while retaining and exploiting knowledge from previously\nencountered domains to handle similar domains in the future. Despite\nsignificant advances, balancing exploration and exploitation in CTTA is still\nchallenging: 1) Existing methods focus on adjusting predictions based on\ndeep-layer outputs of neural networks. However, domain shifts typically affect\nshallow features, which are inefficient to be adjusted from deep predictions,\nleading to dilatory exploration; 2) A single model inevitably forgets knowledge\nof previous domains during the exploration, making it incapable of exploiting\nhistorical knowledge to handle similar future domains. To address these\nchallenges, this paper proposes a mean teacher framework that strikes an\nappropriate Balance between Exploration and Exploitation (BEE) during the CTTA\nprocess. For the former challenge, we introduce a Multi-level Consistency\nRegularization (MCR) loss that aligns the intermediate features of the student\nand teacher models, accelerating adaptation to the current domain. For the\nlatter challenge, we employ a Complementary Anchor Replay (CAR) mechanism to\nreuse historical checkpoints (anchors), recovering complementary knowledge for\ndiverse domains. Experiments show that our method significantly outperforms\nstate-of-the-art methods on several benchmarks, demonstrating its effectiveness\nfor CTTA tasks.",
        "url": "http://arxiv.org/abs/2508.12643v1",
        "published_date": "2025-08-18T06:08:56+00:00",
        "updated_date": "2025-08-18T06:08:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pinci Yang",
            "Peisong Wen",
            "Ke Ma",
            "Qianqian Xu"
        ],
        "ai_categories": [
            "Transformer",
            "AIGC"
        ],
        "tldr": "The paper introduces a method called BEE for Continual Test-Time Adaptation that balances exploration and exploitation to adapt models to changing domains during inference.",
        "tldr_zh": "本文提出了一种名为BEE的方法，用于连续测试时适应，平衡探索和利用以适应模型在推理过程中对变化领域的调整。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.0
    },
    {
        "title": "Foundation Model for Skeleton-Based Human Action Understanding",
        "summary": "Human action understanding serves as a foundational pillar in the field of\nintelligent motion perception. Skeletons serve as a modality- and\ndevice-agnostic representation for human modeling, and skeleton-based action\nunderstanding has potential applications in humanoid robot control and\ninteraction. \\RED{However, existing works often lack the scalability and\ngeneralization required to handle diverse action understanding tasks. There is\nno skeleton foundation model that can be adapted to a wide range of action\nunderstanding tasks}. This paper presents a Unified Skeleton-based Dense\nRepresentation Learning (USDRL) framework, which serves as a foundational model\nfor skeleton-based human action understanding. USDRL consists of a\nTransformer-based Dense Spatio-Temporal Encoder (DSTE), Multi-Grained Feature\nDecorrelation (MG-FD), and Multi-Perspective Consistency Training (MPCT). The\nDSTE module adopts two parallel streams to learn temporal dynamic and spatial\nstructure features. The MG-FD module collaboratively performs feature\ndecorrelation across temporal, spatial, and instance domains to reduce\ndimensional redundancy and enhance information extraction. The MPCT module\nemploys both multi-view and multi-modal self-supervised consistency training.\nThe former enhances the learning of high-level semantics and mitigates the\nimpact of low-level discrepancies, while the latter effectively facilitates the\nlearning of informative multimodal features. We perform extensive experiments\non 25 benchmarks across across 9 skeleton-based action understanding tasks,\ncovering coarse prediction, dense prediction, and transferred prediction. Our\napproach significantly outperforms the current state-of-the-art methods. We\nhope that this work would broaden the scope of research in skeleton-based\naction understanding and encourage more attention to dense prediction tasks.",
        "url": "http://arxiv.org/abs/2508.12586v1",
        "published_date": "2025-08-18T02:42:16+00:00",
        "updated_date": "2025-08-18T02:42:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hongsong Wang",
            "Wanjiang Weng",
            "Junbo Wang",
            "Fang Zhao",
            "Guo-Sen Xie",
            "Xin Geng",
            "Liang Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a Unified Skeleton-based Dense Representation Learning framework for human action understanding, outperforming current state-of-the-art methods.",
        "tldr_zh": "该论文介绍了一种基于骨架的密集表示学习框架，用于人类动作理解，表现出色优于现有最先进方法。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Temporal and Rotational Calibration for Event-Centric Multi-Sensor Systems",
        "summary": "Event cameras generate asynchronous signals in response to pixel-level\nbrightness changes, offering a sensing paradigm with theoretically\nmicrosecond-scale latency that can significantly enhance the performance of\nmulti-sensor systems. Extrinsic calibration is a critical prerequisite for\neffective sensor fusion; however, the configuration that involves event cameras\nremains an understudied topic. In this paper, we propose a motion-based\ntemporal and rotational calibration framework tailored for event-centric\nmulti-sensor systems, eliminating the need for dedicated calibration targets.\nOur method uses as input the rotational motion estimates obtained from event\ncameras and other heterogeneous sensors, respectively. Different from\nconventional approaches that rely on event-to-frame conversion, our method\nefficiently estimates angular velocity from normal flow observations, which are\nderived from the spatio-temporal profile of event data. The overall calibration\npipeline adopts a two-step approach: it first initializes the temporal offset\nand rotational extrinsics by exploiting kinematic correlations in the spirit of\nCanonical Correlation Analysis (CCA), and then refines both temporal and\nrotational parameters through a joint non-linear optimization using a\ncontinuous-time parametrization in SO(3). Extensive evaluations on both\npublicly available and self-collected datasets validate that the proposed\nmethod achieves calibration accuracy comparable to target-based methods, while\nexhibiting superior stability over purely CCA-based methods, and highlighting\nits precision, robustness and flexibility. To facilitate future research, our\nimplementation will be made open-source. Code:\nhttps://github.com/NAIL-HNU/EvMultiCalib.",
        "url": "http://arxiv.org/abs/2508.12564v1",
        "published_date": "2025-08-18T01:53:27+00:00",
        "updated_date": "2025-08-18T01:53:27+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "I.2.9"
        ],
        "authors": [
            "Jiayao Mai",
            "Xiuyuan Lu",
            "Kuan Dai",
            "Shaojie Shen",
            "Yi Zhou"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper presents a calibration framework for event-centric multi-sensor systems using motion-based temporal and rotational calibration without the need for dedicated calibration targets.",
        "tldr_zh": "该论文提出了一个针对事件中心的多传感器系统的校准框架，使用基于运动的时间和旋转校准，而无需专门的校准目标。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "PROD: Palpative Reconstruction of Deformable Objects through Elastostatic Signed Distance Functions",
        "summary": "We introduce PROD (Palpative Reconstruction of Deformables), a novel method\nfor reconstructing the shape and mechanical properties of deformable objects\nusing elastostatic signed distance functions (SDFs). Unlike traditional\napproaches that rely on purely geometric or visual data, PROD integrates\npalpative interaction -- measured through force-controlled surface probing --\nto estimate both the static and dynamic response of soft materials. We model\nthe deformation of an object as an elastostatic process and derive a governing\nPoisson equation for estimating its SDF from a sparse set of pose and force\nmeasurements. By incorporating steady-state elastodynamic assumptions, we show\nthat the undeformed SDF can be recovered from deformed observations with\nprovable convergence. Our approach also enables the estimation of material\nstiffness by analyzing displacement responses to varying force inputs. We\ndemonstrate the robustness of PROD in handling pose errors, non-normal force\napplication, and curvature errors in simulated soft body interactions. These\ncapabilities make PROD a powerful tool for reconstructing deformable objects in\napplications ranging from robotic manipulation to medical imaging and haptic\nfeedback systems.",
        "url": "http://arxiv.org/abs/2508.12554v1",
        "published_date": "2025-08-18T01:13:58+00:00",
        "updated_date": "2025-08-18T01:13:58+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Hamza El-Kebir"
        ],
        "ai_categories": [
            "Other"
        ],
        "tldr": "PROD is a novel method for reconstructing deformable objects using elastostatic signed distance functions, integrating palpative interaction to estimate shape and mechanical properties.",
        "tldr_zh": "PROD是一种利用弹性静态有符号距离函数重建可变形对象的新方法，整合触觉交互来估计形状和机械特性。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Segmenting Thalamic Nuclei: T1 Maps Provide a Reliable and Efficient Solution",
        "summary": "Accurate thalamic nuclei segmentation is crucial for understanding\nneurological diseases, brain functions, and guiding clinical interventions.\nHowever, the optimal inputs for segmentation remain unclear. This study\nsystematically evaluates multiple MRI contrasts, including MPRAGE and FGATIR\nsequences, quantitative PD and T1 maps, and multiple T1-weighted images at\ndifferent inversion times (multi-TI), to determine the most effective inputs.\nFor multi-TI images, we employ a gradient-based saliency analysis with Monte\nCarlo dropout and propose an Overall Importance Score to select the images\ncontributing most to segmentation. A 3D U-Net is trained on each of these\nconfigurations. Results show that T1 maps alone achieve strong quantitative\nperformance and superior qualitative outcomes, while PD maps offer no added\nvalue. These findings underscore the value of T1 maps as a reliable and\nefficient input among the evaluated options, providing valuable guidance for\noptimizing imaging protocols when thalamic structures are of clinical or\nresearch interest.",
        "url": "http://arxiv.org/abs/2508.12508v1",
        "published_date": "2025-08-17T21:59:28+00:00",
        "updated_date": "2025-08-17T21:59:28+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "q-bio.QM"
        ],
        "authors": [
            "Anqi Feng",
            "Zhangxing Bian",
            "Samuel W. Remedios",
            "Savannah P. Hays",
            "Blake E. Dewey",
            "Jiachen Zhuo",
            "Dan Benjamini",
            "Jerry L. Prince"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The study evaluates different MRI contrasts to improve thalamic nuclei segmentation, finding that T1 maps are the most effective input.",
        "tldr_zh": "该研究评估了不同的MRI对比以改进丘脑核分割，发现T1图是最有效的输入。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7.0
    },
    {
        "title": "Empirical Evidences for the Effects of Feature Diversity in Open Set Recognition and Continual Learning",
        "summary": "Open set recognition (OSR) and continual learning are two critical challenges\nin machine learning, focusing respectively on detecting novel classes at\ninference time and updating models to incorporate the new classes. While many\nrecent approaches have addressed these problems, particularly OSR, by\nheuristically promoting feature diversity, few studies have directly examined\nthe role that feature diversity plays in tackling them. In this work, we\nprovide empirical evidence that enhancing feature diversity improves the\nrecognition of open set samples. Moreover, increased feature diversity also\nfacilitates both the retention of previously learned data and the integration\nof new data in continual learning. We hope our findings can inspire further\nresearch into both practical methods and theoretical understanding in these\ndomains.",
        "url": "http://arxiv.org/abs/2508.13005v1",
        "published_date": "2025-08-18T15:25:06+00:00",
        "updated_date": "2025-08-18T15:25:06+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Jiawen Xu",
            "Odej Kao"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "The paper presents empirical evidence that increasing feature diversity improves open set recognition and continual learning.",
        "tldr_zh": "本文提供了实证证据，表明增加特征多样性可以改善开放集识别和持续学习。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Mechanical Automation with Vision: A Design for Rubik's Cube Solver",
        "summary": "The core mechanical system is built around three stepper motors for physical\nmanipulation, a microcontroller for hardware control, a camera and YOLO\ndetection model for real-time cube state detection. A significant software\ncomponent is the development of a user-friendly graphical user interface (GUI)\ndesigned in Unity. The initial state after detection from real-time YOLOv8\nmodel (Precision 0.98443, Recall 0.98419, Box Loss 0.42051, Class Loss 0.2611)\nis virtualized on GUI. To get the solution, the system employs the Kociemba's\nalgorithm while physical manipulation with a single degree of freedom is done\nby combination of stepper motors' interaction with the cube achieving the\naverage solving time of ~2.2 minutes.",
        "url": "http://arxiv.org/abs/2508.12469v1",
        "published_date": "2025-08-17T18:49:46+00:00",
        "updated_date": "2025-08-17T18:49:46+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Abhinav Chalise",
            "Nimesh Gopal Pradhan",
            "Nishan Khanal",
            "Prashant Raj Bista",
            "Dinesh Baniya Kshatri"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes a Rubik's Cube solver using mechanical automation with vision technology, achieving an average solving time of ~2.2 minutes.",
        "tldr_zh": "本文提出了一种使用机械自动化和视觉技术的魔方求解器，平均求解时间为~2.2分钟。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Morphological classification of eclipsing binary stars using computer vision methods",
        "summary": "We present an application of computer vision methods to classify the light\ncurves of eclipsing binaries (EB). We have used pre-trained models based on\nconvolutional neural networks ($\\textit{ResNet50}$) and vision transformers\n($\\textit{vit\\_base\\_patch16\\_224}$), which were fine-tuned on images created\nfrom synthetic datasets. To improve model generalisation and reduce\noverfitting, we developed a novel image representation by transforming\nphase-folded light curves into polar coordinates combined with hexbin\nvisualisation. Our hierarchical approach in the first stage classifies systems\ninto detached and overcontact types, and in the second stage identifies the\npresence or absence of spots. The binary classification models achieved high\naccuracy ($>96\\%$) on validation data across multiple passbands (Gaia~$G$, $I$,\nand $TESS$) and demonstrated strong performance ($>94\\%$, up to $100\\%$ for\n$TESS$) when tested on extensive observational data from the OGLE, DEBCat, and\nWUMaCat catalogues. While the primary binary classification was highly\nsuccessful, the secondary task of automated spot detection performed poorly,\nrevealing a significant limitation of our models for identifying subtle\nphotometric features. This study highlights the potential of computer vision\nfor EB morphological classification in large-scale surveys, but underscores the\nneed for further research into robust, automated spot detection.",
        "url": "http://arxiv.org/abs/2508.12802v1",
        "published_date": "2025-08-18T10:29:19+00:00",
        "updated_date": "2025-08-18T10:29:19+00:00",
        "categories": [
            "cs.CV",
            "astro-ph.IM",
            "astro-ph.SR",
            "I.5.1; J.2"
        ],
        "authors": [
            "Štefan Parimucha",
            "Maksim Gabdeev",
            "Yanna Markus",
            "Martin Vaňko",
            "Pavol Gajdoš"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper applies computer vision methods to classify eclipsing binary stars' light curves, achieving high accuracy in binary classification but highlighting a need for further research in automated spot detection.",
        "tldr_zh": "本文应用计算机视觉方法对食变双星的光变曲线进行分类，取得了较高的二元分类准确率，但也突出了对自动斑点检测进行进一步研究的必要性。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.25
    },
    {
        "title": "A Shift in Perspective on Causality in Domain Generalization",
        "summary": "The promise that causal modelling can lead to robust AI generalization has\nbeen challenged in recent work on domain generalization (DG) benchmarks. We\nrevisit the claims of the causality and DG literature, reconciling apparent\ncontradictions and advocating for a more nuanced theory of the role of\ncausality in generalization. We also provide an interactive demo at\nhttps://chai-uk.github.io/ukairs25-causal-predictors/.",
        "url": "http://arxiv.org/abs/2508.12798v1",
        "published_date": "2025-08-18T10:19:33+00:00",
        "updated_date": "2025-08-18T10:19:33+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Damian Machlanski",
            "Stephanie Riley",
            "Edward Moroshko",
            "Kurt Butler",
            "Panagiotis Dimitrakopoulos",
            "Thomas Melistas",
            "Akchunya Chanchal",
            "Steven McDonagh",
            "Ricardo Silva",
            "Sotirios A. Tsaftaris"
        ],
        "ai_categories": [
            "AIGC"
        ],
        "tldr": "The paper challenges previous assumptions on the role of causality in AI generalization, advocating for a more nuanced understanding of its impact.",
        "tldr_zh": "本文挑战了对人工智能泛化中因果关系作用的先前假设，主张更加细致的理解其影响。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.25
    },
    {
        "title": "Refine-and-Contrast: Adaptive Instance-Aware BEV Representations for Multi-UAV Collaborative Object Detection",
        "summary": "Multi-UAV collaborative 3D detection enables accurate and robust perception\nby fusing multi-view observations from aerial platforms, offering significant\nadvantages in coverage and occlusion handling, while posing new challenges for\ncomputation on resource-constrained UAV platforms. In this paper, we present\nAdaBEV, a novel framework that learns adaptive instance-aware BEV\nrepresentations through a refine-and-contrast paradigm. Unlike existing methods\nthat treat all BEV grids equally, AdaBEV introduces a Box-Guided Refinement\nModule (BG-RM) and an Instance-Background Contrastive Learning (IBCL) to\nenhance semantic awareness and feature discriminability. BG-RM refines only BEV\ngrids associated with foreground instances using 2D supervision and spatial\nsubdivision, while IBCL promotes stronger separation between foreground and\nbackground features via contrastive learning in BEV space. Extensive\nexperiments on the Air-Co-Pred dataset demonstrate that AdaBEV achieves\nsuperior accuracy-computation trade-offs across model scales, outperforming\nother state-of-the-art methods at low resolutions and approaching upper bound\nperformance while maintaining low-resolution BEV inputs and negligible\noverhead.",
        "url": "http://arxiv.org/abs/2508.12684v1",
        "published_date": "2025-08-18T07:37:14+00:00",
        "updated_date": "2025-08-18T07:37:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhongyao Li",
            "Peirui Cheng",
            "Liangjin Zhao",
            "Chen Chen",
            "Yundu Li",
            "Zhechao Wang",
            "Xue Yang",
            "Xian Sun",
            "Zhirui Wang"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces AdaBEV, a framework for adaptive instance-aware bird's eye view (BEV) representations for multi-UAV collaborative object detection.",
        "tldr_zh": "该论文介绍了AdaBEV，一种用于多UAV协同目标检测的自适应实例感知鸟瞰图（BEV）表示的框架。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.25
    },
    {
        "title": "Multi-Phase Automated Segmentation of Dental Structures in CBCT Using a Lightweight Auto3DSeg and SegResNet Implementation",
        "summary": "Cone-beam computed tomography (CBCT) has become an invaluable imaging\nmodality in dentistry, enabling 3D visualization of teeth and surrounding\nstructures for diagnosis and treatment planning. Automated segmentation of\ndental structures in CBCT can efficiently assist in identifying pathology\n(e.g., pulpal or periapical lesions) and facilitate radiation therapy planning\nin head and neck cancer patients. We describe the DLaBella29 team's approach\nfor the MICCAI 2025 ToothFairy3 Challenge, which involves a deep learning\npipeline for multi-class tooth segmentation. We utilized the MONAI Auto3DSeg\nframework with a 3D SegResNet architecture, trained on a subset of the\nToothFairy3 dataset (63 CBCT scans) with 5-fold cross-validation. Key\npreprocessing steps included image resampling to 0.6 mm isotropic resolution\nand intensity clipping. We applied an ensemble fusion using Multi-Label STAPLE\non the 5-fold predictions to infer a Phase 1 segmentation and then conducted\ntight cropping around the easily segmented Phase 1 mandible to perform Phase 2\nsegmentation on the smaller nerve structures. Our method achieved an average\nDice of 0.87 on the ToothFairy3 challenge out-of-sample validation set. This\npaper details the clinical context, data preparation, model development,\nresults of our approach, and discusses the relevance of automated dental\nsegmentation for improving patient care in radiation oncology.",
        "url": "http://arxiv.org/abs/2508.12962v1",
        "published_date": "2025-08-18T14:35:26+00:00",
        "updated_date": "2025-08-18T14:35:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Dominic LaBella",
            "Keshav Jha",
            "Jared Robbins",
            "Esther Yu"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper presents a deep learning pipeline for automated segmentation of dental structures in CBCT scans, achieving high accuracy in tooth segmentation, with potential applications in diagnosis and treatment planning.",
        "tldr_zh": "本文提出了一种用于CBCT扫描中牙齿结构自动分割的深度学习管道，实现了高精度的牙齿分割，具有诊断和治疗规划的潜在应用。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "On the Importance of Behavioral Nuances: Amplifying Non-Obvious Motor Noise Under True Empirical Considerations May Lead to Briefer Assays and Faster Classification Processes",
        "summary": "There is a tradeoff between attaining statistical power with large, difficult\nto gather data sets, and producing highly scalable assays that register brief\ndata samples. Often, as grand-averaging techniques a priori assume\nnormally-distributed parameters and linear, stationary processes in\nbiorhythmic, time series data, important information is lost, averaged out as\ngross data. We developed an affective computing platform that enables taking\nbrief data samples while maintaining personalized statistical power. This is\nachieved by combining a new data type derived from the micropeaks present in\ntime series data registered from brief (5-second-long) face videos with recent\nadvances in AI-driven face-grid estimation methods. By adopting geometric and\nnonlinear dynamical systems approaches to analyze the kinematics, especially\nthe speed data, the new methods capture all facial micropeaks. These include as\nwell the nuances of different affective micro expressions. We offer new ways to\ndifferentiate dynamical and geometric patterns present in autistic individuals\nfrom those found more commonly in neurotypical development.",
        "url": "http://arxiv.org/abs/2508.12742v1",
        "published_date": "2025-08-18T09:05:40+00:00",
        "updated_date": "2025-08-18T09:05:40+00:00",
        "categories": [
            "q-bio.QM",
            "cs.CV",
            "cs.LG",
            "eess.SP",
            "nlin.CD"
        ],
        "authors": [
            "Theodoros Bermperidis",
            "Joe Vero",
            "Elizabeth B Torres"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper discusses a new method for capturing facial micropeaks in time series data from brief face videos to differentiate between autistic individuals and neurotypical development.",
        "tldr_zh": "本文讨论了一种新方法，用于从简短面部视频中捕获时间序列数据中的面部微峰，以区分自闭症个体和正常发展。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]