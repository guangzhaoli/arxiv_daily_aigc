[
    {
        "title": "M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via Self-Supervision",
        "summary": "Medical image retrieval is essential for clinical decision-making and\ntranslational research, relying on discriminative visual representations. Yet,\ncurrent methods remain fragmented, relying on separate architectures and\ntraining strategies for 2D, 3D, and video-based medical data. This\nmodality-specific design hampers scalability and inhibits the development of\nunified representations. To enable unified learning, we curate a large-scale\nhybrid-modality dataset comprising 867,653 medical imaging samples, including\n2D X-rays and ultrasounds, RGB endoscopy videos, and 3D CT scans. Leveraging\nthis dataset, we train M3Ret, a unified visual encoder without any\nmodality-specific customization. It successfully learns transferable\nrepresentations using both generative (MAE) and contrastive (SimDINO)\nself-supervised learning (SSL) paradigms. Our approach sets a new\nstate-of-the-art in zero-shot image-to-image retrieval across all individual\nmodalities, surpassing strong baselines such as DINOv3 and the text-supervised\nBMC-CLIP. More remarkably, strong cross-modal alignment emerges without paired\ndata, and the model generalizes to unseen MRI tasks, despite never observing\nMRI during pretraining, demonstrating the generalizability of purely visual\nself-supervision to unseen modalities. Comprehensive analyses further validate\nthe scalability of our framework across model and data sizes. These findings\ndeliver a promising signal to the medical imaging community, positioning M3Ret\nas a step toward foundation models for visual SSL in multimodal medical image\nunderstanding.",
        "url": "http://arxiv.org/abs/2509.01360v1",
        "published_date": "2025-09-01T10:59:39+00:00",
        "updated_date": "2025-09-01T10:59:39+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Che Liu",
            "Zheng Jiang",
            "Chengyu Fang",
            "Heng Guo",
            "Yan-Jie Zhou",
            "Jiaqi Qu",
            "Le Lu",
            "Minfeng Xu"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper introduces M3Ret, a unified visual encoder for multimodal medical image retrieval, achieving state-of-the-art results using self-supervised learning across different data types.",
        "tldr_zh": "本文介绍了M3Ret，一种统一的视觉编码器，用于多模态医学图像检索，在不同数据类型上使用自监督学习取得了最新成果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "MILO: A Lightweight Perceptual Quality Metric for Image and Latent-Space Optimization",
        "summary": "We present MILO (Metric for Image- and Latent-space Optimization), a\nlightweight, multiscale, perceptual metric for full-reference image quality\nassessment (FR-IQA). MILO is trained using pseudo-MOS (Mean Opinion Score)\nsupervision, in which reproducible distortions are applied to diverse images\nand scored via an ensemble of recent quality metrics that account for visual\nmasking effects. This approach enables accurate learning without requiring\nlarge-scale human-labeled datasets. Despite its compact architecture, MILO\noutperforms existing metrics across standard FR-IQA benchmarks and offers fast\ninference suitable for real-time applications. Beyond quality prediction, we\ndemonstrate the utility of MILO as a perceptual loss in both image and latent\ndomains. In particular, we show that spatial masking modeled by MILO, when\napplied to latent representations from a VAE encoder within Stable Diffusion,\nenables efficient and perceptually aligned optimization. By combining spatial\nmasking with a curriculum learning strategy, we first process perceptually less\nrelevant regions before progressively shifting the optimization to more\nvisually distorted areas. This strategy leads to significantly improved\nperformance in tasks like denoising, super-resolution, and face restoration,\nwhile also reducing computational overhead. MILO thus functions as both a\nstate-of-the-art image quality metric and as a practical tool for perceptual\noptimization in generative pipelines.",
        "url": "http://arxiv.org/abs/2509.01411v1",
        "published_date": "2025-09-01T12:08:30+00:00",
        "updated_date": "2025-09-01T12:08:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Uğur Çoğalan",
            "Mojtaba Bemana",
            "Karol Myszkowski",
            "Hans-Peter Seidel",
            "Colin Groth"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "MILO is a lightweight perceptual quality metric for image and latent-space optimization that outperforms existing metrics and can be used for real-time applications and perceptual optimization in generative pipelines.",
        "tldr_zh": "MILO是一种轻量级的感知质量度量，可用于图像和潜在空间优化，超越了现有的度量，并可用于实时应用和生成管道的感知优化。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "Neural Scene Designer: Self-Styled Semantic Image Manipulation",
        "summary": "Maintaining stylistic consistency is crucial for the cohesion and aesthetic\nappeal of images, a fundamental requirement in effective image editing and\ninpainting. However, existing methods primarily focus on the semantic control\nof generated content, often neglecting the critical task of preserving this\nconsistency. In this work, we introduce the Neural Scene Designer (NSD), a\nnovel framework that enables photo-realistic manipulation of user-specified\nscene regions while ensuring both semantic alignment with user intent and\nstylistic consistency with the surrounding environment. NSD leverages an\nadvanced diffusion model, incorporating two parallel cross-attention mechanisms\nthat separately process text and style information to achieve the dual\nobjectives of semantic control and style consistency. To capture fine-grained\nstyle representations, we propose the Progressive Self-style Representational\nLearning (PSRL) module. This module is predicated on the intuitive premise that\ndifferent regions within a single image share a consistent style, whereas\nregions from different images exhibit distinct styles. The PSRL module employs\na style contrastive loss that encourages high similarity between\nrepresentations from the same image while enforcing dissimilarity between those\nfrom different images. Furthermore, to address the lack of standardized\nevaluation protocols for this task, we establish a comprehensive benchmark.\nThis benchmark includes competing algorithms, dedicated style-related metrics,\nand diverse datasets and settings to facilitate fair comparisons. Extensive\nexperiments conducted on our benchmark demonstrate the effectiveness of the\nproposed framework.",
        "url": "http://arxiv.org/abs/2509.01405v1",
        "published_date": "2025-09-01T11:59:03+00:00",
        "updated_date": "2025-09-01T11:59:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jianman Lin",
            "Tianshui Chen",
            "Chunmei Qing",
            "Zhijing Yang",
            "Shuangping Huang",
            "Yuheng Ren",
            "Liang Lin"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a Neural Scene Designer framework for semantic image manipulation, ensuring both semantic alignment and stylistic consistency. It also proposes a Progressive Self-style Representational Learning module for fine-grained style representation.",
        "tldr_zh": "本文介绍了一种神经场景设计师框架，用于语义图像操作，确保语义对齐和风格一致性。它还提出了一个用于细粒度风格表示的渐进自身风格表征学习模块。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "CompSlider: Compositional Slider for Disentangled Multiple-Attribute Image Generation",
        "summary": "In text-to-image (T2I) generation, achieving fine-grained control over\nattributes - such as age or smile - remains challenging, even with detailed\ntext prompts. Slider-based methods offer a solution for precise control of\nimage attributes. Existing approaches typically train individual adapter for\neach attribute separately, overlooking the entanglement among multiple\nattributes. As a result, interference occurs among different attributes,\npreventing precise control of multiple attributes together. To address this\nchallenge, we aim to disentangle multiple attributes in slider-based generation\nto enbale more reliable and independent attribute manipulation. Our approach,\nCompSlider, can generate a conditional prior for the T2I foundation model to\ncontrol multiple attributes simultaneously. Furthermore, we introduce novel\ndisentanglement and structure losses to compose multiple attribute changes\nwhile maintaining structural consistency within the image. Since CompSlider\noperates in the latent space of the conditional prior and does not require\nretraining the foundation model, it reduces the computational burden for both\ntraining and inference. We evaluate our approach on a variety of image\nattributes and highlight its generality by extending to video generation.",
        "url": "http://arxiv.org/abs/2509.01028v2",
        "published_date": "2025-08-31T23:36:44+00:00",
        "updated_date": "2025-09-03T15:01:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zixin Zhu",
            "Kevin Duarte",
            "Mamshad Nayeem Rizve",
            "Chengyuan Xu",
            "Ratheesh Kalarot",
            "Junsong Yuan"
        ],
        "ai_categories": [
            "AIGC",
            "GAN"
        ],
        "tldr": "CompSlider introduces a method to disentangle multiple attributes for precise image generation control, reducing computational burden and extending to video generation.",
        "tldr_zh": "CompSlider 提出了一种方法，用于解开多个属性，以实现对图像生成的精确控制，减少计算负担并扩展到视频生成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Kwai Keye-VL 1.5 Technical Report",
        "summary": "In recent years, the development of Large Language Models (LLMs) has\nsignificantly advanced, extending their capabilities to multimodal tasks\nthrough Multimodal Large Language Models (MLLMs). However, video understanding\nremains a challenging area due to the dynamic and information-dense nature of\nvideos. Existing models struggle with the trade-off between spatial resolution\nand temporal coverage when processing video content. We present Keye-VL-1.5,\nwhich addresses fundamental challenges in video comprehension through three key\ninnovations. First, we introduce a novel Slow-Fast video encoding strategy that\ndynamically allocates computational resources based on inter-frame similarity,\nprocessing key frames with significant visual changes at higher resolution\n(Slow pathway) while handling relatively static frames with increased temporal\ncoverage at lower resolution (Fast pathway). Second, we implement a progressive\nfour-stage pre-training methodology that systematically extends the model's\ncontext length from 8K to 128K tokens, enabling processing of longer videos and\nmore complex visual content. Third, we develop a comprehensive post-training\npipeline focusing on reasoning enhancement and human preference alignment,\nincorporating a 5-step chain-of-thought data construction process, iterative\nGSPO-based reinforcement learning with progressive prompt hinting for difficult\ncases, and alignment training. Through extensive evaluation on public\nbenchmarks and rigorous internal human assessment, Keye-VL-1.5 demonstrates\nsignificant improvements over existing models, particularly excelling in video\nunderstanding tasks while maintaining competitive performance on general\nmultimodal benchmarks.",
        "url": "http://arxiv.org/abs/2509.01563v1",
        "published_date": "2025-09-01T15:46:58+00:00",
        "updated_date": "2025-09-01T15:46:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Biao Yang",
            "Bin Wen",
            "Boyang Ding",
            "Changyi Liu",
            "Chenglong Chu",
            "Chengru Song",
            "Chongling Rao",
            "Chuan Yi",
            "Da Li",
            "Dunju Zang",
            "Fan Yang",
            "Guorui Zhou",
            "Guowang Zhang",
            "Han Shen",
            "Hao Peng",
            "Haojie Ding",
            "Hao Wang",
            "Hengrui Ju",
            "Jiaming Huang",
            "Jiangxia Cao",
            "Jiankang Chen",
            "Jingyun Hua",
            "Kaibing Chen",
            "Kaiyu Jiang",
            "Kaiyu Tang",
            "Kun Gai",
            "Muhao Wei",
            "Qiang Wang",
            "Ruitao Wang",
            "Sen Na",
            "Shengnan Zhang",
            "Siyang Mao",
            "Sui Huang",
            "Tianke Zhang",
            "Tingting Gao",
            "Wei Chen",
            "Wei Yuan",
            "Xiangyu Wu",
            "Xiao Hu",
            "Xingyu Lu",
            "Yi-Fan Zhang",
            "Yiping Yang",
            "Yulong Chen",
            "Zeyi Lu",
            "Zhenhua Wu",
            "Zhixin Ling",
            "Zhuoran Yang",
            "Ziming Li",
            "Di Xu",
            "Haixuan Gao",
            "Hang Li",
            "Jing Wang",
            "Lejian Ren",
            "Qigen Hu",
            "Qianqian Wang",
            "Shiyao Wang",
            "Xinchen Luo",
            "Yan Li",
            "Yuhang Hu",
            "Zixing Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "Keye-VL-1.5 is a novel Multimodal Large Language Model that addresses challenges in video comprehension through innovative video encoding strategies and pre-training methodologies, demonstrating significant improvements over existing models.",
        "tldr_zh": "Keye-VL-1.5是一种新颖的多模态大语言模型，通过创新的视频编码策略和预训练方法解决了视频理解中的挑战，明显优于现有模型。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "FantasyHSI: Video-Generation-Centric 4D Human Synthesis In Any Scene through A Graph-based Multi-Agent Framework",
        "summary": "Human-Scene Interaction (HSI) seeks to generate realistic human behaviors\nwithin complex environments, yet it faces significant challenges in handling\nlong-horizon, high-level tasks and generalizing to unseen scenes. To address\nthese limitations, we introduce FantasyHSI, a novel HSI framework centered on\nvideo generation and multi-agent systems that operates without paired data. We\nmodel the complex interaction process as a dynamic directed graph, upon which\nwe build a collaborative multi-agent system. This system comprises a scene\nnavigator agent for environmental perception and high-level path planning, and\na planning agent that decomposes long-horizon goals into atomic actions.\nCritically, we introduce a critic agent that establishes a closed-loop feedback\nmechanism by evaluating the deviation between generated actions and the planned\npath. This allows for the dynamic correction of trajectory drifts caused by the\nstochasticity of the generative model, thereby ensuring long-term logical\nconsistency. To enhance the physical realism of the generated motions, we\nleverage Direct Preference Optimization (DPO) to train the action generator,\nsignificantly reducing artifacts such as limb distortion and foot-sliding.\nExtensive experiments on our custom SceneBench benchmark demonstrate that\nFantasyHSI significantly outperforms existing methods in terms of\ngeneralization, long-horizon task completion, and physical realism. Ours\nproject page: https://fantasy-amap.github.io/fantasy-hsi/",
        "url": "http://arxiv.org/abs/2509.01232v1",
        "published_date": "2025-09-01T08:20:50+00:00",
        "updated_date": "2025-09-01T08:20:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lingzhou Mu",
            "Qiang Wang",
            "Fan Jiang",
            "Mengchao Wang",
            "Yaqi Fan",
            "Mu Xu",
            "Kai Zhang"
        ],
        "ai_categories": [
            "GAN",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "FantasyHSI is a novel framework for generating realistic human behaviors in complex scenes using a graph-based multi-agent system without paired data, outperforming existing methods in generalization and realism.",
        "tldr_zh": "FantasyHSI是一个新颖的框架，通过基于图的多智能体系统在复杂场景中生成真实的人类行为，无需配对数据，优于现有方法在泛化和真实性方面的表现。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "FocusDPO: Dynamic Preference Optimization for Multi-Subject Personalized Image Generation via Adaptive Focus",
        "summary": "Multi-subject personalized image generation aims to synthesize customized\nimages containing multiple specified subjects without requiring test-time\noptimization. However, achieving fine-grained independent control over multiple\nsubjects remains challenging due to difficulties in preserving subject fidelity\nand preventing cross-subject attribute leakage. We present FocusDPO, a\nframework that adaptively identifies focus regions based on dynamic semantic\ncorrespondence and supervision image complexity. During training, our method\nprogressively adjusts these focal areas across noise timesteps, implementing a\nweighted strategy that rewards information-rich patches while penalizing\nregions with low prediction confidence. The framework dynamically adjusts focus\nallocation during the DPO process according to the semantic complexity of\nreference images and establishes robust correspondence mappings between\ngenerated and reference subjects. Extensive experiments demonstrate that our\nmethod substantially enhances the performance of existing pre-trained\npersonalized generation models, achieving state-of-the-art results on both\nsingle-subject and multi-subject personalized image synthesis benchmarks. Our\nmethod effectively mitigates attribute leakage while preserving superior\nsubject fidelity across diverse generation scenarios, advancing the frontier of\ncontrollable multi-subject image synthesis.",
        "url": "http://arxiv.org/abs/2509.01181v1",
        "published_date": "2025-09-01T07:06:36+00:00",
        "updated_date": "2025-09-01T07:06:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Qiaoqiao Jin",
            "Siming Fu",
            "Dong She",
            "Weinan Jia",
            "Hualiang Wang",
            "Mu Liu",
            "Jidong Jiang"
        ],
        "ai_categories": [
            "GAN",
            "AIGC"
        ],
        "tldr": "FocusDPO is a framework for multi-subject personalized image generation that dynamically adjusts focus regions to enhance performance and prevent attribute leakage.",
        "tldr_zh": "FocusDPO是一个框架，用于多主题个性化图像生成，动态调整焦点区域以提高性能并防止属性泄漏。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "DynaMind: Reconstructing Dynamic Visual Scenes from EEG by Aligning Temporal Dynamics and Multimodal Semantics to Guided Diffusion",
        "summary": "Reconstruction dynamic visual scenes from electroencephalography (EEG)\nsignals remains a primary challenge in brain decoding, limited by the low\nspatial resolution of EEG, a temporal mismatch between neural recordings and\nvideo dynamics, and the insufficient use of semantic information within brain\nactivity. Therefore, existing methods often inadequately resolve both the\ndynamic coherence and the complex semantic context of the perceived visual\nstimuli. To overcome these limitations, we introduce DynaMind, a novel\nframework that reconstructs video by jointly modeling neural dynamics and\nsemantic features via three core modules: a Regional-aware Semantic Mapper\n(RSM), a Temporal-aware Dynamic Aligner (TDA), and a Dual-Guidance Video\nReconstructor (DGVR). The RSM first utilizes a regional-aware encoder to\nextract multimodal semantic features from EEG signals across distinct brain\nregions, aggregating them into a unified diffusion prior. In the mean time, the\nTDA generates a dynamic latent sequence, or blueprint, to enforce temporal\nconsistency between the feature representations and the original neural\nrecordings. Together, guided by the semantic diffusion prior, the DGVR\ntranslates the temporal-aware blueprint into a high-fidelity video\nreconstruction. On the SEED-DV dataset, DynaMind sets a new state-of-the-art\n(SOTA), boosting reconstructed video accuracies (video- and frame-based) by\n12.5 and 10.3 percentage points, respectively. It also achieves a leap in\npixel-level quality, showing exceptional visual fidelity and temporal coherence\nwith a 9.4% SSIM improvement and a 19.7% FVMD reduction. This marks a critical\nadvancement, bridging the gap between neural dynamics and high-fidelity visual\nsemantics.",
        "url": "http://arxiv.org/abs/2509.01177v1",
        "published_date": "2025-09-01T06:52:08+00:00",
        "updated_date": "2025-09-01T06:52:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.HC",
            "eess.SP"
        ],
        "authors": [
            "Junxiang Liu",
            "Junming Lin",
            "Jiangtong Li",
            "Jie Li"
        ],
        "ai_categories": [
            "Dataset",
            "Multimodality",
            "Diffusion"
        ],
        "tldr": "DynaMind is a novel framework that reconstructs dynamic visual scenes from EEG signals by aligning temporal dynamics and multimodal semantics to guided diffusion, setting a new state-of-the-art in video reconstruction accuracy and quality.",
        "tldr_zh": "DynaMind是一个新颖的框架，通过将时间动态和多模态语义与引导扩散相结合，从EEG信号中重建动态视觉场景，为视频重建准确性和质量树立了新的技术标杆。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "GPSToken: Gaussian Parameterized Spatially-adaptive Tokenization for Image Representation and Generation",
        "summary": "Effective and efficient tokenization plays an important role in image\nrepresentation and generation. Conventional methods, constrained by uniform\n2D/1D grid tokenization, are inflexible to represent regions with varying\nshapes and textures and at different locations, limiting their efficacy of\nfeature representation. In this work, we propose $\\textbf{GPSToken}$, a novel\n$\\textbf{G}$aussian $\\textbf{P}$arameterized $\\textbf{S}$patially-adaptive\n$\\textbf{Token}$ization framework, to achieve non-uniform image tokenization by\nleveraging parametric 2D Gaussians to dynamically model the shape, position,\nand textures of different image regions. We first employ an entropy-driven\nalgorithm to partition the image into texture-homogeneous regions of variable\nsizes. Then, we parameterize each region as a 2D Gaussian (mean for position,\ncovariance for shape) coupled with texture features. A specialized transformer\nis trained to optimize the Gaussian parameters, enabling continuous adaptation\nof position/shape and content-aware feature extraction. During decoding,\nGaussian parameterized tokens are reconstructed into 2D feature maps through a\ndifferentiable splatting-based renderer, bridging our adaptive tokenization\nwith standard decoders for end-to-end training. GPSToken disentangles spatial\nlayout (Gaussian parameters) from texture features to enable efficient\ntwo-stage generation: structural layout synthesis using lightweight networks,\nfollowed by structure-conditioned texture generation. Experiments demonstrate\nthe state-of-the-art performance of GPSToken, which achieves rFID and FID\nscores of 0.65 and 1.50 on image reconstruction and generation tasks using 128\ntokens, respectively. Codes and models of GPSToken can be found at\n$\\href{https://github.com/xtudbxk/GPSToken}{https://github.com/xtudbxk/GPSToken}$.",
        "url": "http://arxiv.org/abs/2509.01109v1",
        "published_date": "2025-09-01T04:01:37+00:00",
        "updated_date": "2025-09-01T04:01:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhengqiang Zhang",
            "Rongyuan Wu",
            "Lingchen Sun",
            "Lei Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "GPSToken proposes a Gaussian Parameterized Spatially-adaptive Tokenization framework for image representation and generation, achieving state-of-the-art performance. It disentangles spatial layout from texture features for more efficient generation.",
        "tldr_zh": "GPSToken提出了一种高斯参数化自适应的标记化框架，用于图像表示和生成，在性能上达到了最先进水平。它将空间布局与纹理特征分离，以实现更高效的生成。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Reinforced Visual Perception with Tools",
        "summary": "Visual reasoning, a cornerstone of human intelligence, encompasses complex\nperceptual and logical processes essential for solving diverse visual problems.\nWhile advances in computer vision have produced powerful models for various\nperceptual tasks, leveraging these for general visual reasoning remains\nchallenging. Prior work demonstrates that augmenting LLMs with vision models\nvia supervised finetuning improves performance, but faces key limitations such\nas expensive data generation, reliance on careful data filtering, and poor\ngeneralization. To address these issues, we propose ReVPT to enhance\nmulti-modal LLMs' abilities to reason about and use visual tools through\nreinforcement learning. We introduce a novel RL algorithm based on GRPO,\ndesigned to train models to reason with a suite of four visual tools. Through\nextensive experiments, we show that our method achieves state-of-the-art\nperformance on several perception-heavy benchmarks, including SAT, CV-Bench,\nBLINK and MMStar, significantly outperforming the supervised and text-based RL\nfinetuning baselines. Notably, Our ReVPT-3B and ReVPT-7B outperform the\ninstruct models by 9.03% and 9.44% on CV-Bench. Finally, we bring to the\ncommunity new insights on RL-based visual tool-usage through extensive\nablations. Our code is available at https://github.com/ls-kelvin/REVPT.",
        "url": "http://arxiv.org/abs/2509.01656v1",
        "published_date": "2025-09-01T17:57:49+00:00",
        "updated_date": "2025-09-01T17:57:49+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Zetong Zhou",
            "Dongping Chen",
            "Zixian Ma",
            "Zhihan Hu",
            "Mingyang Fu",
            "Sinan Wang",
            "Yao Wan",
            "Zhou Zhao",
            "Ranjay Krishna"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes ReVPT, a reinforcement learning approach to enhance visual reasoning capabilities of models using visual tools, achieving state-of-the-art performance on perception-heavy benchmarks.",
        "tldr_zh": "本文提出了一种增强视觉感知能力的强化学习方法ReVPT，通过使用视觉工具，在感知重型基准测试中取得了最新的成绩。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OpenVision 2: A Family of Generative Pretrained Visual Encoders for Multimodal Learning",
        "summary": "This paper provides a simplification on OpenVision's architecture and loss\ndesign for enhancing its training efficiency. Following the prior\nvision-language pretraining works CapPa and AIMv2, as well as modern multimodal\ndesigns like LLaVA, our changes are straightforward: we remove the text encoder\n(and therefore the contrastive loss), retaining only the captioning loss as a\npurely generative training signal. We name this new version OpenVision 2. The\ninitial results are promising: despite this simplification, OpenVision 2\ncompetitively matches the original model's performance on a broad set of\nmultimodal benchmarks while substantially cutting both training time and memory\nconsumption. For example, with ViT-L/14, it reduces training time by about 1.5x\n(from 83h to 57h), and memory usage by about 1.8x (from 24.5GB to 13.8GB,\nequivalently allowing the maximum batch size to grow from 2k to 8k). This\nsuperior training efficiency also allows us to scale far beyond the largest\nvision encoder used in OpenVision, reaching more than 1 billion parameters. We\nhold a strong belief that this lightweight, generative-only paradigm is\ncompelling for future vision encoder development in multimodal foundation\nmodels.",
        "url": "http://arxiv.org/abs/2509.01644v1",
        "published_date": "2025-09-01T17:38:21+00:00",
        "updated_date": "2025-09-01T17:38:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanqing Liu",
            "Xianhang Li",
            "Letian Zhang",
            "Zirui Wang",
            "Zeyu Zheng",
            "Yuyin Zhou",
            "Cihang Xie"
        ],
        "ai_categories": [
            "Multimodality",
            "GAN"
        ],
        "tldr": "The paper introduces OpenVision 2, a simplified version of a visual encoder for multimodal learning that removes the text encoder, leading to improved training efficiency and performance while reducing training time and memory consumption.",
        "tldr_zh": "该论文介绍了 OpenVision 2，这是一个简化版的视觉编码器，用于多模态学习，它移除了文本编码器，提高了训练效率和性能，同时减少了训练时间和内存消耗。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling",
        "summary": "Text-to-image diffusion models are computationally intensive, often requiring\ndozens of forward passes through large transformer backbones. For instance,\nStable Diffusion XL generates high-quality images with 50 evaluations of a\n2.6B-parameter model, an expensive process even for a single batch. Few-step\ndiffusion models reduce this cost to 2-8 denoising steps but still depend on\nlarge, uncompressed U-Net or diffusion transformer backbones, which are often\ntoo costly for full-precision inference without datacenter GPUs. These\nrequirements also limit existing post-training quantization methods that rely\non full-precision calibration. We introduce Q-Sched, a new paradigm for\npost-training quantization that modifies the diffusion model scheduler rather\nthan model weights. By adjusting the few-step sampling trajectory, Q-Sched\nachieves full-precision accuracy with a 4x reduction in model size. To learn\nquantization-aware pre-conditioning coefficients, we propose the JAQ loss,\nwhich combines text-image compatibility with an image quality metric for\nfine-grained optimization. JAQ is reference-free and requires only a handful of\ncalibration prompts, avoiding full-precision inference during calibration.\nQ-Sched delivers substantial gains: a 15.5% FID improvement over the FP16\n4-step Latent Consistency Model and a 16.6% improvement over the FP16 8-step\nPhased Consistency Model, showing that quantization and few-step distillation\nare complementary for high-fidelity generation. A large-scale user study with\nmore than 80,000 annotations further confirms Q-Sched's effectiveness on both\nFLUX.1[schnell] and SDXL-Turbo.",
        "url": "http://arxiv.org/abs/2509.01624v1",
        "published_date": "2025-09-01T17:09:22+00:00",
        "updated_date": "2025-09-01T17:09:22+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Natalia Frumkin",
            "Diana Marculescu"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "GAN"
        ],
        "tldr": "Q-Sched introduces a new post-training quantization method for diffusion models, achieving full-precision accuracy with reduced model size and improved image quality.",
        "tldr_zh": "Q-Sched引入了一种新的用于扩散模型的后训练量化方法，通过减少模型大小并提高图像质量实现全精度准确度。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Improving Large Vision and Language Models by Learning from a Panel of Peers",
        "summary": "Traditional alignment methods for Large Vision and Language Models (LVLMs)\nprimarily rely on human-curated preference data. Human-generated preference\ndata is costly; machine-generated preference data is limited in quality; and\nself-supervised preference data often introduces hallucinations. To overcome\nthese limitations, we propose a novel Panel-of-Peers learning framework\ninspired by collaborative learning among humans. This approach leverages a\npanel of LVLMs, each evaluating and learning from their collective outputs\nthrough an iterative self-improvement process. By simulating a peer review\nsystem, our models generate, assess, and refine outputs in response to a\ncurated set of prompts, mimicking a classroom learning environment. We\ndemonstrate that this methodology enhances model performance without requiring\nextensive human-labeled datasets. Our experiments show significant improvement\nacross multiple benchmarks, demonstrating the potential of peer evaluations as\na scalable alternative to self-supervised alignment. Notably, we show that\nPanel-of-Peers increases the average score on fifteen benchmarks from 48% to\n57%",
        "url": "http://arxiv.org/abs/2509.01610v1",
        "published_date": "2025-09-01T16:43:48+00:00",
        "updated_date": "2025-09-01T16:43:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jefferson Hernandez",
            "Jing Shi",
            "Simon Jenni",
            "Vicente Ordonez",
            "Kushal Kafle"
        ],
        "ai_categories": [
            "Transformer",
            "AIGC"
        ],
        "tldr": "The paper introduces a Panel-of-Peers learning framework for improving Large Vision and Language Models by leveraging a group of models to evaluate and learn from each other, leading to performance enhancements without extensive human-labeled datasets.",
        "tldr_zh": "本文引入了一种基于同行小组的学习框架，通过利用一组模型相互评估和学习，从而提高大型视觉和语言模型的性能，无需大量人工标记的数据集。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "O-DisCo-Edit: Object Distortion Control for Unified Realistic Video Editing",
        "summary": "Diffusion models have recently advanced video editing, yet controllable\nediting remains challenging due to the need for precise manipulation of diverse\nobject properties. Current methods require different control signal for diverse\nediting tasks, which complicates model design and demands significant training\nresources. To address this, we propose O-DisCo-Edit, a unified framework that\nincorporates a novel object distortion control (O-DisCo). This signal, based on\nrandom and adaptive noise, flexibly encapsulates a wide range of editing cues\nwithin a single representation. Paired with a \"copy-form\" preservation module\nfor preserving non-edited regions, O-DisCo-Edit enables efficient,\nhigh-fidelity editing through an effective training paradigm. Extensive\nexperiments and comprehensive human evaluations consistently demonstrate that\nO-DisCo-Edit surpasses both specialized and multitask state-of-the-art methods\nacross various video editing tasks.\nhttps://cyqii.github.io/O-DisCo-Edit.github.io/",
        "url": "http://arxiv.org/abs/2509.01596v1",
        "published_date": "2025-09-01T16:29:39+00:00",
        "updated_date": "2025-09-01T16:29:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuqing Chen",
            "Junjie Wang",
            "Lin Liu",
            "Ruihang Chu",
            "Xiaopeng Zhang",
            "Qi Tian",
            "Yujiu Yang"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces O-DisCo-Edit, a framework for realistic video editing that uses object distortion control based on noise signals to encapsulate various editing cues, surpassing state-of-the-art methods in efficiency and quality.",
        "tldr_zh": "本文介绍了O-DisCo-Edit，这是一个用于逼真视频编辑的框架，利用基于噪声信号的对象失真控制来封装各种编辑提示，优于现有方法的效率和质量。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Acoustic Interference Suppression in Ultrasound images for Real-Time HIFU Monitoring Using an Image-Based Latent Diffusion Model",
        "summary": "High-Intensity Focused Ultrasound (HIFU) is a non-invasive therapeutic\ntechnique widely used for treating various diseases. However, the success and\nsafety of HIFU treatments depend on real-time monitoring, which is often\nhindered by interference when using ultrasound to guide HIFU treatment. To\naddress these challenges, we developed HIFU-ILDiff, a novel deep learning-based\napproach leveraging latent diffusion models to suppress HIFU-induced\ninterference in ultrasound images. The HIFU-ILDiff model employs a Vector\nQuantized Variational Autoencoder (VQ-VAE) to encode noisy ultrasound images\ninto a lower-dimensional latent space, followed by a latent diffusion model\nthat iteratively removes interference. The denoised latent vectors are then\ndecoded to reconstruct high-resolution, interference-free ultrasound images. We\nconstructed a comprehensive dataset comprising 18,872 image pairs from in vitro\nphantoms, ex vivo tissues, and in vivo animal data across multiple imaging\nmodalities and HIFU power levels to train and evaluate the model. Experimental\nresults demonstrate that HIFU-ILDiff significantly outperforms the commonly\nused Notch Filter method, achieving a Structural Similarity Index (SSIM) of\n0.796 and Peak Signal-to-Noise Ratio (PSNR) of 23.780 compared to SSIM of 0.443\nand PSNR of 14.420 for the Notch Filter under in vitro scenarios. Additionally,\nHIFU-ILDiff achieves real-time processing at 15 frames per second, markedly\nfaster than the Notch Filter's 5 seconds per frame. These findings indicate\nthat HIFU-ILDiff is able to denoise HIFU interference in ultrasound guiding\nimages for real-time monitoring during HIFU therapy, which will greatly improve\nthe treatment precision in current clinical applications.",
        "url": "http://arxiv.org/abs/2509.01557v1",
        "published_date": "2025-09-01T15:36:17+00:00",
        "updated_date": "2025-09-01T15:36:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dejia Cai",
            "Yao Ran",
            "Kun Yang",
            "Xinwang Shi",
            "Yingying Zhou",
            "Kexian Wu",
            "Yang Xu",
            "Yi Hu",
            "Xiaowei Zhou"
        ],
        "ai_categories": [
            "AIGC",
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper presents HIFU-ILDiff, a novel deep learning-based method to suppress interference in ultrasound images for real-time monitoring during HIFU therapy.",
        "tldr_zh": "本文介绍了一种名为HIFU-ILDiff的新型深度学习方法，用于在HIFU治疗期间抑制超声图像中的干扰。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unified Supervision For Vision-Language Modeling in 3D Computed Tomography",
        "summary": "General-purpose vision-language models (VLMs) have emerged as promising tools\nin radiology, offering zero-shot capabilities that mitigate the need for large\nlabeled datasets. However, in high-stakes domains like diagnostic radiology,\nthese models often lack the discriminative precision required for reliable\nclinical use. This challenge is compounded by the scarcity and heterogeneity of\npublicly available volumetric CT datasets, which vary widely in annotation\nformats and granularity. To address these limitations, we introduce Uniferum, a\nvolumetric VLM that unifies diverse supervision signals, encoded in\nclassification labels and segmentation masks, into a single training framework.\nBy harmonizing three public 3D CT datasets with distinct annotations, Uniferum\nachieves state-of-the-art performance, improving AUROC on the CT-RATE benchmark\nby 7% compared to CLIP-based and conventional multi-label convolutional models.\nThe model demonstrates robust out-of-distribution generalization, with observed\nevidence of unexpected zero-shot performance on the RAD-CHEST and INSPECT\ndatasets. Our results highlight the effectiveness of integrating heterogeneous\nannotations and body segmentation to enhance model performance, setting a new\ndirection for clinically reliable, data-efficient VLMs in 3D medical imaging.",
        "url": "http://arxiv.org/abs/2509.01554v1",
        "published_date": "2025-09-01T15:30:17+00:00",
        "updated_date": "2025-09-01T15:30:17+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Hao-Chih Lee",
            "Zelong Liu",
            "Hamza Ahmed",
            "Spencer Kim",
            "Sean Huver",
            "Vishwesh Nath",
            "Zahi A. Fayad",
            "Timothy Deyer",
            "Xueyan Mei"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces Uniferum, a unified supervision framework for vision-language models in 3D Computed Tomography, improving performance and generalization in medical imaging.",
        "tldr_zh": "本文介绍了Uniferum，一个针对三维计算机断层扫描中的视觉语言模型的统一监督框架，提高了医学影像领域的性能和泛化能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Variation-aware Vision Token Dropping for Faster Large Vision-Language Models",
        "summary": "Large vision-language models (LVLMs) have demonstrated remarkable\ncapabilities in multimodal understanding tasks. However, the increasing demand\nfor high-resolution image and long-video understanding results in substantial\ntoken counts, leading to reduced inference efficiency. Token compression offers\na direct solution by reducing the number of tokens to be processed, thereby\nimproving computational efficiency. Through extensive analysis, we identify two\ncritical limitations in existing inner-LLM token compression methods:\npositional bias and incompatibility with efficient operators, which hinder\ntheir practical deployment for LVLM acceleration. This paper presents the first\napproach from a token variation perspective, revealing that visual token\nvariations within LLMs exhibit task-agnostic properties. We propose\nVariation-aware Vision Token Dropping (\\textit{i.e.}, \\textbf{V$^2$Drop}),\nwhich progressively removes visual tokens with minimal variation during LVLM\ninference, thereby enhancing computational efficiency. Extensive experiments\nacross multiple models and benchmarks demonstrate that our V$^2$Drop is able to\nmaintain \\textbf{94.0\\%} and \\textbf{98.6\\%} of the original model performance\nfor image and video understanding tasks respectively, while reducing LLM\ngeneration latency by \\textbf{31.5\\%} and \\textbf{74.2\\%}. When combined with\nefficient operators, V$^2$Drop further reduces GPU peak memory usage.",
        "url": "http://arxiv.org/abs/2509.01552v1",
        "published_date": "2025-09-01T15:28:44+00:00",
        "updated_date": "2025-09-01T15:28:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junjie Chen",
            "Xuyang Liu",
            "Zichen Wen",
            "Yiyu Wang",
            "Siteng Huang",
            "Honggang Chen"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper introduces a new method called V$^2$Drop to compress visual tokens in Large Vision-Language Models, improving computational efficiency while maintaining high performance.",
        "tldr_zh": "本文引入了一种名为V$^2$Drop的新方法，用于压缩大型视觉语言模型中的视觉令牌，提高计算效率同时保持高性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Forward-Only Continual Learning",
        "summary": "Catastrophic forgetting remains a central challenge in continual learning\n(CL) with pre-trained models. While existing approaches typically freeze the\nbackbone and fine-tune a small number of parameters to mitigate forgetting,\nthey still rely on iterative error backpropagation and gradient-based\noptimization, which can be computationally intensive and less suitable for\nresource-constrained environments. To address this, we propose FoRo, a\nforward-only, gradient-free continual learning method. FoRo consists of a\nlightweight prompt tuning strategy and a novel knowledge encoding mechanism,\nboth designed without modifying the pre-trained model. Specifically, prompt\nembeddings are inserted at the input layer and optimized using the Covariance\nMatrix Adaptation Evolution Strategy (CMA-ES), which mitigates distribution\nshifts and extracts high-quality task representations. Subsequently,\ntask-specific knowledge is encoded into a knowledge encoding matrix via\nnonlinear random projection and recursive least squares, enabling incremental\nupdates to the classifier without revisiting prior data. Experiments show that\nFoRo significantly reduces average forgetting and improves accuracy. Thanks to\nforward-only learning, FoRo reduces memory usage and run time while maintaining\nhigh knowledge retention across long task sequences. These results suggest that\nFoRo could serve as a promising direction for exploring continual learning with\npre-trained models, especially in real-world multimedia applications where both\nefficiency and effectiveness are critical.",
        "url": "http://arxiv.org/abs/2509.01533v1",
        "published_date": "2025-09-01T15:10:38+00:00",
        "updated_date": "2025-09-01T15:10:38+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Jiao Chen",
            "Jiayi He",
            "Fangfang Chen",
            "Zuohong Lv",
            "Jianhua Tang"
        ],
        "ai_categories": [
            "Transformer",
            "AIGC"
        ],
        "tldr": "The paper proposes a forward-only, gradient-free continual learning method called FoRo, which significantly reduces forgetting and improves accuracy in pre-trained models.",
        "tldr_zh": "本文提出了一种名为FoRo的前向唯一、无梯度的持续学习方法，显著减少了预训练模型的遗忘和提高了准确性。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Continuous-Time Consistency Model for 3D Point Cloud Generation",
        "summary": "Fast and accurate 3D shape generation from point clouds is essential for\napplications in robotics, AR/VR, and digital content creation. We introduce\nConTiCoM-3D, a continuous-time consistency model that synthesizes 3D shapes\ndirectly in point space, without discretized diffusion steps, pre-trained\nteacher models, or latent-space encodings. The method integrates a\nTrigFlow-inspired continuous noise schedule with a Chamfer Distance-based\ngeometric loss, enabling stable training on high-dimensional point sets while\navoiding expensive Jacobian-vector products. This design supports efficient\none- to two-step inference with high geometric fidelity. In contrast to\nprevious approaches that rely on iterative denoising or latent decoders,\nConTiCoM-3D employs a time-conditioned neural network operating entirely in\ncontinuous time, thereby achieving fast generation. Experiments on the ShapeNet\nbenchmark show that ConTiCoM-3D matches or outperforms state-of-the-art\ndiffusion and latent consistency models in both quality and efficiency,\nestablishing it as a practical framework for scalable 3D shape generation.",
        "url": "http://arxiv.org/abs/2509.01492v1",
        "published_date": "2025-09-01T14:11:59+00:00",
        "updated_date": "2025-09-01T14:11:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sebastian Eilermann",
            "René Heesch",
            "Oliver Niggemann"
        ],
        "ai_categories": [
            "GAN",
            "Transformer"
        ],
        "tldr": "ConTiCoM-3D is a continuous-time consistency model for 3D shape generation that outperforms existing models in both quality and efficiency.",
        "tldr_zh": "ConTiCoM-3D 是一个连续时间一致性模型，用于3D形状生成，优于现有模型在质量和效率方面。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Im2Haircut: Single-view Strand-based Hair Reconstruction for Human Avatars",
        "summary": "We present a novel approach for 3D hair reconstruction from single\nphotographs based on a global hair prior combined with local optimization.\nCapturing strand-based hair geometry from single photographs is challenging due\nto the variety and geometric complexity of hairstyles and the lack of ground\ntruth training data. Classical reconstruction methods like multi-view stereo\nonly reconstruct the visible hair strands, missing the inner structure of\nhairstyles and hampering realistic hair simulation. To address this, existing\nmethods leverage hairstyle priors trained on synthetic data. Such data,\nhowever, is limited in both quantity and quality since it requires manual work\nfrom skilled artists to model the 3D hairstyles and create near-photorealistic\nrenderings. To address this, we propose a novel approach that uses both, real\nand synthetic data to learn an effective hairstyle prior. Specifically, we\ntrain a transformer-based prior model on synthetic data to obtain knowledge of\nthe internal hairstyle geometry and introduce real data in the learning process\nto model the outer structure. This training scheme is able to model the visible\nhair strands depicted in an input image, while preserving the general 3D\nstructure of hairstyles. We exploit this prior to create a\nGaussian-splatting-based reconstruction method that creates hairstyles from one\nor more images. Qualitative and quantitative comparisons with existing\nreconstruction pipelines demonstrate the effectiveness and superior performance\nof our method for capturing detailed hair orientation, overall silhouette, and\nbackside consistency. For additional results and code, please refer to\nhttps://im2haircut.is.tue.mpg.de.",
        "url": "http://arxiv.org/abs/2509.01469v1",
        "published_date": "2025-09-01T13:38:08+00:00",
        "updated_date": "2025-09-01T13:38:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Vanessa Sklyarova",
            "Egor Zakharov",
            "Malte Prinzler",
            "Giorgio Becherini",
            "Michael J. Black",
            "Justus Thies"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a method for reconstructing 3D hair from single photographs using a combination of global and local optimization. It leverages both synthetic and real data to learn an effective hairstyle prior for detailed reconstruction.",
        "tldr_zh": "该论文介绍了一种利用全局和局部优化从单张照片重建3D头发的方法。它利用合成数据和真实数据来学习有效的发型先验，以进行详细的重建。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Traces of Image Memorability in Vision Encoders: Activations, Attention Distributions and Autoencoder Losses",
        "summary": "Images vary in how memorable they are to humans. Inspired by findings from\ncognitive science and computer vision, this paper explores the correlates of\nimage memorability in pretrained vision encoders, focusing on latent\nactivations, attention distributions, and the uniformity of image patches. We\nfind that these features correlate with memorability to some extent.\nAdditionally, we explore sparse autoencoder loss over the representations of\nvision transformers as a proxy for memorability, which yields results\noutperforming past methods using convolutional neural network representations.\nOur results shed light on the relationship between model-internal features and\nmemorability. They show that some features are informative predictors of what\nmakes images memorable to humans.",
        "url": "http://arxiv.org/abs/2509.01453v1",
        "published_date": "2025-09-01T13:11:59+00:00",
        "updated_date": "2025-09-01T13:11:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ece Takmaz",
            "Albert Gatt",
            "Jakub Dotlacil"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper explores the relationship between image memorability and vision encoders, finding that certain features correlate with memorability. They introduce a new method using sparse autoencoder loss to predict memorability more effectively.",
        "tldr_zh": "本文探讨了图像记忆性与视觉编码器之间的关系，发现某些特征与记忆性相关。他们引入了一种新的方法，使用稀疏自动编码器损失更有效地预测记忆性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SoccerHigh: A Benchmark Dataset for Automatic Soccer Video Summarization",
        "summary": "Video summarization aims to extract key shots from longer videos to produce\nconcise and informative summaries. One of its most common applications is in\nsports, where highlight reels capture the most important moments of a game,\nalong with notable reactions and specific contextual events. Automatic summary\ngeneration can support video editors in the sports media industry by reducing\nthe time and effort required to identify key segments. However, the lack of\npublicly available datasets poses a challenge in developing robust models for\nsports highlight generation. In this paper, we address this gap by introducing\na curated dataset for soccer video summarization, designed to serve as a\nbenchmark for the task. The dataset includes shot boundaries for 237 matches\nfrom the Spanish, French, and Italian leagues, using broadcast footage sourced\nfrom the SoccerNet dataset. Alongside the dataset, we propose a baseline model\nspecifically designed for this task, which achieves an F1 score of 0.3956 in\nthe test set. Furthermore, we propose a new metric constrained by the length of\neach target summary, enabling a more objective evaluation of the generated\ncontent. The dataset and code are available at\nhttps://ipcv.github.io/SoccerHigh/.",
        "url": "http://arxiv.org/abs/2509.01439v1",
        "published_date": "2025-09-01T12:49:51+00:00",
        "updated_date": "2025-09-01T12:49:51+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "authors": [
            "Artur Díaz-Juan",
            "Coloma Ballester",
            "Gloria Haro"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper introduces SoccerHigh, a benchmark dataset for automatic soccer video summarization, along with a baseline model achieving an F1 score of 0.3956 in the test set.",
        "tldr_zh": "本文介绍了SoccerHigh，一个用于自动足球视频摘要的基准数据集，同时还提出了一个基线模型，在测试集中实现了0.3956的F1分数。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Mamba-CNN: A Hybrid Architecture for Efficient and Accurate Facial Beauty Prediction",
        "summary": "The computational assessment of facial attractiveness, a challenging\nsubjective regression task, is dominated by architectures with a critical\ntrade-off: Convolutional Neural Networks (CNNs) offer efficiency but have\nlimited receptive fields, while Vision Transformers (ViTs) model global context\nat a quadratic computational cost. To address this, we propose Mamba-CNN, a\nnovel and efficient hybrid architecture. Mamba-CNN integrates a lightweight,\nMamba-inspired State Space Model (SSM) gating mechanism into a hierarchical\nconvolutional backbone. This core innovation allows the network to dynamically\nmodulate feature maps and selectively emphasize salient facial features and\ntheir long-range spatial relationships, mirroring human holistic perception\nwhile maintaining computational efficiency. We conducted extensive experiments\non the widely-used SCUT-FBP5500 benchmark, where our model sets a new\nstate-of-the-art. Mamba-CNN achieves a Pearson Correlation (PC) of 0.9187, a\nMean Absolute Error (MAE) of 0.2022, and a Root Mean Square Error (RMSE) of\n0.2610. Our findings validate the synergistic potential of combining CNNs with\nselective SSMs and present a powerful new architectural paradigm for nuanced\nvisual understanding tasks.",
        "url": "http://arxiv.org/abs/2509.01431v1",
        "published_date": "2025-09-01T12:42:04+00:00",
        "updated_date": "2025-09-01T12:42:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Djamel Eddine Boukhari"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes Mamba-CNN, a hybrid architecture combining CNNs with a gating mechanism for facial beauty prediction. It achieves state-of-the-art results on a benchmark dataset.",
        "tldr_zh": "本文提出了Mamba-CNN，一个将CNN与门控机制结合起来用于面部美感预测的混合架构。它在基准数据集上取得了最新的成果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DCA: Graph-Guided Deep Embedding Clustering for Brain Atlases",
        "summary": "Brain atlases are essential for reducing the dimensionality of neuroimaging\ndata and enabling interpretable analysis. However, most existing atlases are\npredefined, group-level templates with limited flexibility and resolution. We\npresent Deep Cluster Atlas (DCA), a graph-guided deep embedding clustering\nframework for generating individualized, voxel-wise brain parcellations. DCA\ncombines a pretrained autoencoder with spatially regularized deep clustering to\nproduce functionally coherent and spatially contiguous regions. Our method\nsupports flexible control over resolution and anatomical scope, and generalizes\nto arbitrary brain structures. We further introduce a standardized benchmarking\nplatform for atlas evaluation, using multiple large-scale fMRI datasets. Across\nmultiple datasets and scales, DCA outperforms state-of-the-art atlases,\nimproving functional homogeneity by 98.8\\% and silhouette coefficient by 29\\%,\nand achieves superior performance in downstream tasks such as autism diagnosis\nand cognitive decoding. Codes and models will be released soon.",
        "url": "http://arxiv.org/abs/2509.01426v1",
        "published_date": "2025-09-01T12:33:32+00:00",
        "updated_date": "2025-09-01T12:33:32+00:00",
        "categories": [
            "q-bio.NC",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Mo Wang",
            "Kaining Peng",
            "Jingsheng Tang",
            "Hongkai Wen",
            "Quanying Liu"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "DCA is a new method for creating individualized brain parcellations with improved performance compared to existing atlases.",
        "tldr_zh": "DCA是一个用于创建个性化脑区分析的新方法，与现有的脑区分析相比，性能更好。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "InfoScale: Unleashing Training-free Variable-scaled Image Generation via Effective Utilization of Information",
        "summary": "Diffusion models (DMs) have become dominant in visual generation but suffer\nperformance drop when tested on resolutions that differ from the training\nscale, whether lower or higher. In fact, the key challenge in generating\nvariable-scale images lies in the differing amounts of information across\nresolutions, which requires information conversion procedures to be varied for\ngenerating variable-scaled images. In this paper, we investigate the issues of\nthree critical aspects in DMs for a unified analysis in variable-scaled\ngeneration: dilated convolution, attention mechanisms, and initial noise.\nSpecifically, 1) dilated convolution in DMs for the higher-resolution\ngeneration loses high-frequency information. 2) Attention for variable-scaled\nimage generation struggles to adjust the information aggregation adaptively. 3)\nThe spatial distribution of information in the initial noise is misaligned with\nvariable-scaled image. To solve the above problems, we propose\n\\textbf{InfoScale}, an information-centric framework for variable-scaled image\ngeneration by effectively utilizing information from three aspects\ncorrespondingly. For information loss in 1), we introduce Progressive Frequency\nCompensation module to compensate for high-frequency information lost by\ndilated convolution in higher-resolution generation. For information\naggregation inflexibility in 2), we introduce Adaptive Information Aggregation\nmodule to adaptively aggregate information in lower-resolution generation and\nachieve an effective balance between local and global information in\nhigher-resolution generation. For information distribution misalignment in 3),\nwe design Noise Adaptation module to re-distribute information in initial noise\nfor variable-scaled generation. Our method is plug-and-play for DMs and\nextensive experiments demonstrate the effectiveness in variable-scaled image\ngeneration.",
        "url": "http://arxiv.org/abs/2509.01421v1",
        "published_date": "2025-09-01T12:27:04+00:00",
        "updated_date": "2025-09-01T12:27:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guohui Zhang",
            "Jiangtong Tan",
            "Linjiang Huang",
            "Zhonghang Yuan",
            "Naishan Zheng",
            "Jie Huang",
            "Feng Zhao"
        ],
        "ai_categories": [
            "GAN"
        ],
        "tldr": "The paper introduces InfoScale, a framework for variable-scaled image generation that effectively utilizes information from dilated convolution, attention mechanisms, and initial noise to address issues in generating images at different resolutions.",
        "tldr_zh": "本文介绍了InfoScale，这是一个用于可变比例图像生成的框架，有效利用了扩张卷积、注意力机制和初始噪音的信息以解决在不同分辨率生成图像时的问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing Partially Relevant Video Retrieval with Robust Alignment Learning",
        "summary": "Partially Relevant Video Retrieval (PRVR) aims to retrieve untrimmed videos\npartially relevant to a given query. The core challenge lies in learning robust\nquery-video alignment against spurious semantic correlations arising from\ninherent data uncertainty: 1) query ambiguity, where the query incompletely\ncharacterizes the target video and often contains uninformative tokens, and 2)\npartial video relevance, where abundant query-irrelevant segments introduce\ncontextual noise in cross-modal alignment. Existing methods often focus on\nenhancing multi-scale clip representations and retrieving the most relevant\nclip. However, the inherent data uncertainty in PRVR renders them vulnerable to\ndistractor videos with spurious similarities, leading to suboptimal\nperformance. To fill this research gap, we propose Robust Alignment Learning\n(RAL) framework, which explicitly models the uncertainty in data. Key\ninnovations include: 1) we pioneer probabilistic modeling for PRVR by encoding\nvideos and queries as multivariate Gaussian distributions. This not only\nquantifies data uncertainty but also enables proxy-level matching to capture\nthe variability in cross-modal correspondences; 2) we consider the\nheterogeneous informativeness of query words and introduce learnable confidence\ngates to dynamically weight similarity. As a plug-and-play solution, RAL can be\nseamlessly integrated into the existing architectures. Extensive experiments\nacross diverse retrieval backbones demonstrate its effectiveness.",
        "url": "http://arxiv.org/abs/2509.01383v1",
        "published_date": "2025-09-01T11:30:43+00:00",
        "updated_date": "2025-09-01T11:30:43+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Long Zhang",
            "Peipei Song",
            "Jianfeng Dong",
            "Kun Li",
            "Xun Yang"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Other"
        ],
        "tldr": "The paper introduces a Robust Alignment Learning framework for Partially Relevant Video Retrieval, addressing data uncertainty and improving alignment between queries and videos.",
        "tldr_zh": "该论文引入了一个Robust Alignment Learning框架，用于部分相关视频检索，解决数据不确定性问题，改善查询和视频之间的对齐。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Uirapuru: Timely Video Analytics for High-Resolution Steerable Cameras on Edge Devices",
        "summary": "Real-time video analytics on high-resolution cameras has become a popular\ntechnology for various intelligent services like traffic control and crowd\nmonitoring. While extensive work has been done on improving analytics accuracy\nwith timing guarantees, virtually all of them target static viewpoint cameras.\nIn this paper, we present Uirapuru, a novel framework for real-time, edge-based\nvideo analytics on high-resolution steerable cameras. The actuation performed\nby those cameras brings significant dynamism to the scene, presenting a\ncritical challenge to existing popular approaches such as frame tiling. To\naddress this problem, Uirapuru incorporates a comprehensive understanding of\ncamera actuation into the system design paired with fast adaptive tiling at a\nper-frame level. We evaluate Uirapuru on a high-resolution video dataset,\naugmented by pan-tilt-zoom (PTZ) movements typical for steerable cameras and on\nreal-world videos collected from an actual PTZ camera. Our experimental results\nshow that Uirapuru provides up to 1.45x improvement in accuracy while\nrespecting specified latency budgets or reaches up to 4.53x inference speedup\nwith on-par accuracy compared to state-of-the-art static camera approaches.",
        "url": "http://arxiv.org/abs/2509.01371v1",
        "published_date": "2025-09-01T11:18:30+00:00",
        "updated_date": "2025-09-01T11:18:30+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "C.3; C.m"
        ],
        "authors": [
            "Guilherme H. Apostolo",
            "Pablo Bauszat",
            "Vinod Nigade",
            "Henri E. Bal",
            "Lin Wang"
        ],
        "ai_categories": [
            "Dataset",
            "Multimodality"
        ],
        "tldr": "Uirapuru is a framework for real-time video analytics on high-resolution steerable cameras, offering improved accuracy and speed compared to traditional approaches.",
        "tldr_zh": "Uirapuru是一个用于在高分辨率可转向摄像机上进行实时视频分析的框架，相较于传统方法提供了更好的准确性和速度。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Identity-Preserving Text-to-Video Generation via Training-Free Prompt, Image, and Guidance Enhancement",
        "summary": "Identity-preserving text-to-video (IPT2V) generation creates videos faithful\nto both a reference subject image and a text prompt. While fine-tuning large\npretrained video diffusion models on ID-matched data achieves state-of-the-art\nresults on IPT2V, data scarcity and high tuning costs hinder broader\nimprovement. We thus introduce a Training-Free Prompt, Image, and Guidance\nEnhancement (TPIGE) framework that bridges the semantic gap between the video\ndescription and the reference image and design sampling guidance that enhances\nidentity preservation and video quality, achieving performance gains at minimal\ncost.Specifically, we first propose Face Aware Prompt Enhancement, using GPT-4o\nto enhance the text prompt with facial details derived from the reference\nimage. We then propose Prompt Aware Reference Image Enhancement, leveraging an\nidentity-preserving image generator to refine the reference image, rectifying\nconflicts with the text prompt. The above mutual refinement significantly\nimproves input quality before video generation. Finally, we propose ID-Aware\nSpatiotemporal Guidance Enhancement, utilizing unified gradients to optimize\nidentity preservation and video quality jointly during generation.Our method\noutperforms prior work and is validated by automatic and human evaluations on a\n1000 video test set, winning first place in the ACM Multimedia 2025\nIdentity-Preserving Video Generation Challenge, demonstrating state-of-the-art\nperformance and strong generality. The code is available at\nhttps://github.com/Andyplus1/IPT2V.git.",
        "url": "http://arxiv.org/abs/2509.01362v1",
        "published_date": "2025-09-01T11:03:13+00:00",
        "updated_date": "2025-09-01T11:03:13+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Jiayi Gao",
            "Changcheng Hua",
            "Qingchao Chen",
            "Yuxin Peng",
            "Yang Liu"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Diffusion"
        ],
        "tldr": "The paper introduces a Training-Free Prompt, Image, and Guidance Enhancement framework for identity-preserving text-to-video generation, achieving state-of-the-art results at minimal cost.",
        "tldr_zh": "本文引入了一个无需训练的提示、图像和引导增强框架，用于保持身份的文本到视频生成，在最小成本下实现了最新技术水平。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "AgroSense: An Integrated Deep Learning System for Crop Recommendation via Soil Image Analysis and Nutrient Profiling",
        "summary": "Meeting the increasing global demand for food security and sustainable\nfarming requires intelligent crop recommendation systems that operate in real\ntime. Traditional soil analysis techniques are often slow, labor-intensive, and\nnot suitable for on-field decision-making. To address these limitations, we\nintroduce AgroSense, a deep-learning framework that integrates soil image\nclassification and nutrient profiling to produce accurate and contextually\nrelevant crop recommendations. AgroSense comprises two main components: a Soil\nClassification Module, which leverages ResNet-18, EfficientNet-B0, and Vision\nTransformer architectures to categorize soil types from images; and a Crop\nRecommendation Module, which employs a Multi-Layer Perceptron, XGBoost,\nLightGBM, and TabNet to analyze structured soil data, including nutrient\nlevels, pH, and rainfall. We curated a multimodal dataset of 10,000 paired\nsamples drawn from publicly available Kaggle repositories, approximately 50,000\nsoil images across seven classes, and 25,000 nutrient profiles for experimental\nevaluation. The fused model achieves 98.0% accuracy, with a precision of 97.8%,\na recall of 97.7%, and an F1-score of 96.75%, while RMSE and MAE drop to 0.32\nand 0.27, respectively. Ablation studies underscore the critical role of\nmultimodal coupling, and statistical validation via t-tests and ANOVA confirms\nthe significance of our improvements. AgroSense offers a practical, scalable\nsolution for real-time decision support in precision agriculture and paves the\nway for future lightweight multimodal AI systems in resource-constrained\nenvironments.",
        "url": "http://arxiv.org/abs/2509.01344v1",
        "published_date": "2025-09-01T10:25:05+00:00",
        "updated_date": "2025-09-01T10:25:05+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Vishal Pandey",
            "Ranjita Das",
            "Debasmita Biswas"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Transformer"
        ],
        "tldr": "AgroSense is a deep learning system that combines soil image analysis and nutrient profiling to recommend crops in real time, achieving high accuracy and performance.",
        "tldr_zh": "AgroSense是一个结合土壤图像分析和营养素分析的深度学习系统，可实时推荐作物，实现了高准确度和性能。",
        "relevance_score": 1,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Street-Level Geolocalization Using Multimodal Large Language Models and Retrieval-Augmented Generation",
        "summary": "Street-level geolocalization from images is crucial for a wide range of\nessential applications and services, such as navigation, location-based\nrecommendations, and urban planning. With the growing popularity of social\nmedia data and cameras embedded in smartphones, applying traditional computer\nvision techniques to localize images has become increasingly challenging, yet\nhighly valuable. This paper introduces a novel approach that integrates\nopen-weight and publicly accessible multimodal large language models with\nretrieval-augmented generation. The method constructs a vector database using\nthe SigLIP encoder on two large-scale datasets (EMP-16 and OSV-5M). Query\nimages are augmented with prompts containing both similar and dissimilar\ngeolocation information retrieved from this database before being processed by\nthe multimodal large language models. Our approach has demonstrated\nstate-of-the-art performance, achieving higher accuracy compared against three\nwidely used benchmark datasets (IM2GPS, IM2GPS3k, and YFCC4k). Importantly, our\nsolution eliminates the need for expensive fine-tuning or retraining and scales\nseamlessly to incorporate new data sources. The effectiveness of\nretrieval-augmented generation-based multimodal large language models in\ngeolocation estimation demonstrated by this paper suggests an alternative path\nto the traditional methods which rely on the training models from scratch,\nopening new possibilities for more accessible and scalable solutions in GeoAI.",
        "url": "http://arxiv.org/abs/2509.01341v1",
        "published_date": "2025-09-01T10:23:48+00:00",
        "updated_date": "2025-09-01T10:23:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yunus Serhat Bicakci",
            "Joseph Shingleton",
            "Anahid Basiri"
        ],
        "ai_categories": [
            "Multimodality",
            "LoRA",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a novel approach for street-level geolocalization using multimodal large language models and retrieval-augmented generation.",
        "tldr_zh": "该论文介绍了一种利用多模态大型语言模型和检索增强生成的街道级地理定位的新方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Image Quality Enhancement and Detection of Small and Dense Objects in Industrial Recycling Processes",
        "summary": "This paper tackles two key challenges: detecting small, dense, and\noverlapping objects (a major hurdle in computer vision) and improving the\nquality of noisy images, especially those encountered in industrial\nenvironments. [1, 2]. Our focus is on evaluating methods built on supervised\ndeep learning. We perform an analysis of these methods, using a newly de-\nveloped dataset comprising over 10k images and 120k in- stances. By evaluating\ntheir performance, accuracy, and com- putational efficiency, we identify the\nmost reliable detection systems and highlight the specific challenges they\naddress in industrial applications. This paper also examines the use of deep\nlearning models to improve image quality in noisy industrial environments. We\nintroduce a lightweight model based on a fully connected convolutional network.\nAddition- ally, we suggest potential future directions for further enhanc- ing\nthe effectiveness of the model. The repository of the dataset and proposed\nmodel can be found at: https://github.com/o-messai/SDOOD,\nhttps://github.com/o-messai/DDSRNet",
        "url": "http://arxiv.org/abs/2509.01332v1",
        "published_date": "2025-09-01T10:14:13+00:00",
        "updated_date": "2025-09-01T10:14:13+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Oussama Messai",
            "Abbass Zein-Eddine",
            "Abdelouahid Bentamou",
            "Mickaël Picq",
            "Nicolas Duquesne",
            "Stéphane Puydarrieux",
            "Yann Gavet"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer",
            "Other"
        ],
        "tldr": "The paper addresses challenges in detecting small, dense objects and improving image quality in noisy industrial environments using supervised deep learning methods.",
        "tldr_zh": "本文使用监督式深度学习方法解决了在嘈杂的工业环境中检测小而密集物体以及提升图像质量的挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Prior-Guided Residual Diffusion: Calibrated and Efficient Medical Image Segmentation",
        "summary": "Ambiguity in medical image segmentation calls for models that capture full\nconditional distributions rather than a single point estimate. We present\nPrior-Guided Residual Diffusion (PGRD), a diffusion-based framework that learns\nvoxel-wise distributions while maintaining strong calibration and practical\nsampling efficiency. PGRD embeds discrete labels as one-hot targets in a\ncontinuous space to align segmentation with diffusion modeling. A coarse prior\npredictor provides step-wise guidance; the diffusion network then learns the\nresidual to the prior, accelerating convergence and improving calibration. A\ndeep diffusion supervision scheme further stabilizes training by supervising\nintermediate time steps. Evaluated on representative MRI and CT datasets, PGRD\nachieves higher Dice scores and lower NLL/ECE values than Bayesian, ensemble,\nProbabilistic U-Net, and vanilla diffusion baselines, while requiring fewer\nsampling steps to reach strong performance.",
        "url": "http://arxiv.org/abs/2509.01330v1",
        "published_date": "2025-09-01T10:13:15+00:00",
        "updated_date": "2025-09-01T10:13:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fuyou Mao",
            "Beining Wu",
            "Yanfeng Jiang",
            "Han Xue",
            "Yan Tang",
            "Hao Zhang"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper introduces Prior-Guided Residual Diffusion (PGRD), a framework for medical image segmentation that captures full conditional distributions efficiently and accurately.",
        "tldr_zh": "该论文介绍了Prior-Guided Residual Diffusion (PGRD)，一种有效而准确地捕获完整条件分布的医学图像分割框架。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multi-Representation Adapter with Neural Architecture Search for Efficient Range-Doppler Radar Object Detection",
        "summary": "Detecting objects efficiently from radar sensors has recently become a\npopular trend due to their robustness against adverse lighting and weather\nconditions compared with cameras. This paper presents an efficient object\ndetection model for Range-Doppler (RD) radar maps. Specifically, we first\nrepresent RD radar maps with multi-representation, i.e., heatmaps and grayscale\nimages, to gather high-level object and fine-grained texture features. Then, we\ndesign an additional Adapter branch, an Exchanger Module with two modes, and a\nPrimary-Auxiliary Fusion Module to effectively extract, exchange, and fuse\nfeatures from the multi-representation inputs, respectively. Furthermore, we\nconstruct a supernet with various width and fusion operations in the Adapter\nbranch for the proposed model and employ a One-Shot Neural Architecture Search\nmethod to further improve the model's efficiency while maintaining high\nperformance. Experimental results demonstrate that our model obtains favorable\naccuracy and efficiency trade-off. Moreover, we achieve new state-of-the-art\nperformance on RADDet and CARRADA datasets with mAP@50 of 71.9 and 57.1,\nrespectively.",
        "url": "http://arxiv.org/abs/2509.01280v1",
        "published_date": "2025-09-01T09:06:53+00:00",
        "updated_date": "2025-09-01T09:06:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhiwei Lin",
            "Weicheng Zheng",
            "Yongtao Wang"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper presents an efficient object detection model for radar sensors using multi-representation and neural architecture search, achieving state-of-the-art performance on relevant datasets.",
        "tldr_zh": "该论文提出了一种利用多重表示和神经架构搜索的雷达传感器效率物体检测模型，在相关数据集上取得了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PRINTER:Deformation-Aware Adversarial Learning for Virtual IHC Staining with In Situ Fidelity",
        "summary": "Tumor spatial heterogeneity analysis requires precise correlation between\nHematoxylin and Eosin H&E morphology and immunohistochemical (IHC) biomarker\nexpression, yet current methods suffer from spatial misalignment in consecutive\nsections, severely compromising in situ pathological interpretation. In order\nto obtain a more accurate virtual staining pattern, We propose PRINTER, a\nweakly-supervised framework that integrates PRototype-drIven content and\nstaiNing patTERn decoupling and deformation-aware adversarial learning\nstrategies designed to accurately learn IHC staining patterns while preserving\nH&E staining details. Our approach introduces three key innovations: (1) A\nprototype-driven staining pattern transfer with explicit content-style\ndecoupling; and (2) A cyclic registration-synthesis framework GapBridge that\nbridges H&E and IHC domains through deformable structural alignment, where\nregistered features guide cross-modal style transfer while synthesized outputs\niteratively refine the registration;(3) Deformation-Aware Adversarial Learning:\nWe propose a training framework where a generator and deformation-aware\nregistration network jointly adversarially optimize a style-focused\ndiscriminator. Extensive experiments demonstrate that PRINTER effectively\nachieves superior performance in preserving H&E staining details and virtual\nstaining fidelity, outperforming state-of-the-art methods. Our work provides a\nrobust and scalable solution for virtual staining, advancing the field of\ncomputational pathology.",
        "url": "http://arxiv.org/abs/2509.01214v1",
        "published_date": "2025-09-01T07:53:05+00:00",
        "updated_date": "2025-09-01T07:53:05+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Yizhe Yuan",
            "Bingsen Xue",
            "Bangzheng Pu",
            "Chengxiang Wang",
            "Cheng Jin"
        ],
        "ai_categories": [
            "GAN",
            "Multimodality",
            "Other"
        ],
        "tldr": "The paper presents PRINTER, a framework for accurate virtual staining in pathology by integrating prototype-driven content and staining pattern decoupling with deformation-aware adversarial learning.",
        "tldr_zh": "该论文提出了PRINTER，一个用于在病理学中准确虚拟染色的框架，通过整合原型驱动内容和染色模式解耦与变形感知对抗学习。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PrediTree: A Multi-Temporal Sub-meter Dataset of Multi-Spectral Imagery Aligned With Canopy Height Maps",
        "summary": "We present PrediTree, the first comprehensive open-source dataset designed\nfor training and evaluating tree height prediction models at sub-meter\nresolution. This dataset combines very high-resolution (0.5m) LiDAR-derived\ncanopy height maps, spatially aligned with multi-temporal and multi-spectral\nimagery, across diverse forest ecosystems in France, totaling 3,141,568 images.\nPrediTree addresses a critical gap in forest monitoring capabilities by\nenabling the training of deep learning methods that can predict tree growth\nbased on multiple past observations. %\\sout{Initially focused on French\nforests, PrediTree is designed as an expanding resource with ongoing efforts to\nincorporate data from other countries. } To make use of this PrediTree dataset,\nwe propose an encoder-decoder framework that requires the multi-temporal\nmulti-spectral imagery and the relative time differences in years between the\ncanopy height map timestamp (target) and each image acquisition date for which\nthis framework predicts the canopy height. The conducted experiments\ndemonstrate that a U-Net architecture trained on the PrediTree dataset provides\nthe highest masked mean squared error of $11.78\\%$, outperforming the next-best\narchitecture, ResNet-50, by around $12\\%$, and cutting the error of the same\nexperiments but on fewer bands (red, green, blue only), by around $30\\%$. This\ndataset is publicly available on \\href{URL}{HuggingFace}, and both processing\nand training codebases are available on \\href{URL}{GitHub}.",
        "url": "http://arxiv.org/abs/2509.01202v1",
        "published_date": "2025-09-01T07:41:27+00:00",
        "updated_date": "2025-09-01T07:41:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hiyam Debary",
            "Mustansar Fiaz",
            "Levente Klein"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "PrediTree is an open-source dataset for tree height prediction models, combining high-resolution canopy height maps with multi-temporal and multi-spectral imagery in French forests.",
        "tldr_zh": "PrediTree是一个针对树高预测模型的开源数据集，在法国森林中结合高分辨率的冠层高度图和多时相、多光谱图像。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Do Video Language Models Really Know Where to Look? Diagnosing Attention Failures in Video Language Models",
        "summary": "Recent advances in multimodal large language models (MLLMs) have led to much\nprogress in video understanding tasks. To avoid the heavy computational cost of\nprocessing all frames, these models typically rely on keyframe sampling methods\nguided by vision-language encoders (\\textit{e.g.,} SigLIP). However, it remains\nunclear whether such encoders can truly identify the most informative frames.\nIn this work, we provide several empirical pieces of evidence revealing that\npopular vision encoders critically suffer from their limited capability to\nidentify where the MLLM should look inside the video to handle the given\ntextual query appropriately. Our findings suggest that the development of\nbetter keyframe identification techniques may be necessary for efficient video\nMLLMs.",
        "url": "http://arxiv.org/abs/2509.01167v1",
        "published_date": "2025-09-01T06:39:08+00:00",
        "updated_date": "2025-09-01T06:39:08+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Hyunjong Ok",
            "Jaeho Lee"
        ],
        "ai_categories": [
            "Multimodality"
        ],
        "tldr": "The paper discusses the limitations of vision encoders in identifying keyframes for video language models, indicating the need for better keyframe identification techniques.",
        "tldr_zh": "本文讨论了视觉编码器在识别视频语言模型的关键帧方面存在的局限性，并指出有必要发展更好的关键帧识别技术。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MetaSSL: A General Heterogeneous Loss for Semi-Supervised Medical Image Segmentation",
        "summary": "Semi-Supervised Learning (SSL) is important for reducing the annotation cost\nfor medical image segmentation models. State-of-the-art SSL methods such as\nMean Teacher, FixMatch and Cross Pseudo Supervision (CPS) are mainly based on\nconsistency regularization or pseudo-label supervision between a reference\nprediction and a supervised prediction. Despite the effectiveness, they have\noverlooked the potential noise in the labeled data, and mainly focus on\nstrategies to generate the reference prediction, while ignoring the\nheterogeneous values of different unlabeled pixels. We argue that effectively\nmining the rich information contained by the two predictions in the loss\nfunction, instead of the specific strategy to obtain a reference prediction, is\nmore essential for SSL, and propose a universal framework MetaSSL based on a\nspatially heterogeneous loss that assigns different weights to pixels by\nsimultaneously leveraging the uncertainty and consistency information between\nthe reference and supervised predictions. Specifically, we split the\npredictions on unlabeled data into four regions with decreasing weights in the\nloss: Unanimous and Confident (UC), Unanimous and Suspicious (US), Discrepant\nand Confident (DC), and Discrepant and Suspicious (DS), where an adaptive\nthreshold is proposed to distinguish confident predictions from suspicious\nones. The heterogeneous loss is also applied to labeled images for robust\nlearning considering the potential annotation noise. Our method is\nplug-and-play and general to most existing SSL methods. The experimental\nresults showed that it improved the segmentation performance significantly when\nintegrated with existing SSL frameworks on different datasets. Code is\navailable at https://github.com/HiLab-git/MetaSSL.",
        "url": "http://arxiv.org/abs/2509.01144v1",
        "published_date": "2025-09-01T05:45:08+00:00",
        "updated_date": "2025-09-01T05:45:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weiren Zhao",
            "Lanfeng Zhong",
            "Xin Liao",
            "Wenjun Liao",
            "Sichuan Zhang",
            "Shaoting Zhang",
            "Guotai Wang"
        ],
        "ai_categories": [
            "Multimodality"
        ],
        "tldr": "MetaSSL proposes a novel heterogeneous loss for semi-supervised medical image segmentation, improving performance by considering uncertainty and consistency information between reference and supervised predictions.",
        "tldr_zh": "MetaSSL 提出了一种新颖的异构损失函数，用于半监督医学图像分割，通过考虑参考和监督预测之间的不确定性和一致性信息，提高了性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RealMat: Realistic Materials with Diffusion and Reinforcement Learning",
        "summary": "Generative models for high-quality materials are particularly desirable to\nmake 3D content authoring more accessible. However, the majority of material\ngeneration methods are trained on synthetic data. Synthetic data provides\nprecise supervision for material maps, which is convenient but also tends to\ncreate a significant visual gap with real-world materials. Alternatively,\nrecent work used a small dataset of real flash photographs to guarantee\nrealism, however such data is limited in scale and diversity. To address these\nlimitations, we propose RealMat, a diffusion-based material generator that\nleverages realistic priors, including a text-to-image model and a dataset of\nrealistic material photos under natural lighting. In RealMat, we first finetune\na pretrained Stable Diffusion XL (SDXL) with synthetic material maps arranged\nin $2 \\times 2$ grids. This way, our model inherits some realism of SDXL while\nlearning the data distribution of the synthetic material grids. Still, this\ncreates a realism gap, with some generated materials appearing synthetic. We\npropose to further finetune our model through reinforcement learning (RL),\nencouraging the generation of realistic materials. We develop a realism reward\nfunction for any material image under natural lighting, by collecting a\nlarge-scale dataset of realistic material images. We show that this approach\nincreases generated materials' realism compared to our base model and related\nwork.",
        "url": "http://arxiv.org/abs/2509.01134v1",
        "published_date": "2025-09-01T05:04:51+00:00",
        "updated_date": "2025-09-01T05:04:51+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Xilong Zhou",
            "Pedro Figueiredo",
            "Miloš Hašan",
            "Valentin Deschaintre",
            "Paul Guerrero",
            "Yiwei Hu",
            "Nima Khademi Kalantari"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces RealMat, a material generator that leverages text-to-image models and real material photos to create realistic materials. It addresses the limitations of current methods by using a diffusion-based approach and reinforcement learning.",
        "tldr_zh": "本文介绍了RealMat，一种材料生成器，利用文本到图像模型和真实材料照片创建逼真材料。 通过使用扩散方法和强化学习，解决了当前方法的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FICGen: Frequency-Inspired Contextual Disentanglement for Layout-driven Degraded Image Generation",
        "summary": "Layout-to-image (L2I) generation has exhibited promising results in natural\ndomains, but suffers from limited generative fidelity and weak alignment with\nuser-provided layouts when applied to degraded scenes (i.e., low-light,\nunderwater). We primarily attribute these limitations to the \"contextual\nillusion dilemma\" in degraded conditions, where foreground instances are\noverwhelmed by context-dominant frequency distributions. Motivated by this, our\npaper proposes a new Frequency-Inspired Contextual Disentanglement Generative\n(FICGen) paradigm, which seeks to transfer frequency knowledge of degraded\nimages into the latent diffusion space, thereby facilitating the rendering of\ndegraded instances and their surroundings via contextual frequency-aware\nguidance. To be specific, FICGen consists of two major steps. Firstly, we\nintroduce a learnable dual-query mechanism, each paired with a dedicated\nfrequency resampler, to extract contextual frequency prototypes from\npre-collected degraded exemplars in the training set. Secondly, a\nvisual-frequency enhanced attention is employed to inject frequency prototypes\ninto the degraded generation process. To alleviate the contextual illusion and\nattribute leakage, an instance coherence map is developed to regulate\nlatent-space disentanglement between individual instances and their\nsurroundings, coupled with an adaptive spatial-frequency aggregation module to\nreconstruct spatial-frequency mixed degraded representations. Extensive\nexperiments on 5 benchmarks involving a variety of degraded scenarios-from\nsevere low-light to mild blur-demonstrate that FICGen consistently surpasses\nexisting L2I methods in terms of generative fidelity, alignment and downstream\nauxiliary trainability.",
        "url": "http://arxiv.org/abs/2509.01107v1",
        "published_date": "2025-09-01T04:00:22+00:00",
        "updated_date": "2025-09-01T04:00:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenzhuang Wang",
            "Yifan Zhao",
            "Mingcan Ma",
            "Ming Liu",
            "Zhonglin Jiang",
            "Yong Chen",
            "Jia Li"
        ],
        "ai_categories": [
            "GAN",
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces a new method, FICGen, for generating high-quality images from degraded scenes by incorporating frequency information into the generation process.",
        "tldr_zh": "该论文提出了一种新方法，FICGen，通过将频率信息纳入生成过程，从退化场景中生成高质量图像。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "An End-to-End Framework for Video Multi-Person Pose Estimation",
        "summary": "Video-based human pose estimation models aim to address scenarios that cannot\nbe effectively solved by static image models such as motion blur, out-of-focus\nand occlusion. Most existing approaches consist of two stages: detecting human\ninstances in each image frame and then using a temporal model for single-person\npose estimation. This approach separates the spatial and temporal dimensions\nand cannot capture the global spatio-temporal context between spatial instances\nfor end-to-end optimization. In addition, it relies on separate detectors and\ncomplex post-processing such as RoI cropping and NMS, which reduces the\ninference efficiency of the video scene. To address the above problems, we\npropose VEPE (Video End-to-End Pose Estimation), a simple and flexible\nframework for end-to-end pose estimation in video. The framework utilizes three\ncrucial spatio-temporal Transformer components: the Spatio-Temporal Pose\nEncoder (STPE), the Spatio-Temporal Deformable Memory Encoder (STDME), and the\nSpatio-Temporal Pose Decoder (STPD). These components are designed to\neffectively utilize temporal context for optimizing human body pose estimation.\nFurthermore, to reduce the mismatch problem during the cross-frame pose query\nmatching process, we propose an instance consistency mechanism, which aims to\nenhance the consistency and discrepancy of the cross-frame instance query and\nrealize the instance tracking function, which in turn accurately guides the\npose query to perform cross-frame matching. Extensive experiments on the\nPosetrack dataset show that our approach outperforms most two-stage models and\nimproves inference efficiency by 300%.",
        "url": "http://arxiv.org/abs/2509.01095v1",
        "published_date": "2025-09-01T03:34:57+00:00",
        "updated_date": "2025-09-01T03:34:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhihong Wei"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a framework called VEPE for end-to-end human pose estimation in videos, outperforming existing models and improving inference efficiency by 300%.",
        "tldr_zh": "本文提出了一个称为VEPE的框架，用于视频中的端到端人体姿势估计，在Posetrack数据集上表现优于现有模型，并提高了300%的推断效率。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Bidirectional Sparse Attention for Faster Video Diffusion Training",
        "summary": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention.",
        "url": "http://arxiv.org/abs/2509.01085v1",
        "published_date": "2025-09-01T03:16:52+00:00",
        "updated_date": "2025-09-01T03:16:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chenlu Zhan",
            "Wen Li",
            "Chuyu Shen",
            "Jun Zhang",
            "Suhui Wu",
            "Hao Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion"
        ],
        "tldr": "The paper introduces a Bidirectional Sparse Attention framework to speed up training for video generation models, significantly reducing computational costs while maintaining generative quality.",
        "tldr_zh": "该论文引入了双向稀疏注意力框架，加速视频生成模型的训练，显著减少计算成本同时保持生成质量。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SpectMamba: Integrating Frequency and State Space Models for Enhanced Medical Image Detection",
        "summary": "Abnormality detection in medical imaging is a critical task requiring both\nhigh efficiency and accuracy to support effective diagnosis. While\nconvolutional neural networks (CNNs) and Transformer-based models are widely\nused, both face intrinsic challenges: CNNs have limited receptive fields,\nrestricting their ability to capture broad contextual information, and\nTransformers encounter prohibitive computational costs when processing\nhigh-resolution medical images. Mamba, a recent innovation in natural language\nprocessing, has gained attention for its ability to process long sequences with\nlinear complexity, offering a promising alternative. Building on this\nfoundation, we present SpectMamba, the first Mamba-based architecture designed\nfor medical image detection. A key component of SpectMamba is the Hybrid\nSpatial-Frequency Attention (HSFA) block, which separately learns high- and\nlow-frequency features. This approach effectively mitigates the loss of\nhigh-frequency information caused by frequency bias and correlates\nfrequency-domain features with spatial features, thereby enhancing the model's\nability to capture global context. To further improve long-range dependencies,\nwe propose the Visual State-Space Module (VSSM) and introduce a novel Hilbert\nCurve Scanning technique to strengthen spatial correlations and local\ndependencies, further optimizing the Mamba framework. Comprehensive experiments\nshow that SpectMamba achieves state-of-the-art performance while being both\neffective and efficient across various medical image detection tasks.",
        "url": "http://arxiv.org/abs/2509.01080v1",
        "published_date": "2025-09-01T02:56:45+00:00",
        "updated_date": "2025-09-01T02:56:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yao Wang",
            "Dong Yang",
            "Zhi Qiao",
            "Wenjian Huang",
            "Liuzhi Yang",
            "Zhen Qian"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "SpectMamba is a novel architecture for medical image detection, leveraging Mamba with Hybrid Spatial-Frequency Attention and Visual State-Space Module for improved performance.",
        "tldr_zh": "SpectMamba是一种用于医学图像检测的新型架构，利用Mamba与混合空间 - 频率注意力和视觉状态空间模块以提高性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated\nsuccess in enhancing LLM reasoning capabilities, but remains limited to\nsingle-turn interactions without tool integration. While recent Agentic\nReinforcement Learning with Tool use (ARLT) approaches have emerged to address\nmulti-turn tool interactions, existing works develop task-specific codebases\nthat suffer from fragmentation, synchronous execution bottlenecks, and limited\nextensibility across domains. These inefficiencies hinder broader community\nadoption and algorithmic innovation. We introduce VerlTool, a unified and\nmodular framework that addresses these limitations through systematic design\nprinciples. VerlTool provides four key contributions: (1) upstream alignment\nwith VeRL ensuring compatibility and simplified maintenance, (2) unified tool\nmanagement via standardized APIs supporting diverse modalities including code\nexecution, search, SQL databases, and vision processing, (3) asynchronous\nrollout execution achieving near 2$\\times$ speedup by eliminating\nsynchronization bottlenecks, and (4) comprehensive evaluation demonstrating\ncompetitive performance across 6 ARLT domains. Our framework formalizes ARLT as\nmulti-turn trajectories with multi-modal observation tokens (text/image/video),\nextending beyond single-turn RLVR paradigms. We train and evaluate models on\nmathematical reasoning, knowledge QA, SQL generation, visual reasoning, web\nsearch, and software engineering tasks, achieving results comparable to\nspecialized systems while providing unified training infrastructure. The\nmodular plugin architecture enables rapid tool integration requiring only\nlightweight Python definitions, significantly reducing development overhead and\nproviding a scalable foundation for tool-augmented RL research. Our code is\nopen-sourced at https://github.com/TIGER-AI-Lab/verl-tool.",
        "url": "http://arxiv.org/abs/2509.01055v1",
        "published_date": "2025-09-01T01:45:18+00:00",
        "updated_date": "2025-09-01T01:45:18+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Dongfu Jiang",
            "Yi Lu",
            "Zhuofeng Li",
            "Zhiheng Lyu",
            "Ping Nie",
            "Haozhe Wang",
            "Alex Su",
            "Hui Chen",
            "Kai Zou",
            "Chao Du",
            "Tianyu Pang",
            "Wenhu Chen"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces VerlTool, a framework for Agentic Reinforcement Learning with Tool use, addressing limitations of existing approaches and demonstrating competitive performance across various domains.",
        "tldr_zh": "本文介绍了VerlTool，这是一个面向工具使用的主体强化学习的框架，解决了现有方法的局限性，并在各个领域展示了竞争性表现。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Integrating Multi-Spectral Imaging with Gaussian Splatting",
        "summary": "We present a study of how to integrate color (RGB) and multi-spectral imagery\n(red, green, red-edge, and near-infrared) into the 3D Gaussian Splatting (3DGS)\nframework, a state-of-the-art explicit radiance-field-based method for fast and\nhigh-fidelity 3D reconstruction from multi-view images. While 3DGS excels on\nRGB data, naive per-band optimization of additional spectra yields poor\nreconstructions due to inconsistently appearing geometry in the spectral\ndomain. This problem is prominent, even though the actual geometry is the same,\nregardless of spectral modality. To investigate this, we evaluate three\nstrategies: 1) Separate per-band reconstruction with no shared structure. 2)\nSplitting optimization, in which we first optimize RGB geometry, copy it, and\nthen fit each new band to the model by optimizing both geometry and band\nrepresentation. 3) Joint, in which the modalities are jointly optimized,\noptionally with an initial RGB-only phase. We showcase through quantitative\nmetrics and qualitative novel-view renderings on multi-spectral datasets the\neffectiveness of our dedicated optimized Joint strategy, increasing overall\nspectral reconstruction as well as enhancing RGB results through spectral\ncross-talk. We therefore suggest integrating multi-spectral data directly into\nthe spherical harmonics color components to compactly model each Gaussian's\nmulti-spectral reflectance. Moreover, our analysis reveals several key\ntrade-offs in when and how to introduce spectral bands during optimization,\noffering practical insights for robust multi-modal 3DGS reconstruction.",
        "url": "http://arxiv.org/abs/2509.00989v1",
        "published_date": "2025-08-31T20:53:35+00:00",
        "updated_date": "2025-08-31T20:53:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Josef Grün",
            "Lukas Meyer",
            "Maximilian Weiherer",
            "Bernhard Egger",
            "Marc Stamminger",
            "Linus Franke"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper explores integrating multi-spectral imaging with Gaussian Splatting for 3D reconstruction, showcasing improved results through a dedicated joint optimization strategy.",
        "tldr_zh": "本文探讨了如何将多光谱成像与高斯分层结合起来进行三维重建，通过专门的联合优化策略展示出更好的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Seeing More, Saying More: Lightweight Language Experts are Dynamic Video Token Compressors",
        "summary": "Recent advancements in large video-language models have revolutionized video\nunderstanding tasks. However, their efficiency is significantly constrained by\nprocessing high volumes of visual tokens. Existing token compression strategies\napply a fixed compression ratio, ignoring the variability in semantic density\namong different video clips. Consequently, this lead to inadequate\nrepresentation of information-rich clips due to insufficient tokens and\nunnecessary computation on static or content-poor ones. To address this, we\npropose LangDC, a Language-aware Dynamic Token Compressor. LangDC leverages a\nlightweight language model to describe video clips, converting them into soft\ncaption tokens as visual representations. Trained with our proposed semantic\ndensity-aware supervision, LangDC aims to 1) cover key visual cues necessary\nfor downstream task reasoning and 2) dynamically adjust compression ratios\nbased on scene richness, reflected by descriptions length. Our design mimics\nhow humans dynamically express what they see: complex scenes (seeing more)\nelicit more detailed language to convey nuances (saying more), whereas simpler\nscenes are described with fewer words. Experimental results show that our\nmethod reduces FLOPs by 49% compared to VideoGPT+ while maintaining competitive\nperformance. Furthermore, qualitative results demonstrate our approach\nadaptively adjusts the token compression ratio based on video segment richness.",
        "url": "http://arxiv.org/abs/2509.00969v1",
        "published_date": "2025-08-31T19:27:29+00:00",
        "updated_date": "2025-08-31T19:27:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiangchen Wang",
            "Jinrui Zhang",
            "Teng Wang",
            "Haigang Zhang",
            "Feng Zheng"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a dynamic token compression method, LangDC, using a lightweight language model to compress video tokens based on scene richness, resulting in reduced computation while maintaining competitive performance.",
        "tldr_zh": "本文提出了一种动态标记压缩方法LangDC，利用轻量级语言模型根据场景丰富度来压缩视频标记，从而减少计算量，同时保持竞争力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association",
        "summary": "We present ViSTA-SLAM as a real-time monocular visual SLAM system that\noperates without requiring camera intrinsics, making it broadly applicable\nacross diverse camera setups. At its core, the system employs a lightweight\nsymmetric two-view association (STA) model as the frontend, which\nsimultaneously estimates relative camera poses and regresses local pointmaps\nfrom only two RGB images. This design reduces model complexity significantly,\nthe size of our frontend is only 35\\% that of comparable state-of-the-art\nmethods, while enhancing the quality of two-view constraints used in the\npipeline. In the backend, we construct a specially designed Sim(3) pose graph\nthat incorporates loop closures to address accumulated drift. Extensive\nexperiments demonstrate that our approach achieves superior performance in both\ncamera tracking and dense 3D reconstruction quality compared to current\nmethods. Github repository: https://github.com/zhangganlin/vista-slam",
        "url": "http://arxiv.org/abs/2509.01584v1",
        "published_date": "2025-09-01T16:12:23+00:00",
        "updated_date": "2025-09-01T16:12:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ganlin Zhang",
            "Shenhan Qian",
            "Xi Wang",
            "Daniel Cremers"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "ViSTA-SLAM is a real-time monocular visual SLAM system that operates without camera intrinsics, achieving superior performance in camera tracking and 3D reconstruction.",
        "tldr_zh": "ViSTA-SLAM是一个实时的单眼视觉SLAM系统，不需要相机内参，在相机跟踪和3D重建方面表现出色。",
        "relevance_score": 2,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "MSA2-Net: Utilizing Self-Adaptive Convolution Module to Extract Multi-Scale Information in Medical Image Segmentation",
        "summary": "The nnUNet segmentation framework adeptly adjusts most hyperparameters in\ntraining scripts automatically, but it overlooks the tuning of internal\nhyperparameters within the segmentation network itself, which constrains the\nmodel's ability to generalize. Addressing this limitation, this study presents\na novel Self-Adaptive Convolution Module that dynamically adjusts the size of\nthe convolution kernels depending on the unique fingerprints of different\ndatasets. This adjustment enables the MSA2-Net, when equipped with this module,\nto proficiently capture both global and local features within the feature maps.\nSelf-Adaptive Convolution Module is strategically integrated into two key\ncomponents of the MSA2-Net: the Multi-Scale Convolution Bridge and the\nMulti-Scale Amalgamation Decoder. In the MSConvBridge, the module enhances the\nability to refine outputs from various stages of the CSWin Transformer during\nthe skip connections, effectively eliminating redundant data that could\npotentially impair the decoder's performance. Simultaneously, the MSADecoder,\nutilizing the module, excels in capturing detailed information of organs\nvarying in size during the decoding phase. This capability ensures that the\ndecoder's output closely reproduces the intricate details within the feature\nmaps, thus yielding highly accurate segmentation images. MSA2-Net, bolstered by\nthis advanced architecture, has demonstrated exceptional performance, achieving\nDice coefficient scores of 86.49\\%, 92.56\\%, 93.37\\%, and 92.98\\% on the\nSynapse, ACDC, Kvasir, and Skin Lesion Segmentation (ISIC2017) datasets,\nrespectively. This underscores MSA2-Net's robustness and precision in medical\nimage segmentation tasks across various datasets.",
        "url": "http://arxiv.org/abs/2509.01498v2",
        "published_date": "2025-09-01T14:19:15+00:00",
        "updated_date": "2025-09-03T02:39:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Chao Deng",
            "Xiaosen Li",
            "Xiao Qin"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel Self-Adaptive Convolution Module for medical image segmentation to adjust convolution kernel sizes dynamically, improving global and local feature extraction.",
        "tldr_zh": "本文介绍了一种新颖的自适应卷积模块，用于医学图像分割，动态调整卷积核大小，改善全局和局部特征提取。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "PointSlice: Accurate and Efficient Slice-Based Representation for 3D Object Detection from Point Clouds",
        "summary": "3D object detection from point clouds plays a critical role in autonomous\ndriving. Currently, the primary methods for point cloud processing are\nvoxel-based and pillarbased approaches. Voxel-based methods offer high accuracy\nthrough fine-grained spatial segmentation but suffer from slower inference\nspeeds. Pillar-based methods enhance inference speed but still fall short of\nvoxel-based methods in accuracy. To address these issues, we propose a novel\npoint cloud processing method, PointSlice, which slices point clouds along the\nhorizontal plane and includes a dedicated detection network. The main\ncontributions of PointSlice are: (1) A new point cloud processing technique\nthat converts 3D point clouds into multiple sets of 2D (x-y) data slices. The\nmodel only learns 2D data distributions, treating the 3D point cloud as\nseparate batches of 2D data, which reduces the number of model parameters and\nenhances inference speed; (2) The introduction of a Slice Interaction Network\n(SIN). To maintain vertical relationships across slices, we incorporate SIN\ninto the 2D backbone network, which improves the model's 3D object perception\ncapability. Extensive experiments demonstrate that PointSlice achieves high\ndetection accuracy and inference speed. On the Waymo dataset, PointSlice is\n1.13x faster and has 0.79x fewer parameters than the state-of-the-art\nvoxel-based method (SAFDNet), with only a 1.2 mAPH accuracy reduction. On the\nnuScenes dataset, we achieve a state-of-the-art detection result of 66.74 mAP.\nOn the Argoverse 2 dataset, PointSlice is 1.10x faster, with 0.66x fewer\nparameters and a 1.0 mAP accuracy reduction. The code will be available at\nhttps://github.com/qifeng22/PointSlice2.",
        "url": "http://arxiv.org/abs/2509.01487v1",
        "published_date": "2025-09-01T14:08:21+00:00",
        "updated_date": "2025-09-01T14:08:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Liu Qifeng",
            "Zhao Dawei",
            "Dong Yabo",
            "Xiao Liang",
            "Wang Juan",
            "Min Chen",
            "Li Fuyang",
            "Jiang Weizhong",
            "Lu Dongming",
            "Nie Yiming"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "PointSlice proposes a novel method for 3D object detection from point clouds, achieving high accuracy and inference speed by slicing point clouds into 2D data slices.",
        "tldr_zh": "PointSlice提出了一种新颖的方法，通过将点云切片成2D数据片段，实现了高准确度和推断速度。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Guided Model-based LiDAR Super-Resolution for Resource-Efficient Automotive scene Segmentation",
        "summary": "High-resolution LiDAR data plays a critical role in 3D semantic segmentation\nfor autonomous driving, but the high cost of advanced sensors limits\nlarge-scale deployment. In contrast, low-cost sensors such as 16-channel LiDAR\nproduce sparse point clouds that degrade segmentation accuracy. To overcome\nthis, we introduce the first end-to-end framework that jointly addresses LiDAR\nsuper-resolution (SR) and semantic segmentation. The framework employs joint\noptimization during training, allowing the SR module to incorporate semantic\ncues and preserve fine details, particularly for smaller object classes. A new\nSR loss function further directs the network to focus on regions of interest.\nThe proposed lightweight, model-based SR architecture uses significantly fewer\nparameters than existing LiDAR SR approaches, while remaining easily compatible\nwith segmentation networks. Experiments show that our method achieves\nsegmentation performance comparable to models operating on high-resolution and\ncostly 64-channel LiDAR data.",
        "url": "http://arxiv.org/abs/2509.01317v1",
        "published_date": "2025-09-01T10:01:40+00:00",
        "updated_date": "2025-09-01T10:01:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Alexandros Gkillas",
            "Nikos Piperigkos",
            "Aris S. Lalos"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces an end-to-end framework for LiDAR super-resolution and semantic segmentation in autonomous driving, achieving comparable performance to high-resolution sensors with significantly fewer parameters.",
        "tldr_zh": "该论文介绍了一种端到端的LiDAR超分辨率和语义分割框架，实现了与高分辨率传感器相当的性能，但参数数量明显减少。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Novel Category Discovery with X-Agent Attention for Open-Vocabulary Semantic Segmentation",
        "summary": "Open-vocabulary semantic segmentation (OVSS) conducts pixel-level\nclassification via text-driven alignment, where the domain discrepancy between\nbase category training and open-vocabulary inference poses challenges in\ndiscriminative modeling of latent unseen category. To address this challenge,\nexisting vision-language model (VLM)-based approaches demonstrate commendable\nperformance through pre-trained multi-modal representations. However, the\nfundamental mechanisms of latent semantic comprehension remain underexplored,\nmaking the bottleneck for OVSS. In this work, we initiate a probing experiment\nto explore distribution patterns and dynamics of latent semantics in VLMs under\ninductive learning paradigms. Building on these insights, we propose X-Agent,\nan innovative OVSS framework employing latent semantic-aware ``agent'' to\norchestrate cross-modal attention mechanisms, simultaneously optimizing latent\nsemantic dynamic and amplifying its perceptibility. Extensive benchmark\nevaluations demonstrate that X-Agent achieves state-of-the-art performance\nwhile effectively enhancing the latent semantic saliency.",
        "url": "http://arxiv.org/abs/2509.01275v2",
        "published_date": "2025-09-01T09:01:58+00:00",
        "updated_date": "2025-09-03T03:02:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahao Li",
            "Yang Lu",
            "Yachao Zhang",
            "Fangyong Wang",
            "Yuan Xie",
            "Yanyun Qu"
        ],
        "ai_categories": [
            "Multimodality"
        ],
        "tldr": "The paper introduces a new framework, X-Agent, for open-vocabulary semantic segmentation, which enhances latent semantic comprehension and achieves state-of-the-art results.",
        "tldr_zh": "本文介绍了一种新的框架X-Agent，用于开放词汇语义分割，增强了潜在语义理解并取得了最先进的结果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "ReCap: Event-Aware Image Captioning with Article Retrieval and Semantic Gaussian Normalization",
        "summary": "Image captioning systems often produce generic descriptions that fail to\ncapture event-level semantics which are crucial for applications like news\nreporting and digital archiving. We present ReCap, a novel pipeline for\nevent-enriched image retrieval and captioning that incorporates broader\ncontextual information from relevant articles to generate narrative-rich,\nfactually grounded captions. Our approach addresses the limitations of standard\nvision-language models that typically focus on visible content while missing\ntemporal, social, and historical contexts. ReCap comprises three integrated\ncomponents: (1) a robust two-stage article retrieval system using DINOv2\nembeddings with global feature similarity for initial candidate selection\nfollowed by patch-level mutual nearest neighbor similarity re-ranking; (2) a\ncontext extraction framework that synthesizes information from article\nsummaries, generic captions, and original source metadata; and (3) a large\nlanguage model-based caption generation system with Semantic Gaussian\nNormalization to enhance fluency and relevance. Evaluated on the OpenEvents V1\ndataset as part of Track 1 in the EVENTA 2025 Grand Challenge, ReCap achieved a\nstrong overall score of 0.54666, ranking 2nd on the private test set. These\nresults highlight ReCap's effectiveness in bridging visual perception with\nreal-world knowledge, offering a practical solution for context-aware image\nunderstanding in high-stakes domains. The code is available at\nhttps://github.com/Noridom1/EVENTA2025-Event-Enriched-Image-Captioning.",
        "url": "http://arxiv.org/abs/2509.01259v1",
        "published_date": "2025-09-01T08:48:33+00:00",
        "updated_date": "2025-09-01T08:48:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Thinh-Phuc Nguyen",
            "Thanh-Hai Nguyen",
            "Gia-Huy Dinh",
            "Lam-Huy Nguyen",
            "Minh-Triet Tran",
            "Trung-Nghia Le"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "ReCap is a novel system for event-enriched image captioning that incorporates contextual information from articles to generate narrative-rich captions, achieving high scores in a competition.",
        "tldr_zh": "ReCap是一个新颖的系统，用于事件丰富的图像字幕生成，结合文章的上下文信息生成富有叙述性的字幕，在竞赛中取得了较高的分数。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Measuring Image-Relation Alignment: Reference-Free Evaluation of VLMs and Synthetic Pre-training for Open-Vocabulary Scene Graph Generation",
        "summary": "Scene Graph Generation (SGG) encodes visual relationships between objects in\nimages as graph structures. Thanks to the advances of Vision-Language Models\n(VLMs), the task of Open-Vocabulary SGG has been recently proposed where models\nare evaluated on their functionality to learn a wide and diverse range of\nrelations. Current benchmarks in SGG, however, possess a very limited\nvocabulary, making the evaluation of open-source models inefficient. In this\npaper, we propose a new reference-free metric to fairly evaluate the\nopen-vocabulary capabilities of VLMs for relation prediction. Another\nlimitation of Open-Vocabulary SGG is the reliance on weakly supervised data of\npoor quality for pre-training. We also propose a new solution for quickly\ngenerating high-quality synthetic data through region-specific prompt tuning of\nVLMs. Experimental results show that pre-training with this new data split can\nbenefit the generalization capabilities of Open-Voc SGG models.",
        "url": "http://arxiv.org/abs/2509.01209v1",
        "published_date": "2025-09-01T07:46:58+00:00",
        "updated_date": "2025-09-01T07:46:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Maëlic Neau",
            "Zoe Falomir",
            "Cédric Buche",
            "Akihiro Sugimoto"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a new metric for evaluating open-vocabulary scene graph generation using Vision-Language Models (VLMs) and introduces a method for generating high-quality synthetic data for pre-training.",
        "tldr_zh": "本文提出了一种评估视觉-语言模型在开放词汇场景图生成中的表现的新度量标准，并介绍了一种用于生成高质量合成数据的方法。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7.5
    },
    {
        "title": "Robix: A Unified Model for Robot Interaction, Reasoning and Planning",
        "summary": "We introduce Robix, a unified model that integrates robot reasoning, task\nplanning, and natural language interaction within a single vision-language\narchitecture. Acting as the high-level cognitive layer in a hierarchical robot\nsystem, Robix dynamically generates atomic commands for the low-level\ncontroller and verbal responses for human interaction, enabling robots to\nfollow complex instructions, plan long-horizon tasks, and interact naturally\nwith human within an end-to-end framework. Robix further introduces novel\ncapabilities such as proactive dialogue, real-time interruption handling, and\ncontext-aware commonsense reasoning during task execution. At its core, Robix\nleverages chain-of-thought reasoning and adopts a three-stage training\nstrategy: (1) continued pretraining to enhance foundational embodied reasoning\nabilities including 3D spatial understanding, visual grounding, and\ntask-centric reasoning; (2) supervised finetuning to model human-robot\ninteraction and task planning as a unified reasoning-action sequence; and (3)\nreinforcement learning to improve reasoning-action consistency and long-horizon\ntask coherence. Extensive experiments demonstrate that Robix outperforms both\nopen-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in\ninteractive task execution, demonstrating strong generalization across diverse\ninstruction types (e.g., open-ended, multi-stage, constrained, invalid, and\ninterrupted) and various user-involved tasks such as table bussing, grocery\nshopping, and dietary filtering.",
        "url": "http://arxiv.org/abs/2509.01106v1",
        "published_date": "2025-09-01T03:53:47+00:00",
        "updated_date": "2025-09-01T03:53:47+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Huang Fang",
            "Mengxi Zhang",
            "Heng Dong",
            "Wei Li",
            "Zixuan Wang",
            "Qifeng Zhang",
            "Xueyun Tian",
            "Yucheng Hu",
            "Hang Li"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "Robix is a unified model for robot interaction, reasoning, and planning, demonstrating superior performance in interactive task execution.",
        "tldr_zh": "Robix是一个统一的机器人交互、推理和规划模型，在交互式任务执行方面表现出色。",
        "relevance_score": 3,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "A Unified Low-level Foundation Model for Enhancing Pathology Image Quality",
        "summary": "Foundation models have revolutionized computational pathology by achieving\nremarkable success in high-level diagnostic tasks, yet the critical challenge\nof low-level image enhancement remains largely unaddressed. Real-world\npathology images frequently suffer from degradations such as noise, blur, and\nlow resolution due to slide preparation artifacts, staining variability, and\nimaging constraints, while the reliance on physical staining introduces\nsignificant costs, delays, and inconsistency. Although existing methods target\nindividual problems like denoising or super-resolution, their task-specific\ndesigns lack the versatility to handle the diverse low-level vision challenges\nencountered in practice. To bridge this gap, we propose the first unified\nLow-level Pathology Foundation Model (LPFM), capable of enhancing image quality\nin restoration tasks, including super-resolution, deblurring, and denoising, as\nwell as facilitating image translation tasks like virtual staining (H&E and\nspecial stains), all through a single adaptable architecture. Our approach\nintroduces a contrastive pre-trained encoder that learns transferable,\nstain-invariant feature representations from 190 million unlabeled pathology\nimages, enabling robust identification of degradation patterns. A unified\nconditional diffusion process dynamically adapts to specific tasks via textual\nprompts, ensuring precise control over output quality. Trained on a curated\ndataset of 87,810 whole slied images (WSIs) across 34 tissue types and 5\nstaining protocols, LPFM demonstrates statistically significant improvements\n(p<0.01) over state-of-the-art methods in most tasks (56/66), achieving Peak\nSignal-to-Noise Ratio (PSNR) gains of 10-15% for image restoration and\nStructural Similarity Index Measure (SSIM) improvements of 12-18% for virtual\nstaining.",
        "url": "http://arxiv.org/abs/2509.01071v1",
        "published_date": "2025-09-01T02:24:34+00:00",
        "updated_date": "2025-09-01T02:24:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziyi Liu",
            "Zhe Xu",
            "Jiabo Ma",
            "Wenqaing Li",
            "Junlin Hou",
            "Fuxiang Huang",
            "Xi Wang",
            "Ronald Cheong Kin Chan",
            "Terence Tsz Wai Wong",
            "Hao Chen"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a unified low-level pathology foundation model for enhancing image quality in pathology images, addressing issues like noise, blur, and low resolution, as well as enabling tasks like super-resolution, deblurring, denoising, and virtual staining.",
        "tldr_zh": "该论文介绍了一种统一的低级病理基础模型，用于增强病理图像的质量，解决了噪声、模糊和低分辨率等问题，同时实现超分辨率、去模糊、去噪和虚拟染色等任务。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "AI-driven Dispensing of Coral Reseeding Devices for Broad-scale Restoration of the Great Barrier Reef",
        "summary": "Coral reefs are on the brink of collapse, with climate change, ocean\nacidification, and pollution leading to a projected 70-90% loss of coral\nspecies within the next decade. Restoration efforts are crucial, but their\nsuccess hinges on introducing automation to upscale efforts. We present\nautomated deployment of coral re-seeding devices powered by artificial\nintelligence, computer vision, and robotics. Specifically, we perform automated\nsubstrate classification, enabling detection of areas of the seafloor suitable\nfor coral growth, thus significantly reducing reliance on human experts and\nincreasing the range and efficiency of restoration. Real-world testing of the\nalgorithms on the Great Barrier Reef leads to deployment accuracy of 77.8%,\nsub-image patch classification of 89.1%, and real-time model inference at 5.5\nframes per second. Further, we present and publicly contribute a large\ncollection of annotated substrate image data to foster future research in this\narea.",
        "url": "http://arxiv.org/abs/2509.01019v1",
        "published_date": "2025-08-31T23:09:51+00:00",
        "updated_date": "2025-08-31T23:09:51+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Scarlett Raine",
            "Benjamin Moshirian",
            "Tobias Fischer"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper presents AI-driven automation for large-scale coral restoration by deploying re-seeding devices using computer vision and robotics on the Great Barrier Reef.",
        "tldr_zh": "该论文介绍了利用人工智能驱动的自动化技术，在大堡礁上部署重新种植设备以进行大规模珊瑚恢复。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Unsupervised Ultra-High-Resolution UAV Low-Light Image Enhancement: A Benchmark, Metric and Framework",
        "summary": "Low light conditions significantly degrade Unmanned Aerial Vehicles (UAVs)\nperformance in critical applications. Existing Low-light Image Enhancement\n(LIE) methods struggle with the unique challenges of aerial imagery, including\nUltra-High Resolution (UHR), lack of paired data, severe non-uniform\nillumination, and deployment constraints. To address these issues, we propose\nthree key contributions. First, we present U3D, the first unsupervised UHR UAV\ndataset for LIE, with a unified evaluation toolkit. Second, we introduce the\nEdge Efficiency Index (EEI), a novel metric balancing perceptual quality with\nkey deployment factors: speed, resolution, model complexity, and memory\nfootprint. Third, we develop U3LIE, an efficient framework with two\ntraining-only designs-Adaptive Pre-enhancement Augmentation (APA) for input\nnormalization and a Luminance Interval Loss (L_int) for exposure control. U3LIE\nachieves SOTA results, processing 4K images at 23.8 FPS on a single GPU, making\nit ideal for real-time on-board deployment. In summary, these contributions\nprovide a holistic solution (dataset, metric, and method) for advancing robust\n24/7 UAV vision. The code and datasets are available at\nhttps://github.com/lwCVer/U3D_Toolkit.",
        "url": "http://arxiv.org/abs/2509.01373v1",
        "published_date": "2025-09-01T11:20:07+00:00",
        "updated_date": "2025-09-01T11:20:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wei Lu",
            "Lingyu Zhu",
            "Si-Bao Chen"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a novel method for enhancing low-light images captured by Unmanned Aerial Vehicles (UAVs) in ultra-high resolution, achieving state-of-the-art results for real-time deployment.",
        "tldr_zh": "本文提出了一种新的方法，用于增强无人机在超高分辨率下捕获的低光图像，实现了实时部署的最新结果。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Cross-Domain Few-Shot Segmentation via Ordinary Differential Equations over Time Intervals",
        "summary": "Cross-domain few-shot segmentation (CD-FSS) not only enables the segmentation\nof unseen categories with very limited samples, but also improves cross-domain\ngeneralization ability within the few-shot segmentation framework. Currently,\nexisting CD-FSS studies typically design multiple independent modules to\nenhance the cross-domain generalization ability of feature representations.\nHowever, the independence among these modules hinders the effective flow of\nknowledge, making it difficult to fully leverage their collective potential. In\ncontrast, this paper proposes an all-in-one module based on ordinary\ndifferential equations and Fourier transform, resulting in a structurally\nconcise method--Few-Shot Segmentation over Time Intervals (FSS-TIs). FSS-TIs\nassumes the existence of an ODE relationship between the spectra (including\namplitude and phase spectra) of domain-specific features and domain-agnostic\nfeatures. This ODE formulation yields an iterative transformation process along\na sequence of time intervals, while simultaneously applying affine\ntransformations with randomized perturbations to the spectra. In doing so, the\nexploration of domain-agnostic feature representation spaces and the simulation\nof diverse potential target-domain distributions are reformulated as an\noptimization process over the intrinsic parameters of the ODE. Moreover, we\nstrictly constrain the support-sample selection during target-domain\nfine-tuning so that it is consistent with the requirements of real-world\nfew-shot segmentation tasks. For evaluation, we introduce five datasets from\nsubstantially different domains and define two sets of cross-domain few-shot\nsegmentation tasks to comprehensively analyze the performance of FSS-TIs.\nExperimental results demonstrate the superiority of FSS-TIs over existing\nCD-FSS methods, and in-depth ablation studies further validate the cross-domain\nadaptability of FSS-TIs.",
        "url": "http://arxiv.org/abs/2509.01299v1",
        "published_date": "2025-09-01T09:35:47+00:00",
        "updated_date": "2025-09-01T09:35:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Huan Ni",
            "Qingshan Liu",
            "Xiaonan Niu",
            "Danfeng Hong",
            "Lingli Zhao",
            "Haiyan Guan"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a new method called FSS-TIs for cross-domain few-shot segmentation using ordinary differential equations and Fourier transform, showing superior performance over existing methods.",
        "tldr_zh": "本文提出了一种名为FSS-TIs的新方法，利用常微分方程和傅里叶变换进行跨领域少样本分割，在现有方法上表现更优。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "DcMatch: Unsupervised Multi-Shape Matching with Dual-Level Consistency",
        "summary": "Establishing point-to-point correspondences across multiple 3D shapes is a\nfundamental problem in computer vision and graphics. In this paper, we\nintroduce DcMatch, a novel unsupervised learning framework for non-rigid\nmulti-shape matching. Unlike existing methods that learn a canonical embedding\nfrom a single shape, our approach leverages a shape graph attention network to\ncapture the underlying manifold structure of the entire shape collection. This\nenables the construction of a more expressive and robust shared latent space,\nleading to more consistent shape-to-universe correspondences via a universe\npredictor. Simultaneously, we represent these correspondences in both the\nspatial and spectral domains and enforce their alignment in the shared universe\nspace through a novel cycle consistency loss. This dual-level consistency\nfosters more accurate and coherent mappings. Extensive experiments on several\nchallenging benchmarks demonstrate that our method consistently outperforms\nprevious state-of-the-art approaches across diverse multi-shape matching\nscenarios. Code is available at https://github.com/YeTianwei/DcMatch.",
        "url": "http://arxiv.org/abs/2509.01204v1",
        "published_date": "2025-09-01T07:43:11+00:00",
        "updated_date": "2025-09-01T07:43:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianwei Ye",
            "Yong Ma",
            "Xiaoguang Mei"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "DcMatch introduces an unsupervised learning framework for non-rigid multi-shape matching, outperforming existing methods by leveraging shape graph attention networks and enforcing dual-level consistency.",
        "tldr_zh": "DcMatch引入了一种非刚性多形状匹配的无监督学习框架，通过利用形状图注意网络并强化双级一致性，优于现有方法。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "PVINet: Point-Voxel Interlaced Network for Point Cloud Compression",
        "summary": "In point cloud compression, the quality of a reconstructed point cloud relies\non both the global structure and the local context, with existing methods\nusually processing global and local information sequentially and lacking\ncommunication between these two types of information. In this paper, we propose\na point-voxel interlaced network (PVINet), which captures global structural\nfeatures and local contextual features in parallel and performs interactions at\neach scale to enhance feature perception efficiency. Specifically, PVINet\ncontains a voxel-based encoder (Ev) for extracting global structural features\nand a point-based encoder (Ep) that models local contexts centered at each\nvoxel. Particularly, a novel conditional sparse convolution is introduced,\nwhich applies point embeddings to dynamically customize kernels for voxel\nfeature extraction, facilitating feature interactions from Ep to Ev. During\ndecoding, a voxel-based decoder employs conditional sparse convolutions to\nincorporate point embeddings as guidance to reconstruct the point cloud.\nExperiments on benchmark datasets show that PVINet delivers competitive\nperformance compared to state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2509.01097v1",
        "published_date": "2025-09-01T03:37:32+00:00",
        "updated_date": "2025-09-01T03:37:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xuan Deng",
            "Xingtao Wang",
            "Xiandong Meng",
            "Xiaopeng Fan",
            "Debin Zhao"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "PVINet proposes a network for point cloud compression that captures global and local features simultaneously to enhance feature perception efficiency.",
        "tldr_zh": "PVINet提出了一种用于点云压缩的网络，同时捕获全局和局部特征，以增强特征感知效率。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Seeing through Unclear Glass: Occlusion Removal with One Shot",
        "summary": "Images taken through window glass are often degraded by contaminants adhered\nto the glass surfaces. Such contaminants cause occlusions that attenuate the\nincoming light and scatter stray light towards the camera. Most of existing\ndeep learning methods for neutralizing the effects of contaminated glasses\nrelied on synthetic training data. Few researchers used real degraded and clean\nimage pairs, but they only considered removing or alleviating the effects of\nrain drops on glasses. This paper is concerned with the more challenging task\nof learning the restoration of images taken through glasses contaminated by a\nwide range of occluders, including muddy water, dirt and other small foreign\nparticles found in reality. To facilitate the learning task we have gone to a\ngreat length to acquire real paired images with and without glass contaminants.\nMore importantly, we propose an all-in-one model to neutralize contaminants of\ndifferent types by utilizing the one-shot test-time adaptation mechanism. It\ninvolves a self-supervised auxiliary learning task to update the trained model\nfor the unique occlusion type of each test image. Experimental results show\nthat the proposed method outperforms the state-of-the-art methods\nquantitatively and qualitatively in cleaning realistic contaminated images,\nespecially the unseen ones.",
        "url": "http://arxiv.org/abs/2509.01033v1",
        "published_date": "2025-09-01T00:01:36+00:00",
        "updated_date": "2025-09-01T00:01:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Qiang Li",
            "Yuanming Cao"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "This paper presents a method for removing occlusions from images taken through contaminated glass, using a one-shot test-time adaptation mechanism. The method outperforms existing techniques in cleaning realistic contaminated images.",
        "tldr_zh": "本文提出了一种方法，用于清除透过受污染玻璃拍摄的图像中的遮挡物，利用一次性测试时间适应机制。该方法在清除现实受污染图像方面优于现有技术。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 7.25
    },
    {
        "title": "TransForSeg: A Multitask Stereo ViT for Joint Stereo Segmentation and 3D Force Estimation in Catheterization",
        "summary": "Recently, the emergence of multitask deep learning models has enhanced\ncatheterization procedures by providing tactile and visual perception data\nthrough an end-to-end architec- ture. This information is derived from a\nsegmentation and force estimation head, which localizes the catheter in X-ray\nimages and estimates the applied pressure based on its deflection within the\nimage. These stereo vision architectures incorporate a CNN- based\nencoder-decoder that captures the dependencies between X-ray images from two\nviewpoints, enabling simultaneous 3D force estimation and stereo segmentation\nof the catheter. With these tasks in mind, this work approaches the problem\nfrom a new perspective. We propose a novel encoder-decoder Vision Transformer\nmodel that processes two input X-ray images as separate sequences. Given\nsequences of X-ray patches from two perspectives, the transformer captures\nlong-range dependencies without the need to gradually expand the receptive\nfield for either image. The embeddings generated by both the encoder and\ndecoder are fed into two shared segmentation heads, while a regression head\nemploys the fused information from the decoder for 3D force estimation. The\nproposed model is a stereo Vision Transformer capable of simultaneously\nsegmenting the catheter from two angles while estimating the generated forces\nat its tip in 3D. This model has undergone extensive experiments on synthetic\nX-ray images with various noise levels and has been compared against\nstate-of-the-art pure segmentation models, vision-based catheter force\nestimation methods, and a multitask catheter segmentation and force estimation\napproach. It outperforms existing models, setting a new state-of-the-art in\nboth catheter segmentation and force estimation.",
        "url": "http://arxiv.org/abs/2509.01605v1",
        "published_date": "2025-09-01T16:36:23+00:00",
        "updated_date": "2025-09-01T16:36:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Pedram Fekri",
            "Mehrdad Zadeh",
            "Javad Dargahi"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer"
        ],
        "tldr": "The paper introduces a multitask deep learning model for catheterization that can simultaneously perform catheter segmentation and 3D force estimation, using a novel stereo Vision Transformer architecture.",
        "tldr_zh": "该论文介绍了一种多任务深度学习模型，可以同时进行导管分割和3D力估计，使用了新颖的立体视觉Transformer架构。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7.0
    },
    {
        "title": "Bangladeshi Street Food Calorie Estimation Using Improved YOLOv8 and Regression Model",
        "summary": "As obesity rates continue to increase, automated calorie tracking has become\na vital tool for people seeking to maintain a healthy lifestyle or adhere to a\ndiet plan. Although numerous research efforts have addressed this issue,\nexisting approaches often face key limitations, such as providing only constant\ncaloric output, struggling with multiple food recognition challenges,\nchallenges in image scaling and normalization, and a predominant focus on\nWestern cuisines. In this paper, we propose a tailored solution that\nspecifically targets Bangladeshi street food. We first construct a diverse\ndataset of popular street foods found across Bangladesh. Then, we develop a\nrefined calorie estimation system by modifying the state-of-the-art vision\nmodel YOLOv8. Our modified model achieves superior classification and\nsegmentation results, with only a slight increase in computational complexity\ncompared to the base variant. Coupled with a machine learning regression model,\nour system achieves an impressive 6.94 mean absolute error (MAE), 11.03 root\nmean squared error (RMSE), and a 96.0% R^2 score in calorie estimation, making\nit both highly effective and accurate for real-world food calorie calculations.",
        "url": "http://arxiv.org/abs/2509.01415v1",
        "published_date": "2025-09-01T12:14:47+00:00",
        "updated_date": "2025-09-01T12:14:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aparup Dhar",
            "MD Tamim Hossain",
            "Pritom Barua"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer",
            "Other"
        ],
        "tldr": "The paper proposes a system for estimating calories in Bangladeshi street food using an improved YOLOv8 model and regression techniques, achieving high accuracy in real-world food calorie calculations.",
        "tldr_zh": "本文提出了一种使用改进的YOLOv8模型和回归技术估算孟加拉街头食品卡路里的系统，实现了高精度的真实世界食物卡路里计算。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Automatic Screening of Parkinson's Disease from Visual Explorations",
        "summary": "Eye movements can reveal early signs of neurodegeneration, including those\nassociated with Parkinson's Disease (PD). This work investigates the utility of\na set of gaze-based features for the automatic screening of PD from different\nvisual exploration tasks. For this purpose, a novel methodology is introduced,\ncombining classic fixation/saccade oculomotor features (e.g., saccade count,\nfixation duration, scanned area) with features derived from gaze clusters\n(i.e., regions with a considerable accumulation of fixations). These features\nare automatically extracted from six exploration tests and evaluated using\ndifferent machine learning classifiers. A Mixture of Experts ensemble is used\nto integrate outputs across tests and both eyes. Results show that ensemble\nmodels outperform individual classifiers, achieving an Area Under the Receiving\nOperating Characteristic Curve (AUC) of 0.95 on a held-out test set. The\nfindings support visual exploration as a non-invasive tool for early automatic\nscreening of PD.",
        "url": "http://arxiv.org/abs/2509.01326v1",
        "published_date": "2025-09-01T10:11:35+00:00",
        "updated_date": "2025-09-01T10:11:35+00:00",
        "categories": [
            "q-bio.NC",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Maria F. Alcala-Durand",
            "J. Camilo Puerta-Acevedo",
            "Julián D. Arias-Londoño",
            "Juan I. Godino-Llorente"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "This paper explores using eye movement patterns to automatically screen for Parkinson's Disease, achieving high accuracy with a novel methodology.",
        "tldr_zh": "本文探讨利用眼动模式自动筛查帕金森病，通过新方法实现高准确性。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "SAR-NAS: Lightweight SAR Object Detection with Neural Architecture Search",
        "summary": "Synthetic Aperture Radar (SAR) object detection faces significant challenges\nfrom speckle noise, small target ambiguities, and on-board computational\nconstraints. While existing approaches predominantly focus on SAR-specific\narchitectural modifications, this paper explores the application of the\nexisting lightweight object detector, i.e., YOLOv10, for SAR object detection\nand enhances its performance through Neural Architecture Search (NAS).\nSpecifically, we employ NAS to systematically optimize the network structure,\nespecially focusing on the backbone architecture search. By constructing an\nextensive search space and leveraging evolutionary search, our method\nidentifies a favorable architecture that balances accuracy, parameter\nefficiency, and computational cost. Notably, this work introduces NAS to SAR\nobject detection for the first time. The experimental results on the\nlarge-scale SARDet-100K dataset demonstrate that our optimized model\noutperforms existing SAR detection methods, achieving superior detection\naccuracy while maintaining lower computational overhead. We hope this work\noffers a novel perspective on leveraging NAS for real-world applications.",
        "url": "http://arxiv.org/abs/2509.01279v1",
        "published_date": "2025-09-01T09:06:13+00:00",
        "updated_date": "2025-09-01T09:06:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinyi Yu",
            "Zhiwei Lin",
            "Yongtao Wang"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces the use of Neural Architecture Search (NAS) to optimize the YOLOv10 object detector for Synthetic Aperture Radar (SAR) object detection, achieving superior accuracy with lower computational cost.",
        "tldr_zh": "本文首次将神经架构搜索应用于优化YOLOv10目标检测器，用于合成孔径雷达（SAR）目标检测，在保持低计算成本的同时实现卓越准确性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Towards More Diverse and Challenging Pre-training for Point Cloud Learning: Self-Supervised Cross Reconstruction with Decoupled Views",
        "summary": "Point cloud learning, especially in a self-supervised way without manual\nlabels, has gained growing attention in both vision and learning communities\ndue to its potential utility in a wide range of applications. Most existing\ngenerative approaches for point cloud self-supervised learning focus on\nrecovering masked points from visible ones within a single view. Recognizing\nthat a two-view pre-training paradigm inherently introduces greater diversity\nand variance, it may thus enable more challenging and informative pre-training.\nInspired by this, we explore the potential of two-view learning in this domain.\nIn this paper, we propose Point-PQAE, a cross-reconstruction generative\nparadigm that first generates two decoupled point clouds/views and then\nreconstructs one from the other. To achieve this goal, we develop a crop\nmechanism for point cloud view generation for the first time and further\npropose a novel positional encoding to represent the 3D relative position\nbetween the two decoupled views. The cross-reconstruction significantly\nincreases the difficulty of pre-training compared to self-reconstruction, which\nenables our method to surpass previous single-modal self-reconstruction methods\nin 3D self-supervised learning. Specifically, it outperforms the\nself-reconstruction baseline (Point-MAE) by 6.5%, 7.0%, and 6.7% in three\nvariants of ScanObjectNN with the Mlp-Linear evaluation protocol. The code is\navailable at https://github.com/aHapBean/Point-PQAE.",
        "url": "http://arxiv.org/abs/2509.01250v1",
        "published_date": "2025-09-01T08:42:17+00:00",
        "updated_date": "2025-09-01T08:42:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiangdong Zhang",
            "Shaofeng Zhang",
            "Junchi Yan"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper proposes a new method for pre-training point cloud learning using self-supervised cross-reconstruction with decoupled views, outperforming previous methods.",
        "tldr_zh": "本文提出了一种新的方法，使用自监督交叉重建和解耦视图进行预训练，优于先前的方法。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 7.0
    },
    {
        "title": "Learning Correlation-aware Aleatoric Uncertainty for 3D Hand Pose Estimation",
        "summary": "3D hand pose estimation is a fundamental task in understanding human hands.\nHowever, accurately estimating 3D hand poses remains challenging due to the\ncomplex movement of hands, self-similarity, and frequent occlusions. In this\nwork, we address two limitations: the inability of existing 3D hand pose\nestimation methods to estimate aleatoric (data) uncertainty, and the lack of\nuncertainty modeling that incorporates joint correlation knowledge, which has\nnot been thoroughly investigated. To this end, we introduce aleatoric\nuncertainty modeling into the 3D hand pose estimation framework, aiming to\nachieve a better trade-off between modeling joint correlations and\ncomputational efficiency. We propose a novel parameterization that leverages a\nsingle linear layer to capture intrinsic correlations among hand joints. This\nis enabled by formulating the hand joint output space as a probabilistic\ndistribution, allowing the linear layer to capture joint correlations. Our\nproposed parameterization is used as a task head layer, and can be applied as\nan add-on module on top of the existing models. Our experiments demonstrate\nthat our parameterization for uncertainty modeling outperforms existing\napproaches. Furthermore, the 3D hand pose estimation model equipped with our\nuncertainty head achieves favorable accuracy in 3D hand pose estimation while\nintroducing new uncertainty modeling capability to the model. The project page\nis available at https://hand-uncertainty.github.io/.",
        "url": "http://arxiv.org/abs/2509.01242v1",
        "published_date": "2025-09-01T08:31:01+00:00",
        "updated_date": "2025-09-01T08:31:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lee Chae-Yeon",
            "Nam Hyeon-Woo",
            "Tae-Hyun Oh"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a method for incorporating aleatoric uncertainty and joint correlation knowledge into 3D hand pose estimation, outperforming existing approaches.",
        "tldr_zh": "该论文提出一种方法，将数据不确定性和关节相关知识融入到3D手部姿势估计中，优于现有方法。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Learn2Reg 2024: New Benchmark Datasets Driving Progress on New Challenges",
        "summary": "Medical image registration is critical for clinical applications, and fair\nbenchmarking of different methods is essential for monitoring ongoing progress.\nTo date, the Learn2Reg 2020-2023 challenges have released several complementary\ndatasets and established metrics for evaluations. However, these editions did\nnot capture all aspects of the registration problem, particularly in terms of\nmodality diversity and task complexity. To address these limitations, the 2024\nedition introduces three new tasks, including large-scale multi-modal\nregistration and unsupervised inter-subject brain registration, as well as the\nfirst microscopy-focused benchmark within Learn2Reg. The new datasets also\ninspired new method developments, including invertibility constraints, pyramid\nfeatures, keypoints alignment and instance optimisation.",
        "url": "http://arxiv.org/abs/2509.01217v1",
        "published_date": "2025-09-01T08:03:07+00:00",
        "updated_date": "2025-09-01T08:03:07+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Lasse Hansen",
            "Wiebke Heyer",
            "Christoph Großbröhmer",
            "Frederic Madesta",
            "Thilo Sentker",
            "Wang Jiazheng",
            "Yuxi Zhang",
            "Hang Zhang",
            "Min Liu",
            "Junyi Wang",
            "Xi Zhu",
            "Yuhua Li",
            "Liwen Wang",
            "Daniil Morozov",
            "Nazim Haouchine",
            "Joel Honkamaa",
            "Pekka Marttinen",
            "Yichao Zhou",
            "Zuopeng Tan",
            "Zhuoyuan Wang",
            "Yi Wang",
            "Hongchao Zhou",
            "Shunbo Hu",
            "Yi Zhang",
            "Qian Tao",
            "Lukas Förner",
            "Thomas Wendler",
            "Bailiang Jian",
            "Christian Wachinger",
            "Jin Kim",
            "Dan Ruan",
            "Marek Wodzinski",
            "Henning Müller",
            "Tony C. W. Mok",
            "Xi Jia",
            "Mikael Brudfors",
            "Seyed-Ahmad Ahmadi",
            "Yunzheng Zhu",
            "William Hsu",
            "Tina Kapur",
            "William M. Wells",
            "Alexandra Golby",
            "Aaron Carass",
            "Harrison Bai",
            "Yihao Liu",
            "Perrine Paul-Gilloteaux",
            "Joakim Lindblad",
            "Nataša Sladoje",
            "Andreas Walter",
            "Junyu Chen",
            "Reuben Dorent",
            "Alessa Hering",
            "Mattias P. Heinrich"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "Learn2Reg 2024 introduces new benchmark datasets and tasks addressing limitations in previous editions, inspiring new method developments for medical image registration.",
        "tldr_zh": "Learn2Reg 2024引入新的基准数据集和任务，解决以往版本的局限性，激发医学图像配准领域的新方法发展。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models for Document Conversion",
        "summary": "High-quality labeled data is essential for training accurate document\nconversion models, particularly in domains with complex formats such as tables,\nformulas, and multi-column text. However, manual annotation is both costly and\ntime-consuming, while automatic labeling using existing models often lacks\naccuracy in handling such challenging scenarios. Consequently, training student\nmodels by distilling outputs from teacher models can significantly limit their\nperformance in real-world applications. In this paper, we propose a fully\nautomated, distillation-free framework comprising two stages for constructing\nhigh-quality document extraction datasets and models capable of handling\ndiverse document formats and layouts. In the first stage, we introduce a method\nfor generating large-scale, diverse synthetic data, which enables a model to\nextract key elements in a unified format with strong initial performance. In\nthe second stage, we present a self-improvement approach that further adapts\nthe model, initially trained on synthetic data, to real-world documents.\nSpecifically, we first use the fine-tuned model to annotate real documents,\nthen apply a suite of filtering strategies to verify annotation quality, and\nfinally retrain the model on the verified dataset. By iteratively repeating\nthis process, we progressively enhance both the model's conversion capabilities\nand the quality of the generated data. We train a public POINTS-1.5 model to\nobtain POINTS-Reader, which surpasses many existing public and proprietary\nmodels of comparable or larger size. Our model is available at\nhttps://github.com/Tencent/POINTS-Reader.",
        "url": "http://arxiv.org/abs/2509.01215v1",
        "published_date": "2025-09-01T07:54:18+00:00",
        "updated_date": "2025-09-01T07:54:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuan Liu",
            "Zhongyin Zhao",
            "Le Tian",
            "Haicheng Wang",
            "Xubing Ye",
            "Yangxiu You",
            "Zilin Yu",
            "Chuhan Wu",
            "Xiao Zhou",
            "Yang Yu",
            "Jie Zhou"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a distillation-free framework for creating high-quality document extraction models, surpassing existing models in performance.",
        "tldr_zh": "本文介绍了一种无需蒸馏的框架，用于创建高质量的文档提取模型，在性能上超过了现有模型。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Generalizable Self-supervised Monocular Depth Estimation with Mixture of Low-Rank Experts for Diverse Endoscopic Scenes",
        "summary": "Self-supervised monocular depth estimation is a significant task for low-cost\nand efficient three-dimensional scene perception in endoscopy. The variety of\nillumination conditions and scene features is still the primary challenge for\ngeneralizable depth estimation in endoscopic scenes. In this work, a\nself-supervised framework is proposed for monocular depth estimation in various\nendoscopy. Firstly, due to various features in endoscopic scenes with different\ntissues, a novel block-wise mixture of dynamic low-rank experts is proposed to\nefficiently finetuning the foundation model for endoscopic depth estimation. In\nthe proposed module, based on the input feature, different experts with a small\namount of trainable parameters are adaptively selected for weighted inference,\nfrom various mixture of low-rank experts which are allocated based on the\ntraining quality of each block. Moreover, a novel self-supervised training\nframework is proposed to jointly cope with the inconsistency of brightness and\nreflectance. The proposed method outperform state-of-the-art works on both\nrealistic and simulated endoscopic datasets. Furthermore, the proposed network\nalso achieves the best generalization based on zero-shot depth estimation on\ndiverse endoscopic scenes. The proposed method could contribute to accurate\nendoscopic perception for minimally invasive measurement and surgery. The code\nwill be released upon acceptance, while the demo video can be found on here:\nhttps://endo-gede.netlify.app/.",
        "url": "http://arxiv.org/abs/2509.01206v1",
        "published_date": "2025-09-01T07:45:12+00:00",
        "updated_date": "2025-09-01T07:45:12+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Liangjing Shao",
            "Benshuang Chen",
            "Chenkang Du",
            "Xueli Liu",
            "Xinrong Chen"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper presents a self-supervised framework for monocular depth estimation in endoscopic scenes, utilizing a mixture of low-rank experts to improve accuracy and generalization.",
        "tldr_zh": "本文提出了一种自监督框架，用于内窥镜场景中的单眼深度估计，利用低秩专家混合物来提高精度和泛化性。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.0
    },
    {
        "title": "SegAssess: Panoramic quality mapping for robust and transferable unsupervised segmentation assessment",
        "summary": "High-quality image segmentation is fundamental to pixel-level geospatial\nanalysis in remote sensing, necessitating robust segmentation quality\nassessment (SQA), particularly in unsupervised settings lacking ground truth.\nAlthough recent deep learning (DL) based unsupervised SQA methods show\npotential, they often suffer from coarse evaluation granularity, incomplete\nassessments, and poor transferability. To overcome these limitations, this\npaper introduces Panoramic Quality Mapping (PQM) as a new paradigm for\ncomprehensive, pixel-wise SQA, and presents SegAssess, a novel deep learning\nframework realizing this approach. SegAssess distinctively formulates SQA as a\nfine-grained, four-class panoramic segmentation task, classifying pixels within\na segmentation mask under evaluation into true positive (TP), false positive\n(FP), true negative (TN), and false negative (FN) categories, thereby\ngenerating a complete quality map. Leveraging an enhanced Segment Anything\nModel (SAM) architecture, SegAssess uniquely employs the input mask as a prompt\nfor effective feature integration via cross-attention. Key innovations include\nan Edge Guided Compaction (EGC) branch with an Aggregated Semantic Filter (ASF)\nmodule to refine predictions near challenging object edges, and an Augmented\nMixup Sampling (AMS) training strategy integrating multi-source masks to\nsignificantly boost cross-domain robustness and zero-shot transferability.\nComprehensive experiments across 32 datasets derived from 6 sources demonstrate\nthat SegAssess achieves state-of-the-art (SOTA) performance and exhibits\nremarkable zero-shot transferability to unseen masks, establishing PQM via\nSegAssess as a robust and transferable solution for unsupervised SQA. The code\nis available at https://github.com/Yangbn97/SegAssess.",
        "url": "http://arxiv.org/abs/2509.01183v1",
        "published_date": "2025-09-01T07:07:48+00:00",
        "updated_date": "2025-09-01T07:07:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bingnan Yang",
            "Mi Zhang",
            "Zhili Zhang",
            "Zhan Zhang",
            "Yuanxin Zhao",
            "Xiangyun Hu",
            "Jianya Gong"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces Panoramic Quality Mapping for assessing image segmentation quality without ground truth, achieving state-of-the-art performance and transferability to unseen data.",
        "tldr_zh": "本文引入了全景质量映射用于评估图像分割质量，无需地面真值，达到了最先进的性能并且对未知数据具有可转移性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MVTrajecter: Multi-View Pedestrian Tracking with Trajectory Motion Cost and Trajectory Appearance Cost",
        "summary": "Multi-View Pedestrian Tracking (MVPT) aims to track pedestrians in the form\nof a bird's eye view occupancy map from multi-view videos. End-to-end methods\nthat detect and associate pedestrians within one model have shown great\nprogress in MVPT. The motion and appearance information of pedestrians is\nimportant for the association, but previous end-to-end MVPT methods rely only\non the current and its single adjacent past timestamp, discarding the past\ntrajectories before that. This paper proposes a novel end-to-end MVPT method\ncalled Multi-View Trajectory Tracker (MVTrajecter) that utilizes information\nfrom multiple timestamps in past trajectories for robust association.\nMVTrajecter introduces trajectory motion cost and trajectory appearance cost to\neffectively incorporate motion and appearance information, respectively. These\ncosts calculate which pedestrians at the current and each past timestamp are\nlikely identical based on the information between those timestamps. Even if a\ncurrent pedestrian could be associated with a false pedestrian at some past\ntimestamp, these costs enable the model to associate that current pedestrian\nwith the correct past trajectory based on other past timestamps. In addition,\nMVTrajecter effectively captures the relationships between multiple timestamps\nleveraging the attention mechanism. Extensive experiments demonstrate the\neffectiveness of each component in MVTrajecter and show that it outperforms the\nprevious state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2509.01157v1",
        "published_date": "2025-09-01T06:27:52+00:00",
        "updated_date": "2025-09-01T06:27:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Taiga Yamane",
            "Ryo Masumura",
            "Satoshi Suzuki",
            "Shota Orihashi"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel end-to-end Multi-View Pedestrian Tracking method called MVTrajecter that utilizes past trajectories for robust association, outperforming previous state-of-the-art methods.",
        "tldr_zh": "本文介绍了一种新的端到端多视角行人跟踪方法，称为MVTrajecter，利用过去的轨迹进行强大的关联，优于先前的最先进方法。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games",
        "summary": "GUI agents powered by LLMs show promise in interacting with diverse digital\nenvironments. Among these, video games offer a valuable testbed due to their\nvaried interfaces, with adventure games posing additional challenges through\ncomplex, narrative-driven interactions. Existing game benchmarks, however, lack\ndiversity and rarely evaluate agents on completing entire storylines. To\naddress this, we introduce FlashAdventure, a benchmark of 34 Flash-based\nadventure games designed to test full story arc completion and tackle the\nobservation-behavior gap: the challenge of remembering and acting on earlier\ngameplay information. We also propose CUA-as-a-Judge, an automated gameplay\nevaluator, and COAST, an agentic framework leveraging long-term clue memory to\nbetter plan and solve sequential tasks. Experiments show current GUI agents\nstruggle with full story arcs, while COAST improves milestone completion by\nbridging the observation-behavior gap. Nonetheless, a marked discrepancy\nbetween humans and best-performing agents warrants continued research efforts\nto narrow this divide.",
        "url": "http://arxiv.org/abs/2509.01052v1",
        "published_date": "2025-09-01T01:33:16+00:00",
        "updated_date": "2025-09-01T01:33:16+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Jaewoo Ahn",
            "Junseo Kim",
            "Heeseung Yun",
            "Jaehyeon Son",
            "Dongmin Park",
            "Jaewoong Cho",
            "Gunhee Kim"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces FlashAdventure, a benchmark for GUI agents to solve full story arcs in diverse adventure games. It proposes an automated gameplay evaluator and an agentic framework to improve performance.",
        "tldr_zh": "本文引入了FlashAdventure，一个用于GUI代理在各种冒险游戏中解决全面情节的基准。它提出了一个自动游戏评估器和一个代理框架，以提高性能。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Chronotome: Real-Time Topic Modeling for Streaming Embedding Spaces",
        "summary": "Many real-world datasets -- from an artist's body of work to a person's\nsocial media history -- exhibit meaningful semantic changes over time that are\ndifficult to capture with existing dimensionality reduction methods. To address\nthis gap, we introduce a visualization technique that combines force-based\nprojection and streaming clustering methods to build a spatial-temporal map of\nembeddings. Applying this technique, we create Chronotome, a tool for\ninteractively exploring evolving themes in time-based data -- in real time. We\ndemonstrate the utility of our approach through use cases on text and image\ndata, showing how it offers a new lens for understanding the aesthetics and\nsemantics of temporal datasets.",
        "url": "http://arxiv.org/abs/2509.01051v1",
        "published_date": "2025-09-01T01:32:25+00:00",
        "updated_date": "2025-09-01T01:32:25+00:00",
        "categories": [
            "cs.HC",
            "cs.CL",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Matte Lim",
            "Catherine Yeh",
            "Martin Wattenberg",
            "Fernanda Viégas",
            "Panagiotis Michalatos"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "Chronotome introduces a visualization technique to capture semantic changes over time in datasets, creating a spatial-temporal map of embeddings for exploring evolving themes in real time.",
        "tldr_zh": "Chronotome引入了一种可视化技术，用于捕捉数据集中随时间变化的语义变化，为实时探索演化主题创建了嵌入的时空地图。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Challenges and Lessons from MIDOG 2025: A Two-Stage Approach to Domain-Robust Mitotic Figure Detection",
        "summary": "Mitotic figure detection remains a challenging task in computational\npathology due to domain variability and morphological complexity. This paper\ndescribes our participation in the MIDOG 2025 challenge, focusing on robust\nmitotic figure detection across diverse tissue domains. We developed a\ntwo-stage pipeline combining Faster R-CNN for candidate detection with an\nensemble of three classifiers (DenseNet-121, EfficientNet-v2,\nInceptionResNet-v2) for false positive reduction. Our best submission achieved\nF1-score 0.2237 (Recall: 0.9528, Precision: 0.1267) using a Faster R-CNN\ntrained solely on MIDOG++ dataset. While our high recall demonstrates effective\nmitotic figure detection, the critically low precision (12.67%) reveals\nfundamental challenges in distinguishing true mitoses from morphologically\nsimilar imposters across diverse domains. Analysis of six submission variants\nshowed that subsequent optimization attempts were counterproductive,\nhighlighting the omplexity of domain generalization in histopathology. This\nwork provides valuable insights into the practical challenges of developing\nrobust mitotic figure detection algorithms and emphasizes the importance of\neffective false positive suppression strategies.",
        "url": "http://arxiv.org/abs/2509.02630v1",
        "published_date": "2025-09-01T17:42:05+00:00",
        "updated_date": "2025-09-01T17:42:05+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Euiseop Song",
            "Jaeyoung Park",
            "Jaewoo Park"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper discusses challenges in detecting mitotic figures in diverse tissue domains, presenting a two-stage approach using Faster R-CNN and ensemble classifiers, with insights into the difficulty of false positive suppression.",
        "tldr_zh": "本文探讨了在不同组织领域检测有丝分裂图像的挑战，提出了一种使用Faster R-CNN和集成分类器的两阶段方法，同时介绍了抑制假阳性的困难。",
        "relevance_score": 5,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6.5
    },
    {
        "title": "User Manual for Model-based Imaging Inverse Problem",
        "summary": "This user manual is intended to provide a detailed description on model-based\noptimization for imaging inverse problem. Theseproblems can be particularly\ncomplex and challenging, especially for individuals without prior exposure to\nconvex optimization orinverse problem theory, like myself. In light of this, I\nam writing this manual to clarify and systematically organize the\nmathematicalrationale underlying imaging inverse problems. This manual might\nnot be accurate in mathmatical notion but more focus on the logicalthinking on\nhow to solve and proceed to solve the problems. If you want to think deep about\nsomething, try to raise questions! Thismanual is seaprated into four sections,\naiming to answer the following four questions: (1) What is inverse imaging\nproblem? (2) Why optimization is used to solve the inverse imaging problem? (3)\nHow to solve the optimization problem? (4) How to implement the optimization\nalgorithm in real imaging system?",
        "url": "http://arxiv.org/abs/2509.01572v1",
        "published_date": "2025-09-01T15:57:20+00:00",
        "updated_date": "2025-09-01T15:57:20+00:00",
        "categories": [
            "math.NA",
            "cs.CV",
            "cs.NA"
        ],
        "authors": [
            "Xiaodong Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "This user manual provides a detailed guide on model-based optimization for imaging inverse problems, focusing on clarifying the mathematical rationale and logical thinking behind solving these complex problems.",
        "tldr_zh": "这本用户手册详细介绍了基于模型的优化在成像逆问题中的应用，重点是澄清解决这些复杂问题背后的数学原理和逻辑思维。",
        "relevance_score": 6,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6.5
    },
    {
        "title": "RibPull: Implicit Occupancy Fields and Medial Axis Extraction for CT Ribcage Scans",
        "summary": "We present RibPull, a methodology that utilizes implicit occupancy fields to\nbridge computational geometry and medical imaging. Implicit 3D representations\nuse continuous functions that handle sparse and noisy data more effectively\nthan discrete methods. While voxel grids are standard for medical imaging, they\nsuffer from resolution limitations, topological information loss, and\ninefficient handling of sparsity. Coordinate functions preserve complex\ngeometrical information and represent a better solution for sparse data\nrepresentation, while allowing for further morphological operations. Implicit\nscene representations enable neural networks to encode entire 3D scenes within\ntheir weights. The result is a continuous function that can implicitly\ncompesate for sparse signals and infer further information about the 3D scene\nby passing any combination of 3D coordinates as input to the model. In this\nwork, we use neural occupancy fields that predict whether a 3D point lies\ninside or outside an object to represent CT-scanned ribcages. We also apply a\nLaplacian-based contraction to extract the medial axis of the ribcage, thus\ndemonstrating a geometrical operation that benefits greatly from continuous\ncoordinate-based 3D scene representations versus voxel-based representations.\nWe evaluate our methodology on 20 medical scans from the RibSeg dataset, which\nis itself an extension of the RibFrac dataset. We will release our code upon\npublication.",
        "url": "http://arxiv.org/abs/2509.01402v1",
        "published_date": "2025-09-01T11:54:50+00:00",
        "updated_date": "2025-09-01T11:54:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Emmanouil Nikolakakis",
            "Amine Ouasfi",
            "Julie Digne",
            "Razvan Marinescu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "RibPull introduces implicit occupancy fields and medial axis extraction for CT ribcage scans, utilizing neural networks and continuous coordinate-based 3D scene representations.",
        "tldr_zh": "RibPull引入了针对CT胸廓扫描的隐式占用场和中轴提取，利用神经网络和连续基于坐标的3D场景表示。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "RT-DETRv2 Explained in 8 Illustrations",
        "summary": "Object detection architectures are notoriously difficult to understand, often\nmore so than large language models. While RT-DETRv2 represents an important\nadvance in real-time detection, most existing diagrams do little to clarify how\nits components actually work and fit together. In this article, we explain the\narchitecture of RT-DETRv2 through a series of eight carefully designed\nillustrations, moving from the overall pipeline down to critical components\nsuch as the encoder, decoder, and multi-scale deformable attention. Our goal is\nto make the existing one genuinely understandable. By visualizing the flow of\ntensors and unpacking the logic behind each module, we hope to provide\nresearchers and practitioners with a clearer mental model of how RT-DETRv2\nworks under the hood.",
        "url": "http://arxiv.org/abs/2509.01241v1",
        "published_date": "2025-09-01T08:28:01+00:00",
        "updated_date": "2025-09-01T08:28:01+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ethan Qi Yang Chua",
            "Jen Hong Tan"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper explains the architecture of RT-DETRv2 using eight illustrations to make it easier to understand for researchers and practitioners.",
        "tldr_zh": "该论文使用八幅插图解释了RT-DETRv2的架构，以便更容易理解。",
        "relevance_score": 4,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6.25
    },
    {
        "title": "Aleatoric Uncertainty from AI-based 6D Object Pose Predictors for Object-relative State Estimation",
        "summary": "Deep Learning (DL) has become essential in various robotics applications due\nto excelling at processing raw sensory data to extract task specific\ninformation from semantic objects. For example, vision-based object-relative\nnavigation relies on a DL-based 6D object pose predictor to provide the\nrelative pose between the object and the robot as measurements to the robot's\nstate estimator. Accurately knowing the uncertainty inherent in such Deep\nNeural Network (DNN) based measurements is essential for probabilistic state\nestimators subsequently guiding the robot's tasks. Thus, in this letter, we\nshow that we can extend any existing DL-based object-relative pose predictor\nfor aleatoric uncertainty inference simply by including two multi-layer\nperceptrons detached from the translational and rotational part of the DL\npredictor. This allows for efficient training while freezing the existing\npre-trained predictor. We then use the inferred 6D pose and its uncertainty as\na measurement and corresponding noise covariance matrix in an extended Kalman\nfilter (EKF). Our approach induces minimal computational overhead such that the\nstate estimator can be deployed on edge devices while benefiting from the\ndynamically inferred measurement uncertainty. This increases the performance of\nthe object-relative state estimation task compared to a fix-covariance\napproach. We conduct evaluations on synthetic data and real-world data to\nunderline the benefits of aleatoric uncertainty inference for the\nobject-relative state estimation task.",
        "url": "http://arxiv.org/abs/2509.01583v1",
        "published_date": "2025-09-01T16:12:10+00:00",
        "updated_date": "2025-09-01T16:12:10+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Thomas Jantos",
            "Stephan Weiss",
            "Jan Steinbrener"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper proposes a method to incorporate uncertainty estimation in object pose prediction for robotic tasks, improving state estimation accuracy.",
        "tldr_zh": "本文提出一种方法，将不确定性估计纳入机器人任务中的物体姿势预测，提高状态估计的准确性。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.0
    },
    {
        "title": "Weather-Dependent Variations in Driver Gaze Behavior: A Case Study in Rainy Conditions",
        "summary": "Rainy weather significantly increases the risk of road accidents due to\nreduced visibility and vehicle traction. Understanding how experienced drivers\nadapt their visual perception through gaze behavior under such conditions is\ncritical for designing robust driver monitoring systems (DMS) and for informing\nadvanced driver assistance systems (ADAS). This case study investigates the eye\ngaze behavior of a driver operating the same highway route under both clear and\nrainy conditions. To this end, gaze behavior was analyzed by a two-step\nclustering approach: first, clustering gaze points within 10-second intervals,\nand then aggregating cluster centroids into meta-clusters. This, along with\nMarkov transition matrices and metrics such as fixation duration, gaze\nelevation, and azimuth distributions, reveals meaningful behavioral shifts.\nWhile the overall gaze behavior focused on the road with occasional mirror\nchecks remains consistent, rainy conditions lead to more frequent dashboard\nglances, longer fixation durations, and higher gaze elevation, indicating\nincreased cognitive focus. These findings offer valuable insight into visual\nattention patterns under adverse conditions and highlight the potential of\nleveraging gaze modeling to aid in the design of more robust ADAS and DMS.",
        "url": "http://arxiv.org/abs/2509.01013v1",
        "published_date": "2025-08-31T22:33:30+00:00",
        "updated_date": "2025-08-31T22:33:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ghazal Farhani",
            "Taufiq Rahman",
            "Dominique Charlebois"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "This paper studies how driver gaze behavior changes in rainy conditions compared to clear conditions, with potential implications for driver monitoring and assistance systems.",
        "tldr_zh": "本文研究了在雨天条件下与晴天条件下驾驶员注视行为的变化，对驾驶员监控和辅助系统可能产生重要影响。",
        "relevance_score": 2,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]