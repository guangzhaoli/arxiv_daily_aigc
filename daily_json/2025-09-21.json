[
    {
        "title": "DA-Font: Few-Shot Font Generation via Dual-Attention Hybrid Integration",
        "summary": "Few-shot font generation aims to create new fonts with a limited number of\nglyph references. It can be used to significantly reduce the labor cost of\nmanual font design. However, due to the variety and complexity of font styles,\nthe results generated by existing methods often suffer from visible defects,\nsuch as stroke errors, artifacts and blurriness. To address these issues, we\npropose DA-Font, a novel framework which integrates a Dual-Attention Hybrid\nModule (DAHM). Specifically, we introduce two synergistic attention blocks: the\ncomponent attention block that leverages component information from content\nimages to guide the style transfer process, and the relation attention block\nthat further refines spatial relationships through interacting the content\nfeature with both original and stylized component-wise representations. These\ntwo blocks collaborate to preserve accurate character shapes and stylistic\ntextures. Moreover, we also design a corner consistency loss and an elastic\nmesh feature loss to better improve geometric alignment. Extensive experiments\nshow that our DA-Font outperforms the state-of-the-art methods across diverse\nfont styles and characters, demonstrating its effectiveness in enhancing\nstructural integrity and local fidelity. The source code can be found at\n\\href{https://github.com/wrchen2001/DA-Font}{\\textit{https://github.com/wrchen2001/DA-Font}}.",
        "url": "http://arxiv.org/abs/2509.16632v1",
        "published_date": "2025-09-20T11:12:15+00:00",
        "updated_date": "2025-09-20T11:12:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weiran Chen",
            "Guiqian Zhu",
            "Ying Li",
            "Yi Ji",
            "Chunping Liu"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Other"
        ],
        "tldr": "DA-Font is a novel framework for few-shot font generation that outperforms existing methods by preserving accurate character shapes and stylistic textures.",
        "tldr_zh": "DA-Font是一个新颖的框架，用于少样本字体生成，通过保留准确的字符形状和风格纹理，优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "V-CECE: Visual Counterfactual Explanations via Conceptual Edits",
        "summary": "Recent black-box counterfactual generation frameworks fail to take into\naccount the semantic content of the proposed edits, while relying heavily on\ntraining to guide the generation process. We propose a novel, plug-and-play\nblack-box counterfactual generation framework, which suggests step-by-step\nedits based on theoretical guarantees of optimal edits to produce human-level\ncounterfactual explanations with zero training. Our framework utilizes a\npre-trained image editing diffusion model, and operates without access to the\ninternals of the classifier, leading to an explainable counterfactual\ngeneration process. Throughout our experimentation, we showcase the explanatory\ngap between human reasoning and neural model behavior by utilizing both\nConvolutional Neural Network (CNN), Vision Transformer (ViT) and Large Vision\nLanguage Model (LVLM) classifiers, substantiated through a comprehensive human\nevaluation.",
        "url": "http://arxiv.org/abs/2509.16567v1",
        "published_date": "2025-09-20T07:53:06+00:00",
        "updated_date": "2025-09-20T07:53:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Nikolaos Spanos",
            "Maria Lymperaiou",
            "Giorgos Filandrianos",
            "Konstantinos Thomas",
            "Athanasios Voulodimos",
            "Giorgos Stamou"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper introduces V-CECE, a novel black-box counterfactual generation framework that generates human-level counterfactual explanations with zero training by suggesting optimal step-by-step edits based on a pre-trained image editing diffusion model, without access to classifier internals.",
        "tldr_zh": "本文介绍了V-CECE，一种新颖的黑盒反事实生成框架，通过建议基于预训练图像编辑扩散模型的最优逐步编辑，生成人类水平的反事实解释，无需训练而生成。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Advancing Reference-free Evaluation of Video Captions with Factual Analysis",
        "summary": "Video captions offer concise snapshots of actors, objects, and actions within\na video, serving as valuable assets for applications such as question answering\nand event localization. However, acquiring human annotations for video captions\nis costly or even impractical, especially when dealing with diverse video\ndomains. Existing models trained on supervised datasets face challenges in\nevaluating performance across different domains due to the reliance on\nreference-based evaluation protocols, which necessitate ground truth captions.\nThis assumption is unrealistic for evaluating videos in the wild. To address\nthese limitations, we propose a reference-free evaluation framework that does\nnot require ground truth captions, focusing on factual grounding to ensure\naccurate assessment of caption quality. We introduce VC-Inspector, a novel\ncaption quality evaluator that is both reference-free and factually grounded.\nUtilizing large language models, we generate pseudo captions of varying quality\nbased on supervised data, which are subsequently used to train a multimodal\nmodel (i.e., Qwen2.5-VL) as the evaluator. Our approach demonstrates superior\nalignment with human judgments on the VATEX-Eval dataset, outperforming\nexisting methods. The performance also generalizes to image caption datasets,\nFlickr8K-Expert and Flickr8K-CF, when viewing images as 1-frame videos.\nOverall, VC-Inspector offers a scalable and generalizable solution for\nevaluating the factual accuracy of video captions, paving the way for more\neffective and objective assessment methodologies in diverse video domains.",
        "url": "http://arxiv.org/abs/2509.16538v1",
        "published_date": "2025-09-20T05:04:41+00:00",
        "updated_date": "2025-09-20T05:04:41+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Shubhashis Roy Dipta",
            "Tz-Ying Wu",
            "Subarna Tripathi"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a reference-free evaluation framework for video captions that focuses on factual grounding, demonstrating superior alignment with human judgments on various datasets.",
        "tldr_zh": "该论文介绍了一个针对视频字幕的无参考评估框架，重点关注事实基础，表现优异，并与各种数据集上的人类判断具有良好的一致性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "Neural Atlas Graphs for Dynamic Scene Decomposition and Editing",
        "summary": "Learning editable high-resolution scene representations for dynamic scenes is\nan open problem with applications across the domains from autonomous driving to\ncreative editing - the most successful approaches today make a trade-off\nbetween editability and supporting scene complexity: neural atlases represent\ndynamic scenes as two deforming image layers, foreground and background, which\nare editable in 2D, but break down when multiple objects occlude and interact.\nIn contrast, scene graph models make use of annotated data such as masks and\nbounding boxes from autonomous-driving datasets to capture complex 3D spatial\nrelationships, but their implicit volumetric node representations are\nchallenging to edit view-consistently. We propose Neural Atlas Graphs (NAGs), a\nhybrid high-resolution scene representation, where every graph node is a\nview-dependent neural atlas, facilitating both 2D appearance editing and 3D\nordering and positioning of scene elements. Fit at test-time, NAGs achieve\nstate-of-the-art quantitative results on the Waymo Open Dataset - by 5 dB PSNR\nincrease compared to existing methods - and make environmental editing possible\nin high resolution and visual quality - creating counterfactual driving\nscenarios with new backgrounds and edited vehicle appearance. We find that the\nmethod also generalizes beyond driving scenes and compares favorably - by more\nthan 7 dB in PSNR - to recent matting and video editing baselines on the DAVIS\nvideo dataset with a diverse set of human and animal-centric scenes.",
        "url": "http://arxiv.org/abs/2509.16336v1",
        "published_date": "2025-09-19T18:24:41+00:00",
        "updated_date": "2025-09-19T18:24:41+00:00",
        "categories": [
            "cs.GR",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Jan Philipp Schneider",
            "Pratik Singh Bisht",
            "Ilya Chugunov",
            "Andreas Kolb",
            "Michael Moeller",
            "Felix Heide"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes a new high-resolution scene representation, Neural Atlas Graphs (NAGs), which combines 2D appearance editing and 3D ordering and positioning, achieving state-of-the-art results on driving scenes and video datasets.",
        "tldr_zh": "本文提出了一种新的高分辨率场景表示，神经图谱图（NAGs），结合了2D外观编辑和3D顺序及定位，实现了在驾驶场景和视频数据集上的最先进结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Efficient Rectified Flow for Image Fusion",
        "summary": "Image fusion is a fundamental and important task in computer vision, aiming\nto combine complementary information from different modalities to fuse images.\nIn recent years, diffusion models have made significant developments in the\nfield of image fusion. However, diffusion models often require complex\ncomputations and redundant inference time, which reduces the applicability of\nthese methods. To address this issue, we propose RFfusion, an efficient\none-step diffusion model for image fusion based on Rectified Flow. We\nincorporate Rectified Flow into the image fusion task to straighten the\nsampling path in the diffusion model, achieving one-step sampling without the\nneed for additional training, while still maintaining high-quality fusion\nresults. Furthermore, we propose a task-specific variational autoencoder (VAE)\narchitecture tailored for image fusion, where the fusion operation is embedded\nwithin the latent space to further reduce computational complexity. To address\nthe inherent discrepancy between conventional reconstruction-oriented VAE\nobjectives and the requirements of image fusion, we introduce a two-stage\ntraining strategy. This approach facilitates the effective learning and\nintegration of complementary information from multi-modal source images,\nthereby enabling the model to retain fine-grained structural details while\nsignificantly enhancing inference efficiency. Extensive experiments demonstrate\nthat our method outperforms other state-of-the-art methods in terms of both\ninference speed and fusion quality. Code is available at\nhttps://github.com/zirui0625/RFfusion.",
        "url": "http://arxiv.org/abs/2509.16549v1",
        "published_date": "2025-09-20T06:21:00+00:00",
        "updated_date": "2025-09-20T06:21:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zirui Wang",
            "Jiayi Zhang",
            "Tianwei Guan",
            "Yuhan Zhou",
            "Xingyuan Li",
            "Minjing Dong",
            "Jinyuan Liu"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Multimodality"
        ],
        "tldr": "RFfusion is an efficient one-step diffusion model for image fusion, integrating Rectified Flow to improve inference efficiency without sacrificing fusion quality.",
        "tldr_zh": "RFfusion是一种高效的一步扩散模型，用于图像融合，集成了修正流技术，以提高推断效率而不损失融合质量。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "HyPlaneHead: Rethinking Tri-plane-like Representations in Full-Head Image Synthesis",
        "summary": "Tri-plane-like representations have been widely adopted in 3D-aware GANs for\nhead image synthesis and other 3D object/scene modeling tasks due to their\nefficiency. However, querying features via Cartesian coordinate projection\noften leads to feature entanglement, which results in mirroring artifacts. A\nrecent work, SphereHead, attempted to address this issue by introducing\nspherical tri-planes based on a spherical coordinate system. While it\nsuccessfully mitigates feature entanglement, SphereHead suffers from uneven\nmapping between the square feature maps and the spherical planes, leading to\ninefficient feature map utilization during rendering and difficulties in\ngenerating fine image details. Moreover, both tri-plane and spherical tri-plane\nrepresentations share a subtle yet persistent issue: feature penetration across\nconvolutional channels can cause interference between planes, particularly when\none plane dominates the others. These challenges collectively prevent\ntri-plane-based methods from reaching their full potential. In this paper, we\nsystematically analyze these problems for the first time and propose innovative\nsolutions to address them. Specifically, we introduce a novel hybrid-plane\n(hy-plane for short) representation that combines the strengths of both planar\nand spherical planes while avoiding their respective drawbacks. We further\nenhance the spherical plane by replacing the conventional theta-phi warping\nwith a novel near-equal-area warping strategy, which maximizes the effective\nutilization of the square feature map. In addition, our generator synthesizes a\nsingle-channel unified feature map instead of multiple feature maps in separate\nchannels, thereby effectively eliminating feature penetration. With a series of\ntechnical improvements, our hy-plane representation enables our method,\nHyPlaneHead, to achieve state-of-the-art performance in full-head image\nsynthesis.",
        "url": "http://arxiv.org/abs/2509.16748v1",
        "published_date": "2025-09-20T17:17:01+00:00",
        "updated_date": "2025-09-20T17:17:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Heyuan Li",
            "Kenkun Liu",
            "Lingteng Qiu",
            "Qi Zuo",
            "Keru Zheng",
            "Zilong Dong",
            "Xiaoguang Han"
        ],
        "ai_categories": [
            "GAN"
        ],
        "tldr": "The paper proposes a new hybrid-plane representation called HyPlaneHead, which combines planar and spherical planes to improve head image synthesis performance.",
        "tldr_zh": "本文提出了一种新的混合平面表示方法HyPlaneHead，结合了平面和球面，以提高头部图像合成性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Pain in 3D: Generating Controllable Synthetic Faces for Automated Pain Assessment",
        "summary": "Automated pain assessment from facial expressions is crucial for\nnon-communicative patients, such as those with dementia. Progress has been\nlimited by two challenges: (i) existing datasets exhibit severe demographic and\nlabel imbalance due to ethical constraints, and (ii) current generative models\ncannot precisely control facial action units (AUs), facial structure, or\nclinically validated pain levels.\n  We present 3DPain, a large-scale synthetic dataset specifically designed for\nautomated pain assessment, featuring unprecedented annotation richness and\ndemographic diversity. Our three-stage framework generates diverse 3D meshes,\ntextures them with diffusion models, and applies AU-driven face rigging to\nsynthesize multi-view faces with paired neutral and pain images, AU\nconfigurations, PSPI scores, and the first dataset-level annotations of\npain-region heatmaps. The dataset comprises 82,500 samples across 25,000 pain\nexpression heatmaps and 2,500 synthetic identities balanced by age, gender, and\nethnicity.\n  We further introduce ViTPain, a Vision Transformer based cross-modal\ndistillation framework in which a heatmap-trained teacher guides a student\ntrained on RGB images, enhancing accuracy, interpretability, and clinical\nreliability. Together, 3DPain and ViTPain establish a controllable, diverse,\nand clinically grounded foundation for generalizable automated pain assessment.",
        "url": "http://arxiv.org/abs/2509.16727v1",
        "published_date": "2025-09-20T15:41:23+00:00",
        "updated_date": "2025-09-20T15:41:23+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Xin Lei Lin",
            "Soroush Mehraban",
            "Abhishek Moturu",
            "Babak Taati"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Transformer"
        ],
        "tldr": "This paper introduces 3DPain, a synthetic dataset for automated pain assessment using facial expressions, and ViTPain, a framework for better accuracy and interpretability. It aims to address challenges in existing datasets and generative models.",
        "tldr_zh": "本文介绍了3DPain，这是一个用于自动疼痛评估的合成数据集，以及ViTPain，一个提高准确性和可解释性的框架。它旨在解决现有数据集和生成模型中的挑战。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Animalbooth: multimodal feature enhancement for animal subject personalization",
        "summary": "Personalized animal image generation is challenging due to rich appearance\ncues and large morphological variability. Existing approaches often exhibit\nfeature misalignment across domains, which leads to identity drift. We present\nAnimalBooth, a framework that strengthens identity preservation with an Animal\nNet and an adaptive attention module, mitigating cross domain alignment errors.\nWe further introduce a frequency controlled feature integration module that\napplies Discrete Cosine Transform filtering in the latent space to guide the\ndiffusion process, enabling a coarse to fine progression from global structure\nto detailed texture. To advance research in this area, we curate AnimalBench, a\nhigh resolution dataset for animal personalization. Extensive experiments show\nthat AnimalBooth consistently outperforms strong baselines on multiple\nbenchmarks and improves both identity fidelity and perceptual quality.",
        "url": "http://arxiv.org/abs/2509.16702v1",
        "published_date": "2025-09-20T14:09:48+00:00",
        "updated_date": "2025-09-20T14:09:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chen Liu",
            "Haitao Wu",
            "Kafeng Wang",
            "Xiaowang Zhang"
        ],
        "ai_categories": [
            "Multimodality",
            "LoRA",
            "Transformer",
            "Dataset"
        ],
        "tldr": "AnimalBooth is a new framework for personalized animal image generation that improves identity preservation and image quality by using a novel attention module and feature integration technique.",
        "tldr_zh": "AnimalBooth是一个新的框架，用于个性化动物图像生成，通过使用新颖的注意力模块和特征融合技术来提高身份保留和图像质量。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "InstanceAssemble: Layout-Aware Image Generation via Instance Assembling Attention",
        "summary": "Diffusion models have demonstrated remarkable capabilities in generating\nhigh-quality images. Recent advancements in Layout-to-Image (L2I) generation\nhave leveraged positional conditions and textual descriptions to facilitate\nprecise and controllable image synthesis. Despite overall progress, current L2I\nmethods still exhibit suboptimal performance. Therefore, we propose\nInstanceAssemble, a novel architecture that incorporates layout conditions via\ninstance-assembling attention, enabling position control with bounding boxes\n(bbox) and multimodal content control including texts and additional visual\ncontent. Our method achieves flexible adaption to existing DiT-based T2I models\nthrough light-weighted LoRA modules. Additionally, we propose a Layout-to-Image\nbenchmark, Denselayout, a comprehensive benchmark for layout-to-image\ngeneration, containing 5k images with 90k instances in total. We further\nintroduce Layout Grounding Score (LGS), an interpretable evaluation metric to\nmore precisely assess the accuracy of L2I generation. Experiments demonstrate\nthat our InstanceAssemble method achieves state-of-the-art performance under\ncomplex layout conditions, while exhibiting strong compatibility with diverse\nstyle LoRA modules.",
        "url": "http://arxiv.org/abs/2509.16691v1",
        "published_date": "2025-09-20T13:37:37+00:00",
        "updated_date": "2025-09-20T13:37:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qiang Xiang",
            "Shuang Sun",
            "Binglei Li",
            "Dejia Song",
            "Huaxia Li",
            "Nemo Chen",
            "Xu Tang",
            "Yao Hu",
            "Junping Zhang"
        ],
        "ai_categories": [
            "AIGC",
            "LoRA",
            "Dataset"
        ],
        "tldr": "InstanceAssemble proposes a novel architecture for layout-aware image generation using instance-assembling attention, achieving state-of-the-art performance under complex layout conditions.",
        "tldr_zh": "InstanceAssemble提出了一种新颖的架构，利用实例组装注意力实现布局感知图像生成，在复杂布局条件下取得了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards a Transparent and Interpretable AI Model for Medical Image Classifications",
        "summary": "The integration of artificial intelligence (AI) into medicine is remarkable,\noffering advanced diagnostic and therapeutic possibilities. However, the\ninherent opacity of complex AI models presents significant challenges to their\nclinical practicality. This paper focuses primarily on investigating the\napplication of explainable artificial intelligence (XAI) methods, with the aim\nof making AI decisions transparent and interpretable. Our research focuses on\nimplementing simulations using various medical datasets to elucidate the\ninternal workings of the XAI model. These dataset-driven simulations\ndemonstrate how XAI effectively interprets AI predictions, thus improving the\ndecision-making process for healthcare professionals. In addition to a survey\nof the main XAI methods and simulations, ongoing challenges in the XAI field\nare discussed. The study highlights the need for the continuous development and\nexploration of XAI, particularly from the perspective of diverse medical\ndatasets, to promote its adoption and effectiveness in the healthcare domain.",
        "url": "http://arxiv.org/abs/2509.16685v1",
        "published_date": "2025-09-20T13:26:31+00:00",
        "updated_date": "2025-09-20T13:26:31+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Binbin Wen",
            "Yihang Wu",
            "Tareef Daqqaq",
            "Ahmad Chaddad"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper focuses on creating a transparent and interpretable AI model for medical image classifications using explainable artificial intelligence methods.",
        "tldr_zh": "本文旨在利用可解释人工智能方法为医学图像分类创建透明且可解释的AI模型。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "IPF-RDA: An Information-Preserving Framework for Robust Data Augmentation",
        "summary": "Data augmentation is widely utilized as an effective technique to enhance the\ngeneralization performance of deep models. However, data augmentation may\ninevitably introduce distribution shifts and noises, which significantly\nconstrain the potential and deteriorate the performance of deep networks. To\nthis end, we propose a novel information-preserving framework, namely IPF-RDA,\nto enhance the robustness of data augmentations in this paper. IPF-RDA combines\nthe proposal of (i) a new class-discriminative information estimation algorithm\nthat identifies the points most vulnerable to data augmentation operations and\ncorresponding importance scores; And (ii) a new information-preserving scheme\nthat preserves the critical information in the augmented samples and ensures\nthe diversity of augmented data adaptively. We divide data augmentation methods\ninto three categories according to the operation types and integrate these\napproaches into our framework accordingly. After being integrated into our\nframework, the robustness of data augmentation methods can be enhanced and\ntheir full potential can be unleashed. Extensive experiments demonstrate that\nalthough being simple, IPF-RDA consistently improves the performance of\nnumerous commonly used state-of-the-art data augmentation methods with popular\ndeep models on a variety of datasets, including CIFAR-10, CIFAR-100,\nTiny-ImageNet, CUHK03, Market1501, Oxford Flower, and MNIST, where its\nperformance and scalability are stressed. The implementation is available at\nhttps://github.com/Jackbrocp/IPF-RDA.",
        "url": "http://arxiv.org/abs/2509.16678v1",
        "published_date": "2025-09-20T13:06:45+00:00",
        "updated_date": "2025-09-20T13:06:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Suorong Yang",
            "Hongchao Yang",
            "Suhan Guo",
            "Furao Shen",
            "Jian Zhao"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces IPF-RDA, a framework for enhancing the robustness of data augmentation in deep learning models by preserving critical information and ensuring diversity in augmented data.",
        "tldr_zh": "本文介绍了IPF-RDA，这是一个用于增强深度学习模型中数据增强鲁棒性的框架，通过保留关键信息和确保增强数据的多样性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MedCutMix: A Data-Centric Approach to Improve Radiology Vision-Language Pre-training with Disease Awareness",
        "summary": "Vision-Language Pre-training (VLP) is drawing increasing interest for its\nability to minimize manual annotation requirements while enhancing semantic\nunderstanding in downstream tasks. However, its reliance on image-text datasets\nposes challenges due to privacy concerns and the high cost of obtaining paired\nannotations. Data augmentation emerges as a viable strategy to address this\nissue, yet existing methods often fall short of capturing the subtle and\ncomplex variations in medical data due to limited diversity. To this end, we\npropose MedCutMix, a novel multi-modal disease-centric data augmentation\nmethod. MedCutMix performs diagnostic sentence CutMix within medical reports\nand establishes the cross-attention between the diagnostic sentence and medical\nimage to guide attentive manifold mix within the imaging modality. Our approach\nsurpasses previous methods across four downstream radiology diagnosis datasets,\nhighlighting its effectiveness in enhancing performance and generalizability in\nradiology VLP.",
        "url": "http://arxiv.org/abs/2509.16673v1",
        "published_date": "2025-09-20T12:51:14+00:00",
        "updated_date": "2025-09-20T12:51:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sinuo Wang",
            "Yutong Xie",
            "Yuyuan Liu",
            "Qi Wu"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a new data augmentation method, MedCutMix, to enhance radiology Vision-Language Pre-training by incorporating disease-centric information from medical reports.",
        "tldr_zh": "本文介绍了一种新的数据增强方法MedCutMix，通过从医疗报告中引入以疾病为中心的信息，增强放射学视觉语言预训练。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs",
        "summary": "Large Vision-Language Models (L-VLMs) have demonstrated remarkable\nperformance in various vision and language tasks, including visual question\nanswering (VQA). However, their high computational cost makes them impractical\nfor resource-constrained settings and inference-heavy applications. In\ncontrast, Small Vision-Language Models (S-VLMs) offer efficiency but suffer\nfrom a significant performance gap compared to their larger counterparts. In\nthis work, we introduce the Model Parity Aligner (MPA), a novel framework\ndesigned to systematically improve S-VLMs by leveraging unlabeled images and\neffective knowledge transfer from L-VLMs. Instead of traditional knowledge\ndistillation methods that rely on labeled training data, MPA employs a\nstrategic parity-based approach that precisely identifies the knowledge\ndisparities between S-VLMs and L-VLMs, and optimizes training by targeting only\nthese disparities. We conduct extensive experiments on four diverse VQA\nbenchmarks, namely TextVQA, ST-VQA, ChartQA, and OKVQA, each of which requires\nspecialized reasoning capabilities such as text recognition, chart\ninterpretation, and commonsense and factual understanding. Our results\ndemonstrate that MPA consistently enhances the performance of S-VLMs on all\nbenchmarks, reducing the performance gap while maintaining computational\nefficiency. We make our code publicly available.",
        "url": "http://arxiv.org/abs/2509.16633v1",
        "published_date": "2025-09-20T11:12:23+00:00",
        "updated_date": "2025-09-20T11:12:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Abhirama Subramanyam Penamakuri",
            "Navlika Singh",
            "Piyush Arora",
            "Anand Mishra"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces Model Parity Aligner (MPA) to improve efficiency and performance of Small Vision-Language Models (S-VLMs) using knowledge transfer from Large VLMs (L-VLMs) without labeled data.",
        "tldr_zh": "本文介绍了模型平等调整器（MPA），通过从大型VLM中传递知识而不使用标记数据，以提高小型视觉语言模型（S-VLMs）的效率和性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation",
        "summary": "We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework\nfor freestyle portrait animation driven by facial landmarks. The main\nchallenges in this task are preserving the identity of the reference portrait,\naccurately transferring target expressions, and maintaining long-term temporal\nconsistency while ensuring generation efficiency. To address identity\npreservation and accurate expression retargeting, we enhance Stable Diffusion\nwith two key components: a expression-aware landmarks as explicit motion\nsignals, which improve motion alignment, support exaggerated expressions, and\nreduce identity leakage; and a fine-grained facial loss that leverages both\nexpression and facial masks to better capture subtle expressions and faithfully\npreserve the reference appearance. With these components, our model supports\ncontrollable and expressive animation across diverse portrait types, including\nreal faces, cartoons, sculptures, and animals. However, diffusion-based\nframeworks typically struggle to efficiently generate long-term stable\nanimation results, which remains a core challenge in this task. To address\nthis, we propose a progressive generation strategy for stable long-term\nanimation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless\nacceleration. These two strategies ensure that our method produces high-quality\nresults efficiently, making it user-friendly and accessible. Finally, we\nintroduce EmojiBench++, a more comprehensive benchmark comprising diverse\nportraits, driving videos, and landmark sequences. Extensive evaluations on\nEmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior\nperformance in both animation quality and controllability. The code, training\ndataset and benchmark will be found in https://follow-your-emoji.github.io/.",
        "url": "http://arxiv.org/abs/2509.16630v1",
        "published_date": "2025-09-20T11:09:01+00:00",
        "updated_date": "2025-09-20T11:09:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yue Ma",
            "Zexuan Yan",
            "Hongyu Liu",
            "Hongfa Wang",
            "Heng Pan",
            "Yingqing He",
            "Junkun Yuan",
            "Ailing Zeng",
            "Chengfei Cai",
            "Heung-Yeung Shum",
            "Zhifeng Li",
            "Wei Liu",
            "Linfeng Zhang",
            "Qifeng Chen"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper introduces a diffusion-based framework for efficient, fine-controllable, and expressive freestyle portrait animation, with a focus on identity preservation, expression retargeting, and long-term stability.",
        "tldr_zh": "本文提出了一种基于扩散的框架，用于高效、细致可控和富有表现力的肖像动画创作，重点关注身份保留、表情转移和长期稳定性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing Scientific Visual Question Answering via Vision-Caption aware Supervised Fine-Tuning",
        "summary": "In this study, we introduce Vision-Caption aware Supervised FineTuning\n(VCASFT), a novel learning paradigm designed to enhance the performance of\nsmaller Vision Language Models(VLMs) on scientific visual question\nanswering(VQA) tasks. VCASFT leverages image captions as zero-shot prompts\nalongside question-answer pairs and instruction-tunes models to yield\nsignificant performance improvements. To comprehensively evaluate VCASFT, we\nbenchmark it on ScienceQA, which consists of questions across diverse\nlanguages, subjects, and fields, demonstrating its adaptability and\neffectiveness in a variety of educational contexts. Additionally, to further\ndemonstrate the effectiveness of this technique on lowresource languages, we\ndeveloped HiSciVQA, a dataset comprising 2,245 high-quality, hand-annotated\nHindi multimodal Q&A pairs. This dataset addresses the critical need for\nlow-resource language Q&A datasets and serves as a foundation for testing\nVCASFT. Additionally, we introduce a novel LLM-based evaluation scheme to\nevaluate VLMs on HiSciVQA which offers deeper insights into model effectiveness\nsurpassing traditional n-gram matching accuracy metrics. We are committed to\nadvancing the field by open-sourcing all code files and the HiSciVQA dataset\nfor the research community.",
        "url": "http://arxiv.org/abs/2509.16628v1",
        "published_date": "2025-09-20T11:07:36+00:00",
        "updated_date": "2025-09-20T11:07:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Janak Kapuriya",
            "Anwar Shaikh",
            "Arnav Goel",
            "Medha Hira",
            "Apoorv Singh",
            "Jay Saraf",
            "Sanjana",
            "Vaibhav Nauriyal",
            "Avinash Anand",
            "Zhengkui Wang",
            "Rajiv Ratn Shah"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a new learning paradigm, VCASFT, to improve smaller Vision Language Models on scientific visual question answering tasks by leveraging image captions. It also presents a new dataset for low-resource language Q&A and evaluation metrics for model effectiveness.",
        "tldr_zh": "本文介绍了一种新的学习范式VCASFT，通过利用图像标题来提高较小的视觉语言模型在科学视觉问题回答任务上的表现。同时，还提出了一个用于低资源语言问答的新数据集和模型有效性评估指标。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Surgical-MambaLLM: Mamba2-enhanced Multimodal Large Language Model for VQLA in Robotic Surgery",
        "summary": "In recent years, Visual Question Localized-Answering in robotic surgery\n(Surgical-VQLA) has gained significant attention for its potential to assist\nmedical students and junior doctors in understanding surgical scenes. Recently,\nthe rapid development of Large Language Models (LLMs) has provided more\npromising solutions for this task. However, current methods struggle to\nestablish complex dependencies between text and visual details, and have\ndifficulty perceiving the spatial information of surgical scenes. To address\nthese challenges, we propose a novel method, Surgical-MambaLLM, which is the\nfirst to combine Mamba2 with LLM in the surgical domain, that leverages\nMamba2's ability to effectively capture cross-modal dependencies and perceive\nspatial information in surgical scenes, thereby enhancing the LLMs'\nunderstanding of surgical images. Specifically, we propose the Cross-modal\nBidirectional Mamba2 Integration (CBMI) module to leverage Mamba2 for effective\nmultimodal fusion, with its cross-modal integration capabilities. Additionally,\ntailored to the geometric characteristics of surgical scenes, we design the\nSurgical Instrument Perception (SIP) scanning mode for Mamba2 to scan the\nsurgical images, enhancing the model's spatial understanding of the surgical\nscene. Extensive experiments demonstrate that our Surgical-MambaLLM model\noutperforms the state-of-the-art methods on the EndoVis17-VQLA and\nEndoVis18-VQLA datasets, significantly improving the performance of the\nSurgical-VQLA task.",
        "url": "http://arxiv.org/abs/2509.16618v1",
        "published_date": "2025-09-20T10:42:29+00:00",
        "updated_date": "2025-09-20T10:42:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Pengfei Hao",
            "Hongqiu Wang",
            "Shuaibo Li",
            "Zhaohu Xing",
            "Guang Yang",
            "Kaishun Wu",
            "Lei Zhu"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces Surgical-MambaLLM, a model combining Mamba2 and LLM for Visual Question Localized-Answering in robotic surgery, outperforming current methods on surgical image understanding tasks.",
        "tldr_zh": "本文介绍了Surgical-MambaLLM模型，结合了Mamba2和LLM，用于机器人手术中的视觉问题定位回答，优于当前方法在手术图像理解任务上的表现。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Describe-to-Score: Text-Guided Efficient Image Complexity Assessment",
        "summary": "Accurately assessing image complexity (IC) is critical for computer vision,\nyet most existing methods rely solely on visual features and often neglect\nhigh-level semantic information, limiting their accuracy and generalization. We\nintroduce vision-text fusion for IC modeling. This approach integrates visual\nand textual semantic features, increasing representational diversity. It also\nreduces the complexity of the hypothesis space, which enhances both accuracy\nand generalization in complexity assessment. We propose the D2S\n(Describe-to-Score) framework, which generates image captions with a\npre-trained vision-language model. We propose the feature alignment and entropy\ndistribution alignment mechanisms, D2S guides semantic information to inform\ncomplexity assessment while bridging the gap between vision and text\nmodalities. D2S utilizes multi-modal information during training but requires\nonly the vision branch during inference, thereby avoiding multi-modal\ncomputational overhead and enabling efficient assessment. Experimental results\ndemonstrate that D2S outperforms existing methods on the IC9600 dataset and\nmaintains competitiveness on no-reference image quality assessment (NR-IQA)\nbenchmark, validating the effectiveness and efficiency of multi-modal fusion in\ncomplexity-related tasks. Code is available at:\nhttps://github.com/xauat-liushipeng/D2S",
        "url": "http://arxiv.org/abs/2509.16609v1",
        "published_date": "2025-09-20T10:17:25+00:00",
        "updated_date": "2025-09-20T10:17:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shipeng Liu",
            "Zhonglin Zhang",
            "Dengfeng Chen",
            "Liang Zhao"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes a vision-text fusion approach for image complexity assessment, outperforming existing methods on IC datasets and NR-IQA benchmarks.",
        "tldr_zh": "本文提出了一种视觉文本融合方法，用于图像复杂度评估，在IC数据集和NR-IQA基准测试中表现优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FakeChain: Exposing Shallow Cues in Multi-Step Deepfake Detection",
        "summary": "Multi-step or hybrid deepfakes, created by sequentially applying different\ndeepfake creation methods such as Face-Swapping, GAN-based generation, and\nDiffusion methods, can pose an emerging and unforseen technical challenge for\ndetection models trained on single-step forgeries. While prior studies have\nmainly focused on detecting isolated single manipulation, little is known about\nthe detection model behavior under such compositional, hybrid, and complex\nmanipulation pipelines. In this work, we introduce \\textbf{FakeChain}, a\nlarge-scale benchmark comprising 1-, 2-, and 3-Step forgeries synthesized using\nfive state-of-the-art representative generators. Using this approach, we\nanalyze detection performance and spectral properties across hybrid\nmanipulation at different step, along with varying generator combinations and\nquality settings. Surprisingly, our findings reveal that detection performance\nhighly depends on the final manipulation type, with F1-score dropping by up to\n\\textbf{58.83\\%} when it differs from training distribution. This clearly\ndemonstrates that detectors rely on last-stage artifacts rather than cumulative\nmanipulation traces, limiting generalization. Such findings highlight the need\nfor detection models to explicitly consider manipulation history and sequences.\nOur results highlight the importance of benchmarks such as FakeChain,\nreflecting growing synthesis complexity and diversity in real-world scenarios.\nOur sample code is available\nhere\\footnote{https://github.com/minjihh/FakeChain}.",
        "url": "http://arxiv.org/abs/2509.16602v1",
        "published_date": "2025-09-20T09:53:50+00:00",
        "updated_date": "2025-09-20T09:53:50+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Minji Heo",
            "Simon S. Woo"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces FakeChain, a benchmark dataset for detecting multi-step deepfakes. It emphasizes the importance of considering manipulation history and sequences for detection models.",
        "tldr_zh": "本文介绍了FakeChain，一个用于检测多步深度伪造的基准数据集。它强调了对检测模型考虑操作历史和顺序的重要性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SQS: Enhancing Sparse Perception Models via Query-based Splatting in Autonomous Driving",
        "summary": "Sparse Perception Models (SPMs) adopt a query-driven paradigm that forgoes\nexplicit dense BEV or volumetric construction, enabling highly efficient\ncomputation and accelerated inference. In this paper, we introduce SQS, a novel\nquery-based splatting pre-training specifically designed to advance SPMs in\nautonomous driving. SQS introduces a plug-in module that predicts 3D Gaussian\nrepresentations from sparse queries during pre-training, leveraging\nself-supervised splatting to learn fine-grained contextual features through the\nreconstruction of multi-view images and depth maps. During fine-tuning, the\npre-trained Gaussian queries are seamlessly integrated into downstream networks\nvia query interaction mechanisms that explicitly connect pre-trained queries\nwith task-specific queries, effectively accommodating the diverse requirements\nof occupancy prediction and 3D object detection. Extensive experiments on\nautonomous driving benchmarks demonstrate that SQS delivers considerable\nperformance gains across multiple query-based 3D perception tasks, notably in\noccupancy prediction and 3D object detection, outperforming prior\nstate-of-the-art pre-training approaches by a significant margin (i.e., +1.3\nmIoU on occupancy prediction and +1.0 NDS on 3D detection).",
        "url": "http://arxiv.org/abs/2509.16588v1",
        "published_date": "2025-09-20T09:25:19+00:00",
        "updated_date": "2025-09-20T09:25:19+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Haiming Zhang",
            "Yiyao Zhu",
            "Wending Zhou",
            "Xu Yan",
            "Yingjie Cai",
            "Bingbing Liu",
            "Shuguang Cui",
            "Zhen Li"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces SQS, a novel query-based splatting pre-training method to enhance Sparse Perception Models in autonomous driving, achieving significant performance gains in occupancy prediction and 3D object detection tasks.",
        "tldr_zh": "该论文引入了SQS，一种新型基于查询的飞溅预训练方法，以增强自动驾驶中的稀疏感知模型，在占用预测和3D目标检测任务中取得显著性能提升。",
        "relevance_score": 1,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "A Novel Metric for Detecting Memorization in Generative Models for Brain MRI Synthesis",
        "summary": "Deep generative models have emerged as a transformative tool in medical\nimaging, offering substantial potential for synthetic data generation. However,\nrecent empirical studies highlight a critical vulnerability: these models can\nmemorize sensitive training data, posing significant risks of unauthorized\npatient information disclosure. Detecting memorization in generative models\nremains particularly challenging, necessitating scalable methods capable of\nidentifying training data leakage across large sets of generated samples. In\nthis work, we propose DeepSSIM, a novel self-supervised metric for quantifying\nmemorization in generative models. DeepSSIM is trained to: i) project images\ninto a learned embedding space and ii) force the cosine similarity between\nembeddings to match the ground-truth SSIM (Structural Similarity Index) scores\ncomputed in the image space. To capture domain-specific anatomical features,\ntraining incorporates structure-preserving augmentations, allowing DeepSSIM to\nestimate similarity reliably without requiring precise spatial alignment. We\nevaluate DeepSSIM in a case study involving synthetic brain MRI data generated\nby a Latent Diffusion Model (LDM) trained under memorization-prone conditions,\nusing 2,195 MRI scans from two publicly available datasets (IXI and CoRR).\nCompared to state-of-the-art memorization metrics, DeepSSIM achieves superior\nperformance, improving F1 scores by an average of +52.03% over the best\nexisting method. Code and data of our approach are publicly available at the\nfollowing link: https://github.com/brAIn-science/DeepSSIM.",
        "url": "http://arxiv.org/abs/2509.16582v1",
        "published_date": "2025-09-20T09:08:08+00:00",
        "updated_date": "2025-09-20T09:08:08+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Antonio Scardace",
            "Lemuel Puglisi",
            "Francesco Guarnera",
            "Sebastiano Battiato",
            "Daniele Ravì"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Diffusion"
        ],
        "tldr": "The paper proposes a novel metric called DeepSSIM to detect memorization in generative models for brain MRI synthesis, showing superior performance compared to existing methods.",
        "tldr_zh": "本文提出了一种名为DeepSSIM的新度量标准，用于检测用于大脑MRI合成的生成模型中的记忆化现象，表现优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Captioning for Text-Video Retrieval via Dual-Group Direct Preference Optimization",
        "summary": "In text-video retrieval, auxiliary captions are often used to enhance video\nunderstanding, bridging the gap between the modalities. While recent advances\nin multi-modal large language models (MLLMs) have enabled strong zero-shot\ncaption generation, we observe that such captions tend to be generic and\nindistinguishable across visually similar videos, limiting their utility for\nfine-grained retrieval. Moreover, conventional captioning approaches are\ntypically evaluated using language generation metrics, such as BLEU, which are\nnot typically tailored for retrieval tasks that require making discriminative\ndistinctions between candidates. To address this, we propose\n$\\textbf{CaRe-DPO}$, a retrieval framework that directly optimizes caption\ngeneration using retrieval relevance scores. At its core is Dual-Group Direct\nPreference Optimization (DG-DPO), a novel learning strategy that supervises\ncaptioning by modeling preferences across groups of distinct video and caption\npairs. In addition, we present an MLLM-based retrieval model that incorporates\nrole-embeddings to better distinguish between textual inputs with different\nfunctional roles, such as an auxiliary caption and a text query. Through\nextensive experiments, we demonstrate that CaRe-DPO significantly enhances\nretrieval performance by effectively leveraging auxiliary knowledge to generate\nfine-grained captions for retrieval. Code is available at\nhttps://github.com/mlvlab/CaReDPO.",
        "url": "http://arxiv.org/abs/2509.16560v1",
        "published_date": "2025-09-20T07:36:53+00:00",
        "updated_date": "2025-09-20T07:36:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ji Soo Lee",
            "Byungoh Ko",
            "Jaewon Cho",
            "Howoong Lee",
            "Jaewoon Byun",
            "Hyunwoo J. Kim"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a retrieval framework that directly optimizes caption generation using retrieval relevance scores, significantly enhancing retrieval performance by generating fine-grained captions.",
        "tldr_zh": "本文提出了一种检索框架，通过检索相关性评分直接优化字幕生成，通过生成细粒度字幕显著提高了检索性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Person Identification from Egocentric Human-Object Interactions using 3D Hand Pose",
        "summary": "Human-Object Interaction Recognition (HOIR) and user identification play a\ncrucial role in advancing augmented reality (AR)-based personalized assistive\ntechnologies. These systems are increasingly being deployed in high-stakes,\nhuman-centric environments such as aircraft cockpits, aerospace maintenance,\nand surgical procedures. This research introduces I2S (Interact2Sign), a multi\nstage framework designed for unobtrusive user identification through human\nobject interaction recognition, leveraging 3D hand pose analysis in egocentric\nvideos. I2S utilizes handcrafted features extracted from 3D hand poses and per\nforms sequential feature augmentation: first identifying the object class,\nfollowed by HOI recognition, and ultimately, user identification. A\ncomprehensive feature extraction and description process was carried out for 3D\nhand poses, organizing the extracted features into semantically meaningful\ncategories: Spatial, Frequency, Kinematic, Orientation, and a novel descriptor\nintroduced in this work, the Inter-Hand Spatial Envelope (IHSE). Extensive\nablation studies were conducted to determine the most effective combination of\nfeatures. The optimal configuration achieved an impressive average F1-score of\n97.52% for user identification, evaluated on a bimanual object manipulation\ndataset derived from the ARCTIC and H2O datasets. I2S demonstrates\nstate-of-the-art performance while maintaining a lightweight model size of\nunder 4 MB and a fast inference time of 0.1 seconds. These characteristics make\nthe proposed framework highly suitable for real-time, on-device authentication\nin security-critical, AR-based systems.",
        "url": "http://arxiv.org/abs/2509.16557v1",
        "published_date": "2025-09-20T07:27:32+00:00",
        "updated_date": "2025-09-20T07:27:32+00:00",
        "categories": [
            "cs.CV",
            "cs.ET",
            "cs.HC",
            "cs.LG"
        ],
        "authors": [
            "Muhammad Hamza",
            "Danish Hamid",
            "Muhammad Tahir Akram"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The research introduces a framework called I2S for user identification through human-object interaction recognition based on 3D hand pose analysis in egocentric videos.",
        "tldr_zh": "该研究引入了一种名为I2S的框架，通过在第一人称视角视频中进行3D手势分析，实现对用户的识别。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "ViTCAE: ViT-based Class-conditioned Autoencoder",
        "summary": "Vision Transformer (ViT) based autoencoders often underutilize the global\nClass token and employ static attention mechanisms, limiting both generative\ncontrol and optimization efficiency. This paper introduces ViTCAE, a framework\nthat addresses these issues by re-purposing the Class token into a generative\nlinchpin. In our architecture, the encoder maps the Class token to a global\nlatent variable that dictates the prior distribution for local, patch-level\nlatent variables, establishing a robust dependency where global semantics\ndirectly inform the synthesis of local details. Drawing inspiration from\nopinion dynamics, we treat each attention head as a dynamical system of\ninteracting tokens seeking consensus. This perspective motivates a\nconvergence-aware temperature scheduler that adaptively anneals each head's\ninfluence function based on its distributional stability. This process enables\na principled head-freezing mechanism, guided by theoretically-grounded\ndiagnostics like an attention evolution distance and a consensus/cluster\nfunctional. This technique prunes converged heads during training to\nsignificantly improve computational efficiency without sacrificing fidelity. By\nunifying a generative Class token with an adaptive attention mechanism rooted\nin multi-agent consensus theory, ViTCAE offers a more efficient and\ncontrollable approach to transformer-based generation.",
        "url": "http://arxiv.org/abs/2509.16554v1",
        "published_date": "2025-09-20T06:48:45+00:00",
        "updated_date": "2025-09-20T06:48:45+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Vahid Jebraeeli",
            "Hamid Krim",
            "Derya Cansever"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "ViTCAE is a framework that combines a generative Class token with an adaptive attention mechanism to offer a more efficient and controllable approach to transformer-based generation.",
        "tldr_zh": "ViTCAE是一种框架，将生成性的Class令牌与自适应注意机制结合起来，提供了一种更高效和可控的基于transformer的生成方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ST-GS: Vision-Based 3D Semantic Occupancy Prediction with Spatial-Temporal Gaussian Splatting",
        "summary": "3D occupancy prediction is critical for comprehensive scene understanding in\nvision-centric autonomous driving. Recent advances have explored utilizing 3D\nsemantic Gaussians to model occupancy while reducing computational overhead,\nbut they remain constrained by insufficient multi-view spatial interaction and\nlimited multi-frame temporal consistency. To overcome these issues, in this\npaper, we propose a novel Spatial-Temporal Gaussian Splatting (ST-GS) framework\nto enhance both spatial and temporal modeling in existing Gaussian-based\npipelines. Specifically, we develop a guidance-informed spatial aggregation\nstrategy within a dual-mode attention mechanism to strengthen spatial\ninteraction in Gaussian representations. Furthermore, we introduce a\ngeometry-aware temporal fusion scheme that effectively leverages historical\ncontext to improve temporal continuity in scene completion. Extensive\nexperiments on the large-scale nuScenes occupancy prediction benchmark showcase\nthat our proposed approach not only achieves state-of-the-art performance but\nalso delivers markedly better temporal consistency compared to existing\nGaussian-based methods.",
        "url": "http://arxiv.org/abs/2509.16552v1",
        "published_date": "2025-09-20T06:36:30+00:00",
        "updated_date": "2025-09-20T06:36:30+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Xiaoyang Yan",
            "Muleilan Pei",
            "Shaojie Shen"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a Spatial-Temporal Gaussian Splatting framework for 3D occupancy prediction in autonomous driving, enhancing spatial and temporal modeling for better performance and consistency.",
        "tldr_zh": "本文提出了一种空间-时间高斯喷溅框架，用于自动驾驶中的3D占用预测，增强了空间和时间建模以实现更好的性能和一致性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Lattice Boltzmann Model for Learning Real-World Pixel Dynamicity",
        "summary": "This work proposes the Lattice Boltzmann Model (LBM) to learn real-world\npixel dynamicity for visual tracking. LBM decomposes visual representations\ninto dynamic pixel lattices and solves pixel motion states through\ncollision-streaming processes. Specifically, the high-dimensional distribution\nof the target pixels is acquired through a multilayer predict-update network to\nestimate the pixel positions and visibility. The predict stage formulates\nlattice collisions among the spatial neighborhood of target pixels and develops\nlattice streaming within the temporal visual context. The update stage\nrectifies the pixel distributions with online visual representations. Compared\nwith existing methods, LBM demonstrates practical applicability in an online\nand real-time manner, which can efficiently adapt to real-world visual tracking\ntasks. Comprehensive evaluations of real-world point tracking benchmarks such\nas TAP-Vid and RoboTAP validate LBM's efficiency. A general evaluation of\nlarge-scale open-world object tracking benchmarks such as TAO, BFT, and OVT-B\nfurther demonstrates LBM's real-world practicality.",
        "url": "http://arxiv.org/abs/2509.16527v1",
        "published_date": "2025-09-20T04:25:27+00:00",
        "updated_date": "2025-09-20T04:25:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Guangze Zheng",
            "Shijie Lin",
            "Haobo Zuo",
            "Si Si",
            "Ming-Shan Wang",
            "Changhong Fu",
            "Jia Pan"
        ],
        "ai_categories": [
            "Other"
        ],
        "tldr": "The paper proposes a Lattice Boltzmann Model for learning real-world pixel dynamics, specifically for visual tracking tasks, demonstrating efficiency in online and real-time applications.",
        "tldr_zh": "本文提出了一种用于学习真实世界像素动态的晶格玻尔兹曼模型，特别是用于视觉跟踪任务，展示了在线实时应用方面的效率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers",
        "summary": "Generating realistic videos with diffusion transformers demands significant\ncomputation, with attention layers the central bottleneck; even producing a\nshort clip requires running a transformer over a very long sequence of\nembeddings, e.g., more than 30K embeddings for a 5-second video, incurring\nsignificant latency. Prior work aims to mitigate this bottleneck by exploiting\nsparsity in the attention layers to reduce computation. However, these works\ntypically rely on block-sparse attention, which skips score computation only\nwhen all entries in a block of attention scores (corresponding to M queries and\nM keys, with M = 64 typically) are zero. This coarse-granular skipping of\nattention scores does not fully exploit sparsity in the attention map and\nleaves room for improvement. In this work, we propose FG-Attn, a sparse\nattention mechanism for long-context diffusion transformers that leverages\nsparsity at a fine granularity. Unlike block-sparse attention, which skips\nentire MxM blocks, our approach skips computations at the granularity of Mx1\nslices of the attention map. Each slice is produced by query-key dot products\nbetween a block of query vectors and a single key. To implement our proposed\nsparse attention mechanism, we develop a new efficient bulk-load operation\ncalled asynchronous-gather load. This load operation gathers a sparse set of\nrelevant key-value vectors from memory and arranges them into packed tiles in\nthe GPU's shared memory. Only a sparse set of keys relevant to those queries\nare loaded into shared memory when computing attention for a block of queries,\nin contrast to loading full blocks of key tokens in block-sparse attention. Our\nfine-grained sparse attention, applied to video diffusion models, achieves an\naverage 1.55X (up to 1.65X) speedup for 5 second, 480p videos, and an average\n1.41X (up to 1.49X) for 5 second, 720p videos on a single H100 GPU.",
        "url": "http://arxiv.org/abs/2509.16518v1",
        "published_date": "2025-09-20T03:48:32+00:00",
        "updated_date": "2025-09-20T03:48:32+00:00",
        "categories": [
            "cs.CV",
            "cs.AR"
        ],
        "authors": [
            "Sankeerth Durvasula",
            "Kavya Sreedhar",
            "Zain Moustafa",
            "Suraj Kothawade",
            "Ashish Gondimalla",
            "Suvinay Subramanian",
            "Narges Shahidi",
            "Nandita Vijaykumar"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer"
        ],
        "tldr": "The paper introduces FG-Attn, a sparse attention mechanism for diffusion transformers, which achieves significant speedups in video generation tasks.",
        "tldr_zh": "本文介绍了FG-Attn，这是一种用于扩散变压器的稀疏注意机制，在视频生成任务中实现了显著的加速。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Seeing Culture: A Benchmark for Visual Reasoning and Grounding",
        "summary": "Multimodal vision-language models (VLMs) have made substantial progress in\nvarious tasks that require a combined understanding of visual and textual\ncontent, particularly in cultural understanding tasks, with the emergence of\nnew cultural datasets. However, these datasets frequently fall short of\nproviding cultural reasoning while underrepresenting many cultures. In this\npaper, we introduce the Seeing Culture Benchmark (SCB), focusing on cultural\nreasoning with a novel approach that requires VLMs to reason on culturally rich\nimages in two stages: i) selecting the correct visual option with\nmultiple-choice visual question answering (VQA), and ii) segmenting the\nrelevant cultural artifact as evidence of reasoning. Visual options in the\nfirst stage are systematically organized into three types: those originating\nfrom the same country, those from different countries, or a mixed group.\nNotably, all options are derived from a singular category for each type.\nProgression to the second stage occurs only after a correct visual option is\nchosen. The SCB benchmark comprises 1,065 images that capture 138 cultural\nartifacts across five categories from seven Southeast Asia countries, whose\ndiverse cultures are often overlooked, accompanied by 3,178 questions, of which\n1,093 are unique and meticulously curated by human annotators. Our evaluation\nof various VLMs reveals the complexities involved in cross-modal cultural\nreasoning and highlights the disparity between visual reasoning and spatial\ngrounding in culturally nuanced scenarios. The SCB serves as a crucial\nbenchmark for identifying these shortcomings, thereby guiding future\ndevelopments in the field of cultural reasoning.\nhttps://github.com/buraksatar/SeeingCulture",
        "url": "http://arxiv.org/abs/2509.16517v1",
        "published_date": "2025-09-20T03:47:49+00:00",
        "updated_date": "2025-09-20T03:47:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.MM"
        ],
        "authors": [
            "Burak Satar",
            "Zhixin Ma",
            "Patrick A. Irawan",
            "Wilfried A. Mulyawan",
            "Jing Jiang",
            "Ee-Peng Lim",
            "Chong-Wah Ngo"
        ],
        "ai_categories": [
            "Dataset",
            "Multimodality"
        ],
        "tldr": "The paper introduces a benchmark for visual reasoning and grounding in cultural understanding tasks, focusing on cultural artifacts from Southeast Asia.",
        "tldr_zh": "本文介绍了一个针对文化理解任务中的视觉推理和定位的基准，重点关注东南亚的文化遗产。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SlowFast-SCI: Slow-Fast Deep Unfolding Learning for Spectral Compressive Imaging",
        "summary": "Humans learn in two complementary ways: a slow, cumulative process that\nbuilds broad, general knowledge, and a fast, on-the-fly process that captures\nspecific experiences. Existing deep-unfolding methods for spectral compressive\nimaging (SCI) mirror only the slow component-relying on heavy pre-training with\nmany unfolding stages-yet they lack the rapid adaptation needed to handle new\noptical configurations. As a result, they falter on out-of-distribution\ncameras, especially in bespoke spectral setups unseen during training. This\ndepth also incurs heavy computation and slow inference. To bridge this gap, we\nintroduce SlowFast-SCI, a dual-speed framework seamlessly integrated into any\ndeep unfolding network beyond SCI systems. During slow learning, we pre-train\nor reuse a priors-based backbone and distill it via imaging guidance into a\ncompact fast-unfolding model. In the fast learning stage, lightweight\nadaptation modules are embedded within each block and trained self-supervised\nat test time via a dual-domain loss-without retraining the backbone. To the\nbest of our knowledge, SlowFast-SCI is the first test-time adaptation-driven\ndeep unfolding framework for efficient, self-adaptive spectral reconstruction.\nIts dual-stage design unites offline robustness with on-the-fly per-sample\ncalibration-yielding over 70% reduction in parameters and FLOPs, up to 5.79 dB\nPSNR improvement on out-of-distribution data, preserved cross-domain\nadaptability, and a 4x faster adaptation speed. In addition, its modularity\nintegrates with any deep-unfolding network, paving the way for self-adaptive,\nfield-deployable imaging and expanded computational imaging modalities. Code\nand models are available at https://github.com/XuanLu11/SlowFast-SCI.",
        "url": "http://arxiv.org/abs/2509.16509v1",
        "published_date": "2025-09-20T03:09:06+00:00",
        "updated_date": "2025-09-20T03:09:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haijin Zeng",
            "Xuan Lu",
            "Yurong Zhang",
            "Yongyong Chen",
            "Jingyong Su",
            "Jie Liu"
        ],
        "ai_categories": [
            "Other",
            "Dataset"
        ],
        "tldr": "The paper introduces SlowFast-SCI, a dual-speed framework for spectral compressive imaging that combines slow pre-training with fast adaptation for efficient spectral reconstruction.",
        "tldr_zh": "本文介绍了SlowFast-SCI，一种用于光谱压缩成像的双速框架，结合了缓慢的预训练和快速的适应，实现了高效的光谱重建。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OS-DiffVSR: Towards One-step Latent Diffusion Model for High-detailed Real-world Video Super-Resolution",
        "summary": "Recently, latent diffusion models has demonstrated promising performance in\nreal-world video super-resolution (VSR) task, which can reconstruct\nhigh-quality videos from distorted low-resolution input through multiple\ndiffusion steps. Compared to image super-resolution (ISR), VSR methods needs to\nprocess each frame in a video, which poses challenges to its inference\nefficiency. However, video quality and inference efficiency have always been a\ntrade-off for the diffusion-based VSR methods. In this work, we propose\nOne-Step Diffusion model for real-world Video Super-Resolution, namely\nOS-DiffVSR. Specifically, we devise a novel adjacent frame adversarial training\nparadigm, which can significantly improve the quality of synthetic videos.\nBesides, we devise a multi-frame fusion mechanism to maintain inter-frame\ntemporal consistency and reduce the flicker in video. Extensive experiments on\nseveral popular VSR benchmarks demonstrate that OS-DiffVSR can even achieve\nbetter quality than existing diffusion-based VSR methods that require dozens of\nsampling steps.",
        "url": "http://arxiv.org/abs/2509.16507v1",
        "published_date": "2025-09-20T03:04:41+00:00",
        "updated_date": "2025-09-20T03:04:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hanting Li",
            "Huaao Tang",
            "Jianhong Han",
            "Tianxiong Zhou",
            "Jiulong Cui",
            "Haizhen Xie",
            "Yan Chen",
            "Jie Hu"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces OS-DiffVSR, a one-step latent diffusion model for high-detailed real-world video super-resolution, outperforming existing methods with fewer steps.",
        "tldr_zh": "本文引入了 OS-DiffVSR，这是一个用于高细节真实世界视频超分辨率的一步潜在扩散模型，在减少步骤的同时胜过了现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RLGF: Reinforcement Learning with Geometric Feedback for Autonomous Driving Video Generation",
        "summary": "Synthetic data is crucial for advancing autonomous driving (AD) systems, yet\ncurrent state-of-the-art video generation models, despite their visual realism,\nsuffer from subtle geometric distortions that limit their utility for\ndownstream perception tasks. We identify and quantify this critical issue,\ndemonstrating a significant performance gap in 3D object detection when using\nsynthetic versus real data. To address this, we introduce Reinforcement\nLearning with Geometric Feedback (RLGF), RLGF uniquely refines video diffusion\nmodels by incorporating rewards from specialized latent-space AD perception\nmodels. Its core components include an efficient Latent-Space Windowing\nOptimization technique for targeted feedback during diffusion, and a\nHierarchical Geometric Reward (HGR) system providing multi-level rewards for\npoint-line-plane alignment, and scene occupancy coherence. To quantify these\ndistortions, we propose GeoScores. Applied to models like DiVE on nuScenes,\nRLGF substantially reduces geometric errors (e.g., VP error by 21\\%, Depth\nerror by 57\\%) and dramatically improves 3D object detection mAP by 12.7\\%,\nnarrowing the gap to real-data performance. RLGF offers a plug-and-play\nsolution for generating geometrically sound and reliable synthetic videos for\nAD development.",
        "url": "http://arxiv.org/abs/2509.16500v1",
        "published_date": "2025-09-20T02:23:36+00:00",
        "updated_date": "2025-09-20T02:23:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianyi Yan",
            "Wencheng Han",
            "Xia Zhou",
            "Xueyang Zhang",
            "Kun Zhan",
            "Cheng-zhong Xu",
            "Jianbing Shen"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces RLGF, a reinforcement learning approach that refines video generation models to produce geometrically sound synthetic videos for autonomous driving development.",
        "tldr_zh": "本文介绍了RLGF，一种强化学习方法，用于改进视频生成模型，以生成几何上正确的合成视频，用于自动驾驶开发。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Octree Latent Diffusion for Semantic 3D Scene Generation and Completion",
        "summary": "The completion, extension, and generation of 3D semantic scenes are an\ninterrelated set of capabilities that are useful for robotic navigation and\nexploration. Existing approaches seek to decouple these problems and solve them\noneoff. Additionally, these approaches are often domain-specific, requiring\nseparate models for different data distributions, e.g. indoor vs. outdoor\nscenes. To unify these techniques and provide cross-domain compatibility, we\ndevelop a single framework that can perform scene completion, extension, and\ngeneration in both indoor and outdoor scenes, which we term Octree Latent\nSemantic Diffusion. Our approach operates directly on an efficient dual octree\ngraph latent representation: a hierarchical, sparse, and memory-efficient\noccupancy structure. This technique disentangles synthesis into two stages: (i)\nstructure diffusion, which predicts binary split signals to construct a coarse\noccupancy octree, and (ii) latent semantic diffusion, which generates semantic\nembeddings decoded by a graph VAE into voxellevel semantic labels. To perform\nsemantic scene completion or extension, our model leverages inference-time\nlatent inpainting, or outpainting respectively. These inference-time methods\nuse partial LiDAR scans or maps to condition generation, without the need for\nretraining or finetuning. We demonstrate highquality structure, coherent\nsemantics, and robust completion from single LiDAR scans, as well as zero-shot\ngeneralization to out-of-distribution LiDAR data. These results indicate that\ncompletion-through-generation in a dual octree graph latent space is a\npractical and scalable alternative to regression-based pipelines for real-world\nrobotic perception tasks.",
        "url": "http://arxiv.org/abs/2509.16483v1",
        "published_date": "2025-09-20T00:53:13+00:00",
        "updated_date": "2025-09-20T00:53:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xujia Zhang",
            "Brendan Crowe",
            "Christoffer Heckman"
        ],
        "ai_categories": [
            "LoRA",
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper introduces a framework called Octree Latent Diffusion for 3D scene generation and completion, aiming to unify and improve current methods for indoor and outdoor scenes.",
        "tldr_zh": "本文介绍了一种名为八叉树潜在扩散的框架，用于3D场景生成和完整性，旨在统一和改进当前针对室内和室外场景的方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Cross-Corpus and Cross-domain Handwriting Assessment of NeuroDegenerative Diseases via Time-Series-to-Image Conversion",
        "summary": "Handwriting is significantly affected by neurological disorders (ND) such as\nParkinson's disease (PD) and Alzheimer's disease (AD). Prior works have\nanalyzed handwriting tasks using feature-based approaches or computer-vision\ntechniques, but these methods have struggled to generalize across multiple\ndatasets, particularly between temporal features represented as time-series and\nimages. We propose a framework that leverages both time-series and images of\nhandwriting through a joint classifier, based on a ResNet50 pretrained on\nImageNet-1k. Binary classification experiments demonstrate state-of-the-art\nperformances on existing time-series and image datasets, with significant\nimprovement on specific drawing and writing tasks from the NeuroLogical Signals\n(NLS) dataset. In particular, the proposed model demonstrates improved\nperformance on Draw Clock and Spiral tasks. Additionally, cross-dataset and\nmulti-dataset experiments were consistently able to achieve high F1 scores, up\nto 98 for PD detection, highlighting the potential of the proposed model to\ngeneralize over different forms of handwriting signals, and enhance the\ndetection of motor deficits in ND.",
        "url": "http://arxiv.org/abs/2509.16474v1",
        "published_date": "2025-09-20T00:00:55+00:00",
        "updated_date": "2025-09-20T00:00:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Gabrielle Chavez",
            "Laureano Moro-Velazquez",
            "Ankur Butala",
            "Najim Dehak",
            "Thomas Thebaud"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a framework that combines time-series and image data of handwriting for neurodegenerative disease assessment, achieving state-of-the-art performance on various tasks and datasets.",
        "tldr_zh": "本文提出了一个将手写时间序列和图像数据结合起来用于神经退行性疾病评估的框架，在各种任务和数据集上取得了领先水平的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Explainable Gait Abnormality Detection Using Dual-Dataset CNN-LSTM Models",
        "summary": "Gait is a key indicator in diagnosing movement disorders, but most models\nlack interpretability and rely on single datasets. We propose a dual-branch\nCNN-LSTM framework a 1D branch on joint-based features from GAVD and a 3D\nbranch on silhouettes from OU-MVLP. Interpretability is provided by SHAP\n(temporal attributions) and Grad-CAM (spatial localization).On held-out sets,\nthe system achieves 98.6% accuracy with strong recall and F1. This approach\nadvances explainable gait analysis across both clinical and biometric domains.",
        "url": "http://arxiv.org/abs/2509.16472v1",
        "published_date": "2025-09-19T23:53:45+00:00",
        "updated_date": "2025-09-19T23:53:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Parth Agarwal",
            "Sangaa Chatterjee",
            "Md Faisal Kabir",
            "Suman Saha"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "Proposes a dual-dataset CNN-LSTM model for gait abnormality detection, providing interpretability using temporal attributions and spatial localization. Achieves high accuracy on held-out sets.",
        "tldr_zh": "提出了一种利用双数据集CNN-LSTM模型进行步态异常检测的方法，通过时间属性和空间定位提供解释性。在保留集上取得了很高的准确率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "KRAST: Knowledge-Augmented Robotic Action Recognition with Structured Text for Vision-Language Models",
        "summary": "Accurate vision-based action recognition is crucial for developing autonomous\nrobots that can operate safely and reliably in complex, real-world\nenvironments. In this work, we advance video-based recognition of indoor daily\nactions for robotic perception by leveraging vision-language models (VLMs)\nenriched with domain-specific knowledge. We adapt a prompt-learning framework\nin which class-level textual descriptions of each action are embedded as\nlearnable prompts into a frozen pre-trained VLM backbone. Several strategies\nfor structuring and encoding these textual descriptions are designed and\nevaluated. Experiments on the ETRI-Activity3D dataset demonstrate that our\nmethod, using only RGB video inputs at test time, achieves over 95\\% accuracy\nand outperforms state-of-the-art approaches. These results highlight the\neffectiveness of knowledge-augmented prompts in enabling robust action\nrecognition with minimal supervision.",
        "url": "http://arxiv.org/abs/2509.16452v1",
        "published_date": "2025-09-19T22:12:49+00:00",
        "updated_date": "2025-09-19T22:12:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Son Hai Nguyen",
            "Diwei Wang",
            "Jinhyeok Jang",
            "Hyewon Seo"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes a method for robotic action recognition using vision-language models enriched with domain-specific knowledge, achieving over 95% accuracy on a dataset.",
        "tldr_zh": "该论文提出了一种利用具有领域特定知识的视觉-语言模型进行机器人动作识别的方法，在数据集上实现了超过95%的准确率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Improved mmFormer for Liver Fibrosis Staging via Missing-Modality Compensation",
        "summary": "In real-world clinical settings, magnetic resonance imaging (MRI) frequently\nsuffers from missing modalities due to equipment variability or patient\ncooperation issues, which can significantly affect model performance. To\naddress this issue, we propose a multimodal MRI classification model based on\nthe mmFormer architecture with an adaptive module for handling arbitrary\ncombinations of missing modalities. Specifically, this model retains the hybrid\nmodality-specific encoders and the modality-correlated encoder from mmFormer to\nextract consistent lesion features across available modalities. In addition, we\nintegrate a missing-modality compensation module which leverages zero-padding,\nmodality availability masks, and a Delta Function with learnable statistical\nparameters to dynamically synthesize proxy features for recovering missing\ninformation. To further improve prediction performance, we adopt a\ncross-validation ensemble strategy by training multiple models on different\nfolds and applying soft voting during inference. This method is evaluated on\nthe test set of Comprehensive Analysis & Computing of REal-world medical images\n(CARE) 2025 challenge, targeting the Liver Fibrosis Staging (LiFS) task based\non non-contrast dynamic MRI scans including T1-weighted imaging (T1WI),\nT2-weighted imaging (T2WI), and diffusion-weighted imaging (DWI). For Cirrhosis\nDetection and Substantial Fibrosis Detection on in-distribution vendors, our\nmodel obtains accuracies of 66.67%, and 74.17%, and corresponding area under\nthe curve (AUC) scores of 71.73% and 68.48%, respectively.",
        "url": "http://arxiv.org/abs/2509.16436v1",
        "published_date": "2025-09-19T21:31:05+00:00",
        "updated_date": "2025-09-19T21:31:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhejia Zhang",
            "Junjie Wang",
            "Le Zhang"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a multimodal MRI classification model for liver fibrosis staging that addresses missing modality issues, achieving high accuracies and AUC scores.",
        "tldr_zh": "该论文介绍了一种多模态MRI分类模型，用于肝纤维化分期，解决了缺失模态问题，实现了高准确性和AUC分数。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AHA -- Predicting What Matters Next: Online Highlight Detection Without Looking Ahead",
        "summary": "Real-time understanding of continuous video streams is essential for\nintelligent agents operating in high-stakes environments, including autonomous\nvehicles, surveillance drones, and disaster response robots. Yet, most existing\nvideo understanding and highlight detection methods assume access to the entire\nvideo during inference, making them unsuitable for online or streaming\nscenarios. In particular, current models optimize for offline summarization,\nfailing to support step-by-step reasoning needed for real-time decision-making.\nWe introduce Aha, an autoregressive highlight detection framework that predicts\nthe relevance of each video frame against a task described in natural language.\nWithout accessing future video frames, Aha utilizes a multimodal\nvision-language model and lightweight, decoupled heads trained on a large,\ncurated dataset of human-centric video labels. To enable scalability, we\nintroduce the Dynamic SinkCache mechanism that achieves constant memory usage\nacross infinite-length streams without degrading performance on standard\nbenchmarks. This encourages the hidden representation to capture high-level\ntask objectives, enabling effective frame-level rankings for informativeness,\nrelevance, and uncertainty with respect to the natural language task. Aha\nachieves state-of-the-art (SOTA) performance on highlight detection benchmarks,\nsurpassing even prior offline, full-context approaches and video-language\nmodels by +5.9% on TVSum and +8.3% on Mr.Hisum in mAP (mean Average Precision).\nWe explore Aha's potential for real-world robotics applications given a\ntask-oriented natural language input and a continuous, robot-centric video.\nBoth experiments demonstrate Aha's potential effectiveness as a real-time\nreasoning module for downstream planning and long-horizon understanding.",
        "url": "http://arxiv.org/abs/2509.16421v1",
        "published_date": "2025-09-19T21:03:00+00:00",
        "updated_date": "2025-09-19T21:03:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Aiden Chang",
            "Celso De Melo",
            "Stephanie M. Lukin"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces Aha, an autoregressive framework for real-time highlight detection in videos without looking ahead, achieving state-of-the-art performance on benchmarks and showing potential for real-world robotics applications.",
        "tldr_zh": "本文介绍了Aha，一种自回归框架，用于实时视频中的亮点检测，不预测未来，实现了对基准测试的最先进性能，并展示了在现实世界机器人应用中的潜力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Introducing Resizable Region Packing Problem in Image Generation, with a Heuristic Solution",
        "summary": "The problem of image data generation in computer vision has traditionally\nbeen a harder problem to solve, than discriminative problems. Such data\ngeneration entails placing relevant objects of appropriate sizes each, at\nmeaningful location in a scene canvas. There have been two classes of popular\napproaches to such generation: graphics based, and generative models-based.\nOptimization problems are known to lurk in the background for both these\nclasses of approaches. In this paper, we introduce a novel, practically useful\nmanifestation of the classical Bin Packing problem in the context of generation\nof synthetic image data. We conjecture that the newly introduced problem,\nResizable Anchored Region Packing(RARP) Problem, is NP-hard, and provide\ndetailed arguments about our conjecture. As a first solution, we present a\nnovel heuristic algorithm that is generic enough and therefore scales and packs\narbitrary number of arbitrary-shaped regions at arbitrary locations, into an\nimage canvas. The algorithm follows greedy approach to iteratively pack region\npairs in a careful way, while obeying the optimization constraints. The\nalgorithm is validated by an implementation that was used to generate a\nlarge-scale synthetic anomaly detection dataset, with highly varying degree of\nbin packing parameters per image sample i.e. RARP instance. Visual inspection\nof such data and checking of the correctness of each solution proves the\neffectiveness of our algorithm. With generative modeling being on rise in deep\nlearning, and synthetic data generation poised to become mainstream, we expect\nthat the newly introduced problem will be valued in the imaging scientific\ncommunity.",
        "url": "http://arxiv.org/abs/2509.16363v1",
        "published_date": "2025-09-19T19:11:31+00:00",
        "updated_date": "2025-09-19T19:11:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hrishikesh Sharma"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a novel Resizable Anchored Region Packing (RARP) Problem in image generation and presents a heuristic algorithm for solving it.",
        "tldr_zh": "本文引入了图像生成中的一种新的可调整锚定区域装箱（RARP）问题，并提出了一个启发式算法来解决它。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FitPro: A Zero-Shot Framework for Interactive Text-based Pedestrian Retrieval in Open World",
        "summary": "Text-based Pedestrian Retrieval (TPR) aims to retrieve specific target\npedestrians in visual scenes according to natural language descriptions.\nAlthough existing methods have achieved progress under constrained settings,\ninteractive retrieval in the open-world scenario still suffers from limited\nmodel generalization and insufficient semantic understanding. To address these\nchallenges, we propose FitPro, an open-world interactive zero-shot TPR\nframework with enhanced semantic comprehension and cross-scene adaptability.\nFitPro has three innovative components: Feature Contrastive Decoding (FCD),\nIncremental Semantic Mining (ISM), and Query-aware Hierarchical Retrieval\n(QHR). The FCD integrates prompt-guided contrastive decoding to generate\nhigh-quality structured pedestrian descriptions from denoised images,\neffectively alleviating semantic drift in zero-shot scenarios. The ISM\nconstructs holistic pedestrian representations from multi-view observations to\nachieve global semantic modeling in multi-turn interactions,thereby improving\nrobustness against viewpoint shifts and fine-grained variations in\ndescriptions. The QHR dynamically optimizes the retrieval pipeline according to\nquery types, enabling efficient adaptation to multi-modal and multi-view\ninputs. Extensive experiments on five public datasets and two evaluation\nprotocols demonstrate that FitPro significantly overcomes the generalization\nlimitations and semantic modeling constraints of existing methods in\ninteractive retrieval, paving the way for practical deployment. The code and\ndata will be released at https://github.com/\nlilo4096/FitPro-Interactive-Person-Retrieval.",
        "url": "http://arxiv.org/abs/2509.16674v1",
        "published_date": "2025-09-20T12:55:18+00:00",
        "updated_date": "2025-09-20T12:55:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zengli Luo",
            "Canlong Zhang",
            "Xiaochun Lu",
            "Zhixin Li"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "FitPro is a zero-shot framework for interactive text-based pedestrian retrieval in open-world scenarios, addressing challenges in model generalization and semantic understanding.",
        "tldr_zh": "FitPro是一个用于开放世界场景中的零-shot交互式基于文本的行人检索的框架，解决了模型泛化和语义理解方面的挑战。",
        "relevance_score": 2,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "ADVEDM:Fine-grained Adversarial Attack against VLM-based Embodied Agents",
        "summary": "Vision-Language Models (VLMs), with their strong reasoning and planning\ncapabilities, are widely used in embodied decision-making (EDM) tasks in\nembodied agents, such as autonomous driving and robotic manipulation. Recent\nresearch has increasingly explored adversarial attacks on VLMs to reveal their\nvulnerabilities. However, these attacks either rely on overly strong\nassumptions, requiring full knowledge of the victim VLM, which is impractical\nfor attacking VLM-based agents, or exhibit limited effectiveness. The latter\nstems from disrupting most semantic information in the image, which leads to a\nmisalignment between the perception and the task context defined by system\nprompts. This inconsistency interrupts the VLM's reasoning process, resulting\nin invalid outputs that fail to affect interactions in the physical world. To\nthis end, we propose a fine-grained adversarial attack framework, ADVEDM, which\nmodifies the VLM's perception of only a few key objects while preserving the\nsemantics of the remaining regions. This attack effectively reduces conflicts\nwith the task context, making VLMs output valid but incorrect decisions and\naffecting the actions of agents, thus posing a more substantial safety threat\nin the physical world. We design two variants of based on this framework,\nADVEDM-R and ADVEDM-A, which respectively remove the semantics of a specific\nobject from the image and add the semantics of a new object into the image. The\nexperimental results in both general scenarios and EDM tasks demonstrate\nfine-grained control and excellent attack performance.",
        "url": "http://arxiv.org/abs/2509.16645v1",
        "published_date": "2025-09-20T11:48:11+00:00",
        "updated_date": "2025-09-20T11:48:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yichen Wang",
            "Hangtao Zhang",
            "Hewen Pan",
            "Ziqi Zhou",
            "Xianlong Wang",
            "Peijin Guo",
            "Lulu Xue",
            "Shengshan Hu",
            "Minghui Li",
            "Leo Yu Zhang"
        ],
        "ai_categories": [
            "GAN",
            "AIGC"
        ],
        "tldr": "The paper introduces ADVEDM, a fine-grained adversarial attack framework targeting VLM-based embodied agents to generate valid but incorrect decisions, posing a safety threat in the physical world.",
        "tldr_zh": "本文介绍了ADVEDM，这是一个细粒度对抗攻击框架，针对VLM-based实体代理，可以生成有效但不正确的决策，对物理世界构成安全威胁。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "Unlocking Hidden Potential in Point Cloud Networks with Attention-Guided Grouping-Feature Coordination",
        "summary": "Point cloud analysis has evolved with diverse network architectures, while\nexisting works predominantly focus on introducing novel structural designs.\nHowever, conventional point-based architectures - processing raw points through\nsequential sampling, grouping, and feature extraction layers - demonstrate\nunderutilized potential. We notice that substantial performance gains can be\nunlocked through strategic module integration rather than structural\nmodifications. In this paper, we propose the Grouping-Feature Coordination\nModule (GF-Core), a lightweight separable component that simultaneously\nregulates both grouping layer and feature extraction layer to enable more\nnuanced feature aggregation. Besides, we introduce a self-supervised\npretraining strategy specifically tailored for point-based inputs to enhance\nmodel robustness in complex point cloud analysis scenarios. On ModelNet40\ndataset, our method elevates baseline networks to 94.0% accuracy, matching\nadvanced frameworks' performance while preserving architectural simplicity. On\nthree variants of the ScanObjectNN dataset, we obtain improvements of 2.96%,\n6.34%, and 6.32% respectively.",
        "url": "http://arxiv.org/abs/2509.16639v1",
        "published_date": "2025-09-20T11:33:19+00:00",
        "updated_date": "2025-09-20T11:33:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shangzhuo Xie",
            "Qianqian Yang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a new module for point cloud networks that improves feature aggregation and introduces a self-supervised pretraining strategy, achieving significant performance gains.",
        "tldr_zh": "本文提出了一种新的点云网络模块，改善了特征聚合，并引入了一种自监督预训练策略，取得了显著的性能提升。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Fusing Spectral Correlation Density Imaging with Deep Learning for Intelligent Fault Diagnosis in Rotating Machinery",
        "summary": "Bearing fault diagnosis in rotating machinery is critical for ensuring\noperational reliability, therefore early fault detection is essential to avoid\ncatastrophic failures and expensive emergency repairs. Traditional methods like\nFast Fourier Transform (FFT) often fail to capture the complex, non-stationary\nnature of vibration signals. This study leverages the cyclostationary\nproperties of vibration data through Spectral Correlation Density (SCD) images\nto enhance fault detection and apply deep learning for classification. Using a\npublicly available dataset with bearing faults seeded in two distinct housings\n(A and B) under varying load conditions (0 Nm, 2 Nm, 4 Nm), we processed\nvibration signals into 2D SCD images to reveal fault-specific periodicities,\nsuch as broadband spectra (2000--8000 Hz) for larger faults. Three\nconvolutional neural network (CNN) models, Custom CNN, ResNet152V2, and\nEfficientNetB0, were developed to classify seven bearing conditions. The custom\nCNN achieved the highest accuracies of 96.58\\% and 94.95\\% on Housing A and B,\nrespectively, followed by ResNet152V2 at 96.49\\% and 95.35\\%, and\nEfficientNetB0 at 94.16\\% and 91.65\\%, respectively. The models' high\naccuracies across different housings demonstrate a robust solution suitable for\ncost-effective condition monitoring deployable near sensing platforms,\ncontributing to applied machine learning for edge intelligence and showcasing\neffective signal processing strategies for handling complex, potentially\nlarge-scale vibration data.",
        "url": "http://arxiv.org/abs/2509.16580v1",
        "published_date": "2025-09-20T08:58:08+00:00",
        "updated_date": "2025-09-20T08:58:08+00:00",
        "categories": [
            "eess.SP",
            "cs.CV"
        ],
        "authors": [
            "Dilshara Herath",
            "Chinthaka Abeyrathne",
            "Chamindu Adithya",
            "Chathura Seneviratne"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer",
            "AIGC"
        ],
        "tldr": "The paper proposes a method combining Spectral Correlation Density (SCD) imaging and deep learning for fault diagnosis in rotating machinery, achieving high accuracies in classification.",
        "tldr_zh": "该论文提出了一种结合了谱相关密度成像和深度学习的方法，用于旋转机械的故障诊断，在分类问题上取得了很高的准确率。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "PM25Vision: A Large-Scale Benchmark Dataset for Visual Estimation of Air Quality",
        "summary": "We introduce PM25Vision (PM25V), the largest and most comprehensive dataset\nto date for estimating air quality - specifically PM2.5 concentrations - from\nstreet-level images. The dataset contains over 11,114 images matched with\ntimestamped and geolocated PM2.5 readings across 3,261 AQI monitoring stations\nand 11 years, significantly exceeding the scale of previous benchmarks. The\nspatial accuracy of this dataset has reached 5 kilometers, far exceeding the\ncity-level accuracy of many datasets. We describe the data collection,\nsynchronization, and cleaning pipelines, and provide baseline model\nperformances using CNN and transformer architectures. Our dataset is publicly\navailable.",
        "url": "http://arxiv.org/abs/2509.16519v1",
        "published_date": "2025-09-20T03:51:45+00:00",
        "updated_date": "2025-09-20T03:51:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yang Han"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "PM25Vision introduces a large-scale dataset for estimating air quality from street-level images, surpassing previous benchmarks in scale and accuracy.",
        "tldr_zh": "PM25Vision介绍了一个用于从街头图像中估计空气质量的大规模数据集，超越了以往在规模和准确度上的基准。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "CommonForms: A Large, Diverse Dataset for Form Field Detection",
        "summary": "This paper introduces CommonForms, a web-scale dataset for form field\ndetection. It casts the problem of form field detection as object detection:\ngiven an image of a page, predict the location and type (Text Input, Choice\nButton, Signature) of form fields. The dataset is constructed by filtering\nCommon Crawl to find PDFs that have fillable elements. Starting with 8 million\ndocuments, the filtering process is used to arrive at a final dataset of\nroughly 55k documents that have over 450k pages. Analysis shows that the\ndataset contains a diverse mixture of languages and domains; one third of the\npages are non-English, and among the 14 classified domains, no domain makes up\nmore than 25% of the dataset.\n  In addition, this paper presents a family of form field detectors,\nFFDNet-Small and FFDNet-Large, which attain a very high average precision on\nthe CommonForms test set. Each model cost less than $500 to train. Ablation\nresults show that high-resolution inputs are crucial for high-quality form\nfield detection, and that the cleaning process improves data efficiency over\nusing all PDFs that have fillable fields in Common Crawl. A qualitative\nanalysis shows that they outperform a popular, commercially available PDF\nreader that can prepare forms. Unlike the most popular commercially available\nsolutions, FFDNet can predict checkboxes in addition to text and signature\nfields. This is, to our knowledge, the first large scale dataset released for\nform field detection, as well as the first open source models. The dataset,\nmodels, and code will be released at https://github.com/jbarrow/commonforms",
        "url": "http://arxiv.org/abs/2509.16506v1",
        "published_date": "2025-09-20T02:55:40+00:00",
        "updated_date": "2025-09-20T02:55:40+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Joe Barrow"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces CommonForms, a large dataset for form field detection, along with form field detectors. It shows high precision and outperforms a popular PDF reader in predicting form fields.",
        "tldr_zh": "该论文介绍了CommonForms，一个用于表单字段检测的大型数据集，以及表单字段检测器。它显示出高精度，并在预测表单字段方面优于一款流行的PDF阅读器。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "The Iconicity of the Generated Image",
        "summary": "How humans interpret and produce images is influenced by the images we have\nbeen exposed to. Similarly, visual generative AI models are exposed to many\ntraining images and learn to generate new images based on this. Given the\nimportance of iconic images in human visual communication, as they are widely\nseen, reproduced, and used as inspiration, we may expect that they may\nsimilarly have a proportionally large influence within the generative AI\nprocess. In this work we explore this question through a three-part analysis,\ninvolving data attribution, semantic similarity analysis, and a user-study. Our\nfindings indicate that iconic images do not have an obvious influence on the\ngenerative process, and that for many icons it is challenging to reproduce an\nimage which resembles it closely. This highlights an important difference in\nhow humans and visual generative AI models draw on and learn from prior visual\ncommunication.",
        "url": "http://arxiv.org/abs/2509.16473v1",
        "published_date": "2025-09-19T23:59:43+00:00",
        "updated_date": "2025-09-19T23:59:43+00:00",
        "categories": [
            "cs.CY",
            "cs.CV"
        ],
        "authors": [
            "Nanne van Noord",
            "Noa Garcia"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper investigates the influence of iconic images on the generative AI process and finds that they do not have a significant impact on image generation.",
        "tldr_zh": "本文研究了标志性图像对生成AI过程的影响，并发现它们对图像生成没有显著影响。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7.5
    },
    {
        "title": "TractoTransformer: Diffusion MRI Streamline Tractography using CNN and Transformer Networks",
        "summary": "White matter tractography is an advanced neuroimaging technique that\nreconstructs the 3D white matter pathways of the brain from diffusion MRI data.\nIt can be framed as a pathfinding problem aiming to infer neural fiber\ntrajectories from noisy and ambiguous measurements, facing challenges such as\ncrossing, merging, and fanning white-matter configurations. In this paper, we\npropose a novel tractography method that leverages Transformers to model the\nsequential nature of white matter streamlines, enabling the prediction of fiber\ndirections by integrating both the trajectory context and current diffusion MRI\nmeasurements. To incorporate spatial information, we utilize CNNs that extract\nmicrostructural features from local neighborhoods around each voxel. By\ncombining these complementary sources of information, our approach improves the\nprecision and completeness of neural pathway mapping compared to traditional\ntractography models. We evaluate our method with the Tractometer toolkit,\nachieving competitive performance against state-of-the-art approaches, and\npresent qualitative results on the TractoInferno dataset, demonstrating strong\ngeneralization to real-world data.",
        "url": "http://arxiv.org/abs/2509.16429v1",
        "published_date": "2025-09-19T21:10:13+00:00",
        "updated_date": "2025-09-19T21:10:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Itzik Waizman",
            "Yakov Gusakov",
            "Itay Benou",
            "Tammy Riklin Raviv"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper presents a novel method using Transformer and CNN networks to improve white matter tractography from diffusion MRI data, achieving competitive performance.",
        "tldr_zh": "本文提出了一种利用Transformer和CNN网络改进扩散MRI数据的白质束追踪方法，取得了竞争性的性能。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Agentic Reasoning for Robust Vision Systems via Increased Test-Time Compute",
        "summary": "Developing trustworthy intelligent vision systems for high-stakes domains,\n\\emph{e.g.}, remote sensing and medical diagnosis, demands broad robustness\nwithout costly retraining. We propose \\textbf{Visual Reasoning Agent (VRA)}, a\ntraining-free, agentic reasoning framework that wraps off-the-shelf\nvision-language models \\emph{and} pure vision systems in a\n\\emph{Think--Critique--Act} loop. While VRA incurs significant additional\ntest-time computation, it achieves up to 40\\% absolute accuracy gains on\nchallenging visual reasoning benchmarks. Future work will optimize query\nrouting and early stopping to reduce inference overhead while preserving\nreliability in vision tasks.",
        "url": "http://arxiv.org/abs/2509.16343v1",
        "published_date": "2025-09-19T18:34:08+00:00",
        "updated_date": "2025-09-19T18:34:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MA"
        ],
        "authors": [
            "Chung-En",
            "Yu",
            "Brian Jalaian",
            "Nathaniel D. Bastian"
        ],
        "ai_categories": [
            "AIGC"
        ],
        "tldr": "The paper introduces a training-free, agentic reasoning framework called Visual Reasoning Agent (VRA) for intelligent vision systems that achieves up to 40% accuracy gains on challenging visual reasoning benchmarks.",
        "tldr_zh": "该论文介绍了一种无需训练的理性推理框架Visual Reasoning Agent（VRA），可在具有挑战性的视觉推理基准上实现高达40%的准确率改进。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "HARE: an entity and relation centric evaluation framework for histopathology reports",
        "summary": "Medical domain automated text generation is an active area of research and\ndevelopment; however, evaluating the clinical quality of generated reports\nremains a challenge, especially in instances where domain-specific metrics are\nlacking, e.g. histopathology. We propose HARE (Histopathology Automated Report\nEvaluation), a novel entity and relation centric framework, composed of a\nbenchmark dataset, a named entity recognition (NER) model, a relation\nextraction (RE) model, and a novel metric, which prioritizes clinically\nrelevant content by aligning critical histopathology entities and relations\nbetween reference and generated reports. To develop the HARE benchmark, we\nannotated 813 de-identified clinical diagnostic histopathology reports and 652\nhistopathology reports from The Cancer Genome Atlas (TCGA) with domain-specific\nentities and relations. We fine-tuned GatorTronS, a domain-adapted language\nmodel to develop HARE-NER and HARE-RE which achieved the highest overall\nF1-score (0.915) among the tested models. The proposed HARE metric outperformed\ntraditional metrics including ROUGE and Meteor, as well as radiology metrics\nsuch as RadGraph-XL, with the highest correlation and the best regression to\nexpert evaluations (higher than the second best method, GREEN, a large language\nmodel based radiology report evaluator, by Pearson $r = 0.168$, Spearman $\\rho\n= 0.161$, Kendall $\\tau = 0.123$, $R^2 = 0.176$, $RMSE = 0.018$). We release\nHARE, datasets, and the models at https://github.com/knowlab/HARE to foster\nadvancements in histopathology report generation, providing a robust framework\nfor improving the quality of reports.",
        "url": "http://arxiv.org/abs/2509.16326v1",
        "published_date": "2025-09-19T18:12:19+00:00",
        "updated_date": "2025-09-19T18:12:19+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Yunsoo Kim",
            "Michal W. S. Ong",
            "Alex Shavick",
            "Honghan Wu",
            "Adam P. Levine"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces HARE, a framework for evaluating histopathology reports, using named entity recognition and relation extraction models to align critical entities and relations between reference and generated reports.",
        "tldr_zh": "本文介绍了HARE，一个用于评估组织病理学报告的框架，使用命名实体识别和关系抽取模型来对齐参考和生成的报告中的关键实体和关系。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Min: Mixture of Noise for Pre-Trained Model-Based Class-Incremental Learning",
        "summary": "Class Incremental Learning (CIL) aims to continuously learn new categories\nwhile retaining the knowledge of old ones. Pre-trained models (PTMs) show\npromising capabilities in CIL. However, existing approaches that apply\nlightweight fine-tuning to backbones still induce parameter drift, thereby\ncompromising the generalization capability of pre-trained models. Parameter\ndrift can be conceptualized as a form of noise that obscures critical patterns\nlearned for previous tasks. However, recent researches have shown that noise is\nnot always harmful. For example, the large number of visual patterns learned\nfrom pre-training can be easily abused by a single task, and introducing\nappropriate noise can suppress some low-correlation features, thus leaving a\nmargin for future tasks. To this end, we propose learning beneficial noise for\nCIL guided by information theory and propose Mixture of Noise (Min), aiming to\nmitigate the degradation of backbone generalization from adapting new tasks.\nSpecifically, task-specific noise is learned from high-dimension features of\nnew tasks. Then, a set of weights is adjusted dynamically for optimal mixture\nof different task noise. Finally, Min embeds the beneficial noise into the\nintermediate features to mask the response of inefficient patterns. Extensive\nexperiments on six benchmark datasets demonstrate that Min achieves\nstate-of-the-art performance in most incremental settings, with particularly\noutstanding results in 50-steps incremental settings. This shows the\nsignificant potential for beneficial noise in continual learning.",
        "url": "http://arxiv.org/abs/2509.16738v1",
        "published_date": "2025-09-20T16:07:20+00:00",
        "updated_date": "2025-09-20T16:07:20+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Kai Jiang",
            "Zhengyan Shi",
            "Dell Zhang",
            "Hongyuan Zhang",
            "Xuelong Li"
        ],
        "ai_categories": [
            "AIGC"
        ],
        "tldr": "Proposes Min, a method using noise to improve pre-trained model-based class-incremental learning, showing state-of-the-art performance in incremental settings.",
        "tldr_zh": "提出 Min 方法，使用噪声来改进基于预训练模型的类增量学习，在增量设置中表现出最先进的性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Text-Scene: A Scene-to-Language Parsing Framework for 3D Scene Understanding",
        "summary": "Enabling agents to understand and interact with complex 3D scenes is a\nfundamental challenge for embodied artificial intelligence systems. While\nMultimodal Large Language Models (MLLMs) have achieved significant progress in\n2D image understanding, extending such capabilities to 3D scenes remains\ndifficult: 1) 3D environment involves richer concepts such as spatial\nrelationships, affordances, physics, layout, and so on, 2) the absence of\nlarge-scale 3D vision-language datasets has posed a significant obstacle. In\nthis paper, we introduce Text-Scene, a framework that automatically parses 3D\nscenes into textual descriptions for scene understanding. Given a 3D scene, our\nmodel identifies object attributes and spatial relationships, and then\ngenerates a coherent summary of the whole scene, bridging the gap between 3D\nobservation and language without requiring human-in-the-loop intervention. By\nleveraging both geometric analysis and MLLMs, Text-Scene produces descriptions\nthat are accurate, detailed, and human-interpretable, capturing object-level\ndetails and global-level context. Experimental results on benchmarks\ndemonstrate that our textual parses can faithfully represent 3D scenes and\nbenefit downstream tasks. To evaluate the reasoning capability of MLLMs, we\npresent InPlan3D, a comprehensive benchmark for 3D task planning, consisting of\n3174 long-term planning tasks across 636 indoor scenes. We emphasize clarity\nand accessibility in our approach, aiming to make 3D scene content\nunderstandable through language. Code and datasets will be released.",
        "url": "http://arxiv.org/abs/2509.16721v1",
        "published_date": "2025-09-20T15:10:45+00:00",
        "updated_date": "2025-09-20T15:10:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Haoyuan Li",
            "Rui Liu",
            "Hehe Fan",
            "Yi Yang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces Text-Scene, a framework that automatically parses 3D scenes into textual descriptions, bridging the gap between 3D observation and language.",
        "tldr_zh": "本文介绍了Text-Scene，这是一个将3D场景自动解析为文本描述的框架，弥合了3D观察和语言之间的差距。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Spectral Compressive Imaging via Chromaticity-Intensity Decomposition",
        "summary": "In coded aperture snapshot spectral imaging (CASSI), the captured measurement\nentangles spatial and spectral information, posing a severely ill-posed inverse\nproblem for hyperspectral images (HSIs) reconstruction. Moreover, the captured\nradiance inherently depends on scene illumination, making it difficult to\nrecover the intrinsic spectral reflectance that remains invariant to lighting\nconditions. To address these challenges, we propose a chromaticity-intensity\ndecomposition framework, which disentangles an HSI into a spatially smooth\nintensity map and a spectrally variant chromaticity cube. The chromaticity\nencodes lighting-invariant reflectance, enriched with high-frequency spatial\ndetails and local spectral sparsity. Building on this decomposition, we develop\nCIDNet, a Chromaticity-Intensity Decomposition unfolding network within a\ndual-camera CASSI system. CIDNet integrates a hybrid spatial-spectral\nTransformer tailored to reconstruct fine-grained and sparse spectral\nchromaticity and a degradation-aware, spatially-adaptive noise estimation\nmodule that captures anisotropic noise across iterative stages. Extensive\nexperiments on both synthetic and real-world CASSI datasets demonstrate that\nour method achieves superior performance in both spectral and chromaticity\nfidelity. Code and models will be publicly available.",
        "url": "http://arxiv.org/abs/2509.16690v1",
        "published_date": "2025-09-20T13:37:14+00:00",
        "updated_date": "2025-09-20T13:37:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaodong Wang",
            "Zijun He",
            "Ping Wang",
            "Lishun Wang",
            "Yanan Hu",
            "Xin Yuan"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a framework for disentangling hyperspectral images for better reconstruction, achieving superior performance in both spectral and chromaticity fidelity.",
        "tldr_zh": "本文提出了一个框架，用于解开高光谱图像，以实现更好的重建，优异的光谱和色度保真度。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "ProtoVQA: An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering",
        "summary": "Visual Question Answering (VQA) is increasingly used in diverse applications\nranging from general visual reasoning to safety-critical domains such as\nmedical imaging and autonomous systems, where models must provide not only\naccurate answers but also explanations that humans can easily understand and\nverify. Prototype-based modeling has shown promise for interpretability by\ngrounding predictions in semantically meaningful regions for purely visual\nreasoning tasks, yet remains underexplored in the context of VQA. We present\nProtoVQA, a unified prototypical framework that (i) learns question-aware\nprototypes that serve as reasoning anchors, connecting answers to\ndiscriminative image regions, (ii) applies spatially constrained matching to\nensure that the selected evidence is coherent and semantically relevant, and\n(iii) supports both answering and grounding tasks through a shared prototype\nbackbone. To assess explanation quality, we propose the Visual-Linguistic\nAlignment Score (VLAS), which measures how well the model's attended regions\nalign with ground-truth evidence. Experiments on Visual7W show that ProtoVQA\nyields faithful, fine-grained explanations while maintaining competitive\naccuracy, advancing the development of transparent and trustworthy VQA systems.",
        "url": "http://arxiv.org/abs/2509.16680v1",
        "published_date": "2025-09-20T13:12:08+00:00",
        "updated_date": "2025-09-20T13:12:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Xingjian Diao",
            "Weiyi Wu",
            "Keyi Kong",
            "Peijun Qing",
            "Xinwen Xu",
            "Ming Cheng",
            "Soroush Vosoughi",
            "Jiang Gui"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer"
        ],
        "tldr": "The paper introduces ProtoVQA, a framework for Visual Question Answering that focuses on providing both accurate answers and explainable reasoning by learning question-aware prototypes that connect answers to image regions.",
        "tldr_zh": "本文介绍了ProtoVQA，一个用于视觉问答的框架，其重点是通过学习问题感知原型来连接答案和图像区域，提供准确答案和可解释推理。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Detection and Simulation of Urban Heat Islands Using a Fine-Tuned Geospatial Foundation Model",
        "summary": "As urbanization and climate change progress, urban heat island effects are\nbecoming more frequent and severe. To formulate effective mitigation plans,\ncities require detailed air temperature data. However, predictive analytics\nmethods based on conventional machine learning models and limited data\ninfrastructure often provide inaccurate predictions, especially in underserved\nareas. In this context, geospatial foundation models trained on unstructured\nglobal data demonstrate strong generalization and require minimal fine-tuning,\noffering an alternative for predictions where traditional approaches are\nlimited. This study fine-tunes a geospatial foundation model to predict urban\nland surface temperatures under future climate scenarios and explores its\nresponse to land cover changes using simulated vegetation strategies. The\nfine-tuned model achieved pixel-wise downscaling errors below 1.74 {\\deg}C and\naligned with ground truth patterns, demonstrating an extrapolation capacity up\nto 3.62 {\\deg}C.",
        "url": "http://arxiv.org/abs/2509.16617v1",
        "published_date": "2025-09-20T10:41:33+00:00",
        "updated_date": "2025-09-20T10:41:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "I.2.6; I.5.4; I.6.8"
        ],
        "authors": [
            "David Kreismann"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper focuses on using geospatial models to predict urban heat island effects and simulate land surface temperature changes under future climate scenarios.",
        "tldr_zh": "本文关注使用地理空间模型来预测城市热岛效应，并在未来气候情景下模拟土地表面温度变化。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Thermal Imaging-based Real-time Fall Detection using Motion Flow and Attention-enhanced Convolutional Recurrent Architecture",
        "summary": "Falls among seniors are a major public health issue. Existing solutions using\nwearable sensors, ambient sensors, and RGB-based vision systems face challenges\nin reliability, user compliance, and practicality. Studies indicate that\nstakeholders, such as older adults and eldercare facilities, prefer\nnon-wearable, passive, privacy-preserving, and real-time fall detection systems\nthat require no user interaction. This study proposes an advanced thermal fall\ndetection method using a Bidirectional Convolutional Long Short-Term Memory\n(BiConvLSTM) model, enhanced with spatial, temporal, feature, self, and general\nattention mechanisms. Through systematic experimentation across hundreds of\nmodel variations exploring the integration of attention mechanisms, recurrent\nmodules, and motion flow, we identified top-performing architectures. Among\nthem, BiConvLSTM achieved state-of-the-art performance with a ROC-AUC of\n$99.7\\%$ on the TSF dataset and demonstrated robust results on TF-66, a newly\nemerged, diverse, and privacy-preserving benchmark. These results highlight the\ngeneralizability and practicality of the proposed model, setting new standards\nfor thermal fall detection and paving the way toward deployable,\nhigh-performance solutions.",
        "url": "http://arxiv.org/abs/2509.16479v1",
        "published_date": "2025-09-20T00:29:43+00:00",
        "updated_date": "2025-09-20T00:29:43+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Christopher Silver",
            "Thangarajah Akilan"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper proposes a thermal imaging-based real-time fall detection system for seniors using advanced attention-enhanced architecture, achieving high performance on benchmark datasets.",
        "tldr_zh": "本文提出了一个基于热成像的实时跌倒检测系统，使用先进的增强型注意力架构，在基准数据集上取得了高性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "LenslessMic: Audio Encryption and Authentication via Lensless Computational Imaging",
        "summary": "With society's increasing reliance on digital data sharing, the protection of\nsensitive information has become critical. Encryption serves as one of the\nprivacy-preserving methods; however, its realization in the audio domain\npredominantly relies on signal processing or software methods embedded into\nhardware. In this paper, we introduce LenslessMic, a hybrid optical\nhardware-based encryption method that utilizes a lensless camera as a physical\nlayer of security applicable to multiple types of audio. We show that\nLenslessMic enables (1) robust authentication of audio recordings and (2)\nencryption strength that can rival the search space of 256-bit digital\nstandards, while maintaining high-quality signals and minimal loss of content\ninformation. The approach is validated with a low-cost Raspberry Pi prototype\nand is open-sourced together with datasets to facilitate research in the area.",
        "url": "http://arxiv.org/abs/2509.16418v1",
        "published_date": "2025-09-19T20:59:30+00:00",
        "updated_date": "2025-09-19T20:59:30+00:00",
        "categories": [
            "cs.CR",
            "cs.AI",
            "cs.CV",
            "cs.SD",
            "eess.AS"
        ],
        "authors": [
            "Petr Grinberg",
            "Eric Bezzam",
            "Paolo Prandoni",
            "Martin Vetterli"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces LenslessMic, a hardware-based audio encryption method using a lensless camera for robust authentication and encryption comparable to 256-bit digital standards.",
        "tldr_zh": "本文介绍了LenslessMic，一种利用无透镜相机的硬件音频加密方法，可用于强大的认证和与256位数字标准相媲美的加密。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "StereoAdapter: Adapting Stereo Depth Estimation to Underwater Scenes",
        "summary": "Underwater stereo depth estimation provides accurate 3D geometry for robotics\ntasks such as navigation, inspection, and mapping, offering metric depth from\nlow-cost passive cameras while avoiding the scale ambiguity of monocular\nmethods. However, existing approaches face two critical challenges: (i)\nparameter-efficiently adapting large vision foundation encoders to the\nunderwater domain without extensive labeled data, and (ii) tightly fusing\nglobally coherent but scale-ambiguous monocular priors with locally metric yet\nphotometrically fragile stereo correspondences. To address these challenges, we\npropose StereoAdapter, a parameter-efficient self-supervised framework that\nintegrates a LoRA-adapted monocular foundation encoder with a recurrent stereo\nrefinement module. We further introduce dynamic LoRA adaptation for efficient\nrank selection and pre-training on the synthetic UW-StereoDepth-40K dataset to\nenhance robustness under diverse underwater conditions. Comprehensive\nevaluations on both simulated and real-world benchmarks show improvements of\n6.11% on TartanAir and 5.12% on SQUID compared to state-of-the-art methods,\nwhile real-world deployment with the BlueROV2 robot further demonstrates the\nconsistent robustness of our approach. Code:\nhttps://github.com/AIGeeksGroup/StereoAdapter. Website:\nhttps://aigeeksgroup.github.io/StereoAdapter.",
        "url": "http://arxiv.org/abs/2509.16415v1",
        "published_date": "2025-09-19T20:57:03+00:00",
        "updated_date": "2025-09-19T20:57:03+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Zhengri Wu",
            "Yiran Wang",
            "Yu Wen",
            "Zeyu Zhang",
            "Biao Wu",
            "Hao Tang"
        ],
        "ai_categories": [
            "AIGC",
            "LoRA",
            "Dataset"
        ],
        "tldr": "The paper introduces StereoAdapter, a framework for adapting stereo depth estimation to underwater scenes, showing significant improvements over existing methods both in simulation and real-world deployment.",
        "tldr_zh": "本文介绍了StereoAdapter，这是一个用于将立体深度估计适应水下场景的框架，在模拟和实际部署中都比现有方法表现出显著的改进。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "CoUn: Empowering Machine Unlearning via Contrastive Learning",
        "summary": "Machine unlearning (MU) aims to remove the influence of specific \"forget\"\ndata from a trained model while preserving its knowledge of the remaining\n\"retain\" data. Existing MU methods based on label manipulation or model weight\nperturbations often achieve limited unlearning effectiveness. To address this,\nwe introduce CoUn, a novel MU framework inspired by the observation that a\nmodel retrained from scratch using only retain data classifies forget data\nbased on their semantic similarity to the retain data. CoUn emulates this\nbehavior by adjusting learned data representations through contrastive learning\n(CL) and supervised learning, applied exclusively to retain data. Specifically,\nCoUn (1) leverages semantic similarity between data samples to indirectly\nadjust forget representations using CL, and (2) maintains retain\nrepresentations within their respective clusters through supervised learning.\nExtensive experiments across various datasets and model architectures show that\nCoUn consistently outperforms state-of-the-art MU baselines in unlearning\neffectiveness. Additionally, integrating our CL module into existing baselines\nempowers their unlearning effectiveness.",
        "url": "http://arxiv.org/abs/2509.16391v1",
        "published_date": "2025-09-19T20:12:49+00:00",
        "updated_date": "2025-09-19T20:12:49+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Yasser H. Khalil",
            "Mehdi Setayesh",
            "Hongliang Li"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "CoUn is a novel Machine Unlearning framework that outperforms existing methods by adjusting learned data representations through contrastive learning and supervised learning on retain data.",
        "tldr_zh": "CoUn是一种新颖的机器遗忘框架，通过对保留数据进行对比学习和监督学习来调整学习数据表示，优于现有方法。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "From Canopy to Ground via ForestGen3D: Learning Cross-Domain Generation of 3D Forest Structure from Aerial-to-Terrestrial LiDAR",
        "summary": "The 3D structure of living and non-living components in ecosystems plays a\ncritical role in determining ecological processes and feedbacks from both\nnatural and human-driven disturbances. Anticipating the effects of wildfire,\ndrought, disease, or atmospheric deposition depends on accurate\ncharacterization of 3D vegetation structure, yet widespread measurement remains\nprohibitively expensive and often infeasible. We introduce ForestGen3D, a novel\ngenerative modeling framework that synthesizes high-fidelity 3D forest\nstructure using only aerial LiDAR (ALS) inputs. ForestGen3D is based on\nconditional denoising diffusion probabilistic models (DDPMs) trained on\nco-registered ALS/TLS (terrestrial LiDAR) data. The model learns to generate\nTLS-like 3D point clouds conditioned on sparse ALS observations, effectively\nreconstructing occluded sub-canopy detail at scale. To ensure ecological\nplausibility, we introduce a geometric containment prior based on the convex\nhull of ALS observations and provide theoretical and empirical guarantees that\ngenerated structures remain spatially consistent. We evaluate ForestGen3D at\ntree, plot, and landscape scales using real-world data from mixed conifer\necosystems, and show that it produces high-fidelity reconstructions that\nclosely match TLS references in terms of geometric similarity and biophysical\nmetrics, such as tree height, DBH, crown diameter and crown volume.\nAdditionally, we demonstrate that the containment property can serve as a\npractical proxy for generation quality in settings where TLS ground truth is\nunavailable. Our results position ForestGen3D as a scalable tool for ecological\nmodeling, wildfire simulation, and structural fuel characterization in ALS-only\nenvironments.",
        "url": "http://arxiv.org/abs/2509.16346v1",
        "published_date": "2025-09-19T18:39:50+00:00",
        "updated_date": "2025-09-19T18:39:50+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Juan Castorena",
            "E. Louise Loudermilk",
            "Scott Pokswinski",
            "Rodman Linn"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "ForestGen3D is a generative modeling framework that can synthesize high-fidelity 3D forest structures using only aerial LiDAR inputs, showing promise for ecological modeling and wildfire simulation in ALS-only environments.",
        "tldr_zh": "ForestGen3D是一个生成建模框架，可以使用仅航空LiDAR输入合成高保真度3D森林结构，具有在仅限于ALS环境中的生态建模和野火模拟方面的潜力。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "CAMBench-QR : A Structure-Aware Benchmark for Post-Hoc Explanations with QR Understanding",
        "summary": "Visual explanations are often plausible but not structurally faithful. We\nintroduce CAMBench-QR, a structure-aware benchmark that leverages the canonical\ngeometry of QR codes (finder patterns, timing lines, module grid) to test\nwhether CAM methods place saliency on requisite substructures while avoiding\nbackground. CAMBench-QR synthesizes QR/non-QR data with exact masks and\ncontrolled distortions, and reports structure-aware metrics (Finder/Timing Mass\nRatios, Background Leakage, coverage AUCs, Distance-to-Structure) alongside\ncausal occlusion, insertion/deletion faithfulness, robustness, and latency. We\nbenchmark representative, efficient CAMs (LayerCAM, EigenGrad-CAM, XGrad-CAM)\nunder two practical regimes of zero-shot and last-block fine-tuning. The\nbenchmark, metrics, and training recipes provide a simple, reproducible\nyardstick for structure-aware evaluation of visual explanations. Hence we\npropose that CAMBENCH-QR can be used as a litmus test of whether visual\nexplanations are truly structure-aware.",
        "url": "http://arxiv.org/abs/2509.16745v1",
        "published_date": "2025-09-20T17:13:38+00:00",
        "updated_date": "2025-09-20T17:13:38+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ritabrata Chakraborty",
            "Avijit Dasgupta",
            "Sandeep Chaurasia"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "CAMBench-QR introduces a structure-aware benchmark using QR codes to evaluate the fidelity of visual explanations, providing metrics for evaluation and training recipes for assessing structure-awareness.",
        "tldr_zh": "CAMBench-QR利用QR码引入结构感知基准，评估视觉解释的逼真程度，提供评估指标和训练配方来评估结构感知。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "When Confidence Fails: Revisiting Pseudo-Label Selection in Semi-supervised Semantic Segmentation",
        "summary": "While significant advances exist in pseudo-label generation for\nsemi-supervised semantic segmentation, pseudo-label selection remains\nunderstudied. Existing methods typically use fixed confidence thresholds to\nretain high-confidence predictions as pseudo-labels. However, these methods\ncannot cope with network overconfidence tendency, where correct and incorrect\npredictions overlap significantly in high-confidence regions, making separation\nchallenging and amplifying model cognitive bias. Meanwhile, the direct\ndiscarding of low-confidence predictions disrupts spatial-semantic continuity,\ncausing critical context loss. We propose Confidence Separable Learning (CSL)\nto address these limitations. CSL formulates pseudo-label selection as a convex\noptimization problem within the confidence distribution feature space,\nestablishing sample-specific decision boundaries to distinguish reliable from\nunreliable predictions. Additionally, CSL introduces random masking of reliable\npixels to guide the network in learning contextual relationships from\nlow-reliability regions, thereby mitigating the adverse effects of discarding\nuncertain predictions. Extensive experimental results on the Pascal,\nCityscapes, and COCO benchmarks show that CSL performs favorably against\nstate-of-the-art methods. Code and model weights are available at\nhttps://github.com/PanLiuCSU/CSL.",
        "url": "http://arxiv.org/abs/2509.16704v1",
        "published_date": "2025-09-20T14:23:09+00:00",
        "updated_date": "2025-09-20T14:23:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pan Liu",
            "Jinshi Liu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces Confidence Separable Learning (CSL) to improve pseudo-label selection in semi-supervised semantic segmentation, outperforming existing methods on benchmark datasets.",
        "tldr_zh": "本论文提出置信度可分学习（CSL），用于改进半监督语义分割的伪标签选择，优于现有方法在基准数据集上的表现。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Segment-to-Act: Label-Noise-Robust Action-Prompted Video Segmentation Towards Embodied Intelligence",
        "summary": "Embodied intelligence relies on accurately segmenting objects actively\ninvolved in interactions. Action-based video object segmentation addresses this\nby linking segmentation with action semantics, but it depends on large-scale\nannotations and prompts that are costly, inconsistent, and prone to multimodal\nnoise such as imprecise masks and referential ambiguity. To date, this\nchallenge remains unexplored. In this work, we take the first step by studying\naction-based video object segmentation under label noise, focusing on two\nsources: textual prompt noise (category flips and within-category noun\nsubstitutions) and mask annotation noise (perturbed object boundaries to mimic\nimprecise supervision). Our contributions are threefold. First, we introduce\ntwo types of label noises for the action-based video object segmentation task.\nSecond, we build up the first action-based video object segmentation under a\nlabel noise benchmark ActiSeg-NL and adapt six label-noise learning strategies\nto this setting, and establish protocols for evaluating them under textual,\nboundary, and mixed noise. Third, we provide a comprehensive analysis linking\nnoise types to failure modes and robustness gains, and we introduce a Parallel\nMask Head Mechanism (PMHM) to address mask annotation noise. Qualitative\nevaluations further reveal characteristic failure modes, including boundary\nleakage and mislocalization under boundary perturbations, as well as occasional\nidentity substitutions under textual flips. Our comparative analysis reveals\nthat different learning strategies exhibit distinct robustness profiles,\ngoverned by a foreground-background trade-off where some achieve balanced\nperformance while others prioritize foreground accuracy at the cost of\nbackground precision. The established benchmark and source code will be made\npublicly available at https://github.com/mylwx/ActiSeg-NL.",
        "url": "http://arxiv.org/abs/2509.16677v1",
        "published_date": "2025-09-20T13:03:43+00:00",
        "updated_date": "2025-09-20T13:03:43+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.RO",
            "eess.IV"
        ],
        "authors": [
            "Wenxin Li",
            "Kunyu Peng",
            "Di Wen",
            "Ruiping Liu",
            "Mengfei Duan",
            "Kai Luo",
            "Kailun Yang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a benchmark for action-based video object segmentation under label noise, exploring different noise types and learning strategies to address the challenges.",
        "tldr_zh": "本文介绍了一个针对标签噪声的基准，探讨了不同噪声类型和学习策略以解决行动驱动的视频目标分割任务中的挑战。",
        "relevance_score": 2,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Towards Anytime Retrieval: A Benchmark for Anytime Person Re-Identification",
        "summary": "In real applications, person re-identification (ReID) is expected to retrieve\nthe target person at any time, including both daytime and nighttime, ranging\nfrom short-term to long-term. However, existing ReID tasks and datasets can not\nmeet this requirement, as they are constrained by available time and only\nprovide training and evaluation for specific scenarios. Therefore, we\ninvestigate a new task called Anytime Person Re-identification (AT-ReID), which\naims to achieve effective retrieval in multiple scenarios based on variations\nin time. To address the AT-ReID problem, we collect the first large-scale\ndataset, AT-USTC, which contains 403k images of individuals wearing multiple\nclothes captured by RGB and IR cameras. Our data collection spans 21 months,\nand 270 volunteers were photographed on average 29.1 times across different\ndates or scenes, 4-15 times more than current datasets, providing conditions\nfor follow-up investigations in AT-ReID. Further, to tackle the new challenge\nof multi-scenario retrieval, we propose a unified model named Uni-AT, which\ncomprises a multi-scenario ReID (MS-ReID) framework for scenario-specific\nfeatures learning, a Mixture-of-Attribute-Experts (MoAE) module to alleviate\ninter-scenario interference, and a Hierarchical Dynamic Weighting (HDW)\nstrategy to ensure balanced training across all scenarios. Extensive\nexperiments show that our model leads to satisfactory results and exhibits\nexcellent generalization to all scenarios.",
        "url": "http://arxiv.org/abs/2509.16635v1",
        "published_date": "2025-09-20T11:20:22+00:00",
        "updated_date": "2025-09-20T11:20:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xulin Li",
            "Yan Lu",
            "Bin Liu",
            "Jiaze Li",
            "Qinhong Yang",
            "Tao Gong",
            "Qi Chu",
            "Mang Ye",
            "Nenghai Yu"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces the Anytime Person Re-identification task and dataset to enable effective person retrieval in various scenarios based on time variations.",
        "tldr_zh": "该论文介绍了任意时间人员再识别任务和数据集，以便根据时间变化实现各种情景下的有效人员检索。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "CGTGait: Collaborative Graph and Transformer for Gait Emotion Recognition",
        "summary": "Skeleton-based gait emotion recognition has received significant attention\ndue to its wide-ranging applications. However, existing methods primarily focus\non extracting spatial and local temporal motion information, failing to capture\nlong-range temporal representations. In this paper, we propose\n\\textbf{CGTGait}, a novel framework that collaboratively integrates graph\nconvolution and transformers to extract discriminative spatiotemporal features\nfor gait emotion recognition. Specifically, CGTGait consists of multiple CGT\nblocks, where each block employs graph convolution to capture frame-level\nspatial topology and the transformer to model global temporal dependencies.\nAdditionally, we introduce a Bidirectional Cross-Stream Fusion (BCSF) module to\neffectively aggregate posture and motion spatiotemporal features, facilitating\nthe exchange of complementary information between the two streams. We evaluate\nour method on two widely used datasets, Emotion-Gait and ELMD, demonstrating\nthat our CGTGait achieves state-of-the-art or at least competitive performance\nwhile reducing computational complexity by approximately \\textbf{82.2\\%} (only\nrequiring 0.34G FLOPs) during testing. Code is available at\n\\small{https://github.com/githubzjj1/CGTGait.}",
        "url": "http://arxiv.org/abs/2509.16623v1",
        "published_date": "2025-09-20T10:48:51+00:00",
        "updated_date": "2025-09-20T10:48:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junjie Zhou",
            "Haijun Xiong",
            "Junhao Lu",
            "Ziyu Lin",
            "Bin Feng"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes CGTGait, a framework combining graph convolution and transformers for gait emotion recognition, achieving state-of-the-art performance with reduced computational complexity.",
        "tldr_zh": "本文提出了CGTGait，一种结合了图卷积和transformers的框架，用于步态情绪识别，在减少计算复杂度的同时实现了最先进的性能。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7.0
    },
    {
        "title": "Eye Gaze Tells You Where to Compute: Gaze-Driven Efficient VLMs",
        "summary": "Vision-Language Models (VLMs) deliver impressive performance in understanding\nvisual content with language instructions. However, redundancy in vision tokens\nresults in the degenerated inference efficiency of VLMs, which hinders\nreal-time use on edge consumer devices such as AR/VR devices. Existing\nefficiency methods commonly prune visual tokens using learned saliency, sparse\nattention schedules, or controller policies, but they often require\narchitectural modification or access to intermediate activations. These\npipelines add inference-time modules that increase compute and memory and often\nlead to an accuracy trade-off. Moreover, they also suffer from misalignment\nbetween the prompts and the region of interest in the images. Without human\nguidance, the model may focus on the wrong regions and miss small,\nhigh-frequency details when prompts or scenes change. In this paper, we propose\nGazeVLM, a training-free framework that uses the human eye gaze as a natural\nsupervisory signal to allocate computation where it matters. By extracting\ngaze-driven regions of interest (ROIs) and optionally combining them with a\nlow-resolution global view, GazeVLM mimics fovea-periphery perception to cut\nredundant visual tokens while preserving task-relevant details. We evaluate the\nvisual question answering tasks on Qwen2.5-VL-3B/7B on the VOILA-COCO benchmark\nwith human gaze. Quality of the answer is assessed by GPT-4o pairwise judging\nand a weighted score over coverage, accuracy, details, and fluency. Efficiency\nis measured by token counts and FLOPs. GazeVLM reduces visual tokens by up to\n93.1%, total tokens by up to 59.6%, and FLOPs by 50%, while keeping better\nanswer quality relative to full-resolution baselines. Our results show that\naligning model computation with human gaze offers a simple, plug-and-play path\ntoward efficient VLM inference on consumer devices.",
        "url": "http://arxiv.org/abs/2509.16476v1",
        "published_date": "2025-09-20T00:16:48+00:00",
        "updated_date": "2025-09-20T00:16:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qinyu Chen",
            "Jiawen Qi"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "AIGC"
        ],
        "tldr": "GazeVLM is a framework that uses human eye gaze to improve efficiency of Vision-Language Models by reducing visual tokens while preserving task-relevant details, leading to better answer quality and reduced computation costs.",
        "tldr_zh": "GazeVLM是一个框架，利用人眼注视来提高视觉语言模型的效率，通过减少视觉令牌同时保留任务相关细节，从而提高答案质量并降低计算成本。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "From Coated to Uncoated: Scanning Electron Microscopy Corrections to Estimate True Surface Pore Size in Nanoporous Membranes",
        "summary": "Scanning electron microscopy (SEM) is the premier method for characterizing\nthe nanoscale surface pores in ultrafiltration (UF) membranes and the support\nlayers of reverse osmosis (RO) membranes. Based on SEM, the conventional\nunderstanding is that membranes typically have low surface porosities of <10%.\nWe hypothesized that high acceleration voltage during SEM imaging and sputter\nmetal coatings required for SEM have led to systematic underestimations of\nporosity and pore size. We showed that imaging a commercial UF membrane at 1,\n5, and 10 kV reduced measured porosity from 10.3% (1 kV) to 6.3% (10 kV), while\nincreasing Pt coating thickness from 1.5 to 5 nm lowered porosity by 54% for\nthe UF membrane (12.9% to 5.8%) and 46% for an RO support (13.1% to 7.0%). To\naccount for coating thickness, we developed a digital correction method that\nsimulates pore dilation, enabling the pore structure to be estimated for\nuncoated membranes. Dilation yielded uncoated porosity values of 23% for the UF\nmembrane and 20% for the RO support, about 3-fold greater than values observed\nwith a 4 nm coating. Mean pore diameters were 2-fold greater for the UF\nmembrane and 1.5-fold greater for the RO support. Critically, dilation-derived\npore-size distributions agreed with low-flux dextran-retention data fitted with\nthe Bungay-Brenner model. Our results suggest that surface porosities and pore\nsizes of nanoporous membranes are much larger than previously understood, with\nmajor implications for structure/transport relationships. For future nanoscale\npore analysis of membranes (and other nanoporous materials), we recommend low\nacceleration voltage (1 kV), minimal coatings (1-2 nm), and digital dilation to\naccount for coating artifacts",
        "url": "http://arxiv.org/abs/2509.16471v1",
        "published_date": "2025-09-19T23:46:08+00:00",
        "updated_date": "2025-09-19T23:46:08+00:00",
        "categories": [
            "cond-mat.mtrl-sci",
            "cs.CV",
            "physics.app-ph",
            "physics.chem-ph",
            "physics.ins-det"
        ],
        "authors": [
            "Sima Zeinali Danalou",
            "Dian Yu",
            "Niher R. Sarker",
            "Hooman Chamani",
            "Jane Y. Howe",
            "Patrick C. Lee",
            "Jay R. Werber"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "The paper challenges the conventional understanding of nanoporous membrane surface porosities and pore sizes by proposing corrections for SEM imaging artifacts, leading to larger estimates of porosity and pore sizes.",
        "tldr_zh": "本文挑战了传统对于纳米孔膜表面孔隙度和孔径大小的认识，提出了扫描电子显微镜成像误差的修正方法，导致对孔隙度和孔径大小的估计更大。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "3D Gaussian Flats: Hybrid 2D/3D Photometric Scene Reconstruction",
        "summary": "Recent advances in radiance fields and novel view synthesis enable creation\nof realistic digital twins from photographs. However, current methods struggle\nwith flat, texture-less surfaces, creating uneven and semi-transparent\nreconstructions, due to an ill-conditioned photometric reconstruction\nobjective. Surface reconstruction methods solve this issue but sacrifice visual\nquality. We propose a novel hybrid 2D/3D representation that jointly optimizes\nconstrained planar (2D) Gaussians for modeling flat surfaces and freeform (3D)\nGaussians for the rest of the scene. Our end-to-end approach dynamically\ndetects and refines planar regions, improving both visual fidelity and\ngeometric accuracy. It achieves state-of-the-art depth estimation on ScanNet++\nand ScanNetv2, and excels at mesh extraction without overfitting to a specific\ncamera model, showing its effectiveness in producing high-quality\nreconstruction of indoor scenes.",
        "url": "http://arxiv.org/abs/2509.16423v1",
        "published_date": "2025-09-19T21:04:36+00:00",
        "updated_date": "2025-09-19T21:04:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Maria Taktasheva",
            "Lily Goli",
            "Alessandro Fiorini",
            "Zhen",
            "Li",
            "Daniel Rebain",
            "Andrea Tagliasacchi"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer"
        ],
        "tldr": "The paper introduces a hybrid 2D/3D representation for photometric scene reconstruction, improving visual fidelity and geometric accuracy by jointly optimizing planar and freeform Gaussians.",
        "tldr_zh": "本文提出了一种混合的2D/3D表示法，用于光度场景重建，通过联合优化平面和自由形式高斯函数，提高了视觉保真度和几何精度。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Accurate Thyroid Cancer Classification using a Novel Binary Pattern Driven Local Discrete Cosine Transform Descriptor",
        "summary": "In this study, we develop a new CAD system for accurate thyroid cancer\nclassification with emphasis on feature extraction. Prior studies have shown\nthat thyroid texture is important for segregating the thyroid ultrasound images\ninto different classes. Based upon our experience with breast cancer\nclassification, we first conjuncture that the Discrete Cosine Transform (DCT)\nis the best descriptor for capturing textural features. Thyroid ultrasound\nimages are particularly challenging as the gland is surrounded by multiple\ncomplex anatomical structures leading to variations in tissue density. Hence,\nwe second conjuncture the importance of localization and propose that the Local\nDCT (LDCT) descriptor captures the textural features best in this context.\nAnother disadvantage of complex anatomy around the thyroid gland is scattering\nof ultrasound waves resulting in noisy and unclear textures. Hence, we third\nconjuncture that one image descriptor is not enough to fully capture the\ntextural features and propose the integration of another popular texture\ncapturing descriptor (Improved Local Binary Pattern, ILBP) with LDCT. ILBP is\nknown to be noise resilient as well. We term our novel descriptor as Binary\nPattern Driven Local Discrete Cosine Transform (BPD-LDCT). Final classification\nis carried out using a non-linear SVM. The proposed CAD system is evaluated on\nthe only two publicly available thyroid cancer datasets, namely TDID and AUITD.\nThe evaluation is conducted in two stages. In Stage I, thyroid nodules are\ncategorized as benign or malignant. In Stage II, the malignant cases are\nfurther sub-classified into TI-RADS (4) and TI-RADS (5). For Stage I\nclassification, our proposed model demonstrates exceptional performance of\nnearly 100% on TDID and 97% on AUITD. In Stage II classification, the proposed\nmodel again attains excellent classification of close to 100% on TDID and 99%\non AUITD.",
        "url": "http://arxiv.org/abs/2509.16382v1",
        "published_date": "2025-09-19T19:54:04+00:00",
        "updated_date": "2025-09-19T19:54:04+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "eess.IV",
            "I.2.1; I.5.2"
        ],
        "authors": [
            "Saurabh Saini",
            "Kapil Ahuja",
            "Marc C. Steinbach",
            "Thomas Wick"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a new CAD system for accurate thyroid cancer classification using a Binary Pattern Driven Local Discrete Cosine Transform descriptor.",
        "tldr_zh": "本文介绍了一种使用二进制模式驱动的局部离散余弦变换描述符进行准确的甲状腺癌分类的新型CAD系统。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "AutoArabic: A Three-Stage Framework for Localizing Video-Text Retrieval Benchmarks",
        "summary": "Video-to-text and text-to-video retrieval are dominated by English benchmarks\n(e.g. DiDeMo, MSR-VTT) and recent multilingual corpora (e.g. RUDDER), yet\nArabic remains underserved, lacking localized evaluation metrics. We introduce\na three-stage framework, AutoArabic, utilizing state-of-the-art large language\nmodels (LLMs) to translate non-Arabic benchmarks into Modern Standard Arabic,\nreducing the manual revision required by nearly fourfold. The framework\nincorporates an error detection module that automatically flags potential\ntranslation errors with 97% accuracy. Applying the framework to DiDeMo, a video\nretrieval benchmark produces DiDeMo-AR, an Arabic variant with 40,144 fluent\nArabic descriptions. An analysis of the translation errors is provided and\norganized into an insightful taxonomy to guide future Arabic localization\nefforts. We train a CLIP-style baseline with identical hyperparameters on the\nArabic and English variants of the benchmark, finding a moderate performance\ngap (about 3 percentage points at Recall@1), indicating that Arabic\nlocalization preserves benchmark difficulty. We evaluate three post-editing\nbudgets (zero/ flagged-only/ full) and find that performance improves\nmonotonically with more post-editing, while the raw LLM output (zero-budget)\nremains usable. To ensure reproducibility to other languages, we made the code\navailable at https://github.com/Tahaalshatiri/AutoArabic.",
        "url": "http://arxiv.org/abs/2509.16438v1",
        "published_date": "2025-09-19T21:35:04+00:00",
        "updated_date": "2025-09-19T21:35:04+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Mohamed Eltahir",
            "Osamah Sarraj",
            "Abdulrahman Alfrihidi",
            "Taha Alshatiri",
            "Mohammed Khurd",
            "Mohammed Bremoo",
            "Tanveer Hussain"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces AutoArabic, a framework for translating and localizing video-text retrieval benchmarks to Arabic, with automated error detection and analysis of translation errors.",
        "tldr_zh": "本文介绍了AutoArabic，这是一个用于将视频-文本检索基准翻译和本地化为阿拉伯语的框架，具有自动化的错误检测和翻译错误分析。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Are VLMs Ready for Lane Topology Awareness in Autonomous Driving?",
        "summary": "Vision-Language Models (VLMs) have recently shown remarkable progress in\nmultimodal reasoning, yet their applications in autonomous driving remain\nlimited. In particular, the ability to understand road topology, a key\nrequirement for safe navigation, has received relatively little attention.\nWhile some recent works have begun to explore VLMs in driving contexts, their\nperformance on topology reasoning is far from satisfactory. In this work, we\nsystematically evaluate VLMs' capabilities in road topology understanding.\nSpecifically, multi-view images are projected into unified ground-plane\ncoordinate system and fused into bird's-eye-view (BEV) lanes. Based on these\nBEV lanes, we formulate four topology-related diagnostic VQA tasks, which\ntogether capture essential components of spatial topology reasoning. Through\nextensive evaluation, we find that while frontier closed-source models (e.g.,\nGPT-4o) achieve relatively high accuracy in some tasks, they still fail in some\ntemporal questions that humans can answer (e.g., GPT-4o achieve only 67.8% in\nvector, a two-class classification problem). Furthermore, we find open-source\nVLMs, even at 30B scale, struggle significantly. These results indicate that\nspatial reasoning remains a fundamental bottleneck for current VLMs. We also\nfind that the model's capability is positively correlated with model size,\nlength of reasoning tokens and shots provided as examples, showing direction\nfor future research.",
        "url": "http://arxiv.org/abs/2509.16654v1",
        "published_date": "2025-09-20T12:02:39+00:00",
        "updated_date": "2025-09-20T12:02:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xin Chen",
            "Jia He",
            "Maozheng Li",
            "Dongliang Xu",
            "Tianyu Wang",
            "Yixiao Chen",
            "Zhixin Lin",
            "Yue Yao"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper evaluates the ability of Vision-Language Models (VLMs) in understanding road topology for autonomous driving and finds current models to be lacking in spatial reasoning skills.",
        "tldr_zh": "本文评估了视觉语言模型（VLMs）在自动驾驶中理解道路拓扑的能力，并发现当前模型在空间推理方面存在不足。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.25
    },
    {
        "title": "Active View Selection for Scene-level Multi-view Crowd Counting and Localization with Limited Labels",
        "summary": "Multi-view crowd counting and localization fuse the input multi-views for\nestimating the crowd number or locations on the ground. Existing methods mainly\nfocus on accurately predicting on the crowd shown in the input views, which\nneglects the problem of choosing the `best' camera views to perceive all crowds\nwell in the scene. Besides, existing view selection methods require massive\nlabeled views and images, and lack the ability for cross-scene settings,\nreducing their application scenarios. Thus, in this paper, we study the view\nselection issue for better scene-level multi-view crowd counting and\nlocalization results with cross-scene ability and limited label demand, instead\nof input-view-level results. We first propose an independent view selection\nmethod (IVS) that considers view and scene geometries in the view selection\nstrategy and conducts the view selection, labeling, and downstream tasks\nindependently. Based on IVS, we also put forward an active view selection\nmethod (AVS) that jointly optimizes the view selection, labeling, and\ndownstream tasks. In AVS, we actively select the labeled views and consider\nboth the view/scene geometries and the predictions of the downstream task\nmodels in the view selection process. Experiments on multi-view counting and\nlocalization tasks demonstrate the cross-scene and the limited label demand\nadvantages of the proposed active view selection method (AVS), outperforming\nexisting methods and with wider application scenarios.",
        "url": "http://arxiv.org/abs/2509.16684v1",
        "published_date": "2025-09-20T13:23:46+00:00",
        "updated_date": "2025-09-20T13:23:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qi Zhang",
            "Bin Li",
            "Antoni B. Chan",
            "Hui Huang"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces an active view selection method for scene-level multi-view crowd counting and localization, addressing the issue of choosing the best camera views with limited labels and cross-scene ability.",
        "tldr_zh": "本文介绍了一种主动视图选择方法，用于场景级多视角人群计数和定位，解决了在有限标签和跨场景能力下选择最佳摄像头视图的问题。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.0
    }
]