[
    {
        "title": "Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models",
        "summary": "Video understanding represents the most challenging frontier in computer\nvision, requiring models to reason about complex spatiotemporal relationships,\nlong-term dependencies, and multimodal evidence. The recent emergence of\nVideo-Large Multimodal Models (Video-LMMs), which integrate visual encoders\nwith powerful decoder-based language models, has demonstrated remarkable\ncapabilities in video understanding tasks. However, the critical phase that\ntransforms these models from basic perception systems into sophisticated\nreasoning engines, post-training, remains fragmented across the literature.\nThis survey provides the first comprehensive examination of post-training\nmethodologies for Video-LMMs, encompassing three fundamental pillars:\nsupervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)\nfrom verifiable objectives, and test-time scaling (TTS) through enhanced\ninference computation. We present a structured taxonomy that clarifies the\nroles, interconnections, and video-specific adaptations of these techniques,\naddressing unique challenges such as temporal localization, spatiotemporal\ngrounding, long video efficiency, and multimodal evidence integration. Through\nsystematic analysis of representative methods, we synthesize key design\nprinciples, insights, and evaluation protocols while identifying critical open\nchallenges in reward design, scalability, and cost-performance optimization. We\nfurther curate essential benchmarks, datasets, and metrics to facilitate\nrigorous assessment of post-training effectiveness. This survey aims to provide\nresearchers and practitioners with a unified framework for advancing Video-LMM\ncapabilities. Additional resources and updates are maintained at:\nhttps://github.com/yunlong10/Awesome-Video-LMM-Post-Training",
        "url": "http://arxiv.org/abs/2510.05034v1",
        "published_date": "2025-10-06T17:10:44+00:00",
        "updated_date": "2025-10-06T17:10:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yunlong Tang",
            "Jing Bi",
            "Pinxin Liu",
            "Zhenyu Pan",
            "Zhangyun Tan",
            "Qianxiang Shen",
            "Jiani Liu",
            "Hang Hua",
            "Junjia Guo",
            "Yunzhong Xiao",
            "Chao Huang",
            "Zhiyuan Wang",
            "Susan Liang",
            "Xinyi Liu",
            "Yizhi Song",
            "Yuhe Nie",
            "Jia-Xing Zhong",
            "Bozheng Li",
            "Daiqing Qi",
            "Ziyun Zeng",
            "Ali Vosoughi",
            "Luchuan Song",
            "Zeliang Zhang",
            "Daiki Shimada",
            "Han Liu",
            "Jiebo Luo",
            "Chenliang Xu"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "This paper explores post-training methodologies for Video-Large Multimodal Models (Video-LMMs) to enhance video understanding capabilities, addressing challenges like temporal localization and multimodal evidence integration.",
        "tldr_zh": "本文探讨了用于增强视频理解能力的视频大型多模态模型（Video-LMMs）的后训练方法，解决了时序定位和多模态证据整合等挑战。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "Bridging Text and Video Generation: A Survey",
        "summary": "Text-to-video (T2V) generation technology holds potential to transform\nmultiple domains such as education, marketing, entertainment, and assistive\ntechnologies for individuals with visual or reading comprehension challenges,\nby creating coherent visual content from natural language prompts. From its\ninception, the field has advanced from adversarial models to diffusion-based\nmodels, yielding higher-fidelity, temporally consistent outputs. Yet challenges\npersist, such as alignment, long-range coherence, and computational efficiency.\nAddressing this evolving landscape, we present a comprehensive survey of\ntext-to-video generative models, tracing their development from early GANs and\nVAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these\nmodels work, what limitations they addressed in their predecessors, and why\nshifts toward new architectural paradigms were necessary to overcome challenges\nin quality, coherence, and control. We provide a systematic account of the\ndatasets, which the surveyed text-to-video models were trained and evaluated\non, and, to support reproducibility and assess the accessibility of training\nsuch models, we detail their training configurations, including their hardware\nspecifications, GPU counts, batch sizes, learning rates, optimizers, epochs,\nand other key hyperparameters. Further, we outline the evaluation metrics\ncommonly used for evaluating such models and present their performance across\nstandard benchmarks, while also discussing the limitations of these metrics and\nthe emerging shift toward more holistic, perception-aligned evaluation\nstrategies. Finally, drawing from our analysis, we outline the current open\nchallenges and propose a few promising future directions, laying out a\nperspective for future researchers to explore and build upon in advancing T2V\nresearch and applications.",
        "url": "http://arxiv.org/abs/2510.04999v1",
        "published_date": "2025-10-06T16:39:05+00:00",
        "updated_date": "2025-10-06T16:39:05+00:00",
        "categories": [
            "cs.GR",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Nilay Kumar",
            "Priyansh Bhandari",
            "G. Maragatham"
        ],
        "ai_categories": [
            "Multimodality",
            "GAN",
            "Diffusion",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper surveys the development of text-to-video generative models, highlighting advancements in quality, coherence, and control, and discussing current challenges and future directions.",
        "tldr_zh": "本文调研了文本到视频生成模型的发展，突出了在质量、连贯性和控制方面的进展，讨论了当前的挑战和未来方向。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "Pulp Motion: Framing-aware multimodal camera and human motion generation",
        "summary": "Treating human motion and camera trajectory generation separately overlooks a\ncore principle of cinematography: the tight interplay between actor performance\nand camera work in the screen space. In this paper, we are the first to cast\nthis task as a text-conditioned joint generation, aiming to maintain consistent\non-screen framing while producing two heterogeneous, yet intrinsically linked,\nmodalities: human motion and camera trajectories. We propose a simple,\nmodel-agnostic framework that enforces multimodal coherence via an auxiliary\nmodality: the on-screen framing induced by projecting human joints onto the\ncamera. This on-screen framing provides a natural and effective bridge between\nmodalities, promoting consistency and leading to more precise joint\ndistribution. We first design a joint autoencoder that learns a shared latent\nspace, together with a lightweight linear transform from the human and camera\nlatents to a framing latent. We then introduce auxiliary sampling, which\nexploits this linear transform to steer generation toward a coherent framing\nmodality. To support this task, we also introduce the PulpMotion dataset, a\nhuman-motion and camera-trajectory dataset with rich captions, and high-quality\nhuman motions. Extensive experiments across DiT- and MAR-based architectures\nshow the generality and effectiveness of our method in generating on-frame\ncoherent human-camera motions, while also achieving gains on textual alignment\nfor both modalities. Our qualitative results yield more cinematographically\nmeaningful framings setting the new state of the art for this task. Code,\nmodels and data are available in our\n\\href{https://www.lix.polytechnique.fr/vista/projects/2025_pulpmotion_courant/}{project\npage}.",
        "url": "http://arxiv.org/abs/2510.05097v1",
        "published_date": "2025-10-06T17:58:34+00:00",
        "updated_date": "2025-10-06T17:58:34+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Robin Courant",
            "Xi Wang",
            "David Loiseaux",
            "Marc Christie",
            "Vicky Kalogeiton"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a framework for generating consistent on-screen human motions and camera trajectories by leveraging multimodal coherence using framing as a bridge between modalities.",
        "tldr_zh": "该论文引入了一个框架，通过利用框架作为模态之间的桥梁实现生成一致的画面上人类动作和摄像机轨迹。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "VChain: Chain-of-Visual-Thought for Reasoning in Video Generation",
        "summary": "Recent video generation models can produce smooth and visually appealing\nclips, but they often struggle to synthesize complex dynamics with a coherent\nchain of consequences. Accurately modeling visual outcomes and state\ntransitions over time remains a core challenge. In contrast, large language and\nmultimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and\nfuture prediction capabilities. To bridge these strengths, we introduce VChain,\na novel inference-time chain-of-visual-thought framework that injects visual\nreasoning signals from multimodal models into video generation. Specifically,\nVChain contains a dedicated pipeline that leverages large multimodal models to\ngenerate a sparse set of critical keyframes as snapshots, which are then used\nto guide the sparse inference-time tuning of a pre-trained video generator only\nat these key moments. Our approach is tuning-efficient, introduces minimal\noverhead and avoids dense supervision. Extensive experiments on complex,\nmulti-step scenarios show that VChain significantly enhances the quality of\ngenerated videos.",
        "url": "http://arxiv.org/abs/2510.05094v1",
        "published_date": "2025-10-06T17:57:59+00:00",
        "updated_date": "2025-10-06T17:57:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziqi Huang",
            "Ning Yu",
            "Gordon Chen",
            "Haonan Qiu",
            "Paul Debevec",
            "Ziwei Liu"
        ],
        "ai_categories": [
            "Multimodality",
            "GAN"
        ],
        "tldr": "VChain introduces a novel chain-of-visual-thought framework to enhance video generation by leveraging keyframes generated from multimodal models for sparse inference-time tuning of a pre-trained video generator, improving video quality significantly.",
        "tldr_zh": "VChain通过利用多模态模型生成的关键帧，为预训练视频生成器的稀疏推理时间调优引入了一种新颖的视觉思维链框架，显著提高了视频质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization",
        "summary": "Tokenizers are a key component of state-of-the-art generative image models,\nextracting the most important features from the signal while reducing data\ndimension and redundancy. Most current tokenizers are based on KL-regularized\nvariational autoencoders (KL-VAE), trained with reconstruction, perceptual and\nadversarial losses. Diffusion decoders have been proposed as a more principled\nalternative to model the distribution over images conditioned on the latent.\nHowever, matching the performance of KL-VAE still requires adversarial losses,\nas well as a higher decoding time due to iterative sampling. To address these\nlimitations, we introduce a new pixel diffusion decoder architecture for\nimproved scaling and training stability, benefiting from transformer components\nand GAN-free training. We use distillation to replicate the performance of the\ndiffusion decoder in an efficient single-step decoder. This makes SSDD the\nfirst diffusion decoder optimized for single-step reconstruction trained\nwithout adversarial losses, reaching higher reconstruction quality and faster\nsampling than KL-VAE. In particular, SSDD improves reconstruction FID from\n$0.87$ to $0.50$ with $1.4\\times$ higher throughput and preserve generation\nquality of DiTs with $3.8\\times$ faster sampling. As such, SSDD can be used as\na drop-in replacement for KL-VAE, and for building higher-quality and faster\ngenerative models.",
        "url": "http://arxiv.org/abs/2510.04961v1",
        "published_date": "2025-10-06T15:57:31+00:00",
        "updated_date": "2025-10-06T15:57:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Théophane Vallaeys",
            "Jakob Verbeek",
            "Matthieu Cord"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer",
            "GAN"
        ],
        "tldr": "SSDD is a new pixel diffusion decoder architecture that improves image tokenization efficiency by using transformer components and GAN-free training. It achieves higher reconstruction quality and faster sampling compared to existing methods like KL-VAE.",
        "tldr_zh": "SSDD是一种新的像素扩散解码器架构，通过使用变压器组件和无GAN训练，提高了图像标记效率。与KL-VAE等现有方法相比，它实现了更高的重建质量和更快的采样速度。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "SFANet: Spatial-Frequency Attention Network for Deepfake Detection",
        "summary": "Detecting manipulated media has now become a pressing issue with the recent\nrise of deepfakes. Most existing approaches fail to generalize across diverse\ndatasets and generation techniques. We thus propose a novel ensemble framework,\ncombining the strengths of transformer-based architectures, such as Swin\nTransformers and ViTs, and texture-based methods, to achieve better detection\naccuracy and robustness. Our method introduces innovative data-splitting,\nsequential training, frequency splitting, patch-based attention, and face\nsegmentation techniques to handle dataset imbalances, enhance high-impact\nregions (e.g., eyes and mouth), and improve generalization. Our model achieves\nstate-of-the-art performance when tested on the DFWild-Cup dataset, a diverse\nsubset of eight deepfake datasets. The ensemble benefits from the\ncomplementarity of these approaches, with transformers excelling in global\nfeature extraction and texturebased methods providing interpretability. This\nwork demonstrates that hybrid models can effectively address the evolving\nchallenges of deepfake detection, offering a robust solution for real-world\napplications.",
        "url": "http://arxiv.org/abs/2510.04630v1",
        "published_date": "2025-10-06T09:35:57+00:00",
        "updated_date": "2025-10-06T09:35:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "authors": [
            "Vrushank Ahire",
            "Aniruddh Muley",
            "Shivam Zample",
            "Siddharth Verma",
            "Pranav Menon",
            "Surbhi Madan",
            "Abhinav Dhall"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces SFANet, a novel ensemble framework combining transformer-based architectures and texture-based methods for deepfake detection, achieving state-of-the-art performance on a diverse dataset.",
        "tldr_zh": "该论文引入了SFANet，一种新颖的集成框架，结合了基于转换器的架构和基于纹理的方法，用于深度伪造检测，在多样数据集上取得了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "Asynchronous Denoising Diffusion Models for Aligning Text-to-Image Generation",
        "summary": "Diffusion models have achieved impressive results in generating high-quality\nimages. Yet, they often struggle to faithfully align the generated images with\nthe input prompts. This limitation arises from synchronous denoising, where all\npixels simultaneously evolve from random noise to clear images. As a result,\nduring generation, the prompt-related regions can only reference the unrelated\nregions at the same noise level, failing to obtain clear context and ultimately\nimpairing text-to-image alignment. To address this issue, we propose\nasynchronous diffusion models -- a novel framework that allocates distinct\ntimesteps to different pixels and reformulates the pixel-wise denoising\nprocess. By dynamically modulating the timestep schedules of individual pixels,\nprompt-related regions are denoised more gradually than unrelated regions,\nthereby allowing them to leverage clearer inter-pixel context. Consequently,\nthese prompt-related regions achieve better alignment in the final images.\nExtensive experiments demonstrate that our asynchronous diffusion models can\nsignificantly improve text-to-image alignment across diverse prompts. The code\nrepository for this work is available at https://github.com/hu-zijing/AsynDM.",
        "url": "http://arxiv.org/abs/2510.04504v1",
        "published_date": "2025-10-06T05:45:56+00:00",
        "updated_date": "2025-10-06T05:45:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zijing Hu",
            "Yunze Tong",
            "Fengda Zhang",
            "Junkun Yuan",
            "Jun Xiao",
            "Kun Kuang"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Multimodality"
        ],
        "tldr": "The paper introduces asynchronous diffusion models to improve text-to-image alignment by allowing different pixels to evolve at different rates based on relevance to the input prompts.",
        "tldr_zh": "该论文引入了异步扩散模型，通过允许不同像素根据输入提示的相关性以不同速率演变来改善文本到图像的对齐。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator",
        "summary": "World models that support controllable\n  and editable spatiotemporal environments are valuable\n  for robotics, enabling scalable training data, repro ducible evaluation, and\nflexible task design. While\n  recent text-to-video models generate realistic dynam ics, they are\nconstrained to 2D views and offer limited\n  interaction. We introduce MorphoSim, a language guided framework that\ngenerates 4D scenes with\n  multi-view consistency and object-level controls. From\n  natural language instructions, MorphoSim produces\n  dynamic environments where objects can be directed,\n  recolored, or removed, and scenes can be observed\n  from arbitrary viewpoints. The framework integrates\n  trajectory-guided generation with feature field dis tillation, allowing edits\nto be applied interactively\n  without full re-generation. Experiments show that Mor phoSim maintains high\nscene fidelity while enabling\n  controllability and editability. The code is available\n  at https://github.com/eric-ai-lab/Morph4D.",
        "url": "http://arxiv.org/abs/2510.04390v1",
        "published_date": "2025-10-05T22:55:17+00:00",
        "updated_date": "2025-10-05T22:55:17+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Xuehai He",
            "Shijie Zhou",
            "Thivyanth Venkateswaran",
            "Kaizhi Zheng",
            "Ziyu Wan",
            "Achuta Kadambi",
            "Xin Eric Wang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "MorphoSim is a language-guided framework that generates 4D scenes with controllable and editable environments for robotics tasks.",
        "tldr_zh": "MorphoSim是一个语言引导的框架，为机器人任务生成可控和可编辑的4D场景。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Visual Representations inside the Language Model",
        "summary": "Despite interpretability work analyzing VIT encoders and transformer\nactivations, we don't yet understand why Multimodal Language Models (MLMs)\nstruggle on perception-heavy tasks. We offer an under-studied perspective by\nexamining how popular MLMs (LLaVA-OneVision, Qwen2.5-VL, and\nLlama-3-LLaVA-NeXT) process their visual key-value tokens. We first study the\nflow of visual information through the language model, finding that image value\ntokens encode sufficient information to perform several perception-heavy tasks\nzero-shot: segmentation, semantic correspondence, temporal correspondence, and\nreferring expression detection. We find that while the language model does\naugment the visual information received from the projection of input visual\nencodings-which we reveal correlates with overall MLM perception capability-it\ncontains less visual information on several tasks than the equivalent visual\nencoder (SigLIP) that has not undergone MLM finetuning. Further, we find that\nthe visual information corresponding to input-agnostic image key tokens in\nlater layers of language models contains artifacts which reduce perception\ncapability of the overall MLM. Next, we discuss controlling visual information\nin the language model, showing that adding a text prefix to the image input\nimproves perception capabilities of visual representations. Finally, we reveal\nthat if language models were able to better control their visual information,\ntheir perception would significantly improve; e.g., in 33.3% of Art Style\nquestions in the BLINK benchmark, perception information present in the\nlanguage model is not surfaced to the output! Our findings reveal insights into\nthe role of key-value tokens in multimodal systems, paving the way for deeper\nmechanistic interpretability of MLMs and suggesting new directions for training\ntheir visual encoder and language model components.",
        "url": "http://arxiv.org/abs/2510.04819v1",
        "published_date": "2025-10-06T14:01:39+00:00",
        "updated_date": "2025-10-06T14:01:39+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Benlin Liu",
            "Amita Kamath",
            "Madeleine Grunde-McLaughlin",
            "Winson Han",
            "Ranjay Krishna"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper explores how Multimodal Language Models process visual information and suggests that controlling visual information within the model can significantly improve perception capabilities.",
        "tldr_zh": "本文探讨了多模态语言模型处理视觉信息的方式，并提出在模型内部控制视觉信息可以显著提高感知能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Paper2Video: Automatic Video Generation from Scientific Papers",
        "summary": "Academic presentation videos have become an essential medium for research\ncommunication, yet producing them remains highly labor-intensive, often\nrequiring hours of slide design, recording, and editing for a short 2 to 10\nminutes video. Unlike natural video, presentation video generation involves\ndistinctive challenges: inputs from research papers, dense multi-modal\ninformation (text, figures, tables), and the need to coordinate multiple\naligned channels such as slides, subtitles, speech, and human talker. To\naddress these challenges, we introduce PaperTalker, the first benchmark of 101\nresearch papers paired with author-created presentation videos, slides, and\nspeaker metadata. We further design four tailored evaluation metrics--Meta\nSimilarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos\nconvey the paper's information to the audience. Building on this foundation, we\npropose PaperTalker, the first multi-agent framework for academic presentation\nvideo generation. It integrates slide generation with effective layout\nrefinement by a novel effective tree search visual choice, cursor grounding,\nsubtitling, speech synthesis, and talking-head rendering, while parallelizing\nslide-wise generation for efficiency. Experiments on Paper2Video demonstrate\nthat the presentation videos produced by our approach are more faithful and\ninformative than existing baselines, establishing a practical step toward\nautomated and ready-to-use academic video generation. Our dataset, agent, and\ncode are available at https://github.com/showlab/Paper2Video.",
        "url": "http://arxiv.org/abs/2510.05096v1",
        "published_date": "2025-10-06T17:58:02+00:00",
        "updated_date": "2025-10-06T17:58:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.MA",
            "cs.MM"
        ],
        "authors": [
            "Zeyu Zhu",
            "Kevin Qinghong Lin",
            "Mike Zheng Shou"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The Paper2Video project introduces PaperTalker, a benchmark for academic presentation video generation from research papers, utilizing a multi-agent framework for improved video quality and fidelity.",
        "tldr_zh": "Paper2Video项目引入了PaperTalker，一个从研究论文生成学术演示视频的基准，利用多代理框架提高视频质量和保真度。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Character Mixing for Video Generation",
        "summary": "Imagine Mr. Bean stepping into Tom and Jerry--can we generate videos where\ncharacters interact naturally across different worlds? We study inter-character\ninteraction in text-to-video generation, where the key challenge is to preserve\neach character's identity and behaviors while enabling coherent cross-context\ninteraction. This is difficult because characters may never have coexisted and\nbecause mixing styles often causes style delusion, where realistic characters\nappear cartoonish or vice versa. We introduce a framework that tackles these\nissues with Cross-Character Embedding (CCE), which learns identity and\nbehavioral logic across multimodal sources, and Cross-Character Augmentation\n(CCA), which enriches training with synthetic co-existence and mixed-style\ndata. Together, these techniques allow natural interactions between previously\nuncoexistent characters without losing stylistic fidelity. Experiments on a\ncurated benchmark of cartoons and live-action series with 10 characters show\nclear improvements in identity preservation, interaction quality, and\nrobustness to style delusion, enabling new forms of generative\nstorytelling.Additional results and videos are available on our project page:\nhttps://tingtingliao.github.io/mimix/.",
        "url": "http://arxiv.org/abs/2510.05093v1",
        "published_date": "2025-10-06T17:57:39+00:00",
        "updated_date": "2025-10-06T17:57:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tingting Liao",
            "Chongjian Ge",
            "Guangyi Liu",
            "Hao Li",
            "Yi Zhou"
        ],
        "ai_categories": [
            "GAN",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a framework for generating videos where characters from different worlds interact naturally without losing their identities or behaviors.",
        "tldr_zh": "该论文介绍了一个框架，用于生成视频，其中来自不同世界的角色可以自然交互，而不会失去其身份或行为。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Factuality Matters: When Image Generation and Editing Meet Structured Visuals",
        "summary": "While modern visual generation models excel at creating aesthetically\npleasing natural images, they struggle with producing or editing structured\nvisuals like charts, diagrams, and mathematical figures, which demand\ncomposition planning, text rendering, and multimodal reasoning for factual\nfidelity. To address this, we present the first comprehensive, systematic\ninvestigation of this domain, encompassing data construction, model training,\nand an evaluation benchmark. First, we construct a large-scale dataset of 1.3\nmillion high-quality structured image pairs derived from executable drawing\nprograms and augmented with chain-of-thought reasoning annotations. Building on\nit, we train a unified model that integrates a VLM with FLUX.1 Kontext via a\nlightweight connector for enhanced multimodal understanding. A three-stage\ntraining curriculum enables progressive feature alignment, knowledge infusion,\nand reasoning-augmented generation, further boosted by an external reasoner at\ninference time. Finally, we introduce StructBench, a novel benchmark for\ngeneration and editing with over 1,700 challenging instances, and an\naccompanying evaluation metric, StructScore, which employs a multi-round Q\\&A\nprotocol to assess fine-grained factual accuracy. Evaluations of 15 models\nreveal that even leading closed-source systems remain far from satisfactory.\nOur model attains strong editing performance, and inference-time reasoning\nyields consistent gains across diverse architectures. By releasing the dataset,\nmodel, and benchmark, we aim to advance unified multimodal foundations for\nstructured visuals.",
        "url": "http://arxiv.org/abs/2510.05091v1",
        "published_date": "2025-10-06T17:56:55+00:00",
        "updated_date": "2025-10-06T17:56:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Le Zhuo",
            "Songhao Han",
            "Yuandong Pu",
            "Boxiang Qiu",
            "Sayak Paul",
            "Yue Liao",
            "Yihao Liu",
            "Jie Shao",
            "Xi Chen",
            "Si Liu",
            "Hongsheng Li"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper addresses the challenge of generating and editing structured visuals like charts and diagrams, presenting a dataset, model, and benchmark to advance multimodal foundations for structured visuals.",
        "tldr_zh": "该论文解决了生成和编辑图表和图表等结构化视觉的挑战，提出了一个数据集、模型和基准，以推进结构化视觉的多模态基础。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder",
        "summary": "Large-scale text-to-image diffusion models have become the backbone of modern\nimage editing, yet text prompts alone do not offer adequate control over the\nediting process. Two properties are especially desirable: disentanglement,\nwhere changing one attribute does not unintentionally alter others, and\ncontinuous control, where the strength of an edit can be smoothly adjusted. We\nintroduce a method for disentangled and continuous editing through token-level\nmanipulation of text embeddings. The edits are applied by manipulating the\nembeddings along carefully chosen directions, which control the strength of the\ntarget attribute. To identify such directions, we employ a Sparse Autoencoder\n(SAE), whose sparse latent space exposes semantically isolated dimensions. Our\nmethod operates directly on text embeddings without modifying the diffusion\nprocess, making it model agnostic and broadly applicable to various image\nsynthesis backbones. Experiments show that it enables intuitive and efficient\nmanipulations with continuous control across diverse attributes and domains.",
        "url": "http://arxiv.org/abs/2510.05081v1",
        "published_date": "2025-10-06T17:51:04+00:00",
        "updated_date": "2025-10-06T17:51:04+00:00",
        "categories": [
            "cs.GR",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Ronen Kamenetsky",
            "Sara Dorfman",
            "Daniel Garibi",
            "Roni Paiss",
            "Or Patashnik",
            "Daniel Cohen-Or"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Diffusion"
        ],
        "tldr": "The paper introduces a method for continuous image editing through token-level manipulation of text embeddings, allowing for disentangled and smooth control over the editing process.",
        "tldr_zh": "本文介绍了一种通过对文本嵌入进行令牌级操作进行连续图像编辑的方法，实现了编辑过程的解缠和平滑控制。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "No-reference Quality Assessment of Contrast-distorted Images using Contrast-enhanced Pseudo Reference",
        "summary": "Contrast change is an important factor that affects the quality of images.\nDuring image capturing, unfavorable lighting conditions can cause contrast\nchange and visual quality loss. While various methods have been proposed to\nassess the quality of images under different distortions such as blur and\nnoise, contrast distortion has been largely overlooked as its visual impact and\nproperties are different from other conventional types of distortions. In this\npaper, we propose a no-reference image quality assessment (NR-IQA) metric for\ncontrast-distorted images. Using a set of contrast enhancement algorithms, we\naim to generate pseudo-reference images that are visually close to the actual\nreference image, such that the NR problem is transformed to a Full-reference\n(FR) assessment with higher accuracy. To this end, a large dataset of\ncontrast-enhanced images is produced to train a classification network that can\nselect the most suitable contrast enhancement algorithm based on image content\nand distortion for pseudo-reference image generation. Finally, the evaluation\nis performed in the FR manner to assess the quality difference between the\ncontrast-enhanced (pseudoreference) and degraded images. Performance evaluation\nof the proposed method on three databases containing contrast distortions\n(CCID2014, TID2013, and CSIQ), indicates the promising performance of the\nproposed method.",
        "url": "http://arxiv.org/abs/2510.05053v1",
        "published_date": "2025-10-06T17:32:48+00:00",
        "updated_date": "2025-10-06T17:32:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mohammad-Ali Mahmoudpour",
            "Saeed Mahmoudpour"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper proposes a method for assessing the quality of contrast-distorted images using contrast-enhanced pseudo-reference images, achieving promising results.",
        "tldr_zh": "该论文提出了一种使用对比增强伪参考图像评估对比失真图像质量的方法，取得了令人期待的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Exploring the Efficacy of Modified Transfer Learning in Identifying Parkinson's Disease Through Drawn Image Patterns",
        "summary": "Parkinson's disease (PD) is a progressive neurodegenerative condition\ncharacterized by the death of dopaminergic neurons, leading to various movement\ndisorder symptoms. Early diagnosis of PD is crucial to prevent adverse effects,\nyet traditional diagnostic methods are often cumbersome and costly. In this\nstudy, a machine learning-based approach is proposed using hand-drawn spiral\nand wave images as potential biomarkers for PD detection. Our methodology\nleverages convolutional neural networks (CNNs), transfer learning, and\nattention mechanisms to improve model performance and resilience against\noverfitting. To enhance the diversity and richness of both spiral and wave\ncategories, the training dataset undergoes augmentation to increase the number\nof images. The proposed architecture comprises three phases: utilizing\npre-trained CNNs, incorporating custom convolutional layers, and ensemble\nvoting. Employing hard voting further enhances performance by aggregating\npredictions from multiple models. Experimental results show promising accuracy\nrates. For spiral images, weighted average precision, recall, and F1-score are\n90%, and for wave images, they are 96.67%. After combining the predictions\nthrough ensemble hard voting, the overall accuracy is 93.3%. These findings\nunderscore the potential of machine learning in early PD diagnosis, offering a\nnon-invasive and cost-effective solution to improve patient outcomes.",
        "url": "http://arxiv.org/abs/2510.05015v1",
        "published_date": "2025-10-06T16:55:07+00:00",
        "updated_date": "2025-10-06T16:55:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nabil Daiyan",
            "Md Rakibul Haque"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper proposes a machine learning-based approach using hand-drawn images for early diagnosis of Parkinson's disease with promising accuracy rates.",
        "tldr_zh": "本文提出了一种利用手绘图像进行帕金森病早期诊断的机器学习方法，具有良好的准确率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ActiveMark: on watermarking of visual foundation models via massive activations",
        "summary": "Being trained on large and vast datasets, visual foundation models (VFMs) can\nbe fine-tuned for diverse downstream tasks, achieving remarkable performance\nand efficiency in various computer vision applications. The high computation\ncost of data collection and training motivates the owners of some VFMs to\ndistribute them alongside the license to protect their intellectual property\nrights. However, a dishonest user of the protected model's copy may illegally\nredistribute it, for example, to make a profit. As a consequence, the\ndevelopment of reliable ownership verification tools is of great importance\ntoday, since such methods can be used to differentiate between a redistributed\ncopy of the protected model and an independent model. In this paper, we propose\nan approach to ownership verification of visual foundation models by\nfine-tuning a small set of expressive layers of a VFM along with a small\nencoder-decoder network to embed digital watermarks into an internal\nrepresentation of a hold-out set of input images. Importantly, the watermarks\nembedded remain detectable in the functional copies of the protected model,\nobtained, for example, by fine-tuning the VFM for a particular downstream task.\nTheoretically and experimentally, we demonstrate that the proposed method\nyields a low probability of false detection of a non-watermarked model and a\nlow probability of false misdetection of a watermarked model.",
        "url": "http://arxiv.org/abs/2510.04966v1",
        "published_date": "2025-10-06T15:58:27+00:00",
        "updated_date": "2025-10-06T15:58:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Anna Chistyakova",
            "Mikhail Pautov"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a method to embed digital watermarks into visual foundation models for ownership verification.",
        "tldr_zh": "本文提出了一种方法，将数字水印嵌入视觉基础模型，用于所有权验证。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion",
        "summary": "Dual-view mammography, including craniocaudal (CC) and mediolateral oblique\n(MLO) projections, offers complementary anatomical views crucial for breast\ncancer diagnosis. However, in real-world clinical workflows, one view may be\nmissing, corrupted, or degraded due to acquisition errors or compression\nartifacts, limiting the effectiveness of downstream analysis. View-to-view\ntranslation can help recover missing views and improve lesion alignment. Unlike\nnatural images, this task in mammography is highly challenging due to large\nnon-rigid deformations and severe tissue overlap in X-ray projections, which\nobscure pixel-level correspondences. In this paper, we propose Column-Aware and\nImplicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view\ntranslation framework based on conditional diffusion model. To address\ncross-view structural misalignment, we first design a column-aware\ncross-attention mechanism that leverages the geometric property that\nanatomically corresponding regions tend to lie in similar column positions\nacross views. A Gaussian-decayed bias is applied to emphasize local column-wise\ncorrelations while suppressing distant mismatches. Furthermore, we introduce an\nimplicit 3D structure reconstruction module that back-projects noisy 2D latents\ninto a coarse 3D feature volume based on breast-view projection geometry. The\nreconstructed 3D structure is refined and injected into the denoising UNet to\nguide cross-view generation with enhanced anatomical awareness. Extensive\nexperiments demonstrate that CA3D-Diff achieves superior performance in\nbidirectional tasks, outperforming state-of-the-art methods in visual fidelity\nand structural consistency. Furthermore, the synthesized views effectively\nimprove single-view malignancy classification in screening settings,\ndemonstrating the practical value of our method in real-world diagnostics.",
        "url": "http://arxiv.org/abs/2510.04947v1",
        "published_date": "2025-10-06T15:48:27+00:00",
        "updated_date": "2025-10-06T15:48:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xin Li",
            "Kaixiang Yang",
            "Qiang Li",
            "Zhiwei Wang"
        ],
        "ai_categories": [
            "Diffusion",
            "Other"
        ],
        "tldr": "The paper proposes a novel framework, CA3D-Diff, for bidirectional mammogram view translation to recover missing views and improve lesion alignment in breast cancer diagnosis.",
        "tldr_zh": "本文提出了一种新的框架CA3D-Diff，用于双向乳腺X线透视图转换，以恢复缺失视图并改善乳腺癌诊断中的病变对齐。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "REN: Anatomically-Informed Mixture-of-Experts for Interstitial Lung Disease Diagnosis",
        "summary": "Mixture-of-Experts (MoE) architectures have significantly contributed to\nscalable machine learning by enabling specialized subnetworks to tackle complex\ntasks efficiently. However, traditional MoE systems lack domain-specific\nconstraints essential for medical imaging, where anatomical structure and\nregional disease heterogeneity strongly influence pathological patterns. Here,\nwe introduce Regional Expert Networks (REN), the first anatomically-informed\nMoE framework tailored specifically for medical image classification. REN\nleverages anatomical priors to train seven specialized experts, each dedicated\nto distinct lung lobes and bilateral lung combinations, enabling precise\nmodeling of region-specific pathological variations. Multi-modal gating\nmechanisms dynamically integrate radiomics biomarkers and deep learning (DL)\nfeatures (CNN, ViT, Mamba) to weight expert contributions optimally. Applied to\ninterstitial lung disease (ILD) classification, REN achieves consistently\nsuperior performance: the radiomics-guided ensemble reached an average AUC of\n0.8646 +/- 0.0467, a +12.5 percent improvement over the SwinUNETR baseline (AUC\n0.7685, p = 0.031). Region-specific experts further revealed that lower-lobe\nmodels achieved AUCs of 0.88-0.90, surpassing DL counterparts (CNN: 0.76-0.79)\nand aligning with known disease progression patterns. Through rigorous\npatient-level cross-validation, REN demonstrates strong generalizability and\nclinical interpretability, presenting a scalable, anatomically-guided approach\nreadily extensible to other structured medical imaging applications.",
        "url": "http://arxiv.org/abs/2510.04923v1",
        "published_date": "2025-10-06T15:35:08+00:00",
        "updated_date": "2025-10-06T15:35:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Alec K. Peltekian",
            "Halil Ertugrul Aktas",
            "Gorkem Durak",
            "Kevin Grudzinski",
            "Bradford C. Bemiss",
            "Carrie Richardson",
            "Jane E. Dematte",
            "G. R. Scott Budinger",
            "Anthony J. Esposito",
            "Alexander Misharin",
            "Alok Choudhary",
            "Ankit Agrawal",
            "Ulas Bagci"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a new anatomically-informed Mixture-of-Experts framework for medical image classification, achieving superior performance in interstitial lung disease diagnosis.",
        "tldr_zh": "本文提出了一种新的解剖结构知识驱动的Mixture-of-Experts框架，用于医学图像分类，在间质肺病诊断中取得了优越的性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CLEAR-IR: Clarity-Enhanced Active Reconstruction of Infrared Imagery",
        "summary": "This paper presents a novel approach for enabling robust robotic perception\nin dark environments using infrared (IR) stream. IR stream is less susceptible\nto noise than RGB in low-light conditions. However, it is dominated by active\nemitter patterns that hinder high-level tasks such as object detection,\ntracking and localisation. To address this, a U-Net-based architecture is\nproposed that reconstructs clean IR images from emitter-populated input,\nimproving both image quality and downstream robotic performance. This approach\noutperforms existing enhancement techniques and enables reliable operation of\nvision-driven robotic systems across illumination conditions from well-lit to\nextreme low-light scenes.",
        "url": "http://arxiv.org/abs/2510.04883v1",
        "published_date": "2025-10-06T15:04:56+00:00",
        "updated_date": "2025-10-06T15:04:56+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Nathan Shankar",
            "Pawel Ladosz",
            "Hujun Yin"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a method for enhancing infrared imagery in dark environments to improve robotic perception using a U-Net-based architecture.",
        "tldr_zh": "本文提出了一种改进红外图像的方法，以提高机器人在黑暗环境中的感知能力，使用基于U-Net的架构。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Flow Matching for Conditional MRI-CT and CBCT-CT Image Synthesis",
        "summary": "Generating synthetic CT (sCT) from MRI or CBCT plays a crucial role in\nenabling MRI-only and CBCT-based adaptive radiotherapy, improving treatment\nprecision while reducing patient radiation exposure. To address this task, we\nadopt a fully 3D Flow Matching (FM) framework, motivated by recent work\ndemonstrating FM's efficiency in producing high-quality images. In our\napproach, a Gaussian noise volume is transformed into an sCT image by\nintegrating a learned FM velocity field, conditioned on features extracted from\nthe input MRI or CBCT using a lightweight 3D encoder. We evaluated the method\non the SynthRAD2025 Challenge benchmark, training separate models for MRI\n$\\rightarrow$ sCT and CBCT $\\rightarrow$ sCT across three anatomical regions:\nabdomen, head and neck, and thorax. Validation and testing were performed\nthrough the challenge submission system. The results indicate that the method\naccurately reconstructs global anatomical structures; however, preservation of\nfine details was limited, primarily due to the relatively low training\nresolution imposed by memory and runtime constraints. Future work will explore\npatch-based training and latent-space flow models to improve resolution and\nlocal structural fidelity.",
        "url": "http://arxiv.org/abs/2510.04823v1",
        "published_date": "2025-10-06T14:07:03+00:00",
        "updated_date": "2025-10-06T14:07:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Arnela Hadzic",
            "Simon Johannes Joham",
            "Martin Urschler"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Dataset"
        ],
        "tldr": "This paper presents a method for generating synthetic CT images from MRI or CBCT scans using a 3D flow matching framework, showing promising results but limited fine detail preservation due to training resolution constraints.",
        "tldr_zh": "该论文提出了一种利用3D流匹配框架从MRI或CBCT扫描生成合成CT图像的方法，显示出有希望的结果，但由于训练分辨率限制，细节保留有限。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AvatarVTON: 4D Virtual Try-On for Animatable Avatars",
        "summary": "We propose AvatarVTON, the first 4D virtual try-on framework that generates\nrealistic try-on results from a single in-shop garment image, enabling free\npose control, novel-view rendering, and diverse garment choices. Unlike\nexisting methods, AvatarVTON supports dynamic garment interactions under\nsingle-view supervision, without relying on multi-view garment captures or\nphysics priors. The framework consists of two key modules: (1) a Reciprocal\nFlow Rectifier, a prior-free optical-flow correction strategy that stabilizes\navatar fitting and ensures temporal coherence; and (2) a Non-Linear Deformer,\nwhich decomposes Gaussian maps into view-pose-invariant and view-pose-specific\ncomponents, enabling adaptive, non-linear garment deformations. To establish a\nbenchmark for 4D virtual try-on, we extend existing baselines with unified\nmodules for fair qualitative and quantitative comparisons. Extensive\nexperiments show that AvatarVTON achieves high fidelity, diversity, and dynamic\ngarment realism, making it well-suited for AR/VR, gaming, and digital-human\napplications.",
        "url": "http://arxiv.org/abs/2510.04822v1",
        "published_date": "2025-10-06T14:06:34+00:00",
        "updated_date": "2025-10-06T14:06:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zicheng Jiang",
            "Jixin Gao",
            "Shengfeng He",
            "Xinzhe Li",
            "Yulong Zheng",
            "Zhaotong Yang",
            "Junyu Dong",
            "Yong Du"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "AvatarVTON is a 4D virtual try-on framework for animatable avatars, enabling realistic garment try-on results with free pose control and diverse choices.",
        "tldr_zh": "AvatarVTON是一个为可动化角色设计的4D虚拟试穿框架，可实现真实的服装试穿结果，具有自由的姿势控制和多样的选择。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Did you just see that? Arbitrary view synthesis for egocentric replay of operating room workflows from ambient sensors",
        "summary": "Observing surgical practice has historically relied on fixed vantage points\nor recollections, leaving the egocentric visual perspectives that guide\nclinical decisions undocumented. Fixed-camera video can capture surgical\nworkflows at the room-scale, but cannot reconstruct what each team member\nactually saw. Thus, these videos only provide limited insights into how\ndecisions that affect surgical safety, training, and workflow optimization are\nmade. Here we introduce EgoSurg, the first framework to reconstruct the\ndynamic, egocentric replays for any operating room (OR) staff directly from\nwall-mounted fixed-camera video, and thus, without intervention to clinical\nworkflow. EgoSurg couples geometry-driven neural rendering with diffusion-based\nview enhancement, enabling high-visual fidelity synthesis of arbitrary and\negocentric viewpoints at any moment. In evaluation across multi-site surgical\ncases and controlled studies, EgoSurg reconstructs person-specific visual\nfields and arbitrary viewpoints with high visual quality and fidelity. By\ntransforming existing OR camera infrastructure into a navigable dynamic 3D\nrecord, EgoSurg establishes a new foundation for immersive surgical data\nscience, enabling surgical practice to be visualized, experienced, and analyzed\nfrom every angle.",
        "url": "http://arxiv.org/abs/2510.04802v1",
        "published_date": "2025-10-06T13:35:51+00:00",
        "updated_date": "2025-10-06T13:35:51+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Han Zhang",
            "Lalithkumar Seenivasan",
            "Jose L. Porras",
            "Roger D. Soberanis-Mukul",
            "Hao Ding",
            "Hongchao Shu",
            "Benjamin D. Killeen",
            "Ankita Ghosh",
            "Lonny Yarmus",
            "Masaru Ishii",
            "Angela Christine Argento",
            "Mathias Unberath"
        ],
        "ai_categories": [
            "Transformer",
            "AIGC"
        ],
        "tldr": "The paper introduces EgoSurg, a framework that reconstructs egocentric replays of surgical workflows from fixed-camera videos in operating rooms, allowing for immersive visualization and analysis of surgical practice.",
        "tldr_zh": "本文介绍了EgoSurg，一个框架可以从固定摄像头视频中重建手术室中的个人视角，使手术实践能够以多角度进行可视化和分析。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "A Comparative Study of Vision Transformers and CNNs for Few-Shot Rigid Transformation and Fundamental Matrix Estimation",
        "summary": "Vision-transformers (ViTs) and large-scale convolution-neural-networks (CNNs)\nhave reshaped computer vision through pretrained feature representations that\nenable strong transfer learning for diverse tasks. However, their efficiency as\nbackbone architectures for geometric estimation tasks involving image\ndeformations in low-data regimes remains an open question. This work considers\ntwo such tasks: 1) estimating 2D rigid transformations between pairs of images\nand 2) predicting the fundamental matrix for stereo image pairs, an important\nproblem in various applications, such as autonomous mobility, robotics, and 3D\nscene reconstruction. Addressing this intriguing question, this work\nsystematically compares large-scale CNNs (ResNet, EfficientNet, CLIP-ResNet)\nwith ViT-based foundation models (CLIP-ViT variants and DINO) in various data\nsize settings, including few-shot scenarios. These pretrained models are\noptimized for classification or contrastive learning, encouraging them to focus\nmostly on high-level semantics. The considered tasks require balancing local\nand global features differently, challenging the straightforward adoption of\nthese models as the backbone. Empirical comparative analysis shows that,\nsimilar to training from scratch, ViTs outperform CNNs during refinement in\nlarge downstream-data scenarios. However, in small data scenarios, the\ninductive bias and smaller capacity of CNNs improve their performance, allowing\nthem to match that of a ViT. Moreover, ViTs exhibit stronger generalization in\ncross-domain evaluation where the data distribution changes. These results\nemphasize the importance of carefully selecting model architectures for\nrefinement, motivating future research towards hybrid architectures that\nbalance local and global representations.",
        "url": "http://arxiv.org/abs/2510.04794v1",
        "published_date": "2025-10-06T13:18:27+00:00",
        "updated_date": "2025-10-06T13:18:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Alon Kaya",
            "Igal Bilik",
            "Inna Stainvas"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "This paper compares Vision Transformers and CNNs for geometric estimation tasks in low-data scenarios, showing ViTs outperform CNNs in large data settings but CNNs can match ViTs in small data scenarios.",
        "tldr_zh": "本文比较了视觉Transformer和卷积神经网络在低数据情况下的几何估计任务中的表现，结果表明在大数据情况下ViTs优于CNNs，但在小数据情况下CNNs可以与ViTs匹敌。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ReactDiff: Fundamental Multiple Appropriate Facial Reaction Diffusion Model",
        "summary": "The automatic generation of diverse and human-like facial reactions in dyadic\ndialogue remains a critical challenge for human-computer interaction systems.\nExisting methods fail to model the stochasticity and dynamics inherent in real\nhuman reactions. To address this, we propose ReactDiff, a novel temporal\ndiffusion framework for generating diverse facial reactions that are\nappropriate for responding to any given dialogue context. Our key insight is\nthat plausible human reactions demonstrate smoothness, and coherence over time,\nand conform to constraints imposed by human facial anatomy. To achieve this,\nReactDiff incorporates two vital priors (spatio-temporal facial kinematics)\ninto the diffusion process: i) temporal facial behavioral kinematics and ii)\nfacial action unit dependencies. These two constraints guide the model toward\nrealistic human reaction manifolds, avoiding visually unrealistic jitters,\nunstable transitions, unnatural expressions, and other artifacts. Extensive\nexperiments on the REACT2024 dataset demonstrate that our approach not only\nachieves state-of-the-art reaction quality but also excels in diversity and\nreaction appropriateness.",
        "url": "http://arxiv.org/abs/2510.04712v1",
        "published_date": "2025-10-06T11:30:40+00:00",
        "updated_date": "2025-10-06T11:30:40+00:00",
        "categories": [
            "cs.CV",
            "cs.HC",
            "cs.MM"
        ],
        "authors": [
            "Luo Cheng",
            "Song Siyang",
            "Yan Siyuan",
            "Yu Zhen",
            "Ge Zongyuan"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "ReactDiff proposes a novel temporal diffusion framework for generating diverse and appropriate facial reactions in dialogue contexts by incorporating facial kinematics constraints.",
        "tldr_zh": "ReactDiff提出了一种新颖的时间扩散框架，通过融合面部运动学约束，生成对话环境中多样且合适的面部反应。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion",
        "summary": "Human-centric generative models designed for AI-driven storytelling must\nbring together two core capabilities: identity consistency and precise control\nover human performance. While recent diffusion-based approaches have made\nsignificant progress in maintaining facial identity, achieving fine-grained\nexpression control without compromising identity remains challenging. In this\nwork, we present a diffusion-based framework that faithfully reimagines any\nsubject under any particular facial expression. Building on an ID-consistent\nface foundation model, we adopt a compositional design featuring an expression\ncross-attention module guided by FLAME blendshape parameters for explicit\ncontrol. Trained on a diverse mixture of image and video data rich in\nexpressive variation, our adapter generalizes beyond basic emotions to subtle\nmicro-expressions and expressive transitions, overlooked by prior works. In\naddition, a pluggable Reference Adapter enables expression editing in real\nimages by transferring the appearance from a reference frame during synthesis.\nExtensive quantitative and qualitative evaluations show that our model\noutperforms existing methods in tailored and identity-consistent expression\ngeneration. Code and models can be found at\nhttps://github.com/foivospar/Arc2Face.",
        "url": "http://arxiv.org/abs/2510.04706v1",
        "published_date": "2025-10-06T11:20:56+00:00",
        "updated_date": "2025-10-06T11:20:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Foivos Paraperas Papantoniou",
            "Stefanos Zafeiriou"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a diffusion-based framework for generating human expressions with precise control while maintaining identity consistency.",
        "tldr_zh": "该论文介绍了一种基于扩散的框架，用于以精确控制生成人类表情，同时保持身份一致性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Label-Efficient Cross-Modality Generalization for Liver Segmentation in Multi-Phase MRI",
        "summary": "Accurate liver segmentation in multi-phase MRI is vital for liver fibrosis\nassessment, yet labeled data is often scarce and unevenly distributed across\nimaging modalities and vendor systems. We propose a label-efficient\nsegmentation approach that promotes cross-modality generalization under\nreal-world conditions, where GED4 hepatobiliary-phase annotations are limited,\nnon-contrast sequences (T1WI, T2WI, DWI) are unlabeled, and spatial\nmisalignment and missing phases are common. Our method integrates a\nfoundation-scale 3D segmentation backbone adapted via fine-tuning, co-training\nwith cross pseudo supervision to leverage unlabeled volumes, and a standardized\npreprocessing pipeline. Without requiring spatial registration, the model\nlearns to generalize across MRI phases and vendors, demonstrating robust\nsegmentation performance in both labeled and unlabeled domains. Our results\nexhibit the effectiveness of our proposed label-efficient baseline for liver\nsegmentation in multi-phase, multi-vendor MRI and highlight the potential of\ncombining foundation model adaptation with co-training for real-world clinical\nimaging tasks.",
        "url": "http://arxiv.org/abs/2510.04705v1",
        "published_date": "2025-10-06T11:19:05+00:00",
        "updated_date": "2025-10-06T11:19:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Quang-Khai Bui-Tran",
            "Minh-Toan Dinh",
            "Thanh-Huy Nguyen",
            "Ba-Thinh Lam",
            "Mai-Anh Vu",
            "Ulas Bagci"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper proposes a label-efficient approach for liver segmentation in MRI, addressing limited annotations and cross-modality generalization.",
        "tldr_zh": "该论文提出了一种在MRI中进行肝脏分割的标签高效方法，解决了标注有限和跨模态泛化的问题。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ConceptSplit: Decoupled Multi-Concept Personalization of Diffusion Models via Token-wise Adaptation and Attention Disentanglement",
        "summary": "In recent years, multi-concept personalization for text-to-image (T2I)\ndiffusion models to represent several subjects in an image has gained much more\nattention. The main challenge of this task is \"concept mixing\", where multiple\nlearned concepts interfere or blend undesirably in the output image. To address\nthis issue, in this paper, we present ConceptSplit, a novel framework to split\nthe individual concepts through training and inference. Our framework comprises\ntwo key components. First, we introduce Token-wise Value Adaptation (ToVA), a\nmerging-free training method that focuses exclusively on adapting the value\nprojection in cross-attention. Based on our empirical analysis, we found that\nmodifying the key projection, a common approach in existing methods, can\ndisrupt the attention mechanism and lead to concept mixing. Second, we propose\nLatent Optimization for Disentangled Attention (LODA), which alleviates\nattention entanglement during inference by optimizing the input latent. Through\nextensive qualitative and quantitative experiments, we demonstrate that\nConceptSplit achieves robust multi-concept personalization, mitigating\nunintended concept interference. Code is available at\nhttps://github.com/KU-VGI/ConceptSplit",
        "url": "http://arxiv.org/abs/2510.04668v1",
        "published_date": "2025-10-06T10:22:46+00:00",
        "updated_date": "2025-10-06T10:22:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Habin Lim",
            "Yeongseob Won",
            "Juwon Seo",
            "Gyeong-Moon Park"
        ],
        "ai_categories": [
            "Multimodality",
            "Diffusion"
        ],
        "tldr": "ConceptSplit proposes a framework to address 'concept mixing' in multi-concept personalization for text-to-image models through Token-wise Value Adaptation and Latent Optimization for Disentangled Attention.",
        "tldr_zh": "ConceptSplit提出了一个框架，通过Token-wise Value Adaptation和Latent Optimization for Disentangled Attention来解决文本到图像模型中的“概念混合”问题。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Social Agent: Mastering Dyadic Nonverbal Behavior Generation via Conversational LLM Agents",
        "summary": "We present Social Agent, a novel framework for synthesizing realistic and\ncontextually appropriate co-speech nonverbal behaviors in dyadic conversations.\nIn this framework, we develop an agentic system driven by a Large Language\nModel (LLM) to direct the conversation flow and determine appropriate\ninteractive behaviors for both participants. Additionally, we propose a novel\ndual-person gesture generation model based on an auto-regressive diffusion\nmodel, which synthesizes coordinated motions from speech signals. The output of\nthe agentic system is translated into high-level guidance for the gesture\ngenerator, resulting in realistic movement at both the behavioral and motion\nlevels. Furthermore, the agentic system periodically examines the movements of\ninterlocutors and infers their intentions, forming a continuous feedback loop\nthat enables dynamic and responsive interactions between the two participants.\nUser studies and quantitative evaluations show that our model significantly\nimproves the quality of dyadic interactions, producing natural, synchronized\nnonverbal behaviors.",
        "url": "http://arxiv.org/abs/2510.04637v1",
        "published_date": "2025-10-06T09:41:37+00:00",
        "updated_date": "2025-10-06T09:41:37+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Zeyi Zhang",
            "Yanju Zhou",
            "Heyuan Yao",
            "Tenglong Ao",
            "Xiaohang Zhan",
            "Libin Liu"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "Other"
        ],
        "tldr": "The paper presents a framework called Social Agent for generating realistic and contextually appropriate nonverbal behaviors in dyadic conversations through conversational LLM agents.",
        "tldr_zh": "本文提出了一种名为Social Agent的框架，通过对话LLM代理生成现实和情境适当的双人对话中的非言语行为。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SONA: Learning Conditional, Unconditional, and Mismatching-Aware Discriminator",
        "summary": "Deep generative models have made significant advances in generating complex\ncontent, yet conditional generation remains a fundamental challenge. Existing\nconditional generative adversarial networks often struggle to balance the dual\nobjectives of assessing authenticity and conditional alignment of input samples\nwithin their conditional discriminators. To address this, we propose a novel\ndiscriminator design that integrates three key capabilities: unconditional\ndiscrimination, matching-aware supervision to enhance alignment sensitivity,\nand adaptive weighting to dynamically balance all objectives. Specifically, we\nintroduce Sum of Naturalness and Alignment (SONA), which employs separate\nprojections for naturalness (authenticity) and alignment in the final layer\nwith an inductive bias, supported by dedicated objective functions and an\nadaptive weighting mechanism. Extensive experiments on class-conditional\ngeneration tasks show that \\ours achieves superior sample quality and\nconditional alignment compared to state-of-the-art methods. Furthermore, we\ndemonstrate its effectiveness in text-to-image generation, confirming the\nversatility and robustness of our approach.",
        "url": "http://arxiv.org/abs/2510.04576v1",
        "published_date": "2025-10-06T08:26:06+00:00",
        "updated_date": "2025-10-06T08:26:06+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "stat.ML"
        ],
        "authors": [
            "Yuhta Takida",
            "Satoshi Hayakawa",
            "Takashi Shibuya",
            "Masaaki Imaizumi",
            "Naoki Murata",
            "Bac Nguyen",
            "Toshimitsu Uesaka",
            "Chieh-Hsin Lai",
            "Yuki Mitsufuji"
        ],
        "ai_categories": [
            "GAN"
        ],
        "tldr": "The paper introduces a novel discriminator design called SONA for conditional generation tasks, achieving superior sample quality and conditional alignment compared to existing methods.",
        "tldr_zh": "该论文引入了一种名为SONA的新颖鉴别器设计，用于条件生成任务，在样本质量和条件对齐方面优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Conditional Representation Learning for Customized Tasks",
        "summary": "Conventional representation learning methods learn a universal representation\nthat primarily captures dominant semantics, which may not always align with\ncustomized downstream tasks. For instance, in animal habitat analysis,\nresearchers prioritize scene-related features, whereas universal embeddings\nemphasize categorical semantics, leading to suboptimal results. As a solution,\nexisting approaches resort to supervised fine-tuning, which however incurs high\ncomputational and annotation costs. In this paper, we propose Conditional\nRepresentation Learning (CRL), aiming to extract representations tailored to\narbitrary user-specified criteria. Specifically, we reveal that the semantics\nof a space are determined by its basis, thereby enabling a set of descriptive\nwords to approximate the basis for a customized feature space. Building upon\nthis insight, given a user-specified criterion, CRL first employs a large\nlanguage model (LLM) to generate descriptive texts to construct the semantic\nbasis, then projects the image representation into this conditional feature\nspace leveraging a vision-language model (VLM). The conditional representation\nbetter captures semantics for the specific criterion, which could be utilized\nfor multiple customized tasks. Extensive experiments on classification and\nretrieval tasks demonstrate the superiority and generality of the proposed CRL.\nThe code is available at https://github.com/XLearning-SCU/2025-NeurIPS-CRL.",
        "url": "http://arxiv.org/abs/2510.04564v1",
        "published_date": "2025-10-06T08:00:59+00:00",
        "updated_date": "2025-10-06T08:00:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Honglin Liu",
            "Chao Sun",
            "Peng Hu",
            "Yunfan Li",
            "Xi Peng"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces Conditional Representation Learning (CRL) to tailor representations for specific tasks, improving performance without high computational costs.",
        "tldr_zh": "本文引入条件表示学习（CRL）以为特定任务定制表示，提高性能而不增加高计算成本。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Post-training quantization of vision encoders needs prefixing registers",
        "summary": "Transformer-based vision encoders -- such as CLIP -- are central to\nmultimodal intelligence, powering applications from autonomous web agents to\nrobotic control. Since these applications often demand real-time processing of\nmassive visual data, reducing the inference cost of vision encoders is\ncritical. Post-training quantization offers a practical path, but remains\nchallenging even at 8-bit precision due to massive-scale activations (i.e.,\noutliers). In this work, we propose $\\textit{RegCache}$, a training-free\nalgorithm to mitigate outliers in vision encoders, enabling quantization with\nsignificantly smaller accuracy drops. The proposed RegCache introduces\noutlier-prone yet semantically meaningless prefix tokens to the target vision\nencoder, which prevents other tokens from having outliers. Notably, we observe\nthat outliers in vision encoders behave differently from those in language\nmodels, motivating two technical innovations: middle-layer prefixing and token\ndeletion. Experiments show that our method consistently improves the accuracy\nof quantized models across both text-supervised and self-supervised vision\nencoders.",
        "url": "http://arxiv.org/abs/2510.04547v1",
        "published_date": "2025-10-06T07:27:46+00:00",
        "updated_date": "2025-10-06T07:27:46+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Seunghyeon Kim",
            "Jinho Kim",
            "Taesun Yeom",
            "Wonpyo Park",
            "Kyuyeun Kim",
            "Jaeho Lee"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality"
        ],
        "tldr": "The paper introduces a new algorithm called RegCache to reduce outliers in vision encoders, enabling better quantization with smaller accuracy drops.",
        "tldr_zh": "本文介绍了一种名为 RegCache 的新算法，用于减少视觉编码器中的异常值，从而实现更好的量化效果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "C3Editor: Achieving Controllable Consistency in 2D Model for 3D Editing",
        "summary": "Existing 2D-lifting-based 3D editing methods often encounter challenges\nrelated to inconsistency, stemming from the lack of view-consistent 2D editing\nmodels and the difficulty of ensuring consistent editing across multiple views.\nTo address these issues, we propose C3Editor, a controllable and consistent\n2D-lifting-based 3D editing framework. Given an original 3D representation and\na text-based editing prompt, our method selectively establishes a\nview-consistent 2D editing model to achieve superior 3D editing results. The\nprocess begins with the controlled selection of a ground truth (GT) view and\nits corresponding edited image as the optimization target, allowing for\nuser-defined manual edits. Next, we fine-tune the 2D editing model within the\nGT view and across multiple views to align with the GT-edited image while\nensuring multi-view consistency. To meet the distinct requirements of GT view\nfitting and multi-view consistency, we introduce separate LoRA modules for\ntargeted fine-tuning. Our approach delivers more consistent and controllable 2D\nand 3D editing results than existing 2D-lifting-based methods, outperforming\nthem in both qualitative and quantitative evaluations.",
        "url": "http://arxiv.org/abs/2510.04539v1",
        "published_date": "2025-10-06T07:07:14+00:00",
        "updated_date": "2025-10-06T07:07:14+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Zeng Tao",
            "Zheng Ding",
            "Zeyuan Chen",
            "Xiang Zhang",
            "Leizhi Li",
            "Zhuowen Tu"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "C3Editor proposes a 2D-lifting-based 3D editing framework to achieve controllable consistency and superior editing results compared to existing methods.",
        "tldr_zh": "C3Editor提出了一种基于2D-lifting的3D编辑框架，以实现可控的一致性和优越的编辑结果，相较于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "3Dify: a Framework for Procedural 3D-CG Generation Assisted by LLMs Using MCP and RAG",
        "summary": "This paper proposes \"3Dify,\" a procedural 3D computer graphics (3D-CG)\ngeneration framework utilizing Large Language Models (LLMs). The framework\nenables users to generate 3D-CG content solely through natural language\ninstructions. 3Dify is built upon Dify, an open-source platform for AI\napplication development, and incorporates several state-of-the-art LLM-related\ntechnologies such as the Model Context Protocol (MCP) and Retrieval-Augmented\nGeneration (RAG). For 3D-CG generation support, 3Dify automates the operation\nof various Digital Content Creation (DCC) tools via MCP. When DCC tools do not\nsupport MCP-based interaction, the framework employs the Computer-Using Agent\n(CUA) method to automate Graphical User Interface (GUI) operations. Moreover,\nto enhance image generation quality, 3Dify allows users to provide feedback by\nselecting preferred images from multiple candidates. The LLM then learns\nvariable patterns from these selections and applies them to subsequent\ngenerations. Furthermore, 3Dify supports the integration of locally deployed\nLLMs, enabling users to utilize custom-developed models and to reduce both time\nand monetary costs associated with external API calls by leveraging their own\ncomputational resources.",
        "url": "http://arxiv.org/abs/2510.04536v1",
        "published_date": "2025-10-06T07:00:06+00:00",
        "updated_date": "2025-10-06T07:00:06+00:00",
        "categories": [
            "cs.GR",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Shun-ichiro Hayashi",
            "Daichi Mukunoki",
            "Tetsuya Hoshino",
            "Satoshi Ohshima",
            "Takahiro Katagiri"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "GAN"
        ],
        "tldr": "3Dify is a framework for generating 3D computer graphics using natural language instructions and incorporating advanced technologies like MCP and RAG.",
        "tldr_zh": "3Dify是一个利用自然语言指令生成3D计算机图形的框架，包括MCP和RAG等先进技术。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion Sampling",
        "summary": "Recent diffusion models achieve the state-of-the-art performance in image\ngeneration, but often suffer from semantic inconsistencies or hallucinations.\nWhile various inference-time guidance methods can enhance generation, they\noften operate indirectly by relying on external signals or architectural\nmodifications, which introduces additional computational overhead. In this\npaper, we propose Tangential Amplifying Guidance (TAG), a more efficient and\ndirect guidance method that operates solely on trajectory signals without\nmodifying the underlying diffusion model. TAG leverages an intermediate sample\nas a projection basis and amplifies the tangential components of the estimated\nscores with respect to this basis to correct the sampling trajectory. We\nformalize this guidance process by leveraging a first-order Taylor expansion,\nwhich demonstrates that amplifying the tangential component steers the state\ntoward higher-probability regions, thereby reducing inconsistencies and\nenhancing sample quality. TAG is a plug-and-play, architecture-agnostic module\nthat improves diffusion sampling fidelity with minimal computational addition,\noffering a new perspective on diffusion guidance.",
        "url": "http://arxiv.org/abs/2510.04533v1",
        "published_date": "2025-10-06T06:53:29+00:00",
        "updated_date": "2025-10-06T06:53:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hyunmin Cho",
            "Donghoon Ahn",
            "Susung Hong",
            "Jee Eun Kim",
            "Seungryong Kim",
            "Kyong Hwan Jin"
        ],
        "ai_categories": [
            "Diffusion"
        ],
        "tldr": "The paper introduces TAG, a guidance method that improves image generation by correcting sampling trajectories, reducing inconsistencies and enhancing sample quality.",
        "tldr_zh": "本文介绍了TAG，一种通过纠正采样轨迹来改善图像生成的引导方法，减少不一致性并提高样本质量。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TBStar-Edit: From Image Editing Pattern Shifting to Consistency Enhancement",
        "summary": "Recent advances in image generation and editing technologies have enabled\nstate-of-the-art models to achieve impressive results in general domains.\nHowever, when applied to e-commerce scenarios, these general models often\nencounter consistency limitations. To address this challenge, we introduce\nTBStar-Edit, an new image editing model tailored for the e-commerce domain.\nThrough rigorous data engineering, model architecture design and training\nstrategy, TBStar-Edit achieves precise and high-fidelity image editing while\nmaintaining the integrity of product appearance and layout. Specifically, for\ndata engineering, we establish a comprehensive data construction pipeline,\nencompassing data collection, construction, filtering, and augmentation, to\nacquire high-quality, instruction-following, and strongly consistent editing\ndata to support model training. For model architecture design, we design a\nhierarchical model framework consisting of a base model, pattern shifting\nmodules, and consistency enhancement modules. For model training, we adopt a\ntwo-stage training strategy to enhance the consistency preservation: first\nstage for editing pattern shifting, and second stage for consistency\nenhancement. Each stage involves training different modules with separate\ndatasets. Finally, we conduct extensive evaluations of TBStar-Edit on a\nself-proposed e-commerce benchmark, and the results demonstrate that\nTBStar-Edit outperforms existing general-domain editing models in both\nobjective metrics (VIE Score) and subjective user preference.",
        "url": "http://arxiv.org/abs/2510.04483v1",
        "published_date": "2025-10-06T04:46:42+00:00",
        "updated_date": "2025-10-06T04:46:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hao Fang",
            "Zechao Zhan",
            "Weixin Feng",
            "Ziwei Huang",
            "XuBin Li",
            "Tiezheng Ge"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "TBStar-Edit is a new image editing model specifically designed for e-commerce, achieving precise editing while preserving product appearance and layout consistency.",
        "tldr_zh": "TBStar-Edit是一种专为电子商务设计的新型图像编辑模型，实现了精确的编辑同时保持产品外观和布局的一致性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VaseVQA-3D: Benchmarking 3D VLMs on Ancient Greek Pottery",
        "summary": "Vision-Language Models (VLMs) have achieved significant progress in\nmultimodal understanding tasks, demonstrating strong capabilities particularly\nin general tasks such as image captioning and visual reasoning. However, when\ndealing with specialized cultural heritage domains like 3D vase artifacts,\nexisting models face severe data scarcity issues and insufficient domain\nknowledge limitations. Due to the lack of targeted training data, current VLMs\nstruggle to effectively handle such culturally significant specialized tasks.\nTo address these challenges, we propose the VaseVQA-3D dataset, which serves as\nthe first 3D visual question answering dataset for ancient Greek pottery\nanalysis, collecting 664 ancient Greek vase 3D models with corresponding\nquestion-answer data and establishing a complete data construction pipeline. We\nfurther develop the VaseVLM model, enhancing model performance in vase artifact\nanalysis through domain-adaptive training. Experimental results validate the\neffectiveness of our approach, where we improve by 12.8% on R@1 metrics and by\n6.6% on lexical similarity compared with previous state-of-the-art on the\nVaseVQA-3D dataset, significantly improving the recognition and understanding\nof 3D vase artifacts, providing new technical pathways for digital heritage\npreservation research.",
        "url": "http://arxiv.org/abs/2510.04479v1",
        "published_date": "2025-10-06T04:28:39+00:00",
        "updated_date": "2025-10-06T04:28:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nonghai Zhang",
            "Zeyu Zhang",
            "Jiazi Wang",
            "Yang Zhao",
            "Hao Tang"
        ],
        "ai_categories": [
            "Dataset",
            "Multimodality"
        ],
        "tldr": "This paper introduces the VaseVQA-3D dataset and VaseVLM model to improve the recognition and understanding of 3D vase artifacts in ancient Greek pottery.",
        "tldr_zh": "本文介绍了VaseVQA-3D数据集和VaseVLM模型，以提高对古希腊陶器中3D花瓶文物的识别和理解。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "REAR: Rethinking Visual Autoregressive Models via Generator-Tokenizer Consistency Regularization",
        "summary": "Visual autoregressive (AR) generation offers a promising path toward unifying\nvision and language models, yet its performance remains suboptimal against\ndiffusion models. Prior work often attributes this gap to tokenizer limitations\nand rasterization ordering. In this work, we identify a core bottleneck from\nthe perspective of generator-tokenizer inconsistency, i.e., the AR-generated\ntokens may not be well-decoded by the tokenizer. To address this, we propose\nreAR, a simple training strategy introducing a token-wise regularization\nobjective: when predicting the next token, the causal transformer is also\ntrained to recover the visual embedding of the current token and predict the\nembedding of the target token under a noisy context. It requires no changes to\nthe tokenizer, generation order, inference pipeline, or external models.\nDespite its simplicity, reAR substantially improves performance. On ImageNet,\nit reduces gFID from 3.02 to 1.86 and improves IS to 316.9 using a standard\nrasterization-based tokenizer. When applied to advanced tokenizers, it achieves\na gFID of 1.42 with only 177M parameters, matching the performance with larger\nstate-of-the-art diffusion models (675M).",
        "url": "http://arxiv.org/abs/2510.04450v1",
        "published_date": "2025-10-06T02:48:13+00:00",
        "updated_date": "2025-10-06T02:48:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qiyuan He",
            "Yicong Li",
            "Haotian Ye",
            "Jinghao Wang",
            "Xinyao Liao",
            "Pheng-Ann Heng",
            "Stefano Ermon",
            "James Zou",
            "Angela Yao"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper introduces reAR, a training strategy to improve visual autoregressive models by addressing generator-tokenizer inconsistency, significantly enhancing performance on ImageNet.",
        "tldr_zh": "本文介绍了reAR，一种训练策略，通过解决生成器-标记器不一致性来改善视觉自回归模型，在ImageNet上显著提高性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Partial Information Decomposition via Normalizing Flows in Latent Gaussian Distributions",
        "summary": "The study of multimodality has garnered significant interest in fields where\nthe analysis of interactions among multiple information sources can enhance\npredictive modeling, data fusion, and interpretability. Partial information\ndecomposition (PID) has emerged as a useful information-theoretic framework to\nquantify the degree to which individual modalities independently, redundantly,\nor synergistically convey information about a target variable. However,\nexisting PID methods depend on optimizing over a joint distribution constrained\nby estimated pairwise probability distributions, which are costly and\ninaccurate for continuous and high-dimensional modalities. Our first key\ninsight is that the problem can be solved efficiently when the pairwise\ndistributions are multivariate Gaussians, and we refer to this problem as\nGaussian PID (GPID). We propose a new gradient-based algorithm that\nsubstantially improves the computational efficiency of GPID based on an\nalternative formulation of the underlying optimization problem. To generalize\nthe applicability to non-Gaussian data, we learn information-preserving\nencoders to transform random variables of arbitrary input distributions into\npairwise Gaussian random variables. Along the way, we resolved an open problem\nregarding the optimality of joint Gaussian solutions for GPID. Empirical\nvalidation in diverse synthetic examples demonstrates that our proposed method\nprovides more accurate and efficient PID estimates than existing baselines. We\nfurther evaluate a series of large-scale multimodal benchmarks to show its\nutility in real-world applications of quantifying PID in multimodal datasets\nand selecting high-performing models.",
        "url": "http://arxiv.org/abs/2510.04417v1",
        "published_date": "2025-10-06T01:08:34+00:00",
        "updated_date": "2025-10-06T01:08:34+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.IT",
            "math.IT"
        ],
        "authors": [
            "Wenyuan Zhao",
            "Adithya Balachandran",
            "Chao Tian",
            "Paul Pu Liang"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a new method, Gaussian PID, for efficiently decomposing partial information in multimodal data using Gaussian distributions. The method outperforms existing approaches in accuracy and computational efficiency.",
        "tldr_zh": "该论文引入了一种新方法，高斯PID，用于使用高斯分布在多模态数据中高效分解部分信息。该方法在准确性和计算效率方面优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CodeFormer++: Blind Face Restoration Using Deformable Registration and Deep Metric Learning",
        "summary": "Blind face restoration (BFR) has attracted increasing attention with the rise\nof generative methods. Most existing approaches integrate generative priors\ninto the restoration pro- cess, aiming to jointly address facial detail\ngeneration and identity preservation. However, these methods often suffer from\na trade-off between visual quality and identity fidelity, leading to either\nidentity distortion or suboptimal degradation removal. In this paper, we\npresent CodeFormer++, a novel framework that maximizes the utility of\ngenerative priors for high-quality face restoration while preserving identity.\nWe decompose BFR into three sub-tasks: (i) identity- preserving face\nrestoration, (ii) high-quality face generation, and (iii) dynamic fusion of\nidentity features with realistic texture details. Our method makes three key\ncontributions: (1) a learning-based deformable face registration module that\nsemantically aligns generated and restored faces; (2) a texture guided\nrestoration network to dynamically extract and transfer the texture of\ngenerated face to boost the quality of identity-preserving restored face; and\n(3) the integration of deep metric learning for BFR with the generation of\ninformative positive and hard negative samples to better fuse identity-\npreserving and generative features. Extensive experiments on real-world and\nsynthetic datasets demonstrate that, the pro- posed CodeFormer++ achieves\nsuperior performance in terms of both visual fidelity and identity consistency.",
        "url": "http://arxiv.org/abs/2510.04410v1",
        "published_date": "2025-10-06T00:53:50+00:00",
        "updated_date": "2025-10-06T00:53:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Venkata Bharath Reddy Reddem",
            "Akshay P Sarashetti",
            "Ranjith Merugu",
            "Amit Satish Unde"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces CodeFormer++, a framework for blind face restoration that prioritizes high-quality restoration while preserving identity through deformable registration and deep metric learning.",
        "tldr_zh": "本文介绍了CodeFormer++，一个盲面部恢复的框架，通过可变形配准和深度度量学习优先考虑高质量的恢复，同时保留身份信息。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GenAR: Next-Scale Autoregressive Generation for Spatial Gene Expression Prediction",
        "summary": "Spatial Transcriptomics (ST) offers spatially resolved gene expression but\nremains costly. Predicting expression directly from widely available\nHematoxylin and Eosin (H&E) stained images presents a cost-effective\nalternative. However, most computational approaches (i) predict each gene\nindependently, overlooking co-expression structure, and (ii) cast the task as\ncontinuous regression despite expression being discrete counts. This mismatch\ncan yield biologically implausible outputs and complicate downstream analyses.\nWe introduce GenAR, a multi-scale autoregressive framework that refines\npredictions from coarse to fine. GenAR clusters genes into hierarchical groups\nto expose cross-gene dependencies, models expression as codebook-free discrete\ntoken generation to directly predict raw counts, and conditions decoding on\nfused histological and spatial embeddings. From an information-theoretic\nperspective, the discrete formulation avoids log-induced biases and the\ncoarse-to-fine factorization aligns with a principled conditional\ndecomposition. Extensive experimental results on four Spatial Transcriptomics\ndatasets across different tissue types demonstrate that GenAR achieves\nstate-of-the-art performance, offering potential implications for precision\nmedicine and cost-effective molecular profiling. Code is publicly available at\nhttps://github.com/oyjr/genar.",
        "url": "http://arxiv.org/abs/2510.04315v1",
        "published_date": "2025-10-05T18:28:21+00:00",
        "updated_date": "2025-10-05T18:28:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiarui Ouyang",
            "Yihui Wang",
            "Yihang Gao",
            "Yingxue Xu",
            "Shu Yang",
            "Hao Chen"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "GenAR introduces a multi-scale autoregressive framework for predicting spatial gene expression from H&E images, achieving state-of-the-art performance across different tissue types.",
        "tldr_zh": "GenAR 提出了一种多尺度自回归框架，用于从H&E图像预测空间基因表达，实现了跨不同组织类型的最先进性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Beyond Random: Automatic Inner-loop Optimization in Dataset Distillation",
        "summary": "The growing demand for efficient deep learning has positioned dataset\ndistillation as a pivotal technique for compressing training dataset while\npreserving model performance. However, existing inner-loop optimization methods\nfor dataset distillation typically rely on random truncation strategies, which\nlack flexibility and often yield suboptimal results. In this work, we observe\nthat neural networks exhibit distinct learning dynamics across different\ntraining stages-early, middle, and late-making random truncation ineffective.\nTo address this limitation, we propose Automatic Truncated Backpropagation\nThrough Time (AT-BPTT), a novel framework that dynamically adapts both\ntruncation positions and window sizes according to intrinsic gradient behavior.\nAT-BPTT introduces three key components: (1) a probabilistic mechanism for\nstage-aware timestep selection, (2) an adaptive window sizing strategy based on\ngradient variation, and (3) a low-rank Hessian approximation to reduce\ncomputational overhead. Extensive experiments on CIFAR-10, CIFAR-100,\nTiny-ImageNet, and ImageNet-1K show that AT-BPTT achieves state-of-the-art\nperformance, improving accuracy by an average of 6.16% over baseline methods.\nMoreover, our approach accelerates inner-loop optimization by 3.9x while saving\n63% memory cost.",
        "url": "http://arxiv.org/abs/2510.04838v1",
        "published_date": "2025-10-06T14:22:28+00:00",
        "updated_date": "2025-10-06T14:22:28+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Muquan Li",
            "Hang Gou",
            "Dongyang Zhang",
            "Shuang Liang",
            "Xiurui Xie",
            "Deqiang Ouyang",
            "Ke Qin"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper introduces Automatic Truncated Backpropagation Through Time (AT-BPTT), a framework for inner-loop optimization in dataset distillation, achieving state-of-the-art performance and accelerated optimization.",
        "tldr_zh": "本文介绍了自动截断时间反向传播（AT-BPTT），这是一个用于数据集精简中的内循环优化框架，实现了最新技术的性能和加速优化。",
        "relevance_score": 6,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7.75
    },
    {
        "title": "Neuroplastic Modular Framework: Cross-Domain Image Classification of Garbage and Industrial Surfaces",
        "summary": "Efficient and accurate classification of waste and industrial surface defects\nis essential for ensuring sustainable waste management and maintaining high\nstandards in quality control. This paper introduces the Neuroplastic Modular\nClassifier, a novel hybrid architecture designed for robust and adaptive image\nclassification in dynamic environments. The model combines a ResNet-50 backbone\nfor localized feature extraction with a Vision Transformer (ViT) to capture\nglobal semantic context. Additionally, FAISS-based similarity retrieval is\nincorporated to provide a memory-like reference to previously encountered data,\nenriching the model's feature space. A key innovation of our architecture is\nthe neuroplastic modular design composed of expandable, learnable blocks that\ndynamically grow during training when performance plateaus. Inspired by\nbiological learning systems, this mechanism allows the model to adapt to data\ncomplexity over time, improving generalization. Beyond garbage classification,\nwe validate the model on the Kolektor Surface Defect Dataset 2 (KolektorSDD2),\nwhich involves industrial defect detection on metal surfaces. Experimental\nresults across domains show that the proposed architecture outperforms\ntraditional static models in both accuracy and adaptability. The Neuroplastic\nModular Classifier offers a scalable, high-performance solution for real-world\nimage classification, with strong applicability in both environmental and\nindustrial domains.",
        "url": "http://arxiv.org/abs/2510.05071v1",
        "published_date": "2025-10-06T17:47:45+00:00",
        "updated_date": "2025-10-06T17:47:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Debojyoti Ghosh",
            "Soumya K Ghosh",
            "Adrijit Goswami"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a Neuroplastic Modular Classifier for image classification of waste and industrial surface defects, showcasing improved adaptability and accuracy compared to traditional models.",
        "tldr_zh": "本文引入了一种神经可塑性模块分类器，用于对垃圾和工业表面缺陷进行图像分类，展示出比传统模型更好的适应性和准确性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "SegMASt3R: Geometry Grounded Segment Matching",
        "summary": "Segment matching is an important intermediate task in computer vision that\nestablishes correspondences between semantically or geometrically coherent\nregions across images. Unlike keypoint matching, which focuses on localized\nfeatures, segment matching captures structured regions, offering greater\nrobustness to occlusions, lighting variations, and viewpoint changes. In this\npaper, we leverage the spatial understanding of 3D foundation models to tackle\nwide-baseline segment matching, a challenging setting involving extreme\nviewpoint shifts. We propose an architecture that uses the inductive bias of\nthese 3D foundation models to match segments across image pairs with up to 180\ndegree view-point change. Extensive experiments show that our approach\noutperforms state-of-the-art methods, including the SAM2 video propagator and\nlocal feature matching methods, by upto 30% on the AUPRC metric, on ScanNet++\nand Replica datasets. We further demonstrate benefits of the proposed model on\nrelevant downstream tasks, including 3D instance segmentation and image-goal\nnavigation. Project Page: https://segmast3r.github.io/",
        "url": "http://arxiv.org/abs/2510.05051v1",
        "published_date": "2025-10-06T17:31:32+00:00",
        "updated_date": "2025-10-06T17:31:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rohit Jayanti",
            "Swayam Agrawal",
            "Vansh Garg",
            "Siddharth Tourani",
            "Muhammad Haris Khan",
            "Sourav Garg",
            "Madhava Krishna"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces SegMASt3R, a method for segment matching in computer vision using 3D foundation models to handle extreme viewpoint shifts. It outperforms existing methods by up to 30% on certain metrics.",
        "tldr_zh": "该论文介绍了SegMASt3R，一种在计算机视觉中使用3D基础模型处理极端视角变化的分段匹配方法。在某些指标上超过现有方法高达30%。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "Beyond Appearance: Transformer-based Person Identification from Conversational Dynamics",
        "summary": "This paper investigates the performance of transformer-based architectures\nfor person identification in natural, face-to-face conversation scenario. We\nimplement and evaluate a two-stream framework that separately models spatial\nconfigurations and temporal motion patterns of 133 COCO WholeBody keypoints,\nextracted from a subset of the CANDOR conversational corpus. Our experiments\ncompare pre-trained and from-scratch training, investigate the use of velocity\nfeatures, and introduce a multi-scale temporal transformer for hierarchical\nmotion modeling. Results demonstrate that domain-specific training\nsignificantly outperforms transfer learning, and that spatial configurations\ncarry more discriminative information than temporal dynamics. The spatial\ntransformer achieves 95.74% accuracy, while the multi-scale temporal\ntransformer achieves 93.90%. Feature-level fusion pushes performance to 98.03%,\nconfirming that postural and dynamic information are complementary. These\nfindings highlight the potential of transformer architectures for person\nidentification in natural interactions and provide insights for future\nmultimodal and cross-cultural studies.",
        "url": "http://arxiv.org/abs/2510.04753v1",
        "published_date": "2025-10-06T12:31:15+00:00",
        "updated_date": "2025-10-06T12:31:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Masoumeh Chapariniya",
            "Teodora Vukovic",
            "Sarah Ebling",
            "Volker Dellwo"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper explores using transformer-based models for person identification in conversational dynamics, showing that domain-specific training outperforms transfer learning and spatial configurations are more informative than temporal dynamics.",
        "tldr_zh": "本文探讨了使用基于Transformer的模型进行对话动力学中的人员识别，结果表明领域特定训练优于迁移学习，空间配置比时间动态更具信息性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "A Spatial-Spectral-Frequency Interactive Network for Multimodal Remote Sensing Classification",
        "summary": "Deep learning-based methods have achieved significant success in remote\nsensing Earth observation data analysis. Numerous feature fusion techniques\naddress multimodal remote sensing image classification by integrating global\nand local features. However, these techniques often struggle to extract\nstructural and detail features from heterogeneous and redundant multimodal\nimages. With the goal of introducing frequency domain learning to model key and\nsparse detail features, this paper introduces the spatial-spectral-frequency\ninteraction network (S$^2$Fin), which integrates pairwise fusion modules across\nthe spatial, spectral, and frequency domains. Specifically, we propose a\nhigh-frequency sparse enhancement transformer that employs sparse\nspatial-spectral attention to optimize the parameters of the high-frequency\nfilter. Subsequently, a two-level spatial-frequency fusion strategy is\nintroduced, comprising an adaptive frequency channel module that fuses\nlow-frequency structures with enhanced high-frequency details, and a\nhigh-frequency resonance mask that emphasizes sharp edges via phase similarity.\nIn addition, a spatial-spectral attention fusion module further enhances\nfeature extraction at intermediate layers of the network. Experiments on four\nbenchmark multimodal datasets with limited labeled data demonstrate that\nS$^2$Fin performs superior classification, outperforming state-of-the-art\nmethods. The code is available at https://github.com/HaoLiu-XDU/SSFin.",
        "url": "http://arxiv.org/abs/2510.04628v1",
        "published_date": "2025-10-06T09:33:35+00:00",
        "updated_date": "2025-10-06T09:33:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hao Liu",
            "Yunhao Gao",
            "Wei Li",
            "Mingyang Zhang",
            "Maoguo Gong",
            "Lorenzo Bruzzone"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a spatial-spectral-frequency interactive network for multimodal remote sensing classification, outperforming state-of-the-art methods on benchmark datasets.",
        "tldr_zh": "本文介绍了一种用于多模式遥感分类的空间-光谱-频率交互网络，在基准数据集上表现优于最先进方法。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "Real-time Prediction of Urban Sound Propagation with Conditioned Normalizing Flows",
        "summary": "Accurate and fast urban noise prediction is pivotal for public health and for\nregulatory workflows in cities, where the Environmental Noise Directive\nmandates regular strategic noise maps and action plans, often needed in\npermission workflows, right-of-way allocation, and construction scheduling.\nPhysics-based solvers are too slow for such time-critical, iterative \"what-if\"\nstudies. We evaluate conditional Normalizing Flows (Full-Glow) for generating\nfor generating standards-compliant urban sound-pressure maps from 2D urban\nlayouts in real time per 256x256 map on a single RTX 4090), enabling\ninteractive exploration directly on commodity hardware. On datasets covering\nBaseline, Diffraction, and Reflection regimes, our model accelerates map\ngeneration by >2000 times over a reference solver while improving NLoS accuracy\nby up to 24% versus prior deep models; in Baseline NLoS we reach 0.65 dB MAE\nwith high structural fidelity. The model reproduces diffraction and\ninterference patterns and supports instant recomputation under source or\ngeometry changes, making it a practical engine for urban planning, compliance\nmapping, and operations (e.g., temporary road closures, night-work variance\nassessments).",
        "url": "http://arxiv.org/abs/2510.04510v1",
        "published_date": "2025-10-06T06:00:08+00:00",
        "updated_date": "2025-10-06T06:00:08+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Achim Eckerle",
            "Martin Spitznagel",
            "Janis Keuper"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Diffusion"
        ],
        "tldr": "The paper presents a method using conditioned normalizing flows for real-time prediction of urban sound propagation, enabling fast generation of sound-pressure maps for urban planning purposes.",
        "tldr_zh": "该论文提出了一种使用条件归一化流进行城市声音传播实时预测的方法，能够快速生成用于城市规划目的的声压地图。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "A.I.R.: Enabling Adaptive, Iterative, and Reasoning-based Frame Selection For Video Question Answering",
        "summary": "Effectively applying Vision-Language Models (VLMs) to Video Question\nAnswering (VideoQA) hinges on selecting a concise yet comprehensive set of\nframes, as processing entire videos is computationally infeasible. However,\ncurrent frame selection methods face a critical trade-off: approaches relying\non lightweight similarity models, such as CLIP, often fail to capture the\nnuances of complex queries, resulting in inaccurate similarity scores that\ncannot reflect the authentic query-frame relevance, which further undermines\nframe selection. Meanwhile, methods that leverage a VLM for deeper analysis\nachieve higher accuracy but incur prohibitive computational costs. To address\nthese limitations, we propose A.I.R., a training-free approach for Adaptive,\nIterative, and Reasoning-based frame selection. We leverage a powerful VLM to\nperform deep, semantic analysis on complex queries, and this analysis is\ndeployed within a cost-effective iterative loop that processes only a small\nbatch of the most high-potential frames at a time. Extensive experiments on\nvarious VideoQA benchmarks demonstrate that our approach outperforms existing\nframe selection methods, significantly boosts the performance of the foundation\nVLM, and achieves substantial gains in computational efficiency over other\nVLM-based techniques.",
        "url": "http://arxiv.org/abs/2510.04428v1",
        "published_date": "2025-10-06T01:51:13+00:00",
        "updated_date": "2025-10-06T01:51:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuanhao Zou",
            "Shengji Jin",
            "Andong Deng",
            "Youpeng Zhao",
            "Jun Wang",
            "Chen Chen"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality"
        ],
        "tldr": "The paper proposes a new approach called A.I.R. for frame selection in Video Question Answering, outperforming existing methods in accuracy and computational efficiency.",
        "tldr_zh": "该论文提出了一种名为A.I.R.的新方法，用于视频问答中的帧选择，其在准确性和计算效率方面优于现有方法。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Your Vision-Language Model Can't Even Count to 20: Exposing the Failures of VLMs in Compositional Counting",
        "summary": "Vision-Language Models (VLMs) have become a central focus of today's AI\ncommunity, owing to their impressive abilities gained from training on\nlarge-scale vision-language data from the Web. These models have demonstrated\nstrong performance across diverse tasks, including image understanding, video\nunderstanding, complex visual reasoning, and embodied AI. Despite these\nnoteworthy successes, a fundamental question remains: Can VLMs count objects\ncorrectly? In this paper, we introduce a simple yet effective benchmark,\nVLMCountBench, designed under a minimalist setting with only basic geometric\nshapes (e.g., triangles, circles) and their compositions, focusing exclusively\non counting tasks without interference from other factors. We adopt strict\nindependent variable control and systematically study the effects of simple\nproperties such as color, size, and prompt refinement in a controlled ablation.\nOur empirical results reveal that while VLMs can count reliably when only one\nshape type is present, they exhibit substantial failures when multiple shape\ntypes are combined (i.e., compositional counting). This highlights a\nfundamental empirical limitation of current VLMs and motivates important\ndirections for future research.",
        "url": "http://arxiv.org/abs/2510.04401v1",
        "published_date": "2025-10-06T00:11:24+00:00",
        "updated_date": "2025-10-06T00:11:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xuyang Guo",
            "Zekai Huang",
            "Zhenmei Shi",
            "Zhao Song",
            "Jiahao Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper exposes the failures of Vision-Language Models in compositional counting tasks by introducing a new benchmark focused on counting geometric shapes.",
        "tldr_zh": "本文通过引入一个新的基准测试，重点关注几何形状的计数，揭示了视觉-语言模型在组合计数任务中的失败。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7.5
    },
    {
        "title": "Unsupervised Active Learning via Natural Feature Progressive Framework",
        "summary": "The effectiveness of modern deep learning models is predicated on the\navailability of large-scale, human-annotated datasets, a process that is\nnotoriously expensive and time-consuming. While Active Learning (AL) offers a\nstrategic solution by labeling only the most informative and representative\ndata, its iterative nature still necessitates significant human involvement.\nUnsupervised Active Learning (UAL) presents an alternative by shifting the\nannotation burden to a single, post-selection step. Unfortunately, prevailing\nUAL methods struggle to achieve state-of-the-art performance. These approaches\ntypically rely on local, gradient-based scoring for sample importance\nestimation, which not only makes them vulnerable to ambiguous and noisy data\nbut also hinders their capacity to select samples that adequately represent the\nfull data distribution. Moreover, their use of shallow, one-shot linear\nselection falls short of a true UAL paradigm. In this paper, we propose the\nNatural Feature Progressive Framework (NFPF), a UAL method that revolutionizes\nhow sample importance is measured. At its core, NFPF employs a Specific Feature\nLearning Machine (SFLM) to effectively quantify each sample's contribution to\nmodel performance. We further utilize the SFLM to define a powerful\nReconstruction Difference metric for initial sample selection. Our\ncomprehensive experiments show that NFPF significantly outperforms all\nestablished UAL methods and achieves performance on par with supervised AL\nmethods on vision datasets. Detailed ablation studies and qualitative\nvisualizations provide compelling evidence for NFPF's superior performance,\nenhanced robustness, and improved data distribution coverage.",
        "url": "http://arxiv.org/abs/2510.04939v1",
        "published_date": "2025-10-06T15:44:33+00:00",
        "updated_date": "2025-10-06T15:44:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Yuxi Liu",
            "Catherine Lalman",
            "Yimin Yang"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a new Unsupervised Active Learning method called NFPF, which outperforms established methods and achieves results comparable to supervised AL methods on vision datasets.",
        "tldr_zh": "本文介绍了一种名为NFPF的新型无监督主动学习方法，该方法优于已有方法，在视觉数据集上实现了与监督学习方法可比的结果。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "In-Field Mapping of Grape Yield and Quality with Illumination-Invariant Deep Learning",
        "summary": "This paper presents an end-to-end, IoT-enabled robotic system for the\nnon-destructive, real-time, and spatially-resolved mapping of grape yield and\nquality (Brix, Acidity) in vineyards. The system features a comprehensive\nanalytical pipeline that integrates two key modules: a high-performance model\nfor grape bunch detection and weight estimation, and a novel deep learning\nframework for quality assessment from hyperspectral (HSI) data. A critical\nbarrier to in-field HSI is the ``domain shift\" caused by variable illumination.\nTo overcome this, our quality assessment is powered by the Light-Invariant\nSpectral Autoencoder (LISA), a domain-adversarial framework that learns\nillumination-invariant features from uncalibrated data. We validated the\nsystem's robustness on a purpose-built HSI dataset spanning three distinct\nillumination domains: controlled artificial lighting (lab), and variable\nnatural sunlight captured in the morning and afternoon. Results show the\ncomplete pipeline achieves a recall (0.82) for bunch detection and a $R^2$\n(0.76) for weight prediction, while the LISA module improves quality prediction\ngeneralization by over 20% compared to the baselines. By combining these robust\nmodules, the system successfully generates high-resolution, georeferenced data\nof both grape yield and quality, providing actionable, data-driven insights for\nprecision viticulture.",
        "url": "http://arxiv.org/abs/2510.04864v1",
        "published_date": "2025-10-06T14:51:24+00:00",
        "updated_date": "2025-10-06T14:51:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ciem Cornelissen",
            "Sander De Coninck",
            "Axel Willekens",
            "Sam Leroux",
            "Pieter Simoens"
        ],
        "ai_categories": [
            "Multimodality",
            "GAN",
            "Dataset"
        ],
        "tldr": "This paper introduces a robotic system for mapping grape yield and quality in vineyards using deep learning and hyperspectral imaging technology.",
        "tldr_zh": "本文介绍了一种使用深度学习和高光谱成像技术在葡萄园中进行葡萄产量和质量映射的机器人系统。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "From Actions to Kinesics: Extracting Human Psychological States through Bodily Movements",
        "summary": "Understanding the dynamic relationship between humans and the built\nenvironment is a key challenge in disciplines ranging from environmental\npsychology to reinforcement learning (RL). A central obstacle in modeling these\ninteractions is the inability to capture human psychological states in a way\nthat is both generalizable and privacy preserving. Traditional methods rely on\ntheoretical models or questionnaires, which are limited in scope, static, and\nlabor intensive. We present a kinesics recognition framework that infers the\ncommunicative functions of human activity -- known as kinesics -- directly from\n3D skeleton joint data. Combining a spatial-temporal graph convolutional\nnetwork (ST-GCN) with a convolutional neural network (CNN), the framework\nleverages transfer learning to bypass the need for manually defined mappings\nbetween physical actions and psychological categories. The approach preserves\nuser anonymity while uncovering latent structures in bodily movements that\nreflect cognitive and emotional states. Our results on the Dyadic User\nEngagemenT (DUET) dataset demonstrate that this method enables scalable,\naccurate, and human-centered modeling of behavior, offering a new pathway for\nenhancing RL-driven simulations of human-environment interaction.",
        "url": "http://arxiv.org/abs/2510.04844v1",
        "published_date": "2025-10-06T14:31:53+00:00",
        "updated_date": "2025-10-06T14:31:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Cheyu Lin",
            "Katherine A. Flanigan"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a framework to infer human psychological states through bodily movements using a combination of neural networks, offering a new approach that maintains privacy and enhances simulations of human-environment interaction.",
        "tldr_zh": "本文介绍了一种通过身体动作推断人类心理状态的框架，使用神经网络的组合来提供一种新的方法，以保护隐私并提升人与环境互动的模拟。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Detailed Aerial Mapping of Photovoltaic Power Plants Through Semantically Significant Keypoints",
        "summary": "An accurate and up-to-date model of a photovoltaic (PV) power plant is\nessential for its optimal operation and maintenance. However, such a model may\nnot be easily available. This work introduces a novel approach for PV power\nplant mapping based on aerial overview images. It enables the automation of the\nmapping process while removing the reliance on third-party data. The presented\nmapping method takes advantage of the structural layout of the power plants to\nachieve detailed modeling down to the level of individual PV modules. The\napproach relies on visual segmentation of PV modules in overview images and the\ninference of structural information in each image, assigning modules to\nindividual benches, rows, and columns. We identify visual keypoints related to\nthe layout and use these to merge detections from multiple images while\nmaintaining their structural integrity. The presented method was experimentally\nverified and evaluated on two different power plants. The final fusion of 3D\npositions and semantic structures results in a compact georeferenced model\nsuitable for power plant maintenance.",
        "url": "http://arxiv.org/abs/2510.04840v1",
        "published_date": "2025-10-06T14:25:03+00:00",
        "updated_date": "2025-10-06T14:25:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Viktor Kozák",
            "Jan Chudoba",
            "Libor Přeučil"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel method for detailed mapping of photovoltaic power plants using aerial images, enabling automation and accurate modeling down to individual PV modules.",
        "tldr_zh": "该论文介绍了一种利用航拍图像进行太阳能光伏发电厂详细绘图的新方法，实现了自动化和精确建模到单个光伏模块。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Beyond the Seen: Bounded Distribution Estimation for Open-Vocabulary Learning",
        "summary": "Open-vocabulary learning requires modeling the data distribution in open\nenvironments, which consists of both seen-class and unseen-class data.\n  Existing methods estimate the distribution in open environments using\nseen-class data, where the absence of unseen classes makes the estimation error\ninherently unidentifiable.\n  Intuitively, learning beyond the seen classes is crucial for distribution\nestimation to bound the estimation error.\n  We theoretically demonstrate that the distribution can be effectively\nestimated by generating unseen-class data, through which the estimation error\nis upper-bounded.\n  Building on this theoretical insight, we propose a novel open-vocabulary\nlearning method, which generates unseen-class data for estimating the\ndistribution in open environments. The method consists of a class-domain-wise\ndata generation pipeline and a distribution alignment algorithm. The data\ngeneration pipeline generates unseen-class data under the guidance of a\nhierarchical semantic tree and domain information inferred from the seen-class\ndata, facilitating accurate distribution estimation. With the generated data,\nthe distribution alignment algorithm estimates and maximizes the posterior\nprobability to enhance generalization in open-vocabulary learning. Extensive\nexperiments on $11$ datasets demonstrate that our method outperforms baseline\napproaches by up to $14\\%$, highlighting its effectiveness and superiority.",
        "url": "http://arxiv.org/abs/2510.04770v1",
        "published_date": "2025-10-06T12:43:59+00:00",
        "updated_date": "2025-10-06T12:43:59+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Xiaomeng Fan",
            "Yuchuan Mao",
            "Zhi Gao",
            "Yuwei Wu",
            "Jin Chen",
            "Yunde Jia"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a method for open-vocabulary learning that estimates the data distribution by generating unseen-class data, outperforming baseline approaches by up to 14% on 11 datasets.",
        "tldr_zh": "该论文介绍了一种用于开放词汇学习的方法，通过生成未见类数据估计数据分布，在11个数据集上胜过基线方法高达14%。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction",
        "summary": "The 3D occupancy prediction task has witnessed remarkable progress in recent\nyears, playing a crucial role in vision-based autonomous driving systems. While\ntraditional methods are limited to fixed semantic categories, recent approaches\nhave moved towards predicting text-aligned features to enable open-vocabulary\ntext queries in real-world scenes. However, there exists a trade-off in\ntext-aligned scene modeling: sparse Gaussian representation struggles to\ncapture small objects in the scene, while dense representation incurs\nsignificant computational overhead. To address these limitations, we present\nPG-Occ, an innovative Progressive Gaussian Transformer Framework that enables\nopen-vocabulary 3D occupancy prediction. Our framework employs progressive\nonline densification, a feed-forward strategy that gradually enhances the 3D\nGaussian representation to capture fine-grained scene details. By iteratively\nenhancing the representation, the framework achieves increasingly precise and\ndetailed scene understanding. Another key contribution is the introduction of\nan anisotropy-aware sampling strategy with spatio-temporal fusion, which\nadaptively assigns receptive fields to Gaussians at different scales and\nstages, enabling more effective feature aggregation and richer scene\ninformation capture. Through extensive evaluations, we demonstrate that PG-Occ\nachieves state-of-the-art performance with a relative 14.3% mIoU improvement\nover the previous best performing method. Code and pretrained models will be\nreleased upon publication on our project page:\nhttps://yanchi-3dv.github.io/PG-Occ",
        "url": "http://arxiv.org/abs/2510.04759v1",
        "published_date": "2025-10-06T12:36:07+00:00",
        "updated_date": "2025-10-06T12:36:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Chi Yan",
            "Dan Xu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a Progressive Gaussian Transformer framework for 3D occupancy prediction, achieving state-of-the-art performance with an innovative sampling strategy.",
        "tldr_zh": "本文介绍了一种用于3D占用预测的渐进高斯变换器框架，通过创新的采样策略实现了最先进的性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "ExposureEngine: Oriented Logo Detection and Sponsor Visibility Analytics in Sports Broadcasts",
        "summary": "Quantifying sponsor visibility in sports broadcasts is a critical marketing\ntask traditionally hindered by manual, subjective, and unscalable analysis\nmethods. While automated systems offer an alternative, their reliance on\naxis-aligned Horizontal Bounding Box (HBB) leads to inaccurate exposuremetrics\nwhen logos appear rotated or skewed due to dynamic camera angles and\nperspective distortions. This paper introduces ExposureEngine, an end-to-end\nsystem designed for accurate, rotation-aware sponsor visibility analytics in\nsports broadcasts, demonstrated in a soccer case study. Our approach predicts\nOriented Bounding Box (OBB) to provide a geometrically precise fit to each logo\nregardless of the orientation on-screen. To train and evaluate our detector, we\ndeveloped a new dataset comprising 1,103 frames from Swedish elite soccer,\nfeaturing 670 unique sponsor logos annotated with OBBs. Our model achieves a\nmean Average Precision (mAP@0.5) of 0.859, with a precision of 0.96 and recall\nof 0.87, demonstrating robust performance in localizing logos under diverse\nbroadcast conditions. The system integrates these detections into an analytical\npipeline that calculates precise visibility metrics, such as exposure duration\nand on-screen coverage. Furthermore, we incorporate a language-driven agentic\nlayer, enabling users to generate reports, summaries, and media content through\nnatural language queries. The complete system, including the dataset and the\nanalytics dashboard, provides a comprehensive solution for auditable and\ninterpretable sponsor measurement in sports media. An overview of the\nExposureEngine is available online: https://youtu.be/tRw6OBISuW4 .",
        "url": "http://arxiv.org/abs/2510.04739v1",
        "published_date": "2025-10-06T12:11:53+00:00",
        "updated_date": "2025-10-06T12:11:53+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Mehdi Houshmand Sarkhoosh",
            "Frøy Øye",
            "Henrik Nestor Sørlie",
            "Nam Hoang Vu",
            "Dag Johansen",
            "Cise Midoglu",
            "Tomas Kupka",
            "Pål Halvorsen"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer",
            "AIGC",
            "Other"
        ],
        "tldr": "The paper introduces ExposureEngine, a system for accurate sponsor visibility analytics in sports broadcasts using Oriented Bounding Boxes (OBB) for logos, achieving high precision and recall in detecting logos under diverse conditions.",
        "tldr_zh": "本文介绍了ExposureEngine，一种用于在体育转播中准确检测赞助商可见性的系统，利用定向界框（OBB）来检测Logo，在各种条件下实现高精度和召回率。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Watch and Learn: Learning to Use Computers from Online Videos",
        "summary": "Computer use agents (CUAs) need to plan task workflows grounded in diverse,\never-changing applications and environments, but learning is hindered by the\nscarcity of large-scale, high-quality training data in the target application.\nExisting datasets are domain-specific, static, and costly to annotate, while\ncurrent synthetic data generation methods often yield simplistic or misaligned\ntask demonstrations. To address these limitations, we introduce Watch & Learn\n(W&L), a framework that converts human demonstration videos readily available\non the Internet into executable UI trajectories at scale. Instead of directly\ngenerating trajectories or relying on ad hoc reasoning heuristics, we cast the\nproblem as an inverse dynamics objective: predicting the user's action from\nconsecutive screen states. This formulation reduces manual engineering, is\neasier to learn, and generalizes more robustly across applications. Concretely,\nwe develop an inverse dynamics labeling pipeline with task-aware video\nretrieval, generate over 53k high-quality trajectories from raw web videos, and\ndemonstrate that these trajectories improve CUAs both as in-context\ndemonstrations and as supervised training data. On the challenging OSWorld\nbenchmark, UI trajectories extracted with W&L consistently enhance both\ngeneral-purpose and state-of-the-art frameworks in-context, and deliver\nstronger gains for open-source models under supervised training. These results\nhighlight web-scale human demonstration videos as a practical and scalable\nfoundation for advancing CUAs towards real-world deployment.",
        "url": "http://arxiv.org/abs/2510.04673v1",
        "published_date": "2025-10-06T10:29:00+00:00",
        "updated_date": "2025-10-06T10:29:00+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Chan Hee Song",
            "Yiwen Song",
            "Palash Goyal",
            "Yu Su",
            "Oriana Riva",
            "Hamid Palangi",
            "Tomas Pfister"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces Watch & Learn (W&L), a framework that converts human demonstration videos from the Internet into executable UI trajectories to improve computer use agents' learning process.",
        "tldr_zh": "该论文介绍了Watch & Learn (W&L)框架，将来自互联网的人类演示视频转换为可执行的UI轨迹，以改进计算机使用者的学习过程。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Fast Witness Persistence for MRI Volumes via Hybrid Landmarking",
        "summary": "We introduce a scalable witness-based persistent homology pipeline for\nfull-brain MRI volumes that couples density-aware landmark selection with a\nGPU-ready witness filtration. Candidates are scored by a hybrid metric that\nbalances geometric coverage against inverse kernel density, yielding landmark\nsets that shrink mean pairwise distances by 30-60% over random or density-only\nbaselines while preserving topological features. Benchmarks on BrainWeb, IXI,\nand synthetic manifolds execute in under ten seconds on a single NVIDIA RTX\n4090 GPU, avoiding the combinatorial blow-up of Cech, Vietoris-Rips, and alpha\nfiltrations. The package is distributed on PyPI as whale-tda (installable via\npip); source and issues are hosted at https://github.com/jorgeLRW/whale. The\nrelease also exposes a fast preset (mri_deep_dive_fast) for exploratory sweeps,\nand ships with reproducibility-focused scripts and artifacts for drop-in use in\nmedical imaging workflows.",
        "url": "http://arxiv.org/abs/2510.04553v1",
        "published_date": "2025-10-06T07:34:21+00:00",
        "updated_date": "2025-10-06T07:34:21+00:00",
        "categories": [
            "cs.CG",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Jorge Leonardo Ruiz Williams"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer",
            "Other"
        ],
        "tldr": "The paper introduces a witness-based persistent homology pipeline for full-brain MRI volumes, achieving faster processing times on GPUs with better results compared to existing methods.",
        "tldr_zh": "该论文介绍了一种适用于全脑MRI体积的见证基持续同调流水线，通过GPU实现更快的处理速度，并与现有方法相比取得更好的结果。",
        "relevance_score": 5,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering",
        "summary": "Recent multimodal LLMs have shown promise in chart-based visual question\nanswering, but their performance declines sharply on unannotated charts, those\nrequiring precise visual interpretation rather than relying on textual\nshortcuts. To address this, we introduce ChartAgent, a novel agentic framework\nthat explicitly performs visual reasoning directly within the chart's spatial\ndomain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively\ndecomposes queries into visual subtasks and actively manipulates and interacts\nwith chart images through specialized actions such as drawing annotations,\ncropping regions (e.g., segmenting pie slices, isolating bars), and localizing\naxes, using a library of chart-specific vision tools to fulfill each subtask.\nThis iterative reasoning process closely mirrors human cognitive strategies for\nchart comprehension. ChartAgent achieves state-of-the-art accuracy on the\nChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%\nabsolute gain overall and 17.31% on unannotated, numerically intensive queries.\nFurthermore, our analyses show that ChartAgent is (a) effective across diverse\nchart types, (b) achieve the highest scores across varying visual and reasoning\ncomplexity levels, and (c) serves as a plug-and-play framework that boosts\nperformance across diverse underlying LLMs. Our work is among the first to\ndemonstrate visually grounded reasoning for chart understanding using\ntool-augmented multimodal agents.",
        "url": "http://arxiv.org/abs/2510.04514v1",
        "published_date": "2025-10-06T06:05:36+00:00",
        "updated_date": "2025-10-06T06:05:36+00:00",
        "categories": [
            "cs.AI",
            "cs.CE",
            "cs.CL",
            "cs.CV",
            "stat.ME"
        ],
        "authors": [
            "Rachneet Kaur",
            "Nishan Srishankar",
            "Zhen Zeng",
            "Sumitra Ganesh",
            "Manuela Veloso"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "ChartAgent introduces a novel agentic framework for visually grounded reasoning in chart question answering, achieving state-of-the-art accuracy on benchmark tests.",
        "tldr_zh": "ChartAgent引入新颖的智能框架，用于图表问题回答中的视觉推理，在基准测试中取得了最新的准确性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Diffusion^2: Dual Diffusion Model with Uncertainty-Aware Adaptive Noise for Momentary Trajectory Prediction",
        "summary": "Accurate pedestrian trajectory prediction is crucial for ensuring safety and\nefficiency in autonomous driving and human-robot interaction scenarios. Earlier\nstudies primarily utilized sufficient observational data to predict future\ntrajectories. However, in real-world scenarios, such as pedestrians suddenly\nemerging from blind spots, sufficient observational data is often unavailable\n(i.e. momentary trajectory), making accurate prediction challenging and\nincreasing the risk of traffic accidents. Therefore, advancing research on\npedestrian trajectory prediction under extreme scenarios is critical for\nenhancing traffic safety. In this work, we propose a novel framework termed\nDiffusion^2, tailored for momentary trajectory prediction. Diffusion^2 consists\nof two sequentially connected diffusion models: one for backward prediction,\nwhich generates unobserved historical trajectories, and the other for forward\nprediction, which forecasts future trajectories. Given that the generated\nunobserved historical trajectories may introduce additional noise, we propose a\ndual-head parameterization mechanism to estimate their aleatoric uncertainty\nand design a temporally adaptive noise module that dynamically modulates the\nnoise scale in the forward diffusion process. Empirically, Diffusion^2 sets a\nnew state-of-the-art in momentary trajectory prediction on ETH/UCY and Stanford\nDrone datasets.",
        "url": "http://arxiv.org/abs/2510.04365v1",
        "published_date": "2025-10-05T21:19:33+00:00",
        "updated_date": "2025-10-05T21:19:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuhao Luo",
            "Yuang Zhang",
            "Kehua Chen",
            "Xinyu Zheng",
            "Shucheng Zhang",
            "Sikai Chen",
            "Yinhai Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper presents a novel framework called Diffusion^2 for predicting momentary pedestrian trajectories in extreme scenarios, setting a new state-of-the-art in trajectory prediction.",
        "tldr_zh": "该论文提出了一种名为Diffusion^2的新颖框架，用于在极端情况下预测行人短时轨迹，创立了轨迹预测的新领域。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks",
        "summary": "Parameter-efficient fine-tuning (PEFT) methods have become the standard\nparadigm for adapting large-scale models. Among these techniques,\nWeight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both the\nlearning capacity and training stability of the vanilla Low-Rank Adaptation\n(LoRA) method by explicitly decomposing pre-trained weights into magnitude and\ndirectional components. In this work, we propose DoRAN, a new variant of DoRA\ndesigned to further stabilize training and boost the sample efficiency of DoRA.\nOur approach includes two key stages: (i) injecting noise into the denominator\nof DoRA's weight decomposition, which serves as an adaptive regularizer to\nmitigate instabilities; and (ii) replacing static low-rank matrices with\nauxiliary networks that generate them dynamically, enabling parameter coupling\nacross layers and yielding better sample efficiency in both theory and\npractice. Comprehensive experiments on vision and language benchmarks show that\nDoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines. These\nresults underscore the effectiveness of combining stabilization through\nnoise-based regularization with network-based parameter generation, offering a\npromising direction for robust and efficient fine-tuning of foundation models.",
        "url": "http://arxiv.org/abs/2510.04331v1",
        "published_date": "2025-10-05T19:27:48+00:00",
        "updated_date": "2025-10-05T19:27:48+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Nghiem T. Diep",
            "Hien Dang",
            "Tuan Truong",
            "Tan Dinh",
            "Huy Nguyen",
            "Nhat Ho"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper introduces DoRAN, a method that stabilizes weight-decomposed low-rank adaptation using noise injection and auxiliary networks for efficient fine-tuning of large-scale models.",
        "tldr_zh": "本文引入了DoRAN方法，通过噪声注入和辅助网络来稳定权重分解低秩适应，以实现大规模模型的高效微调。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation",
        "summary": "A fundamental challenge in embodied intelligence is developing expressive and\ncompact state representations for efficient world modeling and decision making.\nHowever, existing methods often fail to achieve this balance, yielding\nrepresentations that are either overly redundant or lacking in task-critical\ninformation. We propose an unsupervised approach that learns a highly\ncompressed two-token state representation using a lightweight encoder and a\npre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong\ngenerative prior. Our representation is efficient, interpretable, and\nintegrates seamlessly into existing VLA-based models, improving performance by\n14.3% on LIBERO and 30% in real-world task success with minimal inference\noverhead. More importantly, we find that the difference between these tokens,\nobtained via latent interpolation, naturally serves as a highly effective\nlatent action, which can be further decoded into executable robot actions. This\nemergent capability reveals that our representation captures structured\ndynamics without explicit supervision. We name our method StaMo for its ability\nto learn generalizable robotic Motion from compact State representation, which\nis encoded from static images, challenging the prevalent dependence to learning\nlatent action on complex architectures and video data. The resulting latent\nactions also enhance policy co-training, outperforming prior methods by 10.4%\nwith improved interpretability. Moreover, our approach scales effectively\nacross diverse data sources, including real-world robot data, simulation, and\nhuman egocentric video.",
        "url": "http://arxiv.org/abs/2510.05057v1",
        "published_date": "2025-10-06T17:37:24+00:00",
        "updated_date": "2025-10-06T17:37:24+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Mingyu Liu",
            "Jiuhe Shu",
            "Hui Chen",
            "Zeju Li",
            "Canyu Zhao",
            "Jiange Yang",
            "Shenyuan Gao",
            "Hao Chen",
            "Chunhua Shen"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "GAN"
        ],
        "tldr": "The paper introduces an unsupervised approach called StaMo that learns a compressed state representation for robot motion from static images, improving performance and interpretability.",
        "tldr_zh": "本文介绍了一种名为StaMo的无监督方法，从静态图像中学习机器人运动的压缩状态表示，提高了性能和可解释性。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.0
    },
    {
        "title": "BenthiCat: An opti-acoustic dataset for advancing benthic classification and habitat mapping",
        "summary": "Benthic habitat mapping is fundamental for understanding marine ecosystems,\nguiding conservation efforts, and supporting sustainable resource management.\nYet, the scarcity of large, annotated datasets limits the development and\nbenchmarking of machine learning models in this domain. This paper introduces a\nthorough multi-modal dataset, comprising about a million side-scan sonar (SSS)\ntiles collected along the coast of Catalonia (Spain), complemented by\nbathymetric maps and a set of co-registered optical images from targeted\nsurveys using an autonomous underwater vehicle (AUV). Approximately \\num{36000}\nof the SSS tiles have been manually annotated with segmentation masks to enable\nsupervised fine-tuning of classification models. All the raw sensor data,\ntogether with mosaics, are also released to support further exploration and\nalgorithm development. To address challenges in multi-sensor data fusion for\nAUVs, we spatially associate optical images with corresponding SSS tiles,\nfacilitating self-supervised, cross-modal representation learning. Accompanying\nopen-source preprocessing and annotation tools are provided to enhance\naccessibility and encourage research. This resource aims to establish a\nstandardized benchmark for underwater habitat mapping, promoting advancements\nin autonomous seafloor classification and multi-sensor integration.",
        "url": "http://arxiv.org/abs/2510.04876v1",
        "published_date": "2025-10-06T15:00:20+00:00",
        "updated_date": "2025-10-06T15:00:20+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "I.2.6; I.4.6; I.5.1; I.5.4"
        ],
        "authors": [
            "Hayat Rajani",
            "Valerio Franchi",
            "Borja Martinez-Clavel Valles",
            "Raimon Ramos",
            "Rafael Garcia",
            "Nuno Gracias"
        ],
        "ai_categories": [
            "Dataset",
            "Multimodality"
        ],
        "tldr": "Introduction of a multi-modal dataset for benthic habitat mapping aimed at advancing autonomous seafloor classification and multi-sensor integration.",
        "tldr_zh": "介绍了一个多模态数据集，旨在推动自主海床分类和多传感器集成。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "ERDE: Entropy-Regularized Distillation for Early-exit",
        "summary": "Although deep neural networks and in particular Convolutional Neural Networks\nhave demonstrated state-of-the-art performance in image classification with\nrelatively high efficiency, they still exhibit high computational costs, often\nrendering them impractical for real-time and edge applications. Therefore, a\nmultitude of compression techniques have been developed to reduce these costs\nwhile maintaining accuracy. In addition, dynamic architectures have been\nintroduced to modulate the level of compression at execution time, which is a\ndesirable property in many resource-limited application scenarios. The proposed\nmethod effectively integrates two well-established optimization techniques:\nearly exits and knowledge distillation, where a reduced student early-exit\nmodel is trained from a more complex teacher early-exit model. The primary\ncontribution of this research lies in the approach for training the student\nearly-exit model. In comparison to the conventional Knowledge Distillation\nloss, our approach incorporates a new entropy-based loss for images where the\nteacher's classification was incorrect. The proposed method optimizes the\ntrade-off between accuracy and efficiency, thereby achieving significant\nreductions in computational complexity without compromising classification\nperformance. The validity of this approach is substantiated by experimental\nresults on image classification datasets CIFAR10, CIFAR100 and SVHN, which\nfurther opens new research perspectives for Knowledge Distillation in other\ncontexts.",
        "url": "http://arxiv.org/abs/2510.04856v1",
        "published_date": "2025-10-06T14:45:41+00:00",
        "updated_date": "2025-10-06T14:45:41+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Martial Guidez",
            "Stefan Duffner",
            "Yannick Alpou",
            "Oscar Röth",
            "Christophe Garcia"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "ERDE is a method that combines early exits and knowledge distillation in neural networks using an entropy-based loss to achieve significant reductions in computational complexity without compromising classification performance.",
        "tldr_zh": "ERDE是一种结合了早退出和知识蒸馏的神经网络方法，使用基于熵的损失实现了显著降低计算复杂性而不影响分类性能。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.0
    },
    {
        "title": "Hands-Free Heritage: Automated 3D Scanning for Cultural Heritage Digitization",
        "summary": "High-fidelity 3D scanning is essential for preserving cultural heritage\nartefacts, supporting documentation, analysis, and long-term conservation.\nHowever, conventional methods typically require specialized expertise and\nmanual intervention to maintain optimal scanning conditions and coverage. We\npresent an automated two-robot scanning system that eliminates the need for\nhandheld or semi-automatic workflows by combining coordinated robotic\nmanipulation with high-resolution 3D scanning. Our system parameterizes the\nscanning space into distinct regions, enabling coordinated motion planning\nbetween a scanner-equipped robot and a tray-handling robot. Optimized\ntrajectory planning and waypoint distribution ensure comprehensive surface\ncoverage, minimize occlusions, and balance reconstruction accuracy with system\nefficiency. Experimental results show that our approach achieves significantly\nlower Chamfer Distance and higher F-score compared to baseline methods,\noffering superior geometric accuracy, improved digitization efficiency, and\nreduced reliance on expert operators.",
        "url": "http://arxiv.org/abs/2510.04781v1",
        "published_date": "2025-10-06T12:58:41+00:00",
        "updated_date": "2025-10-06T12:58:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Javed Ahmad",
            "Federico Dassiè",
            "Selene Frascella",
            "Gabriele Marchello",
            "Ferdinando Cannella",
            "Arianna Traviglia"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper presents an automated two-robot scanning system for 3D scanning cultural heritage artefacts, achieving better accuracy and efficiency compared to traditional methods.",
        "tldr_zh": "该论文提出了一种用于3D扫描文化遗产文物的自动化双机器人扫描系统，相较传统方法获得更好的准确性和效率。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Anomaly-Aware YOLO: A Frugal yet Robust Approach to Infrared Small Target Detection",
        "summary": "Infrared Small Target Detection (IRSTD) is a challenging task in defense\napplications, where complex backgrounds and tiny target sizes often result in\nnumerous false alarms using conventional object detectors. To overcome this\nlimitation, we propose Anomaly-Aware YOLO (AA-YOLO), which integrates a\nstatistical anomaly detection test into its detection head. By treating small\ntargets as unexpected patterns against the background, AA-YOLO effectively\ncontrols the false alarm rate. Our approach not only achieves competitive\nperformance on several IRSTD benchmarks, but also demonstrates remarkable\nrobustness in scenarios with limited training data, noise, and domain shifts.\nFurthermore, since only the detection head is modified, our design is highly\ngeneric and has been successfully applied across various YOLO backbones,\nincluding lightweight models. It also provides promising results when\nintegrated into an instance segmentation YOLO. This versatility makes AA-YOLO\nan attractive solution for real-world deployments where resources are\nconstrained. The code will be publicly released.",
        "url": "http://arxiv.org/abs/2510.04741v1",
        "published_date": "2025-10-06T12:13:56+00:00",
        "updated_date": "2025-10-06T12:13:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Alina Ciocarlan",
            "Sylvie Le Hégarat-Mascle",
            "Sidonie Lefebvre"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces Anomaly-Aware YOLO (AA-YOLO) for Infrared Small Target Detection, achieving competitive performance and robustness in various scenarios with limited training data.",
        "tldr_zh": "该论文介绍了适用于红外小目标检测的Anomaly-Aware YOLO（AA-YOLO），在各种场景中实现了有竞争力的性能和稳健性，包括有限的训练数据。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Object-Centric Representation Learning for Enhanced 3D Scene Graph Prediction",
        "summary": "3D Semantic Scene Graph Prediction aims to detect objects and their semantic\nrelationships in 3D scenes, and has emerged as a crucial technology for\nrobotics and AR/VR applications. While previous research has addressed dataset\nlimitations and explored various approaches including Open-Vocabulary settings,\nthey frequently fail to optimize the representational capacity of object and\nrelationship features, showing excessive reliance on Graph Neural Networks\ndespite insufficient discriminative capability. In this work, we demonstrate\nthrough extensive analysis that the quality of object features plays a critical\nrole in determining overall scene graph accuracy. To address this challenge, we\ndesign a highly discriminative object feature encoder and employ a contrastive\npretraining strategy that decouples object representation learning from the\nscene graph prediction. This design not only enhances object classification\naccuracy but also yields direct improvements in relationship prediction.\nNotably, when plugging in our pretrained encoder into existing frameworks, we\nobserve substantial performance improvements across all evaluation metrics.\nAdditionally, whereas existing approaches have not fully exploited the\nintegration of relationship information, we effectively combine both geometric\nand semantic features to achieve superior relationship prediction.\nComprehensive experiments on the 3DSSG dataset demonstrate that our approach\nsignificantly outperforms previous state-of-the-art methods. Our code is\npublicly available at https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes.",
        "url": "http://arxiv.org/abs/2510.04714v1",
        "published_date": "2025-10-06T11:33:09+00:00",
        "updated_date": "2025-10-06T11:33:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "KunHo Heo",
            "GiHyun Kim",
            "SuYeon Kim",
            "MyeongAh Cho"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper proposes a method for improving 3D scene graph prediction by enhancing object representation learning, leading to significant performance improvements over existing methods.",
        "tldr_zh": "本文提出了一种通过增强目标表示学习来改善3D场景图预测的方法，从而显著提高了现有方法的性能。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "MoME: Estimating Psychological Traits from Gait with Multi-Stage Mixture of Movement Experts",
        "summary": "Gait encodes rich biometric and behavioural information, yet leveraging the\nmanner of walking to infer psychological traits remains a challenging and\nunderexplored problem. We introduce a hierarchical Multi-Stage Mixture of\nMovement Experts (MoME) architecture for multi-task prediction of psychological\nattributes from gait sequences represented as 2D poses. MoME processes the\nwalking cycle in four stages of movement complexity, employing lightweight\nexpert models to extract spatio-temporal features and task-specific gating\nmodules to adaptively weight experts across traits and stages. Evaluated on the\nPsyMo benchmark covering 17 psychological traits, our method outperforms\nstate-of-the-art gait analysis models, achieving a 37.47% weighted F1 score at\nthe run level and 44.6% at the subject level. Our experiments show that\nintegrating auxiliary tasks such as identity recognition, gender prediction,\nand BMI estimation further improves psychological trait estimation. Our\nfindings demonstrate the viability of multi-task gait-based learning for\npsychological trait estimation and provide a foundation for future research on\nmovement-informed psychological inference.",
        "url": "http://arxiv.org/abs/2510.04654v1",
        "published_date": "2025-10-06T09:58:43+00:00",
        "updated_date": "2025-10-06T09:58:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Andy Cǎtrunǎ",
            "Adrian Cosma",
            "Emilian Rǎdoi"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a novel method called MoME for predicting psychological traits from gait sequences, outperforming existing models. It demonstrates the potential of using gait for psychological trait estimation.",
        "tldr_zh": "该论文介绍了一种名为MoME的新方法，用于从步态序列预测心理特征，胜过现有模型。它展示了利用步态进行心理特征估计的潜力。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "EduPersona: Benchmarking Subjective Ability Boundaries of Virtual Student Agents",
        "summary": "As large language models are increasingly integrated into education, virtual\nstudent agents are becoming vital for classroom simulation and teacher\ntraining. Yet their classroom-oriented subjective abilities remain largely\nunassessed, limiting understanding of model boundaries and hindering\ntrustworthy deployment. We present EduPersona, a large-scale benchmark spanning\ntwo languages, three subjects, and ten persona types based on the Big Five\ntheory. The dataset contains 1,308 authentic classroom dialogue rounds,\ncorresponding to 12,814 teacher-student Q&A turns, and is further expanded\nthrough persona stylization into roughly 10 times larger scale (128k turns),\nproviding a solid foundation for evaluation. Building on this resource, we\ndecompose hard-to-quantify subjective performance into three progressive tasks:\nTASK1 basic coherence (whether behavior, emotion, expression, and voice align\nwith classroom context), TASK2 student realism, and TASK3 long-term persona\nconsistency, thereby establishing an evaluation framework grounded in\neducational theory and research value. We conduct systematic experiments on\nthree representative LLMs, comparing their original versions with ten\npersona-fine-tuned variants trained on EduPersona. Results show consistent and\nsignificant average improvements across all tasks: TASK1 +33.6%, TASK2 +30.6%,\nand TASK3 +14.9%. These improvements highlight the dataset's effectiveness and\nresearch value, while also revealing the heterogeneous difficulty of persona\nmodeling. In summary, EduPersona delivers the first classroom benchmark\ncentered on subjective abilities, establishes a decoupled and verifiable\nresearch paradigm, and we will open-source both the dataset and the framework\nto support the broader research community in advancing trustworthy and\nhuman-like AI for education.",
        "url": "http://arxiv.org/abs/2510.04648v1",
        "published_date": "2025-10-06T09:52:18+00:00",
        "updated_date": "2025-10-06T09:52:18+00:00",
        "categories": [
            "cs.CV",
            "cs.CY"
        ],
        "authors": [
            "Buyuan Zhu",
            "Shiyu Hu",
            "Yiping Ma",
            "Yuanming Zhang",
            "Kang Hao Cheong"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces EduPersona, a benchmark dataset for evaluating the subjective abilities of virtual student agents in classroom simulations, showing significant improvements in performance after fine-tuning with persona types based on the Big Five theory.",
        "tldr_zh": "该论文介绍了EduPersona，一个用于评估虚拟学生代理在课堂模拟中的主观能力的基准数据集，显示在使用基于大五人格理论的人物类型进行微调后，性能有显著改善。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole Slide Image Diagnosis Behavior",
        "summary": "Diagnosing a whole-slide image is an interactive, multi-stage process\ninvolving changes in magnification and movement between fields. Although recent\npathology foundation models are strong, practical agentic systems that decide\nwhat field to examine next, adjust magnification, and deliver explainable\ndiagnoses are still lacking. The blocker is data: scalable, clinically aligned\nsupervision of expert viewing behavior that is tacit and experience-based, not\nwritten in textbooks or online, and therefore absent from large language model\ntraining. We introduce the AI Session Recorder, which works with standard WSI\nviewers to unobtrusively record routine navigation and convert the viewer logs\ninto standardized behavioral commands (inspect or peek at discrete\nmagnifications) and bounding boxes. A lightweight human-in-the-loop review\nturns AI-drafted rationales into the Pathology-CoT dataset, a form of paired\n\"where to look\" and \"why it matters\" supervision produced at roughly six times\nlower labeling time. Using this behavioral data, we build Pathologist-o3, a\ntwo-stage agent that first proposes regions of interest and then performs\nbehavior-guided reasoning. On gastrointestinal lymph-node metastasis detection,\nit achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding the\nstate-of-the-art OpenAI o3 model and generalizing across backbones. To our\nknowledge, this constitutes one of the first behavior-grounded agentic systems\nin pathology. Turning everyday viewer logs into scalable, expert-validated\nsupervision, our framework makes agentic pathology practical and establishes a\npath to human-aligned, upgradeable clinical AI.",
        "url": "http://arxiv.org/abs/2510.04587v1",
        "published_date": "2025-10-06T08:44:04+00:00",
        "updated_date": "2025-10-06T08:44:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sheng Wang",
            "Ruiming Wu",
            "Charles Herndon",
            "Yihang Liu",
            "Shunsuke Koga",
            "Jeanne Shen",
            "Zhi Huang"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces an agentic system for diagnosing whole-slide images in pathology based on expert behavior, achieving high accuracy and establishing a path towards practical clinical AI.",
        "tldr_zh": "该论文介绍了一种基于专家行为的病理学全切片图像诊断智能系统，实现了高准确度，并为实际临床人工智能打开了一条道路。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models",
        "summary": "Bridging clinical diagnostic reasoning with AI remains a central challenge in\nmedical imaging. We introduce MedCLM, an automated pipeline that converts\ndetection datasets into large-scale medical visual question answering (VQA)\ndata with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ\nsegmentation and structured rationales. These contextual signals enable medical\nvision-language models to generate question-answer pairs with step-by-step\nreasoning. To utilize this data effectively, we propose an Integrated\nCoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes\nfor visual grounding, a Medium stage that encourages implicit localization, and\na Hard stage for weakly supervised reasoning. Experimental results demonstrate\nthat MedCLM attains state-of-the-art performance on several medical VQA\nbenchmarks, providing a scalable framework for developing clinically aligned\nmedical vision-language models.",
        "url": "http://arxiv.org/abs/2510.04477v1",
        "published_date": "2025-10-06T04:26:39+00:00",
        "updated_date": "2025-10-06T04:26:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Soo Yong Kim",
            "Suin Cho",
            "Vincent-Daniel Yun",
            "Gyeongyeon Hwang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "MedCLM introduces a pipeline for medical visual question answering with step-by-step reasoning using CoT strategy, achieving state-of-the-art performance.",
        "tldr_zh": "MedCLM引入了一种用于医学图像问题回答的流水线，利用CoT策略进行逐步推理，取得了最先进的表现。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "RAP: 3D Rasterization Augmented End-to-End Planning",
        "summary": "Imitation learning for end-to-end driving trains policies only on expert\ndemonstrations. Once deployed in a closed loop, such policies lack recovery\ndata: small mistakes cannot be corrected and quickly compound into failures. A\npromising direction is to generate alternative viewpoints and trajectories\nbeyond the logged path. Prior work explores photorealistic digital twins via\nneural rendering or game engines, but these methods are prohibitively slow and\ncostly, and thus mainly used for evaluation. In this work, we argue that\nphotorealism is unnecessary for training end-to-end planners. What matters is\nsemantic fidelity and scalability: driving depends on geometry and dynamics,\nnot textures or lighting. Motivated by this, we propose 3D Rasterization, which\nreplaces costly rendering with lightweight rasterization of annotated\nprimitives, enabling augmentations such as counterfactual recovery maneuvers\nand cross-agent view synthesis. To transfer these synthetic views effectively\nto real-world deployment, we introduce a Raster-to-Real feature-space alignment\nthat bridges the sim-to-real gap. Together, these components form Rasterization\nAugmented Planning (RAP), a scalable data augmentation pipeline for planning.\nRAP achieves state-of-the-art closed-loop robustness and long-tail\ngeneralization, ranking first on four major benchmarks: NAVSIM v1/v2, Waymo\nOpen Dataset Vision-based E2E Driving, and Bench2Drive. Our results show that\nlightweight rasterization with feature alignment suffices to scale E2E\ntraining, offering a practical alternative to photorealistic rendering. Project\npage: https://alan-lanfeng.github.io/RAP/.",
        "url": "http://arxiv.org/abs/2510.04333v1",
        "published_date": "2025-10-05T19:31:24+00:00",
        "updated_date": "2025-10-05T19:31:24+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Lan Feng",
            "Yang Gao",
            "Eloi Zablocki",
            "Quanyi Li",
            "Wuyang Li",
            "Sichao Liu",
            "Matthieu Cord",
            "Alexandre Alahi"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces 3D Rasterization Augmented Planning (RAP), a method for training end-to-end driving policies using lightweight rendering techniques and feature alignment, achieving state-of-the-art results in various benchmarks.",
        "tldr_zh": "这篇论文介绍了一种名为3D Rasterization Augmented Planning (RAP)的方法，使用轻量级渲染技术和特征对齐来训练端到端驾驶策略，在各种基准测试中取得了最先进的结果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CARE-PD: A Multi-Site Anonymized Clinical Dataset for Parkinson's Disease Gait Assessment",
        "summary": "Objective gait assessment in Parkinson's Disease (PD) is limited by the\nabsence of large, diverse, and clinically annotated motion datasets. We\nintroduce CARE-PD, the largest publicly available archive of 3D mesh gait data\nfor PD, and the first multi-site collection spanning 9 cohorts from 8 clinical\ncenters. All recordings (RGB video or motion capture) are converted into\nanonymized SMPL meshes via a harmonized preprocessing pipeline. CARE-PD\nsupports two key benchmarks: supervised clinical score prediction (estimating\nUnified Parkinson's Disease Rating Scale, UPDRS, gait scores) and unsupervised\nmotion pretext tasks (2D-to-3D keypoint lifting and full-body 3D\nreconstruction). Clinical prediction is evaluated under four generalization\nprotocols: within-dataset, cross-dataset, leave-one-dataset-out, and\nmulti-dataset in-domain adaptation. To assess clinical relevance, we compare\nstate-of-the-art motion encoders with a traditional gait-feature baseline,\nfinding that encoders consistently outperform handcrafted features. Pretraining\non CARE-PD reduces MPJPE (from 60.8mm to 7.5mm) and boosts PD severity macro-F1\nby 17 percentage points, underscoring the value of clinically curated, diverse\ntraining data. CARE-PD and all benchmark code are released for non-commercial\nresearch at https://neurips2025.care-pd.ca/.",
        "url": "http://arxiv.org/abs/2510.04312v1",
        "published_date": "2025-10-05T18:14:50+00:00",
        "updated_date": "2025-10-05T18:14:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Vida Adeli",
            "Ivan Klabucar",
            "Javad Rajabi",
            "Benjamin Filtjens",
            "Soroush Mehraban",
            "Diwei Wang",
            "Hyewon Seo",
            "Trung-Hieu Hoang",
            "Minh N. Do",
            "Candice Muller",
            "Claudia Oliveira",
            "Daniel Boari Coelho",
            "Pieter Ginis",
            "Moran Gilat",
            "Alice Nieuwboer",
            "Joke Spildooren",
            "Lucas Mckay",
            "Hyeokhyen Kwon",
            "Gari Clifford",
            "Christine Esper",
            "Stewart Factor",
            "Imari Genias",
            "Amirhossein Dadashzadeh",
            "Leia Shum",
            "Alan Whone",
            "Majid Mirmehdi",
            "Andrea Iaboni",
            "Babak Taati"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "CARE-PD introduces a large dataset of 3D gait data for Parkinson's Disease, supporting clinical score prediction and motion tasks.",
        "tldr_zh": "CARE-PD引入了一个用于帕金森病的大型3D步态数据集，支持临床评分预测和运动任务。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "A Semantics-Aware Hierarchical Self-Supervised Approach to Classification of Remote Sensing Images",
        "summary": "Deep learning has become increasingly important in remote sensing image\nclassification due to its ability to extract semantic information from complex\ndata. Classification tasks often include predefined label hierarchies that\nrepresent the semantic relationships among classes. However, these hierarchies\nare frequently overlooked, and most approaches focus only on fine-grained\nclassification schemes. In this paper, we present a novel Semantics-Aware\nHierarchical Consensus (SAHC) method for learning hierarchical features and\nrelationships by integrating hierarchy-specific classification heads within a\ndeep network architecture, each specialized in different degrees of class\ngranularity. The proposed approach employs trainable hierarchy matrices, which\nguide the network through the learning of the hierarchical structure in a\nself-supervised manner. Furthermore, we introduce a hierarchical consensus\nmechanism to ensure consistent probability distributions across different\nhierarchical levels. This mechanism acts as a weighted ensemble being able to\neffectively leverage the inherent structure of the hierarchical classification\ntask. The proposed SAHC method is evaluated on three benchmark datasets with\ndifferent degrees of hierarchical complexity on different tasks, using distinct\nbackbone architectures to effectively emphasize its adaptability. Experimental\nresults show both the effectiveness of the proposed approach in guiding network\nlearning and the robustness of the hierarchical consensus for remote sensing\nimage classification tasks.",
        "url": "http://arxiv.org/abs/2510.04916v1",
        "published_date": "2025-10-06T15:30:39+00:00",
        "updated_date": "2025-10-06T15:30:39+00:00",
        "categories": [
            "cs.CV",
            "I.4.6; I.4.8; I.4.10"
        ],
        "authors": [
            "Giulio Weikmann",
            "Gianmarco Perantoni",
            "Lorenzo Bruzzone"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper presents a novel approach for hierarchical classification of remote sensing images, using a Semantics-Aware Hierarchical Consensus (SAHC) method to learn hierarchical features and relationships in a self-supervised manner.",
        "tldr_zh": "本文提出了一种新的方法，用于遥感图像的层级分类，通过Semantics-Aware Hierarchical Consensus (SAHC) 方法，在自监督的方式下学习层级特征和关系。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Comparative Analysis of YOLOv5, Faster R-CNN, SSD, and RetinaNet for Motorbike Detection in Kigali Autonomous Driving Context",
        "summary": "In Kigali, Rwanda, motorcycle taxis are a primary mode of transportation,\noften navigating unpredictably and disregarding traffic rules, posing\nsignificant challenges for autonomous driving systems. This study compares four\nobject detection models--YOLOv5, Faster R-CNN, SSD, and RetinaNet--for\nmotorbike detection using a custom dataset of 198 images collected in Kigali.\nImplemented in PyTorch with transfer learning, the models were evaluated for\naccuracy, localization, and inference speed to assess their suitability for\nreal-time navigation in resource-constrained settings. We identify\nimplementation challenges, including dataset limitations and model\ncomplexities, and recommend simplified architectures for future work to enhance\naccessibility for autonomous systems in developing countries like Rwanda.",
        "url": "http://arxiv.org/abs/2510.04912v1",
        "published_date": "2025-10-06T15:26:08+00:00",
        "updated_date": "2025-10-06T15:26:08+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Ngeyen Yinkfu",
            "Sunday Nwovu",
            "Jonathan Kayizzi",
            "Angelique Uwamahoro"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper compares four object detection models for motorbike detection in Kigali, Rwanda, focusing on real-time navigation suitability in resource-constrained settings.",
        "tldr_zh": "该论文比较了四种目标检测模型，用于在卢旺达基加利进行摩托车检测，侧重于在资源受限环境中的实时导航适用性。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Benchmark on Monocular Metric Depth Estimation in Wildlife Setting",
        "summary": "Camera traps are widely used for wildlife monitoring, but extracting accurate\ndistance measurements from monocular images remains challenging due to the lack\nof depth information. While monocular depth estimation (MDE) methods have\nadvanced significantly, their performance in natural wildlife environments has\nnot been systematically evaluated. This work introduces the first benchmark for\nmonocular metric depth estimation in wildlife monitoring conditions. We\nevaluate four state-of-the-art MDE methods (Depth Anything V2, ML Depth Pro,\nZoeDepth, and Metric3D) alongside a geometric baseline on 93 camera trap images\nwith ground truth distances obtained using calibrated ChARUCO patterns. Our\nresults demonstrate that Depth Anything V2 achieves the best overall\nperformance with a mean absolute error of 0.454m and correlation of 0.962,\nwhile methods like ZoeDepth show significant degradation in outdoor natural\nenvironments (MAE: 3.087m). We find that median-based depth extraction\nconsistently outperforms mean-based approaches across all deep learning\nmethods. Additionally, we analyze computational efficiency, with ZoeDepth being\nfastest (0.17s per image) but least accurate, while Depth Anything V2 provides\nan optimal balance of accuracy and speed (0.22s per image). This benchmark\nestablishes performance baselines for wildlife applications and provides\npractical guidance for implementing depth estimation in conservation monitoring\nsystems.",
        "url": "http://arxiv.org/abs/2510.04723v1",
        "published_date": "2025-10-06T11:43:34+00:00",
        "updated_date": "2025-10-06T11:43:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Niccolò Niccoli",
            "Lorenzo Seidenari",
            "Ilaria Greco",
            "Francesco Rovero"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper introduces a benchmark for monocular depth estimation in wildlife settings, evaluating state-of-the-art methods and providing insights on performance and computational efficiency.",
        "tldr_zh": "本文在野生动物监测中引入了单眼深度估计的基准，评估了最先进的方法，并提供了关于性能和计算效率的见解。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Do Superpixel Segmentation Methods Influence Deforestation Image Classification?",
        "summary": "Image segmentation is a crucial step in various visual applications,\nincluding environmental monitoring through remote sensing. In the context of\nthe ForestEyes project, which combines citizen science and machine learning to\ndetect deforestation in tropical forests, image segments are used for labeling\nby volunteers and subsequent model training. Traditionally, the Simple Linear\nIterative Clustering (SLIC) algorithm is adopted as the segmentation method.\nHowever, recent studies have indicated that other superpixel-based methods\noutperform SLIC in remote sensing image segmentation, and might suggest that\nthey are more suitable for the task of detecting deforested areas. In this\nsense, this study investigated the impact of the four best segmentation\nmethods, together with SLIC, on the training of classifiers for the target\napplication. Initially, the results showed little variation in performance\namong segmentation methods, even when selecting the top five classifiers using\nthe PyCaret AutoML library. However, by applying a classifier fusion approach\n(ensemble of classifiers), noticeable improvements in balanced accuracy were\nobserved, highlighting the importance of both the choice of segmentation method\nand the combination of machine learning-based models for deforestation\ndetection tasks.",
        "url": "http://arxiv.org/abs/2510.04645v1",
        "published_date": "2025-10-06T09:46:17+00:00",
        "updated_date": "2025-10-06T09:46:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hugo Resende",
            "Fabio A. Faria",
            "Eduardo B. Neto",
            "Isabela Borlido",
            "Victor Sundermann",
            "Silvio Jamil F. Guimarães",
            "Álvaro L. Fazenda"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The study investigates the impact of different superpixel segmentation methods on training classifiers for deforestation detection, showing improvement with classifier fusion.",
        "tldr_zh": "本研究探讨不同超像素分割方法对于林地砍伐检测中分类器训练的影响，通过分类器融合实现改进。",
        "relevance_score": 2,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6.5
    },
    {
        "title": "Adaptive double-phase Rudin--Osher--Fatemi denoising model",
        "summary": "We propose a new image denoising model based on a variable-growth total\nvariation regularization of double-phase type with adaptive weight. It is\ndesigned to reduce staircasing with respect to the classical\nRudin--Osher--Fatemi model, while preserving the edges of the image in a\nsimilar fashion. We implement the model and test its performance on synthetic\nand natural images in 1D and 2D over a range of noise levels.",
        "url": "http://arxiv.org/abs/2510.04382v1",
        "published_date": "2025-10-05T22:26:06+00:00",
        "updated_date": "2025-10-05T22:26:06+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.NA",
            "math.NA"
        ],
        "authors": [
            "Wojciech Górny",
            "Michał Łasica",
            "Alexandros Matsoukas"
        ],
        "ai_categories": [
            "Multimodality"
        ],
        "tldr": "The paper introduces a new image denoising model that aims to reduce staircasing while preserving image edges compared to existing models.",
        "tldr_zh": "本文介绍了一种新的图像去噪模型，旨在减少楼梯效应，并保留图像边缘与现有模型相比。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Latent Uncertainty Representations for Video-based Driver Action and Intention Recognition",
        "summary": "Deep neural networks (DNNs) are increasingly applied to safety-critical tasks\nin resource-constrained environments, such as video-based driver action and\nintention recognition. While last layer probabilistic deep learning (LL-PDL)\nmethods can detect out-of-distribution (OOD) instances, their performance\nvaries. As an alternative to last layer approaches, we propose extending\npre-trained DNNs with transformation layers to produce multiple latent\nrepresentations to estimate the uncertainty. We evaluate our latent uncertainty\nrepresentation (LUR) and repulsively trained LUR (RLUR) approaches against\neight PDL methods across four video-based driver action and intention\nrecognition datasets, comparing classification performance, calibration, and\nuncertainty-based OOD detection. We also contribute 28,000 frame-level action\nlabels and 1,194 video-level intention labels for the NuScenes dataset. Our\nresults show that LUR and RLUR achieve comparable in-distribution\nclassification performance to other LL-PDL approaches. For uncertainty-based\nOOD detection, LUR matches top-performing PDL methods while being more\nefficient to train and easier to tune than approaches that require Markov-Chain\nMonte Carlo sampling or repulsive training procedures.",
        "url": "http://arxiv.org/abs/2510.05006v1",
        "published_date": "2025-10-06T16:50:02+00:00",
        "updated_date": "2025-10-06T16:50:02+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Koen Vellenga",
            "H. Joe Steinhauer",
            "Jonas Andersson",
            "Anders Sjögren"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a new method for estimating uncertainty in video-based driver action and intention recognition using pre-trained deep neural networks with transformation layers. Results show comparable performance to existing methods with more efficiency.",
        "tldr_zh": "本文提出了一种新的方法，使用预训练的深度神经网络与转换层来估计视频驾驶员动作和意图识别中的不确定性。结果表明，性能与现有方法相当，同时更加高效。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "On Structured State-Space Duality",
        "summary": "Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence\nbetween a simple Structured State-Space Model (SSM) and a masked attention\nmechanism. In particular, a state-space model with a scalar-times-identity\nstate matrix is equivalent to a masked self-attention with a $1$-semiseparable\ncausal mask. Consequently, the same sequence transformation (model) has two\nalgorithmic realizations: as a linear-time $O(T)$ recurrence or as a\nquadratic-time $O(T^2)$ attention. In this note, we formalize and generalize\nthis duality: (i) we extend SSD from the scalar-identity case to general\ndiagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs\nmatch the scalar case's training complexity lower bounds while supporting\nricher dynamics; (iii) we establish a necessary and sufficient condition under\nwhich an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we\nshow that such duality fails to extend to standard softmax attention due to\nrank explosion. Together, these results tighten bridge between recurrent SSMs\nand Transformers, and widen the design space for expressive yet efficient\nsequence models.",
        "url": "http://arxiv.org/abs/2510.04944v1",
        "published_date": "2025-10-06T15:46:50+00:00",
        "updated_date": "2025-10-06T15:46:50+00:00",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.CV",
            "stat.ML"
        ],
        "authors": [
            "Jerry Yao-Chieh Hu",
            "Xiwen Zhang",
            "Weimin Wu",
            "Han Liu"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper introduces Structured State-Space Duality (SSD) which connects state-space models with masked attention mechanisms, offering two algorithmic realizations for the same sequence transformation.",
        "tldr_zh": "本文介绍了结构化状态空间对偶（SSD），它将状态空间模型与掩蔽注意机制连接起来，为相同序列转换提供了两种算法实现。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "Read the Room: Inferring Social Context Through Dyadic Interaction Recognition in Cyber-physical-social Infrastructure Systems",
        "summary": "Cyber-physical systems (CPS) integrate sensing, computing, and control to\nimprove infrastructure performance, focusing on economic goals like performance\nand safety. However, they often neglect potential human-centered (or\n''social'') benefits. Cyber-physical-social infrastructure systems (CPSIS) aim\nto address this by aligning CPS with social objectives. This involves defining\nsocial benefits, understanding human interactions with each other and\ninfrastructure, developing privacy-preserving measurement methods, modeling\nthese interactions for prediction, linking them to social benefits, and\nactuating the physical environment to foster positive social outcomes. This\npaper delves into recognizing dyadic human interactions using real-world data,\nwhich is the backbone to measuring social behavior. This lays a foundation to\naddress the need to enhance understanding of the deeper meanings and mutual\nresponses inherent in human interactions. While RGB cameras are informative for\ninteraction recognition, privacy concerns arise. Depth sensors offer a\nprivacy-conscious alternative by analyzing skeletal movements. This study\ncompares five skeleton-based interaction recognition algorithms on a dataset of\n12 dyadic interactions. Unlike single-person datasets, these interactions,\ncategorized into communication types like emblems and affect displays, offer\ninsights into the cultural and emotional aspects of human interactions.",
        "url": "http://arxiv.org/abs/2510.04854v1",
        "published_date": "2025-10-06T14:40:22+00:00",
        "updated_date": "2025-10-06T14:40:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Cheyu Lin",
            "John Martins",
            "Katherine A. Flanigan",
            "Ph. D"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper focuses on recognizing human interactions in cyber-physical-social infrastructure systems using depth sensors for privacy-conscious analysis.",
        "tldr_zh": "本文主要关注使用深度传感器对维护隐私的分析来识别人类在网络物理社会基础设施系统中的互动。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "Federated Learning for Surgical Vision in Appendicitis Classification: Results of the FedSurg EndoVis 2024 Challenge",
        "summary": "Purpose: The FedSurg challenge was designed to benchmark the state of the art\nin federated learning for surgical video classification. Its goal was to assess\nhow well current methods generalize to unseen clinical centers and adapt\nthrough local fine-tuning while enabling collaborative model development\nwithout sharing patient data. Methods: Participants developed strategies to\nclassify inflammation stages in appendicitis using a preliminary version of the\nmulti-center Appendix300 video dataset. The challenge evaluated two tasks:\ngeneralization to an unseen center and center-specific adaptation after\nfine-tuning. Submitted approaches included foundation models with linear\nprobing, metric learning with triplet loss, and various FL aggregation schemes\n(FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and\nExpected Cost, with ranking robustness evaluated via bootstrapping and\nstatistical testing. Results: In the generalization task, performance across\ncenters was limited. In the adaptation task, all teams improved after\nfine-tuning, though ranking stability was low. The ViViT-based submission\nachieved the strongest overall performance. The challenge highlighted\nlimitations in generalization, sensitivity to class imbalance, and difficulties\nin hyperparameter tuning in decentralized training, while spatiotemporal\nmodeling and context-aware preprocessing emerged as promising strategies.\nConclusion: The FedSurg Challenge establishes the first benchmark for\nevaluating FL strategies in surgical video classification. Findings highlight\nthe trade-off between local personalization and global robustness, and\nunderscore the importance of architecture choice, preprocessing, and loss\ndesign. This benchmarking offers a reference point for future development of\nimbalance-aware, adaptive, and robust FL methods in clinical surgical AI.",
        "url": "http://arxiv.org/abs/2510.04772v1",
        "published_date": "2025-10-06T12:48:46+00:00",
        "updated_date": "2025-10-06T12:48:46+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Max Kirchner",
            "Hanna Hoffmann",
            "Alexander C. Jenke",
            "Oliver L. Saldanha",
            "Kevin Pfeiffer",
            "Weam Kanjo",
            "Julia Alekseenko",
            "Claas de Boer",
            "Santhi Raj Kolamuri",
            "Lorenzo Mazza",
            "Nicolas Padoy",
            "Sophia Bano",
            "Annika Reinke",
            "Lena Maier-Hein",
            "Danail Stoyanov",
            "Jakob N. Kather",
            "Fiona R. Kolbinger",
            "Sebastian Bodenstedt",
            "Stefanie Speidel"
        ],
        "ai_categories": [
            "Dataset",
            "GAN"
        ],
        "tldr": "The paper presents the results of the FedSurg Challenge which evaluates federated learning strategies for surgical video classification in appendicitis. It highlights the trade-off between local personalization and global robustness in decentralized training.",
        "tldr_zh": "该论文介绍了FedSurg挑战的结果，评估了在阑尾炎中进行手术视频分类的联邦学习策略。它强调了去中心化训练中本地个性化和全局鲁棒性之间的权衡。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "SPEGNet: Synergistic Perception-Guided Network for Camouflaged Object Detection",
        "summary": "Camouflaged object detection segments objects with intrinsic similarity and\nedge disruption. Current detection methods rely on accumulated complex\ncomponents. Each approach adds components such as boundary modules, attention\nmechanisms, and multi-scale processors independently. This accumulation creates\na computational burden without proportional gains. To manage this complexity,\nthey process at reduced resolutions, eliminating fine details essential for\ncamouflage. We present SPEGNet, addressing fragmentation through a unified\ndesign. The architecture integrates multi-scale features via channel\ncalibration and spatial enhancement. Boundaries emerge directly from\ncontext-rich representations, maintaining semantic-spatial alignment.\nProgressive refinement implements scale-adaptive edge modulation with peak\ninfluence at intermediate resolutions. This design strikes a balance between\nboundary precision and regional consistency. SPEGNet achieves 0.887 $S_\\alpha$\non CAMO, 0.890 on COD10K, and 0.895 on NC4K, with real-time inference speed.\nOur approach excels across scales, from tiny, intricate objects to large,\npattern-similar ones, while handling occlusion and ambiguous boundaries. Code,\nmodel weights, and results are available on\n\\href{https://github.com/Baber-Jan/SPEGNet}{https://github.com/Baber-Jan/SPEGNet}.",
        "url": "http://arxiv.org/abs/2510.04472v1",
        "published_date": "2025-10-06T04:06:40+00:00",
        "updated_date": "2025-10-06T04:06:40+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "eess.IV"
        ],
        "authors": [
            "Baber Jan",
            "Saeed Anwar",
            "Aiman H. El-Maleh",
            "Abdul Jabbar Siddiqui",
            "Abdul Bais"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "SPEGNet is a camouflaged object detection network that integrates multi-scale features and progressive refinement for improved performance.",
        "tldr_zh": "SPEGNet是一种伪装物体检测网络，通过整合多尺度特征和渐进细化来提高性能。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "The method of the approximate inverse for limited-angle CT",
        "summary": "Limited-angle computerized tomography stands for one of the most difficult\nchallenges in imaging. Although it opens the way to faster data acquisition in\nindustry and less dangerous scans in medicine, standard approaches, such as the\nfiltered backprojection (FBP) algorithm or the widely used total-variation\nfunctional, often produce various artefacts that hinder the diagnosis. With the\nrise of deep learning, many modern techniques have proven themselves successful\nin removing such artefacts but at the cost of large datasets. In this paper, we\npropose a new model-driven approach based on the method of the approximate\ninverse, which could serve as new starting point for learning strategies in the\nfuture. In contrast to FBP-type approaches, our reconstruction step consists in\nevaluating linear functionals on the measured data using reconstruction kernels\nthat are precomputed as solution of an auxiliary problem. With this problem\nbeing uniquely solvable, the derived limited-angle reconstruction kernel (LARK)\nis able to fully reconstruct the object without the well-known streak\nartefacts, even for large limited angles. However, it inherits severe\nill-conditioning which leads to a different kind of artefacts arising from the\nsingular functions of the limited-angle Radon transform. The problem becomes\nparticularly challenging when working on semi-discrete (real or analytical)\nmeasurements. We develop a general regularization strategy, named constrained\nlimited-angle reconstruction kernel (CLARK), by combining spectral filter, the\nmethod of the approximate inverse and custom edge-preserving denoising in order\nto stabilize the whole process. We further derive and interpret error estimates\nfor the application on real, i.e. semi-discrete, data and we validate our\napproach on synthetic and real data.",
        "url": "http://arxiv.org/abs/2510.04369v1",
        "published_date": "2025-10-05T21:24:44+00:00",
        "updated_date": "2025-10-05T21:24:44+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.NA",
            "math.NA"
        ],
        "authors": [
            "Bernadette Hahn",
            "Gael Rigaud",
            "Richard Schmähl"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a new method for limited-angle computerized tomography reconstruction, aiming to reduce artefacts and improve image quality.",
        "tldr_zh": "本文介绍了一种新的用于有限角度计算机断层扫描重建的方法，旨在减少伪影并改善图像质量。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.0
    },
    {
        "title": "μDeepIQA: deep learning-based fast and robust image quality assessment with local predictions for optical microscopy",
        "summary": "Optical microscopy is one of the most widely used techniques in research\nstudies for life sciences and biomedicine. These applications require reliable\nexperimental pipelines to extract valuable knowledge from the measured samples\nand must be supported by image quality assessment (IQA) to ensure correct\nprocessing and analysis of the image data. IQA methods are implemented with\nvariable complexity. However, while most quality metrics have a straightforward\nimplementation, they might be time consuming and computationally expensive when\nevaluating a large dataset. In addition, quality metrics are often designed for\nwell-defined image features and may be unstable for images out of the ideal\ndomain.\n  To overcome these limitations, recent works have proposed deep learning-based\nIQA methods, which can provide superior performance, increased generalizability\nand fast prediction. Our method, named $\\mathrm{\\mu}$DeepIQA, is inspired by\nprevious studies and applies a deep convolutional neural network designed for\nIQA on natural images to optical microscopy measurements. We retrained the same\narchitecture to predict individual quality metrics and global quality scores\nfor optical microscopy data. The resulting models provide fast and stable\npredictions of image quality by generalizing quality estimation even outside\nthe ideal range of standard methods. In addition, $\\mathrm{\\mu}$DeepIQA\nprovides patch-wise prediction of image quality and can be used to visualize\nspatially varying quality in a single image. Our study demonstrates that\noptical microscopy-based studies can benefit from the generalizability of deep\nlearning models due to their stable performance in the presence of outliers,\nthe ability to assess small image patches, and rapid predictions.",
        "url": "http://arxiv.org/abs/2510.04859v1",
        "published_date": "2025-10-06T14:48:36+00:00",
        "updated_date": "2025-10-06T14:48:36+00:00",
        "categories": [
            "cs.CV",
            "physics.data-an",
            "q-bio.QM"
        ],
        "authors": [
            "Elena Corbetta",
            "Thomas Bocklitz"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ]
    }
]