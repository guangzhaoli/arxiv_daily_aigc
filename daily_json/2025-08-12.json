[
    {
        "title": "Grouped Speculative Decoding for Autoregressive Image Generation",
        "summary": "Recently, autoregressive (AR) image models have demonstrated remarkable\ngenerative capabilities, positioning themselves as a compelling alternative to\ndiffusion models. However, their sequential nature leads to long inference\ntimes, limiting their practical scalability. In this work, we introduce Grouped\nSpeculative Decoding (GSD), a novel, training-free acceleration method for AR\nimage models. While recent studies have explored Speculative Decoding (SD) as a\nmeans to speed up AR image generation, existing approaches either provide only\nmodest acceleration or require additional training. Our in-depth analysis\nreveals a fundamental difference between language and image tokens: image\ntokens exhibit inherent redundancy and diversity, meaning multiple tokens can\nconvey valid semantics. However, traditional SD methods are designed to accept\nonly a single most-likely token, which fails to leverage this difference,\nleading to excessive false-negative rejections. To address this, we propose a\nnew SD strategy that evaluates clusters of visually valid tokens rather than\nrelying on a single target token. Additionally, we observe that static\nclustering based on embedding distance is ineffective, which motivates our\ndynamic GSD approach. Extensive experiments show that GSD accelerates AR image\nmodels by an average of 3.7x while preserving image quality-all without\nrequiring any additional training. The source code is available at\nhttps://github.com/junhyukso/GSD",
        "url": "http://arxiv.org/abs/2508.07747v1",
        "published_date": "2025-08-11T08:27:57+00:00",
        "updated_date": "2025-08-11T08:27:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junhyuk So",
            "Juncheol Shin",
            "Hyunho Kook",
            "Eunhyeok Park"
        ],
        "ai_categories": [
            "AIGC"
        ],
        "tldr": "The paper introduces Grouped Speculative Decoding (GSD), a training-free acceleration method for autoregressive image models, achieving 3.7x speedup without compromising image quality.",
        "tldr_zh": "本文介绍了一种名为Grouped Speculative Decoding（GSD）的无需训练的加速方法，用于自回归图像模型，实现了3.7倍的加速而不影响图像质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 10,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "A Physics-Driven Neural Network with Parameter Embedding for Generating Quantitative MR Maps from Weighted Images",
        "summary": "We propose a deep learning-based approach that integrates MRI sequence\nparameters to improve the accuracy and generalizability of quantitative image\nsynthesis from clinical weighted MRI. Our physics-driven neural network embeds\nMRI sequence parameters -- repetition time (TR), echo time (TE), and inversion\ntime (TI) -- directly into the model via parameter embedding, enabling the\nnetwork to learn the underlying physical principles of MRI signal formation.\nThe model takes conventional T1-weighted, T2-weighted, and T2-FLAIR images as\ninput and synthesizes T1, T2, and proton density (PD) quantitative maps.\nTrained on healthy brain MR images, it was evaluated on both internal and\nexternal test datasets. The proposed method achieved high performance with PSNR\nvalues exceeding 34 dB and SSIM values above 0.92 for all synthesized parameter\nmaps. It outperformed conventional deep learning models in accuracy and\nrobustness, including data with previously unseen brain structures and lesions.\nNotably, our model accurately synthesized quantitative maps for these unseen\npathological regions, highlighting its superior generalization capability.\nIncorporating MRI sequence parameters via parameter embedding allows the neural\nnetwork to better learn the physical characteristics of MR signals,\nsignificantly enhancing the performance and reliability of quantitative MRI\nsynthesis. This method shows great potential for accelerating qMRI and\nimproving its clinical utility.",
        "url": "http://arxiv.org/abs/2508.08123v1",
        "published_date": "2025-08-11T16:01:12+00:00",
        "updated_date": "2025-08-11T16:01:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lingjing Chen",
            "Chengxiu Zhang",
            "Yinqiao Yi",
            "Yida Wang",
            "Yang Song",
            "Xu Yan",
            "Shengfang Xu",
            "Dalin Zhu",
            "Mengqiu Cao",
            "Yan Zhou",
            "Chenglong Wang",
            "Guang Yang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a physics-driven neural network that integrates MRI sequence parameters to generate accurate and generalizable quantitative MR maps from clinical weighted MRI images.",
        "tldr_zh": "该论文引入了一种物理驱动的神经网络，通过整合MRI序列参数，从临床加权MRI图像生成准确且具有普适性的定量MR图。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation",
        "summary": "Generating high-fidelity human videos that match user-specified identities is\nimportant yet challenging in the field of generative AI. Existing methods often\nrely on an excessive number of training parameters and lack compatibility with\nother AIGC tools. In this paper, we propose Stand-In, a lightweight and\nplug-and-play framework for identity preservation in video generation.\nSpecifically, we introduce a conditional image branch into the pre-trained\nvideo generation model. Identity control is achieved through restricted\nself-attentions with conditional position mapping, and can be learned quickly\nwith only 2000 pairs. Despite incorporating and training just $\\sim$1\\%\nadditional parameters, our framework achieves excellent results in video\nquality and identity preservation, outperforming other full-parameter training\nmethods. Moreover, our framework can be seamlessly integrated for other tasks,\nsuch as subject-driven video generation, pose-referenced video generation,\nstylization, and face swapping.",
        "url": "http://arxiv.org/abs/2508.07901v1",
        "published_date": "2025-08-11T12:17:38+00:00",
        "updated_date": "2025-08-11T12:17:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bowen Xue",
            "Qixin Yan",
            "Wenjing Wang",
            "Hao Liu",
            "Chen Li"
        ],
        "ai_categories": [
            "AIGC",
            "GAN"
        ],
        "tldr": "The paper introduces Stand-In, a lightweight framework for identity preservation in video generation that achieves excellent results with minimal additional parameters and can be used for various tasks like subject-driven video generation and face swapping.",
        "tldr_zh": "该论文介绍了Stand-In，一种轻量级身份保留框架，用于视频生成，具有出色的结果，并且可以用于主体驱动视频生成和面部交换等各种任务。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "TBAC-UniImage: Unified Understanding and Generation by Ladder-Side Diffusion Tuning",
        "summary": "This paper introduces TBAC-UniImage, a novel unified model for multimodal\nunderstanding and generation. We achieve this by deeply integrating a\npre-trained Diffusion Model, acting as a generative ladder, with a Multimodal\nLarge Language Model (MLLM). Previous diffusion-based unified models face two\nprimary limitations. One approach uses only the MLLM's final hidden state as\nthe generative condition. This creates a shallow connection, as the generator\nis isolated from the rich, hierarchical representations within the MLLM's\nintermediate layers. The other approach, pretraining a unified generative\narchitecture from scratch, is computationally expensive and prohibitive for\nmany researchers. To overcome these issues, our work explores a new paradigm.\nInstead of relying on a single output, we use representations from multiple,\ndiverse layers of the MLLM as generative conditions for the diffusion model.\nThis method treats the pre-trained generator as a ladder, receiving guidance\nfrom various depths of the MLLM's understanding process. Consequently,\nTBAC-UniImage achieves a much deeper and more fine-grained unification of\nunderstanding and generation.",
        "url": "http://arxiv.org/abs/2508.08098v1",
        "published_date": "2025-08-11T15:37:22+00:00",
        "updated_date": "2025-08-11T15:37:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junzhe Xu",
            "Yuyang Yin",
            "Xi Chen"
        ],
        "ai_categories": [
            "Multimodality",
            "Diffusion",
            "Other"
        ],
        "tldr": "TBAC-UniImage is a unified model for multimodal understanding and generation, integrating a pre-trained Diffusion Model with a Multimodal Large Language Model (MLLM) to achieve deeper and more fine-grained unification.",
        "tldr_zh": "TBAC-UniImage是一个用于多模态理解和生成的统一模型，通过将预训练的扩散模型与多模态大语言模型（MLLM）深度集成，实现了更深度和更精细化的统一。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Matrix-3D: Omnidirectional Explorable 3D World Generation",
        "summary": "Explorable 3D world generation from a single image or text prompt forms a\ncornerstone of spatial intelligence. Recent works utilize video model to\nachieve wide-scope and generalizable 3D world generation. However, existing\napproaches often suffer from a limited scope in the generated scenes. In this\nwork, we propose Matrix-3D, a framework that utilize panoramic representation\nfor wide-coverage omnidirectional explorable 3D world generation that combines\nconditional video generation and panoramic 3D reconstruction. We first train a\ntrajectory-guided panoramic video diffusion model that employs scene mesh\nrenders as condition, to enable high-quality and geometrically consistent scene\nvideo generation. To lift the panorama scene video to 3D world, we propose two\nseparate methods: (1) a feed-forward large panorama reconstruction model for\nrapid 3D scene reconstruction and (2) an optimization-based pipeline for\naccurate and detailed 3D scene reconstruction. To facilitate effective\ntraining, we also introduce the Matrix-Pano dataset, the first large-scale\nsynthetic collection comprising 116K high-quality static panoramic video\nsequences with depth and trajectory annotations. Extensive experiments\ndemonstrate that our proposed framework achieves state-of-the-art performance\nin panoramic video generation and 3D world generation. See more in\nhttps://matrix-3d.github.io.",
        "url": "http://arxiv.org/abs/2508.08086v1",
        "published_date": "2025-08-11T15:29:57+00:00",
        "updated_date": "2025-08-11T15:29:57+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Zhongqi Yang",
            "Wenhang Ge",
            "Yuqi Li",
            "Jiaqi Chen",
            "Haoyuan Li",
            "Mengyin An",
            "Fei Kang",
            "Hua Xue",
            "Baixin Xu",
            "Yuyang Yin",
            "Eric Li",
            "Yang Liu",
            "Yikai Wang",
            "Hao-Xiang Guo",
            "Yahui Zhou"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a framework called Matrix-3D for generating wide-coverage omnidirectional 3D worlds from single images or text prompts, achieving state-of-the-art performance in panoramic video and 3D world generation.",
        "tldr_zh": "该论文介绍了一种名为Matrix-3D的框架，用于从单个图像或文本提示生成广覆盖全向3D世界，在全景视频和3D世界生成方面取得了最新成果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation",
        "summary": "Visual effects (VFX) are essential visual enhancements fundamental to modern\ncinematic production. Although video generation models offer cost-efficient\nsolutions for VFX production, current methods are constrained by per-effect\nLoRA training, which limits generation to single effects. This fundamental\nlimitation impedes applications that require spatially controllable composite\neffects, i.e., the concurrent generation of multiple effects at designated\nlocations. However, integrating diverse effects into a unified framework faces\nmajor challenges: interference from effect variations and spatial\nuncontrollability during multi-VFX joint training. To tackle these challenges,\nwe propose Omni-Effects, a first unified framework capable of generating\nprompt-guided effects and spatially controllable composite effects. The core of\nour framework comprises two key innovations: (1) LoRA-based Mixture of Experts\n(LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects\nwithin a unified model while effectively mitigating cross-task interference.\n(2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the\ntext token, enabling precise spatial control. Furthermore, we introduce an\nIndependent-Information Flow (IIF) module integrated within the SAP, isolating\nthe control signals corresponding to individual effects to prevent any unwanted\nblending. To facilitate this research, we construct a comprehensive VFX dataset\nOmni-VFX via a novel data collection pipeline combining image editing and\nFirst-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX\nevaluation framework for validating model performance. Extensive experiments\ndemonstrate that Omni-Effects achieves precise spatial control and diverse\neffect generation, enabling users to specify both the category and location of\ndesired effects.",
        "url": "http://arxiv.org/abs/2508.07981v1",
        "published_date": "2025-08-11T13:41:24+00:00",
        "updated_date": "2025-08-11T13:41:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Fangyuan Mao",
            "Aiming Hao",
            "Jintao Chen",
            "Dongxia Liu",
            "Xiaokun Feng",
            "Jiashu Zhu",
            "Meiqi Wu",
            "Chubin Chen",
            "Jiahong Wu",
            "Xiangxiang Chu"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes Omni-Effects, a unified framework for generating spatially controllable visual effects, integrating diverse effects and enabling users to specify both the category and location of desired effects.",
        "tldr_zh": "本文提出了Omni-Effects，一个统一框架用于生成具有空间可控性的视觉效果，集成了多种效果，使用户能够指定所需效果的类别和位置。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Mem4D: Decoupling Static and Dynamic Memory for Dynamic Scene Reconstruction",
        "summary": "Reconstructing dense geometry for dynamic scenes from a monocular video is a\ncritical yet challenging task. Recent memory-based methods enable efficient\nonline reconstruction, but they fundamentally suffer from a Memory Demand\nDilemma: The memory representation faces an inherent conflict between the\nlong-term stability required for static structures and the rapid, high-fidelity\ndetail retention needed for dynamic motion. This conflict forces existing\nmethods into a compromise, leading to either geometric drift in static\nstructures or blurred, inaccurate reconstructions of dynamic objects. To\naddress this dilemma, we propose Mem4D, a novel framework that decouples the\nmodeling of static geometry and dynamic motion. Guided by this insight, we\ndesign a dual-memory architecture: 1) The Transient Dynamics Memory (TDM)\nfocuses on capturing high-frequency motion details from recent frames, enabling\naccurate and fine-grained modeling of dynamic content; 2) The Persistent\nStructure Memory (PSM) compresses and preserves long-term spatial information,\nensuring global consistency and drift-free reconstruction for static elements.\nBy alternating queries to these specialized memories, Mem4D simultaneously\nmaintains static geometry with global consistency and reconstructs dynamic\nelements with high fidelity. Experiments on challenging benchmarks demonstrate\nthat our method achieves state-of-the-art or competitive performance while\nmaintaining high efficiency. Codes will be publicly available.",
        "url": "http://arxiv.org/abs/2508.07908v1",
        "published_date": "2025-08-11T12:23:31+00:00",
        "updated_date": "2025-08-11T12:23:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xudong Cai",
            "Shuo Wang",
            "Peng Wang",
            "Yongcai Wang",
            "Zhaoxin Fan",
            "Wanting Li",
            "Tianbao Zhang",
            "Jianrong Tao",
            "Yeying Jin",
            "Deying Li"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "Mem4D proposes a novel framework that decouples static geometry and dynamic motion memory for dynamic scene reconstruction, achieving state-of-the-art performance on challenging benchmarks.",
        "tldr_zh": "Mem4D提出了一种新的框架，将静态几何和动态运动记忆进行解耦，用于动态场景重建，在具有挑战性的基准测试中实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Dream4D: Lifting Camera-Controlled I2V towards Spatiotemporally Consistent 4D Generation",
        "summary": "The synthesis of spatiotemporally coherent 4D content presents fundamental\nchallenges in computer vision, requiring simultaneous modeling of high-fidelity\nspatial representations and physically plausible temporal dynamics. Current\napproaches often struggle to maintain view consistency while handling complex\nscene dynamics, particularly in large-scale environments with multiple\ninteracting elements. This work introduces Dream4D, a novel framework that\nbridges this gap through a synergy of controllable video generation and neural\n4D reconstruction. Our approach seamlessly combines a two-stage architecture:\nit first predicts optimal camera trajectories from a single image using\nfew-shot learning, then generates geometrically consistent multi-view sequences\nvia a specialized pose-conditioned diffusion process, which are finally\nconverted into a persistent 4D representation. This framework is the first to\nleverage both rich temporal priors from video diffusion models and geometric\nawareness of the reconstruction models, which significantly facilitates 4D\ngeneration and shows higher quality (e.g., mPSNR, mSSIM) over existing methods.",
        "url": "http://arxiv.org/abs/2508.07769v1",
        "published_date": "2025-08-11T08:55:47+00:00",
        "updated_date": "2025-08-11T08:55:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoyan Liu",
            "Kangrui Li",
            "Jiaxin Liu"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Dataset"
        ],
        "tldr": "Dream4D introduces a novel framework for generating spatiotemporally consistent 4D content using controllable video generation and neural 4D reconstruction.",
        "tldr_zh": "Dream4D提出了一种新颖的框架，利用可控视频生成和神经4D重建来生成时空一致的4D内容。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering",
        "summary": "We propose a novel training-free image generation algorithm that precisely\ncontrols the occlusion relationships between objects in an image. Existing\nimage generation methods typically rely on prompts to influence occlusion,\nwhich often lack precision. While layout-to-image methods provide control over\nobject locations, they fail to address occlusion relationships explicitly.\nGiven a pre-trained image diffusion model, our method leverages volume\nrendering principles to \"render\" the scene in latent space, guided by occlusion\nrelationships and the estimated transmittance of objects. This approach does\nnot require retraining or fine-tuning the image diffusion model, yet it enables\naccurate occlusion control due to its physics-grounded foundation. In extensive\nexperiments, our method significantly outperforms existing approaches in terms\nof occlusion accuracy. Furthermore, we demonstrate that by adjusting the\nopacities of objects or concepts during rendering, our method can achieve a\nvariety of effects, such as altering the transparency of objects, the density\nof mass (e.g., forests), the concentration of particles (e.g., rain, fog), the\nintensity of light, and the strength of lens effects, etc.",
        "url": "http://arxiv.org/abs/2508.07647v1",
        "published_date": "2025-08-11T05:57:59+00:00",
        "updated_date": "2025-08-11T05:57:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaohang Zhan",
            "Dingming Liu"
        ],
        "ai_categories": [
            "GAN",
            "AIGC"
        ],
        "tldr": "LaRender is a training-free image generation algorithm that controls occlusion relationships between objects in images with high precision using volume rendering principles in latent space.",
        "tldr_zh": "LaRender是一种无需训练的图像生成算法，通过在潜在空间中使用体积渲染原理精准控制图像中物体之间的遮挡关系。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "LaVieID: Local Autoregressive Diffusion Transformers for Identity-Preserving Video Creation",
        "summary": "In this paper, we present LaVieID, a novel \\underline{l}ocal\n\\underline{a}utoregressive \\underline{vi}d\\underline{e}o diffusion framework\ndesigned to tackle the challenging \\underline{id}entity-preserving\ntext-to-video task. The key idea of LaVieID is to mitigate the loss of identity\ninformation inherent in the stochastic global generation process of diffusion\ntransformers (DiTs) from both spatial and temporal perspectives. Specifically,\nunlike the global and unstructured modeling of facial latent states in existing\nDiTs, LaVieID introduces a local router to explicitly represent latent states\nby weighted combinations of fine-grained local facial structures. This\nalleviates undesirable feature interference and encourages DiTs to capture\ndistinctive facial characteristics. Furthermore, a temporal autoregressive\nmodule is integrated into LaVieID to refine denoised latent tokens before video\ndecoding. This module divides latent tokens temporally into chunks, exploiting\ntheir long-range temporal dependencies to predict biases for rectifying tokens,\nthereby significantly enhancing inter-frame identity consistency. Consequently,\nLaVieID can generate high-fidelity personalized videos and achieve\nstate-of-the-art performance. Our code and models are available at\nhttps://github.com/ssugarwh/LaVieID.",
        "url": "http://arxiv.org/abs/2508.07603v1",
        "published_date": "2025-08-11T04:13:32+00:00",
        "updated_date": "2025-08-11T04:13:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenhui Song",
            "Hanhui Li",
            "Jiehui Huang",
            "Panwen Hu",
            "Yuhao Cheng",
            "Long Chen",
            "Yiqiang Yan",
            "Xiaodan Liang"
        ],
        "ai_categories": [
            "LoRA",
            "Diffusion",
            "Transformer",
            "GAN"
        ],
        "tldr": "LaVieID is a novel framework for identity-preserving video creation that leverages local autoregressive diffusion transformers, achieving high-fidelity personalized videos and state-of-the-art performance.",
        "tldr_zh": "LaVieID是一个新颖的框架，用于保持身份并生成视频，利用本地自回归扩散变换器，实现高保真个性化视频和最先进性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "From Prediction to Explanation: Multimodal, Explainable, and Interactive Deepfake Detection Framework for Non-Expert Users",
        "summary": "The proliferation of deepfake technologies poses urgent challenges and\nserious risks to digital integrity, particularly within critical sectors such\nas forensics, journalism, and the legal system. While existing detection\nsystems have made significant progress in classification accuracy, they\ntypically function as black-box models, offering limited transparency and\nminimal support for human reasoning. This lack of interpretability hinders\ntheir usability in real-world decision-making contexts, especially for\nnon-expert users. In this paper, we present DF-P2E (Deepfake: Prediction to\nExplanation), a novel multimodal framework that integrates visual, semantic,\nand narrative layers of explanation to make deepfake detection interpretable\nand accessible. The framework consists of three modular components: (1) a\ndeepfake classifier with Grad-CAM-based saliency visualisation, (2) a visual\ncaptioning module that generates natural language summaries of manipulated\nregions, and (3) a narrative refinement module that uses a fine-tuned Large\nLanguage Model (LLM) to produce context-aware, user-sensitive explanations. We\ninstantiate and evaluate the framework on the DF40 benchmark, the most diverse\ndeepfake dataset to date. Experiments demonstrate that our system achieves\ncompetitive detection performance while providing high-quality explanations\naligned with Grad-CAM activations. By unifying prediction and explanation in a\ncoherent, human-aligned pipeline, this work offers a scalable approach to\ninterpretable deepfake detection, advancing the broader vision of trustworthy\nand transparent AI systems in adversarial media environments.",
        "url": "http://arxiv.org/abs/2508.07596v1",
        "published_date": "2025-08-11T03:55:47+00:00",
        "updated_date": "2025-08-11T03:55:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shahroz Tariq",
            "Simon S. Woo",
            "Priyanka Singh",
            "Irena Irmalasari",
            "Saakshi Gupta",
            "Dev Gupta"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a framework for explainable deepfake detection using visual, semantic, and narrative explanations, achieving competitive detection performance and high-quality explanations.",
        "tldr_zh": "本文介绍了一个可解释的深度伪造检测框架，使用视觉、语义和叙事解释，实现了竞争性的检测性能和高质量的解释。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Enhanced Generative Structure Prior for Chinese Text Image Super-resolution",
        "summary": "Faithful text image super-resolution (SR) is challenging because each\ncharacter has a unique structure and usually exhibits diverse font styles and\nlayouts. While existing methods primarily focus on English text, less attention\nhas been paid to more complex scripts like Chinese. In this paper, we introduce\na high-quality text image SR framework designed to restore the precise strokes\nof low-resolution (LR) Chinese characters. Unlike methods that rely on\ncharacter recognition priors to regularize the SR task, we propose a novel\nstructure prior that offers structure-level guidance to enhance visual quality.\nOur framework incorporates this structure prior within a StyleGAN model,\nleveraging its generative capabilities for restoration. To maintain the\nintegrity of character structures while accommodating various font styles and\nlayouts, we implement a codebook-based mechanism that restricts the generative\nspace of StyleGAN. Each code in the codebook represents the structure of a\nspecific character, while the vector $w$ in StyleGAN controls the character's\nstyle, including typeface, orientation, and location. Through the collaborative\ninteraction between the codebook and style, we generate a high-resolution\nstructure prior that aligns with LR characters both spatially and structurally.\nExperiments demonstrate that this structure prior provides robust,\ncharacter-specific guidance, enabling the accurate restoration of clear strokes\nin degraded characters, even for real-world LR Chinese text with irregular\nlayouts. Our code and pre-trained models will be available at\nhttps://github.com/csxmli2016/MARCONetPlusPlus",
        "url": "http://arxiv.org/abs/2508.07537v1",
        "published_date": "2025-08-11T01:34:45+00:00",
        "updated_date": "2025-08-11T01:34:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoming Li",
            "Wangmeng Zuo",
            "Chen Change Loy"
        ],
        "ai_categories": [
            "GAN",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a framework for enhancing Chinese text image super-resolution using a structure prior in a StyleGAN model, improving visual quality and stroke restoration.",
        "tldr_zh": "本文介绍了一种利用StyleGAN模型中的结构先验来增强中文文本图像超分辨率的框架，提高视觉质量和笔画恢复。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "SAGOnline: Segment Any Gaussians Online",
        "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for explicit\n3D scene representation, yet achieving efficient and consistent 3D segmentation\nremains challenging. Current methods suffer from prohibitive computational\ncosts, limited 3D spatial reasoning, and an inability to track multiple objects\nsimultaneously. We present Segment Any Gaussians Online (SAGOnline), a\nlightweight and zero-shot framework for real-time 3D segmentation in Gaussian\nscenes that addresses these limitations through two key innovations: (1) a\ndecoupled strategy that integrates video foundation models (e.g., SAM2) for\nview-consistent 2D mask propagation across synthesized views; and (2) a\nGPU-accelerated 3D mask generation and Gaussian-level instance labeling\nalgorithm that assigns unique identifiers to 3D primitives, enabling lossless\nmulti-object tracking and segmentation across views. SAGOnline achieves\nstate-of-the-art performance on NVOS (92.7% mIoU) and Spin-NeRF (95.2% mIoU)\nbenchmarks, outperforming Feature3DGS, OmniSeg3D-gs, and SA3D by 15--1500 times\nin inference speed (27 ms/frame). Qualitative results demonstrate robust\nmulti-object segmentation and tracking in complex scenes. Our contributions\ninclude: (i) a lightweight and zero-shot framework for 3D segmentation in\nGaussian scenes, (ii) explicit labeling of Gaussian primitives enabling\nsimultaneous segmentation and tracking, and (iii) the effective adaptation of\n2D video foundation models to the 3D domain. This work allows real-time\nrendering and 3D scene understanding, paving the way for practical AR/VR and\nrobotic applications.",
        "url": "http://arxiv.org/abs/2508.08219v1",
        "published_date": "2025-08-11T17:38:50+00:00",
        "updated_date": "2025-08-11T17:38:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wentao Sun",
            "Quanyun Wu",
            "Hanqing Xu",
            "Kyle Gao",
            "Zhengsen Xu",
            "Yiping Chen",
            "Dedong Zhang",
            "Lingfei Ma",
            "John S. Zelek",
            "Jonathan Li"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "SAGOnline is a framework for real-time 3D segmentation in Gaussian scenes, achieving state-of-the-art performance and enabling multi-object tracking and segmentation.",
        "tldr_zh": "SAGOnline是一个用于高斯场景中的实时3D分割的框架，实现了最先进的性能并实现了多对象跟踪和分割。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "TRIDE: A Text-assisted Radar-Image weather-aware fusion network for Depth Estimation",
        "summary": "Depth estimation, essential for autonomous driving, seeks to interpret the 3D\nenvironment surrounding vehicles. The development of radar sensors, known for\ntheir cost-efficiency and robustness, has spurred interest in radar-camera\nfusion-based solutions. However, existing algorithms fuse features from these\nmodalities without accounting for weather conditions, despite radars being\nknown to be more robust than cameras under adverse weather. Additionally, while\nVision-Language models have seen rapid advancement, utilizing language\ndescriptions alongside other modalities for depth estimation remains an open\nchallenge. This paper first introduces a text-generation strategy along with\nfeature extraction and fusion techniques that can assist monocular depth\nestimation pipelines, leading to improved accuracy across different algorithms\non the KITTI dataset. Building on this, we propose TRIDE, a radar-camera fusion\nalgorithm that enhances text feature extraction by incorporating radar point\ninformation. To address the impact of weather on sensor performance, we\nintroduce a weather-aware fusion block that adaptively adjusts radar weighting\nbased on current weather conditions. Our method, benchmarked on the nuScenes\ndataset, demonstrates performance gains over the state-of-the-art, achieving a\n12.87% improvement in MAE and a 9.08% improvement in RMSE. Code:\nhttps://github.com/harborsarah/TRIDE",
        "url": "http://arxiv.org/abs/2508.08038v1",
        "published_date": "2025-08-11T14:39:41+00:00",
        "updated_date": "2025-08-11T14:39:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Huawei Sun",
            "Zixu Wang",
            "Hao Feng",
            "Julius Ott",
            "Lorenzo Servadei",
            "Robert Wille"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "TRIDE introduces a radar-image fusion network for depth estimation that incorporates text descriptions and adapts to weather conditions, showing significant performance improvements over existing methods.",
        "tldr_zh": "TRIDE引入了一种雷达-图像融合网络用于深度估计，整合了文本描述并根据天气条件进行调整，在现有方法上取得了显著性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and Deformable 3D Gaussian Reconstruction",
        "summary": "Computer vision-based technologies significantly enhance surgical automation\nby advancing tool tracking, detection, and localization. However, Current\ndata-driven approaches are data-voracious, requiring large, high-quality\nlabeled image datasets, which limits their application in surgical data\nscience. Our Work introduces a novel dynamic Gaussian Splatting technique to\naddress the data scarcity in surgical image datasets. We propose a dynamic\nGaussian model to represent dynamic surgical scenes, enabling the rendering of\nsurgical instruments from unseen viewpoints and deformations with real tissue\nbackgrounds. We utilize a dynamic training adjustment strategy to address\nchallenges posed by poorly calibrated camera poses from real-world scenarios.\nAdditionally, we propose a method based on dynamic Gaussians for automatically\ngenerating annotations for our synthetic data. For evaluation, we constructed a\nnew dataset featuring seven scenes with 14,000 frames of tool and camera motion\nand tool jaw articulation, with a background of an ex-vivo porcine model. Using\nthis dataset, we synthetically replicate the scene deformation from the ground\ntruth data, allowing direct comparisons of synthetic image quality.\nExperimental results illustrate that our method generates photo-realistic\nlabeled image datasets with the highest values in Peak-Signal-to-Noise Ratio\n(29.87). We further evaluate the performance of medical-specific neural\nnetworks trained on real and synthetic images using an unseen real-world image\ndataset. Our results show that the performance of models trained on synthetic\nimages generated by the proposed method outperforms those trained with\nstate-of-the-art standard data augmentation by 10%, leading to an overall\nimprovement in model performances by nearly 15%.",
        "url": "http://arxiv.org/abs/2508.07897v1",
        "published_date": "2025-08-11T12:13:05+00:00",
        "updated_date": "2025-08-11T12:13:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "I.3.3"
        ],
        "authors": [
            "Tianle Zeng",
            "Junlei Hu",
            "Gerardo Loza Galindo",
            "Sharib Ali",
            "Duygu Sarikaya",
            "Pietro Valdastri",
            "Dominic Jones"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel method using dynamic Gaussian splatting to generate synthetic images of surgical instruments with real tissue backgrounds for training neural networks. These synthetic images outperform real images in training medical-specific models.",
        "tldr_zh": "该论文引入了一种新方法，利用动态高斯点生成术中器械的合成图像，并配有真实组织背景，用于训练神经网络。这些合成图像在训练医学模型时优于真实图像。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "DiTVR: Zero-Shot Diffusion Transformer for Video Restoration",
        "summary": "Video restoration aims to reconstruct high quality video sequences from low\nquality inputs, addressing tasks such as super resolution, denoising, and\ndeblurring. Traditional regression based methods often produce unrealistic\ndetails and require extensive paired datasets, while recent generative\ndiffusion models face challenges in ensuring temporal consistency. We introduce\nDiTVR, a zero shot video restoration framework that couples a diffusion\ntransformer with trajectory aware attention and a wavelet guided, flow\nconsistent sampler. Unlike prior 3D convolutional or frame wise diffusion\napproaches, our attention mechanism aligns tokens along optical flow\ntrajectories, with particular emphasis on vital layers that exhibit the highest\nsensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically\nselects relevant tokens based on motion correspondences across frames. The flow\nguided sampler injects data consistency only into low-frequency bands,\npreserving high frequency priors while accelerating convergence. DiTVR\nestablishes a new zero shot state of the art on video restoration benchmarks,\ndemonstrating superior temporal consistency and detail preservation while\nremaining robust to flow noise and occlusions.",
        "url": "http://arxiv.org/abs/2508.07811v1",
        "published_date": "2025-08-11T09:54:45+00:00",
        "updated_date": "2025-08-11T09:54:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sicheng Gao",
            "Nancy Mehta",
            "Zongwei Wu",
            "Radu Timofte"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "Dataset"
        ],
        "tldr": "DiTVR introduces a zero-shot video restoration framework using diffusion transformer with trajectory aware attention and flow guided sampler to achieve state-of-the-art results in video restoration tasks.",
        "tldr_zh": "DiTVR引入了一种零样本视频恢复框架，使用扩散变压器、轨迹感知注意力和流引导采样器，在视频恢复任务中取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "SOFA: Deep Learning Framework for Simulating and Optimizing Atrial Fibrillation Ablation",
        "summary": "Atrial fibrillation (AF) is a prevalent cardiac arrhythmia often treated with\ncatheter ablation procedures, but procedural outcomes are highly variable.\nEvaluating and improving ablation efficacy is challenging due to the complex\ninteraction between patient-specific tissue and procedural factors. This paper\nasks two questions: Can AF recurrence be predicted by simulating the effects of\nprocedural parameters? How should we ablate to reduce AF recurrence? We propose\nSOFA (Simulating and Optimizing Atrial Fibrillation Ablation), a novel\ndeep-learning framework that addresses these questions. SOFA first simulates\nthe outcome of an ablation strategy by generating a post-ablation image\ndepicting scar formation, conditioned on a patient's pre-ablation LGE-MRI and\nthe specific procedural parameters used (e.g., ablation locations, duration,\ntemperature, power, and force). During this simulation, it predicts AF\nrecurrence risk. Critically, SOFA then introduces an optimization scheme that\nrefines these procedural parameters to minimize the predicted risk. Our method\nleverages a multi-modal, multi-view generator that processes 2.5D\nrepresentations of the atrium. Quantitative evaluations show that SOFA\naccurately synthesizes post-ablation images and that our optimization scheme\nleads to a 22.18\\% reduction in the model-predicted recurrence risk. To the\nbest of our knowledge, SOFA is the first framework to integrate the simulation\nof procedural effects, recurrence prediction, and parameter optimization,\noffering a novel tool for personalizing AF ablation.",
        "url": "http://arxiv.org/abs/2508.07621v1",
        "published_date": "2025-08-11T05:01:54+00:00",
        "updated_date": "2025-08-11T05:01:54+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yunsung Chung",
            "Chanho Lim",
            "Ghassan Bidaoui",
            "Christian Massad",
            "Nassir Marrouche",
            "Jihun Hamm"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a deep-learning framework, SOFA, for simulating and optimizing atrial fibrillation ablation procedures to personalize treatment.",
        "tldr_zh": "本文介绍了一个深度学习框架SOFA，用于模拟和优化心房颤动消融手术，以个性化治疗。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning",
        "summary": "Current audio-visual (AV) benchmarks focus on final answer accuracy,\noverlooking the underlying reasoning process. This makes it difficult to\ndistinguish genuine comprehension from correct answers derived through flawed\nreasoning or hallucinations. To address this, we introduce AURA (Audio-visual\nUnderstanding and Reasoning Assessment), a benchmark for evaluating the\ncross-modal reasoning capabilities of Audio-Visual Large Language Models\n(AV-LLMs) and Omni-modal Language Models (OLMs). AURA includes questions across\nsix challenging cognitive domains, such as causality, timbre and pitch, tempo\nand AV synchronization, unanswerability, implicit distractions, and skill\nprofiling, explicitly designed to be unanswerable from a single modality. This\nforces models to construct a valid logical path grounded in both audio and\nvideo, setting AURA apart from AV datasets that allow uni-modal shortcuts. To\nassess reasoning traces, we propose a novel metric, AuraScore, which addresses\nthe lack of robust tools for evaluating reasoning fidelity. It decomposes\nreasoning into two aspects: (i) Factual Consistency - whether reasoning is\ngrounded in perceptual evidence, and (ii) Core Inference - the logical validity\nof each reasoning step. Evaluations of SOTA models on AURA reveal a critical\nreasoning gap: although models achieve high accuracy (up to 92% on some tasks),\ntheir Factual Consistency and Core Inference scores fall below 45%. This\ndiscrepancy highlights that models often arrive at correct answers through\nflawed logic, underscoring the need for our benchmark and paving the way for\nmore robust multimodal evaluation.",
        "url": "http://arxiv.org/abs/2508.07470v1",
        "published_date": "2025-08-10T20:06:42+00:00",
        "updated_date": "2025-08-10T20:06:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Siminfar Samakoush Galougah",
            "Rishie Raj",
            "Sanjoy Chowdhury",
            "Sayan Nag",
            "Ramani Duraiswami"
        ],
        "ai_categories": [
            "Multimodality"
        ],
        "tldr": "AURA introduces a benchmark for evaluating cross-modal reasoning capabilities of models by focusing on underlying reasoning processes rather than final answer accuracy.",
        "tldr_zh": "AURA介绍了一个用于评估模型的跨模态推理能力的基准，重点关注底层推理过程，而不是最终答案的准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Learning an Implicit Physics Model for Image-based Fluid Simulation",
        "summary": "Humans possess an exceptional ability to imagine 4D scenes, encompassing both\nmotion and 3D geometry, from a single still image. This ability is rooted in\nour accumulated observations of similar scenes and an intuitive understanding\nof physics. In this paper, we aim to replicate this capacity in neural\nnetworks, specifically focusing on natural fluid imagery. Existing methods for\nthis task typically employ simplistic 2D motion estimators to animate the\nimage, leading to motion predictions that often defy physical principles,\nresulting in unrealistic animations. Our approach introduces a novel method for\ngenerating 4D scenes with physics-consistent animation from a single image. We\npropose the use of a physics-informed neural network that predicts motion for\neach surface point, guided by a loss term derived from fundamental physical\nprinciples, including the Navier-Stokes equations. To capture appearance, we\npredict feature-based 3D Gaussians from the input image and its estimated\ndepth, which are then animated using the predicted motions and rendered from\nany desired camera perspective. Experimental results highlight the\neffectiveness of our method in producing physically plausible animations,\nshowcasing significant performance improvements over existing methods. Our\nproject page is https://physfluid.github.io/ .",
        "url": "http://arxiv.org/abs/2508.08254v1",
        "published_date": "2025-08-11T17:59:58+00:00",
        "updated_date": "2025-08-11T17:59:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Emily Yue-Ting Jia",
            "Jiageng Mao",
            "Zhiyuan Gao",
            "Yajie Zhao",
            "Yue Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a physics-informed neural network to generate 4D scenes with physics-consistent animation from a single image, showcasing significant improvements in producing realistic animations.",
        "tldr_zh": "本文提出了一种物理信息神经网络，用于从单个图像生成带有物理一致动画的4D场景，显示出在生成逼真动画方面的显著改进。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation",
        "summary": "Current diffusion models for audio-driven avatar video generation struggle to\nsynthesize long videos with natural audio synchronization and identity\nconsistency. This paper presents StableAvatar, the first end-to-end video\ndiffusion transformer that synthesizes infinite-length high-quality videos\nwithout post-processing. Conditioned on a reference image and audio,\nStableAvatar integrates tailored training and inference modules to enable\ninfinite-length video generation. We observe that the main reason preventing\nexisting models from generating long videos lies in their audio modeling. They\ntypically rely on third-party off-the-shelf extractors to obtain audio\nembeddings, which are then directly injected into the diffusion model via\ncross-attention. Since current diffusion backbones lack any audio-related\npriors, this approach causes severe latent distribution error accumulation\nacross video clips, leading the latent distribution of subsequent segments to\ndrift away from the optimal distribution gradually. To address this,\nStableAvatar introduces a novel Time-step-aware Audio Adapter that prevents\nerror accumulation via time-step-aware modulation. During inference, we propose\na novel Audio Native Guidance Mechanism to further enhance the audio\nsynchronization by leveraging the diffusion's own evolving joint audio-latent\nprediction as a dynamic guidance signal. To enhance the smoothness of the\ninfinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy\nthat fuses latent over time. Experiments on benchmarks show the effectiveness\nof StableAvatar both qualitatively and quantitatively.",
        "url": "http://arxiv.org/abs/2508.08248v1",
        "published_date": "2025-08-11T17:58:24+00:00",
        "updated_date": "2025-08-11T17:58:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuyuan Tu",
            "Yueming Pan",
            "Yinming Huang",
            "Xintong Han",
            "Zhen Xing",
            "Qi Dai",
            "Chong Luo",
            "Zuxuan Wu",
            "Yu-Gang Jiang"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "This paper introduces StableAvatar, an end-to-end video diffusion transformer that generates infinite-length high-quality videos without post-processing by addressing the issue of audio synchronization and identity consistency.",
        "tldr_zh": "本文介绍了StableAvatar，一种端到端视频扩散变换器，通过解决音频同步和身份一致性问题，生成无限长度高质量视频。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Cut2Next: Generating Next Shot via In-Context Tuning",
        "summary": "Effective multi-shot generation demands purposeful, film-like transitions and\nstrict cinematic continuity. Current methods, however, often prioritize basic\nvisual consistency, neglecting crucial editing patterns (e.g., shot/reverse\nshot, cutaways) that drive narrative flow for compelling storytelling. This\nyields outputs that may be visually coherent but lack narrative sophistication\nand true cinematic integrity. To bridge this, we introduce Next Shot Generation\n(NSG): synthesizing a subsequent, high-quality shot that critically conforms to\nprofessional editing patterns while upholding rigorous cinematic continuity.\nOur framework, Cut2Next, leverages a Diffusion Transformer (DiT). It employs\nin-context tuning guided by a novel Hierarchical Multi-Prompting strategy. This\nstrategy uses Relational Prompts to define overall context and inter-shot\nediting styles. Individual Prompts then specify per-shot content and\ncinematographic attributes. Together, these guide Cut2Next to generate\ncinematically appropriate next shots. Architectural innovations, Context-Aware\nCondition Injection (CACI) and Hierarchical Attention Mask (HAM), further\nintegrate these diverse signals without introducing new parameters. We\nconstruct RawCuts (large-scale) and CuratedCuts (refined) datasets, both with\nhierarchical prompts, and introduce CutBench for evaluation. Experiments show\nCut2Next excels in visual consistency and text fidelity. Crucially, user\nstudies reveal a strong preference for Cut2Next, particularly for its adherence\nto intended editing patterns and overall cinematic continuity, validating its\nability to generate high-quality, narratively expressive, and cinematically\ncoherent subsequent shots.",
        "url": "http://arxiv.org/abs/2508.08244v1",
        "published_date": "2025-08-11T17:56:59+00:00",
        "updated_date": "2025-08-11T17:56:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jingwen He",
            "Hongbo Liu",
            "Jiajun Li",
            "Ziqi Huang",
            "Yu Qiao",
            "Wanli Ouyang",
            "Ziwei Liu"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper introduces Cut2Next, a framework for generating next shots in videos that conform to professional editing patterns, enhancing narrative flow and cinematic continuity.",
        "tldr_zh": "该论文介绍了Cut2Next，一个用于生成符合专业编辑模式的视频下一镜头的框架，增强叙事流畅性和影视连贯性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OMGSR: You Only Need One Mid-timestep Guidance for Real-World Image Super-Resolution",
        "summary": "Denoising Diffusion Probabilistic Models (DDPM) and Flow Matching (FM)\ngenerative models show promising potential for one-step Real-World Image\nSuper-Resolution (Real-ISR). Recent one-step Real-ISR models typically inject a\nLow-Quality (LQ) image latent distribution at the initial timestep. However, a\nfundamental gap exists between the LQ image latent distribution and the\nGaussian noisy latent distribution, limiting the effective utilization of\ngenerative priors. We observe that the noisy latent distribution at DDPM/FM\nmid-timesteps aligns more closely with the LQ image latent distribution. Based\non this insight, we present One Mid-timestep Guidance Real-ISR (OMGSR), a\nuniversal framework applicable to DDPM/FM-based generative models. OMGSR\ninjects the LQ image latent distribution at a pre-computed mid-timestep,\nincorporating the proposed Latent Distribution Refinement loss to alleviate the\nlatent distribution gap. We also design the Overlap-Chunked LPIPS/GAN loss to\neliminate checkerboard artifacts in image generation. Within this framework, we\ninstantiate OMGSR for DDPM/FM-based generative models with two variants:\nOMGSR-S (SD-Turbo) and OMGSR-F (FLUX.1-dev). Experimental results demonstrate\nthat OMGSR-S/F achieves balanced/excellent performance across quantitative and\nqualitative metrics at 512-resolution. Notably, OMGSR-F establishes\noverwhelming dominance in all reference metrics. We further train a\n1k-resolution OMGSR-F to match the default resolution of FLUX.1-dev, which\nyields excellent results, especially in the details of the image generation. We\nalso generate 2k-resolution images by the 1k-resolution OMGSR-F using our\ntwo-stage Tiled VAE & Diffusion.",
        "url": "http://arxiv.org/abs/2508.08227v1",
        "published_date": "2025-08-11T17:44:59+00:00",
        "updated_date": "2025-08-11T17:44:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhiqiang Wu",
            "Zhaomang Sun",
            "Tong Zhou",
            "Bingtao Fu",
            "Ji Cong",
            "Yitong Dong",
            "Huaqi Zhang",
            "Xuan Tang",
            "Mingsong Chen",
            "Xian Wei"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Diffusion"
        ],
        "tldr": "OMGSR proposes a new method for Real-ISR by injecting the LQ image latent distribution at a mid-timestep, achieving excellent results at high resolutions.",
        "tldr_zh": "OMGSR提出了一种在中间时间步骤注入LQ图像潜在分布的新方法，以高分辨率取得出色的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Learning User Preferences for Image Generation Model",
        "summary": "User preference prediction requires a comprehensive and accurate\nunderstanding of individual tastes. This includes both surface-level\nattributes, such as color and style, and deeper content-related aspects, such\nas themes and composition. However, existing methods typically rely on general\nhuman preferences or assume static user profiles, often neglecting individual\nvariability and the dynamic, multifaceted nature of personal taste. To address\nthese limitations, we propose an approach built upon Multimodal Large Language\nModels, introducing contrastive preference loss and preference tokens to learn\npersonalized user preferences from historical interactions. The contrastive\npreference loss is designed to effectively distinguish between user ''likes''\nand ''dislikes'', while the learnable preference tokens capture shared interest\nrepresentations among existing users, enabling the model to activate\ngroup-specific preferences and enhance consistency across similar users.\nExtensive experiments demonstrate our model outperforms other methods in\npreference prediction accuracy, effectively identifying users with similar\naesthetic inclinations and providing more precise guidance for generating\nimages that align with individual tastes. The project page is\n\\texttt{https://learn-user-pref.github.io/}.",
        "url": "http://arxiv.org/abs/2508.08220v1",
        "published_date": "2025-08-11T17:39:42+00:00",
        "updated_date": "2025-08-11T17:39:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenyi Mo",
            "Ying Ba",
            "Tianyu Zhang",
            "Yalong Bai",
            "Biye Li"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality"
        ],
        "tldr": "The paper introduces a method using Multimodal Large Language Models to learn user preferences for image generation, outperforming existing methods in accuracy and personalized guidance for generating images aligned with individual tastes.",
        "tldr_zh": "本文介绍了一种使用多模态大型语言模型的方法，用于学习用户偏好的图像生成，其在准确性和生成与个人口味一致的图像方面胜过现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model",
        "summary": "Precise spatial modeling in the operating room (OR) is foundational to many\nclinical tasks, supporting intraoperative awareness, hazard avoidance, and\nsurgical decision-making. While existing approaches leverage large-scale\nmultimodal datasets for latent-space alignment to implicitly learn spatial\nrelationships, they overlook the 3D capabilities of MLLMs. However, this\napproach raises two issues: (1) Operating rooms typically lack multiple video\nand audio sensors, making multimodal 3D data difficult to obtain; (2) Training\nsolely on readily available 2D data fails to capture fine-grained details in\ncomplex scenes. To address this gap, we introduce Spatial-ORMLLM, the first\nlarge vision-language model for 3D spatial reasoning in operating rooms using\nonly RGB modality to infer volumetric and semantic cues, enabling downstream\nmedical tasks with detailed and holistic spatial context. Spatial-ORMLLM\nincorporates a Spatial-Enhanced Feature Fusion Block, which integrates 2D\nmodality inputs with rich 3D spatial knowledge extracted by the estimation\nalgorithm and then feeds the combined features into the visual tower. By\nemploying a unified end-to-end MLLM framework, it combines powerful spatial\nfeatures with textual features to deliver robust 3D scene reasoning without any\nadditional expert annotations or sensor inputs. Experiments on multiple\nbenchmark clinical datasets demonstrate that Spatial-ORMLLM achieves\nstate-of-the-art performance and generalizes robustly to previously unseen\nsurgical scenarios and downstream tasks.",
        "url": "http://arxiv.org/abs/2508.08199v1",
        "published_date": "2025-08-11T17:17:20+00:00",
        "updated_date": "2025-08-11T17:17:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Peiqi He",
            "Zhenhao Zhang",
            "Yixiang Zhang",
            "Xiongjun Zhao",
            "Shaoliang Peng"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "Spatial-ORMLLM introduces a large vision-language model for 3D spatial reasoning in operating rooms using only RGB modality, achieving state-of-the-art performance on multiple benchmark clinical datasets.",
        "tldr_zh": "Spatial-ORMLLM引入了一个大型视觉-语言模型，用于在手术室内进行3D空间推理，仅使用RGB模态，在多个基准临床数据集上取得了最新技术表现。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Reinforcement Learning in Vision: A Survey",
        "summary": "Recent advances at the intersection of reinforcement learning (RL) and visual\nintelligence have enabled agents that not only perceive complex visual scenes\nbut also reason, generate, and act within them. This survey offers a critical\nand up-to-date synthesis of the field. We first formalize visual RL problems\nand trace the evolution of policy-optimization strategies from RLHF to\nverifiable reward paradigms, and from Proximal Policy Optimization to Group\nRelative Policy Optimization. We then organize more than 200 representative\nworks into four thematic pillars: multi-modal large language models, visual\ngeneration, unified model frameworks, and vision-language-action models. For\neach pillar we examine algorithmic design, reward engineering, benchmark\nprogress, and we distill trends such as curriculum-driven training,\npreference-aligned diffusion, and unified reward modeling. Finally, we review\nevaluation protocols spanning set-level fidelity, sample-level preference, and\nstate-level stability, and we identify open challenges that include sample\nefficiency, generalization, and safe deployment. Our goal is to provide\nresearchers and practitioners with a coherent map of the rapidly expanding\nlandscape of visual RL and to highlight promising directions for future\ninquiry. Resources are available at:\nhttps://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.",
        "url": "http://arxiv.org/abs/2508.08189v1",
        "published_date": "2025-08-11T17:08:55+00:00",
        "updated_date": "2025-08-11T17:08:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weijia Wu",
            "Chen Gao",
            "Joya Chen",
            "Kevin Qinghong Lin",
            "Qingwei Meng",
            "Yiming Zhang",
            "Yuke Qiu",
            "Hong Zhou",
            "Mike Zheng Shou"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper surveys the intersection of reinforcement learning and visual intelligence, discussing policy optimization strategies and examining various thematic pillars in visual RL.",
        "tldr_zh": "该论文调查了强化学习与视觉智能的交叉领域，讨论了政策优化策略，并研究了视觉RL中的各种主题支柱。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "3D Human Mesh Estimation from Single View RGBD",
        "summary": "Despite significant progress in 3D human mesh estimation from RGB images;\nRGBD cameras, offering additional depth data, remain underutilized. In this\npaper, we present a method for accurate 3D human mesh estimation from a single\nRGBD view, leveraging the affordability and widespread adoption of RGBD cameras\nfor real-world applications. A fully supervised approach for this problem,\nrequires a dataset with RGBD image and 3D mesh label pairs. However, collecting\nsuch a dataset is costly and challenging, hence, existing datasets are small,\nand limited in pose and shape diversity. To overcome this data scarcity, we\nleverage existing Motion Capture (MoCap) datasets. We first obtain complete 3D\nmeshes from the body models found in MoCap datasets, and create partial,\nsingle-view versions of them by projection to a virtual camera. This simulates\nthe depth data provided by an RGBD camera from a single viewpoint. Then, we\ntrain a masked autoencoder to complete the partial, single-view mesh. During\ninference, our method, which we name as M$^3$ for ``Masked Mesh Modeling'',\nmatches the depth values coming from the sensor to vertices of a template human\nmesh, which creates a partial, single-view mesh. We effectively recover parts\nof the 3D human body mesh model that are not visible, resulting in a full body\nmesh. M$^3$ achieves 16.8 mm and 22.0 mm per-vertex-error (PVE) on the SURREAL\nand CAPE datasets, respectively; outperforming existing methods that use\nfull-body point clouds as input. We obtain a competitive 70.9 PVE on the BEHAVE\ndataset, outperforming a recently published RGB based method by 18.4 mm,\nhighlighting the usefulness of depth data. Code will be released.",
        "url": "http://arxiv.org/abs/2508.08178v1",
        "published_date": "2025-08-11T16:59:14+00:00",
        "updated_date": "2025-08-11T16:59:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ozhan Suat",
            "Bedirhan Uguz",
            "Batuhan Karagoz",
            "Muhammed Can Keles",
            "Emre Akbas"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a method for accurate 3D human mesh estimation from single RGBD views, leveraging depth data for better results. They achieve competitive performance on different datasets compared to existing methods.",
        "tldr_zh": "本文介绍了一种从单个RGBD视图准确估计3D人体网格的方法，利用深度数据获得更好的结果。他们在不同数据集上的表现与现有方法相比具有竞争力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision",
        "summary": "Accurately grounding regions of interest (ROIs) is critical for diagnosis and\ntreatment planning in medical imaging. While multimodal large language models\n(MLLMs) combine visual perception with natural language, current\nmedical-grounding pipelines still rely on supervised fine-tuning with explicit\nspatial hints, making them ill-equipped to handle the implicit queries common\nin clinical practice. This work makes three core contributions. We first define\nUnified Medical Reasoning Grounding (UMRG), a novel vision-language task that\ndemands clinical reasoning and pixel-level grounding. Second, we release\nU-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside\nimplicit clinical queries and reasoning traces, spanning 10 modalities, 15\nsuper-categories, and 108 specific categories. Finally, we introduce\nMedReasoner, a modular framework that distinctly separates reasoning from\nsegmentation: an MLLM reasoner is optimized with reinforcement learning, while\na frozen segmentation expert converts spatial prompts into masks, with\nalignment achieved through format and accuracy rewards. MedReasoner achieves\nstate-of-the-art performance on U-MRG-14K and demonstrates strong\ngeneralization to unseen clinical queries, underscoring the significant promise\nof reinforcement learning for interpretable medical grounding.",
        "url": "http://arxiv.org/abs/2508.08177v1",
        "published_date": "2025-08-11T16:59:06+00:00",
        "updated_date": "2025-08-11T16:59:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhonghao Yan",
            "Muxi Diao",
            "Yuxuan Yang",
            "Jiayuan Xu",
            "Kaizhou Zhang",
            "Ruoyan Jing",
            "Lele Yang",
            "Yanxi Liu",
            "Kongming Liang",
            "Zhanyu Ma"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a framework called MedReasoner that utilizes reinforcement learning to improve medical image segmentation with implicit clinical queries, achieving state-of-the-art results on a new dataset.",
        "tldr_zh": "该论文介绍了一个名为MedReasoner的框架，利用强化学习来改善医学图像分割，通过隐式临床查询，在新数据集上取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CD-TVD: Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data",
        "summary": "Large-scale scientific simulations require significant resources to generate\nhigh-resolution time-varying data (TVD). While super-resolution is an efficient\npost-processing strategy to reduce costs, existing methods rely on a large\namount of HR training data, limiting their applicability to diverse simulation\nscenarios. To address this constraint, we proposed CD-TVD, a novel framework\nthat combines contrastive learning and an improved diffusion-based\nsuper-resolution model to achieve accurate 3D super-resolution from limited\ntime-step high-resolution data. During pre-training on historical simulation\ndata, the contrastive encoder and diffusion superresolution modules learn\ndegradation patterns and detailed features of high-resolution and\nlow-resolution samples. In the training phase, the improved diffusion model\nwith a local attention mechanism is fine-tuned using only one newly generated\nhigh-resolution timestep, leveraging the degradation knowledge learned by the\nencoder. This design minimizes the reliance on large-scale high-resolution\ndatasets while maintaining the capability to recover fine-grained details.\nExperimental results on fluid and atmospheric simulation datasets confirm that\nCD-TVD delivers accurate and resource-efficient 3D super-resolution, marking a\nsignificant advancement in data augmentation for large-scale scientific\nsimulations. The code is available at\nhttps://github.com/Xin-Gao-private/CD-TVD.",
        "url": "http://arxiv.org/abs/2508.08173v1",
        "published_date": "2025-08-11T16:51:28+00:00",
        "updated_date": "2025-08-11T16:51:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chongke Bi",
            "Xin Gao",
            "Jiangkang Deng",
            "Guan"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes CD-TVD, a framework for 3D super-resolution using limited high-resolution time-varying data, achieving accurate results in large-scale scientific simulations.",
        "tldr_zh": "本文提出了CD-TVD，这是一个利用有限高分辨率时变数据进行3D超分辨的框架，在大规模科学模拟中取得准确的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction",
        "summary": "Reinforcement learning for training end-to-end autonomous driving models in\nclosed-loop simulations is gaining growing attention. However, most simulation\nenvironments differ significantly from real-world conditions, creating a\nsubstantial simulation-to-reality (sim2real) gap. To bridge this gap, some\napproaches utilize scene reconstruction techniques to create photorealistic\nenvironments as a simulator. While this improves realistic sensor simulation,\nthese methods are inherently constrained by the distribution of the training\ndata, making it difficult to render high-quality sensor data for novel\ntrajectories or corner case scenarios. Therefore, we propose ReconDreamer-RL, a\nframework designed to integrate video diffusion priors into scene\nreconstruction to aid reinforcement learning, thereby enhancing end-to-end\nautonomous driving training. Specifically, in ReconDreamer-RL, we introduce\nReconSimulator, which combines the video diffusion prior for appearance\nmodeling and incorporates a kinematic model for physical modeling, thereby\nreconstructing driving scenarios from real-world data. This narrows the\nsim2real gap for closed-loop evaluation and reinforcement learning. To cover\nmore corner-case scenarios, we introduce the Dynamic Adversary Agent (DAA),\nwhich adjusts the trajectories of surrounding vehicles relative to the ego\nvehicle, autonomously generating corner-case traffic scenarios (e.g., cut-in).\nFinally, the Cousin Trajectory Generator (CTG) is proposed to address the issue\nof training data distribution, which is often biased toward simple\nstraight-line movements. Experiments show that ReconDreamer-RL improves\nend-to-end autonomous driving training, outperforming imitation learning\nmethods with a 5x reduction in the Collision Ratio.",
        "url": "http://arxiv.org/abs/2508.08170v1",
        "published_date": "2025-08-11T16:45:55+00:00",
        "updated_date": "2025-08-11T16:45:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chaojun Ni",
            "Guosheng Zhao",
            "Xiaofeng Wang",
            "Zheng Zhu",
            "Wenkang Qin",
            "Xinze Chen",
            "Guanghong Jia",
            "Guan Huang",
            "Wenjun Mei"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces ReconDreamer-RL, a framework that integrates video diffusion priors into scene reconstruction to improve reinforcement learning for autonomous driving, outperforming imitation learning methods.",
        "tldr_zh": "本文介绍了ReconDreamer-RL，一种将视频扩散先验信息融入场景重建以改进自动驾驶强化学习的框架，表现优于模仿学习方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Pindrop it! Audio and Visual Deepfake Countermeasures for Robust Detection and Fine Grained-Localization",
        "summary": "The field of visual and audio generation is burgeoning with new\nstate-of-the-art methods. This rapid proliferation of new techniques\nunderscores the need for robust solutions for detecting synthetic content in\nvideos. In particular, when fine-grained alterations via localized\nmanipulations are performed in visual, audio, or both domains, these subtle\nmodifications add challenges to the detection algorithms. This paper presents\nsolutions for the problems of deepfake video classification and localization.\nThe methods were submitted to the ACM 1M Deepfakes Detection Challenge,\nachieving the best performance in the temporal localization task and a top four\nranking in the classification task for the TestA split of the evaluation\ndataset.",
        "url": "http://arxiv.org/abs/2508.08141v1",
        "published_date": "2025-08-11T16:14:17+00:00",
        "updated_date": "2025-08-11T16:14:17+00:00",
        "categories": [
            "cs.CV",
            "cs.SD",
            "eess.AS"
        ],
        "authors": [
            "Nicholas Klein",
            "Hemlata Tak",
            "James Fullwood",
            "Krishna Regmi",
            "Leonidas Spinoulas",
            "Ganesh Sivaraman",
            "Tianxiang Chen",
            "Elie Khoury"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper presents solutions for detecting deepfake videos with fine-grained alterations in audio and visual domains, achieving top performance in a deepfake detection challenge.",
        "tldr_zh": "本文提出了针对音频和视觉领域微调修改的深度伪造视频的检测解决方案，在深度伪造检测挑战中取得了最佳表现。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FantasyStyle: Controllable Stylized Distillation for 3D Gaussian Splatting",
        "summary": "The success of 3DGS in generative and editing applications has sparked\ngrowing interest in 3DGS-based style transfer. However, current methods still\nface two major challenges: (1) multi-view inconsistency often leads to style\nconflicts, resulting in appearance smoothing and distortion; and (2) heavy\nreliance on VGG features, which struggle to disentangle style and content from\nstyle images, often causing content leakage and excessive stylization. To\ntackle these issues, we introduce \\textbf{FantasyStyle}, a 3DGS-based style\ntransfer framework, and the first to rely entirely on diffusion model\ndistillation. It comprises two key components: (1) \\textbf{Multi-View Frequency\nConsistency}. We enhance cross-view consistency by applying a 3D filter to\nmulti-view noisy latent, selectively reducing low-frequency components to\nmitigate stylized prior conflicts. (2) \\textbf{Controllable Stylized\nDistillation}. To suppress content leakage from style images, we introduce\nnegative guidance to exclude undesired content. In addition, we identify the\nlimitations of Score Distillation Sampling and Delta Denoising Score in 3D\nstyle transfer and remove the reconstruction term accordingly. Building on\nthese insights, we propose a controllable stylized distillation that leverages\nnegative guidance to more effectively optimize the 3D Gaussians. Extensive\nexperiments demonstrate that our method consistently outperforms\nstate-of-the-art approaches, achieving higher stylization quality and visual\nrealism across various scenes and styles.",
        "url": "http://arxiv.org/abs/2508.08136v1",
        "published_date": "2025-08-11T16:11:08+00:00",
        "updated_date": "2025-08-11T16:11:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yitong Yang",
            "Yinglin Wang",
            "Changshuo Wang",
            "Huajie Wang",
            "Shuting He"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "FantasyStyle introduces a 3DGS-based style transfer framework that relies on diffusion model distillation to address challenges in multi-view inconsistency and content leakage, outperforming existing methods.",
        "tldr_zh": "FantasyStyle引入了一种基于3DGS的风格转移框架，依赖于扩散模型蒸馏来解决多视角不一致性和内容泄漏的挑战，优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control",
        "summary": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement.",
        "url": "http://arxiv.org/abs/2508.08134v1",
        "published_date": "2025-08-11T16:10:00+00:00",
        "updated_date": "2025-08-11T16:10:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zeqian Long",
            "Mingzhe Zheng",
            "Kunyu Feng",
            "Xinhua Zhang",
            "Hongyu Liu",
            "Harry Yang",
            "Linfeng Zhang",
            "Qifeng Chen",
            "Yue Ma"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper presents a framework called Follow-Your-Shape for shape-aware image editing, which enables precise and controlled editing of object shapes while preserving non-target content. It outperforms existing methods in tasks requiring large-scale shape replacement.",
        "tldr_zh": "本文提出了一种名为Follow-Your-Shape的框架，用于形状感知图像编辑，能够在保留非目标内容的同时实现对对象形状的精确和可控编辑。在需要大规模形状替换的任务中表现优异。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Vision-Based Localization and LLM-based Navigation for Indoor Environments",
        "summary": "Indoor navigation remains a complex challenge due to the absence of reliable\nGPS signals and the architectural intricacies of large enclosed environments.\nThis study presents an indoor localization and navigation approach that\nintegrates vision-based localization with large language model (LLM)-based\nnavigation. The localization system utilizes a ResNet-50 convolutional neural\nnetwork fine-tuned through a two-stage process to identify the user's position\nusing smartphone camera input. To complement localization, the navigation\nmodule employs an LLM, guided by a carefully crafted system prompt, to\ninterpret preprocessed floor plan images and generate step-by-step directions.\nExperimental evaluation was conducted in a realistic office corridor with\nrepetitive features and limited visibility to test localization robustness. The\nmodel achieved high confidence and an accuracy of 96% across all tested\nwaypoints, even under constrained viewing conditions and short-duration\nqueries. Navigation tests using ChatGPT on real building floor maps yielded an\naverage instruction accuracy of 75%, with observed limitations in zero-shot\nreasoning and inference time. This research demonstrates the potential for\nscalable, infrastructure-free indoor navigation using off-the-shelf cameras and\npublicly available floor plans, particularly in resource-constrained settings\nlike hospitals, airports, and educational institutions.",
        "url": "http://arxiv.org/abs/2508.08120v1",
        "published_date": "2025-08-11T15:59:09+00:00",
        "updated_date": "2025-08-11T15:59:09+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Keyan Rahimi",
            "Md. Wasiul Haque",
            "Sagar Dasgupta",
            "Mizanur Rahman"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a vision-based localization and LLM-based navigation system for indoor environments using off-the-shelf cameras and publicly available floor plans.",
        "tldr_zh": "本文提出了一种基于视觉定位和LLM导航的室内定位和导航系统，利用现成的摄像头和公开的楼层平面图。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MDD-Net: Multimodal Depression Detection through Mutual Transformer",
        "summary": "Depression is a major mental health condition that severely impacts the\nemotional and physical well-being of individuals. The simple nature of data\ncollection from social media platforms has attracted significant interest in\nproperly utilizing this information for mental health research. A Multimodal\nDepression Detection Network (MDD-Net), utilizing acoustic and visual data\nobtained from social media networks, is proposed in this work where mutual\ntransformers are exploited to efficiently extract and fuse multimodal features\nfor efficient depression detection. The MDD-Net consists of four core modules:\nan acoustic feature extraction module for retrieving relevant acoustic\nattributes, a visual feature extraction module for extracting significant\nhigh-level patterns, a mutual transformer for computing the correlations among\nthe generated features and fusing these features from multiple modalities, and\na detection layer for detecting depression using the fused feature\nrepresentations. The extensive experiments are performed using the multimodal\nD-Vlog dataset, and the findings reveal that the developed multimodal\ndepression detection network surpasses the state-of-the-art by up to 17.37% for\nF1-Score, demonstrating the greater performance of the proposed system. The\nsource code is accessible at\nhttps://github.com/rezwanh001/Multimodal-Depression-Detection.",
        "url": "http://arxiv.org/abs/2508.08093v1",
        "published_date": "2025-08-11T15:32:56+00:00",
        "updated_date": "2025-08-11T15:32:56+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Md Rezwanul Haque",
            "Md. Milon Islam",
            "S M Taslim Uddin Raju",
            "Hamdi Altaheri",
            "Lobna Nassar",
            "Fakhri Karray"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces MDD-Net, a Multimodal Depression Detection Network that utilizes acoustic and visual data from social media networks to detect depression with high accuracy.",
        "tldr_zh": "本文介绍了MDD-Net，一种利用社交媒体网络中的声学和视觉数据进行抑郁症检测的多模态深度网络。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Investigating the Design Space of Visual Grounding in Multimodal Large Language Model",
        "summary": "Fine-grained multimodal capability in Multimodal Large Language Models\n(MLLMs) has emerged as a critical research direction, particularly for tackling\nthe visual grounding (VG) problem. Despite the strong performance achieved by\nexisting approaches, they often employ disparate design choices when\nfine-tuning MLLMs for VG, lacking systematic verification to support these\ndesigns. To bridge this gap, this paper presents a comprehensive study of\nvarious design choices that impact the VG performance of MLLMs. We conduct our\nanalysis using LLaVA-1.5, which has been widely adopted in prior empirical\nstudies of MLLMs. While more recent models exist, we follow this convention to\nensure our findings remain broadly applicable and extendable to other\narchitectures. We cover two key aspects: (1) exploring different visual\ngrounding paradigms in MLLMs, identifying the most effective design, and\nproviding our insights; and (2) conducting ablation studies on the design of\ngrounding data to optimize MLLMs' fine-tuning for the VG task. Finally, our\nfindings contribute to a stronger MLLM for VG, achieving improvements of +5.6%\n/ +6.9% / +7.0% on RefCOCO/+/g over the LLaVA-1.5.",
        "url": "http://arxiv.org/abs/2508.08066v1",
        "published_date": "2025-08-11T15:10:52+00:00",
        "updated_date": "2025-08-11T15:10:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Weitai Kang",
            "Weiming Zhuang",
            "Zhizhong Li",
            "Yan Yan",
            "Lingjuan Lyu"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper investigates different design choices for visual grounding in Multimodal Large Language Models, achieving significant improvements in performance.",
        "tldr_zh": "本文研究了在多模态大语言模型中的视觉定位的不同设计选择，取得了性能显著提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PrIINeR: Towards Prior-Informed Implicit Neural Representations for Accelerated MRI",
        "summary": "Accelerating Magnetic Resonance Imaging (MRI) reduces scan time but often\ndegrades image quality. While Implicit Neural Representations (INRs) show\npromise for MRI reconstruction, they struggle at high acceleration factors due\nto weak prior constraints, leading to structural loss and aliasing artefacts.\nTo address this, we propose PrIINeR, an INR-based MRI reconstruction method\nthat integrates prior knowledge from pre-trained deep learning models into the\nINR framework. By combining population-level knowledge with instance-based\noptimization and enforcing dual data consistency, PrIINeR aligns both with the\nacquired k-space data and the prior-informed reconstruction. Evaluated on the\nNYU fastMRI dataset, our method not only outperforms state-of-the-art INR-based\napproaches but also improves upon several learning-based state-of-the-art\nmethods, significantly improving structural preservation and fidelity while\neffectively removing aliasing artefacts.PrIINeR bridges deep learning and\nINR-based techniques, offering a more reliable solution for high-quality,\naccelerated MRI reconstruction. The code is publicly available on\nhttps://github.com/multimodallearning/PrIINeR.",
        "url": "http://arxiv.org/abs/2508.08058v1",
        "published_date": "2025-08-11T14:59:09+00:00",
        "updated_date": "2025-08-11T14:59:09+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Ziad Al-Haj Hemidi",
            "Eytan Kats",
            "Mattias P. Heinrich"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "PrIINeR proposes a novel INR-based MRI reconstruction method that integrates prior knowledge from pre-trained deep learning models to improve image quality and accelerate MRI.",
        "tldr_zh": "PrIINeR提出了一种新颖的基于INR的MRI重建方法，将来自预训练深度学习模型的先验知识整合到其中，以提高图像质量和加速MRI。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "S^2VG: 3D Stereoscopic and Spatial Video Generation via Denoising Frame Matrix",
        "summary": "While video generation models excel at producing high-quality monocular\nvideos, generating 3D stereoscopic and spatial videos for immersive\napplications remains an underexplored challenge. We present a pose-free and\ntraining-free method that leverages an off-the-shelf monocular video generation\nmodel to produce immersive 3D videos. Our approach first warps the generated\nmonocular video into pre-defined camera viewpoints using estimated depth\ninformation, then applies a novel \\textit{frame matrix} inpainting framework.\nThis framework utilizes the original video generation model to synthesize\nmissing content across different viewpoints and timestamps, ensuring spatial\nand temporal consistency without requiring additional model fine-tuning.\nMoreover, we develop a \\dualupdate~scheme that further improves the quality of\nvideo inpainting by alleviating the negative effects propagated from\ndisoccluded areas in the latent space. The resulting multi-view videos are then\nadapted into stereoscopic pairs or optimized into 4D Gaussians for spatial\nvideo synthesis. We validate the efficacy of our proposed method by conducting\nexperiments on videos from various generative models, such as Sora, Lumiere,\nWALT, and Zeroscope. The experiments demonstrate that our method has a\nsignificant improvement over previous methods. Project page at:\nhttps://daipengwa.github.io/S-2VG_ProjectPage/",
        "url": "http://arxiv.org/abs/2508.08048v1",
        "published_date": "2025-08-11T14:50:03+00:00",
        "updated_date": "2025-08-11T14:50:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Peng Dai",
            "Feitong Tan",
            "Qiangeng Xu",
            "Yihua Huang",
            "David Futschik",
            "Ruofei Du",
            "Sean Fanello",
            "Yinda Zhang",
            "Xiaojuan Qi"
        ],
        "ai_categories": [
            "GAN",
            "Transformer"
        ],
        "tldr": "The paper proposes a method for generating 3D stereoscopic and spatial videos by leveraging a monocular video generation model and a novel frame matrix inpainting framework.",
        "tldr_zh": "该论文提出了一种通过利用单眼视频生成模型和新颖的帧矩阵修复框架来生成3D立体和空间视频的方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Sample-aware RandAugment: Search-free Automatic Data Augmentation for Effective Image Recognition",
        "summary": "Automatic data augmentation (AutoDA) plays an important role in enhancing the\ngeneralization of neural networks. However, mainstream AutoDA methods often\nencounter two challenges: either the search process is excessively\ntime-consuming, hindering practical application, or the performance is\nsuboptimal due to insufficient policy adaptation during training. To address\nthese issues, we propose Sample-aware RandAugment (SRA), an asymmetric,\nsearch-free AutoDA method that dynamically adjusts augmentation policies while\nmaintaining straightforward implementation. SRA incorporates a heuristic\nscoring module that evaluates the complexity of the original training data,\nenabling the application of tailored augmentations for each sample.\nAdditionally, an asymmetric augmentation strategy is employed to maximize the\npotential of this scoring module. In multiple experimental settings, SRA\nnarrows the performance gap between search-based and search-free AutoDA\nmethods, achieving a state-of-the-art Top-1 accuracy of 78.31\\% on ImageNet\nwith ResNet-50. Notably, SRA demonstrates good compatibility with existing\naugmentation pipelines and solid generalization across new tasks, without\nrequiring hyperparameter tuning. The pretrained models leveraging SRA also\nenhance recognition in downstream object detection tasks. SRA represents a\npromising step towards simpler, more effective, and practical AutoDA designs\napplicable to a variety of future tasks. Our code is available at\n\\href{https://github.com/ainieli/Sample-awareRandAugment}{https://github.com/ainieli/Sample-awareRandAugment",
        "url": "http://arxiv.org/abs/2508.08004v1",
        "published_date": "2025-08-11T14:09:01+00:00",
        "updated_date": "2025-08-11T14:09:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Anqi Xiao",
            "Weichen Yu",
            "Hongyuan Yu"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper introduces Sample-aware RandAugment, a search-free automatic data augmentation method that dynamically adjusts augmentation policies for each sample, achieving state-of-the-art results on ImageNet with ResNet-50.",
        "tldr_zh": "该论文介绍了Sample-aware RandAugment，一种无需搜索的自动数据增强方法，可以动态调整每个样本的增强策略，在ImageNet上使用ResNet-50实现了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models",
        "summary": "Group Activity Detection (GAD) involves recognizing social groups and their\ncollective behaviors in videos. Vision Foundation Models (VFMs), like DinoV2,\noffer excellent features, but are pretrained primarily on object-centric data\nand remain underexplored for modeling group dynamics. While they are a\npromising alternative to highly task-specific GAD architectures that require\nfull fine-tuning, our initial investigation reveals that simply swapping CNN\nbackbones used in these methods with VFMs brings little gain, underscoring the\nneed for structured, group-aware reasoning on top.\n  We introduce Prompt-driven Group Activity Detection (ProGraD) -- a method\nthat bridges this gap through 1) learnable group prompts to guide the VFM\nattention toward social configurations, and 2) a lightweight two-layer\nGroupContext Transformer that infers actor-group associations and collective\nbehavior. We evaluate our approach on two recent GAD benchmarks: Cafe, which\nfeatures multiple concurrent social groups, and Social-CAD, which focuses on\nsingle-group interactions. While we surpass state-of-the-art in both settings,\nour method is especially effective in complex multi-group scenarios, where we\nyield a gain of 6.5\\% (Group mAP\\@1.0) and 8.2\\% (Group mAP\\@0.5) using only\n10M trainable parameters. Furthermore, our experiments reveal that ProGraD\nproduces interpretable attention maps, offering insights into actor-group\nreasoning. Code and models will be released.",
        "url": "http://arxiv.org/abs/2508.07996v1",
        "published_date": "2025-08-11T13:59:22+00:00",
        "updated_date": "2025-08-11T13:59:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Thinesh Thiyakesan Ponbagavathi",
            "Chengzheng Yang",
            "Alina Roitberg"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a method called ProGraD for group activity detection in videos using Vision Foundation Models, achieving state-of-the-art performance especially in complex multi-group scenarios.",
        "tldr_zh": "该论文介绍了一种名为ProGraD的方法，使用视觉基础模型进行视频中的群体活动检测，在复杂的多组场景中取得了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "The Escalator Problem: Identifying Implicit Motion Blindness in AI for Accessibility",
        "summary": "Multimodal Large Language Models (MLLMs) hold immense promise as assistive\ntechnologies for the blind and visually impaired (BVI) community. However, we\nidentify a critical failure mode that undermines their trustworthiness in\nreal-world applications. We introduce the Escalator Problem -- the inability of\nstate-of-the-art models to perceive an escalator's direction of travel -- as a\ncanonical example of a deeper limitation we term Implicit Motion Blindness.\nThis blindness stems from the dominant frame-sampling paradigm in video\nunderstanding, which, by treating videos as discrete sequences of static\nimages, fundamentally struggles to perceive continuous, low-signal motion. As a\nposition paper, our contribution is not a new model but rather to: (I) formally\narticulate this blind spot, (II) analyze its implications for user trust, and\n(III) issue a call to action. We advocate for a paradigm shift from purely\nsemantic recognition towards robust physical perception and urge the\ndevelopment of new, human-centered benchmarks that prioritize safety,\nreliability, and the genuine needs of users in dynamic environments.",
        "url": "http://arxiv.org/abs/2508.07989v1",
        "published_date": "2025-08-11T13:53:09+00:00",
        "updated_date": "2025-08-11T13:53:09+00:00",
        "categories": [
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Xiantao Zhang"
        ],
        "ai_categories": [
            "Multimodality",
            "AIGC"
        ],
        "tldr": "The paper introduces the Escalator Problem in AI for accessibility, highlighting the limitations in perceiving continuous motion in videos which affects user trust. It calls for a shift towards physical perception in models.",
        "tldr_zh": "本文介绍了AI在辅助功能方面面临的'自动扶梯问题'，强调了视频中连续运动感知的局限性，影响了用户信任。呼吁模型转向物理感知。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis",
        "summary": "Forensic cause-of-death determination faces systemic challenges, including\nworkforce shortages and diagnostic variability, particularly in high-volume\nsystems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic\nAgenT), a multi-agent AI framework that automates and standardizes death\ninvestigations through a domain-adapted large language model. FEAT's\napplication-oriented architecture integrates: (i) a central Planner for task\ndecomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a\nMemory & Reflection module for iterative refinement, and (iv) a Global Solver\nfor conclusion synthesis. The system employs tool-augmented reasoning,\nhierarchical retrieval-augmented generation, forensic-tuned LLMs, and\nhuman-in-the-loop feedback to ensure legal and medical validity. In evaluations\nacross diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI\nsystems in both long-form autopsy analyses and concise cause-of-death\nconclusions. It demonstrated robust generalization across six geographic\nregions and achieved high expert concordance in blinded validations. Senior\npathologists validated FEAT's outputs as comparable to those of human experts,\nwith improved detection of subtle evidentiary nuances. To our knowledge, FEAT\nis the first LLM-based AI agent system dedicated to forensic medicine, offering\nscalable, consistent death certification while maintaining expert-level rigor.\nBy integrating AI efficiency with human oversight, this work could advance\nequitable access to reliable medicolegal services while addressing critical\ncapacity constraints in forensic systems.",
        "url": "http://arxiv.org/abs/2508.07950v1",
        "published_date": "2025-08-11T13:05:59+00:00",
        "updated_date": "2025-08-11T13:05:59+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.MA"
        ],
        "authors": [
            "Chen Shen",
            "Wanqing Zhang",
            "Kehan Li",
            "Erwen Huang",
            "Haitao Bi",
            "Aiying Fan",
            "Yiwen Shen",
            "Hongmei Dong",
            "Ji Zhang",
            "Yuming Shao",
            "Zengjia Liu",
            "Xinshe Liu",
            "Tao Li",
            "Chunxia Yan",
            "Shuanliang Fan",
            "Di Wu",
            "Jianhua Ma",
            "Bin Cong",
            "Zhenyuan Wang",
            "Chunfeng Lian"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "FEAT is a multi-agent AI system that automates and standardizes death investigations using a large language model, outperforming state-of-the-art systems in autopsy analysis and cause-of-death conclusions.",
        "tldr_zh": "FEAT是一个多智能体AI系统，利用大型语言模型自动化和标准化死因调查，在尸检分析和死因结论方面表现优于最先进的系统。",
        "relevance_score": 1,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Safeguarding Generative AI Applications in Preclinical Imaging through Hybrid Anomaly Detection",
        "summary": "Generative AI holds great potentials to automate and enhance data synthesis\nin nuclear medicine. However, the high-stakes nature of biomedical imaging\nnecessitates robust mechanisms to detect and manage unexpected or erroneous\nmodel behavior. We introduce development and implementation of a hybrid anomaly\ndetection framework to safeguard GenAI models in BIOEMTECH's eyes(TM) systems.\nTwo applications are demonstrated: Pose2Xray, which generates synthetic X-rays\nfrom photographic mouse images, and DosimetrEYE, which estimates 3D radiation\ndose maps from 2D SPECT/CT scans. In both cases, our outlier detection (OD)\nenhances reliability, reduces manual oversight, and supports real-time quality\ncontrol. This approach strengthens the industrial viability of GenAI in\npreclinical settings by increasing robustness, scalability, and regulatory\ncompliance.",
        "url": "http://arxiv.org/abs/2508.07923v1",
        "published_date": "2025-08-11T12:35:44+00:00",
        "updated_date": "2025-08-11T12:35:44+00:00",
        "categories": [
            "cs.CV",
            "cs.HC",
            "cs.LG"
        ],
        "authors": [
            "Jakub Binda",
            "Valentina Paneta",
            "Vasileios Eleftheriadis",
            "Hongkyou Chung",
            "Panagiotis Papadimitroulas",
            "Neo Christopher Chung"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces a hybrid anomaly detection framework to safeguard Generative AI models in biomedical imaging applications, enhancing reliability and reducing manual oversight.",
        "tldr_zh": "该论文介绍了一种混合异常检测框架，用于在生物医学成像应用中保护生成式人工智能模型，提高可靠性，减少手动监督。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "RSVLM-QA: A Benchmark Dataset for Remote Sensing Vision Language Model-based Question Answering",
        "summary": "Visual Question Answering (VQA) in remote sensing (RS) is pivotal for\ninterpreting Earth observation data. However, existing RS VQA datasets are\nconstrained by limitations in annotation richness, question diversity, and the\nassessment of specific reasoning capabilities. This paper introduces RSVLM-QA\ndataset, a new large-scale, content-rich VQA dataset for the RS domain.\nRSVLM-QA is constructed by integrating data from several prominent RS\nsegmentation and detection datasets: WHU, LoveDA, INRIA, and iSAID. We employ\nan innovative dual-track annotation generation pipeline. Firstly, we leverage\nLarge Language Models (LLMs), specifically GPT-4.1, with meticulously designed\nprompts to automatically generate a suite of detailed annotations including\nimage captions, spatial relations, and semantic tags, alongside complex\ncaption-based VQA pairs. Secondly, to address the challenging task of object\ncounting in RS imagery, we have developed a specialized automated process that\nextracts object counts directly from the original segmentation data; GPT-4.1\nthen formulates natural language answers from these counts, which are paired\nwith preset question templates to create counting QA pairs. RSVLM-QA comprises\n13,820 images and 162,373 VQA pairs, featuring extensive annotations and\ndiverse question types. We provide a detailed statistical analysis of the\ndataset and a comparison with existing RS VQA benchmarks, highlighting the\nsuperior depth and breadth of RSVLM-QA's annotations. Furthermore, we conduct\nbenchmark experiments on Six mainstream Vision Language Models (VLMs),\ndemonstrating that RSVLM-QA effectively evaluates and challenges the\nunderstanding and reasoning abilities of current VLMs in the RS domain. We\nbelieve RSVLM-QA will serve as a pivotal resource for the RS VQA and VLM\nresearch communities, poised to catalyze advancements in the field.",
        "url": "http://arxiv.org/abs/2508.07918v1",
        "published_date": "2025-08-11T12:32:48+00:00",
        "updated_date": "2025-08-11T12:32:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xing Zi",
            "Jinghao Xiao",
            "Yunxiao Shi",
            "Xian Tao",
            "Jun Li",
            "Ali Braytee",
            "Mukesh Prasad"
        ],
        "ai_categories": [
            "Dataset",
            "GAN",
            "Transformer"
        ],
        "tldr": "RSVLM-QA introduces a new large-scale dataset for Remote Sensing Vision Language Model-based Question Answering, addressing limitations in existing datasets and challenging current Vision Language Models in the RS domain.",
        "tldr_zh": "RSVLM-QA为遥感视觉语言模型问题回答引入了一个新的大规模数据集，解决了现有数据集的局限性，并挑战了RS领域当前的视觉语言模型。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Generative Video Matting",
        "summary": "Video matting has traditionally been limited by the lack of high-quality\nground-truth data. Most existing video matting datasets provide only\nhuman-annotated imperfect alpha and foreground annotations, which must be\ncomposited to background images or videos during the training stage. Thus, the\ngeneralization capability of previous methods in real-world scenarios is\ntypically poor. In this work, we propose to solve the problem from two\nperspectives. First, we emphasize the importance of large-scale pre-training by\npursuing diverse synthetic and pseudo-labeled segmentation datasets. We also\ndevelop a scalable synthetic data generation pipeline that can render diverse\nhuman bodies and fine-grained hairs, yielding around 200 video clips with a\n3-second duration for fine-tuning. Second, we introduce a novel video matting\napproach that can effectively leverage the rich priors from pre-trained video\ndiffusion models. This architecture offers two key advantages. First, strong\npriors play a critical role in bridging the domain gap between synthetic and\nreal-world scenes. Second, unlike most existing methods that process video\nmatting frame-by-frame and use an independent decoder to aggregate temporal\ninformation, our model is inherently designed for video, ensuring strong\ntemporal consistency. We provide a comprehensive quantitative evaluation across\nthree benchmark datasets, demonstrating our approach's superior performance,\nand present comprehensive qualitative results in diverse real-world scenes,\nillustrating the strong generalization capability of our method. The code is\navailable at https://github.com/aim-uofa/GVM.",
        "url": "http://arxiv.org/abs/2508.07905v1",
        "published_date": "2025-08-11T12:18:55+00:00",
        "updated_date": "2025-08-11T12:18:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yongtao Ge",
            "Kangyang Xie",
            "Guangkai Xu",
            "Mingyu Liu",
            "Li Ke",
            "Longtao Huang",
            "Hui Xue",
            "Hao Chen",
            "Chunhua Shen"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Diffusion"
        ],
        "tldr": "The paper introduces a novel video matting approach that leverages synthetic data and pre-trained models to improve generalization in real-world scenarios, showcasing superior performance and generalization capabilities.",
        "tldr_zh": "本文介绍了一种新的视频抠像方法，利用合成数据和预训练模型提高在真实场景中的泛化能力，展示出卓越的性能和泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TAP: Parameter-efficient Task-Aware Prompting for Adverse Weather Removal",
        "summary": "Image restoration under adverse weather conditions has been extensively\nexplored, leading to numerous high-performance methods. In particular, recent\nadvances in All-in-One approaches have shown impressive results by training on\nmulti-task image restoration datasets. However, most of these methods rely on\ndedicated network modules or parameters for each specific degradation type,\nresulting in a significant parameter overhead. Moreover, the relatedness across\ndifferent restoration tasks is often overlooked. In light of these issues, we\npropose a parameter-efficient All-in-One image restoration framework that\nleverages task-aware enhanced prompts to tackle various adverse weather\ndegradations.Specifically, we adopt a two-stage training paradigm consisting of\na pretraining phase and a prompt-tuning phase to mitigate parameter conflicts\nacross tasks. We first employ supervised learning to acquire general\nrestoration knowledge, and then adapt the model to handle specific degradation\nvia trainable soft prompts. Crucially, we enhance these task-specific prompts\nin a task-aware manner. We apply low-rank decomposition to these prompts to\ncapture both task-general and task-specific characteristics, and impose\ncontrastive constraints to better align them with the actual inter-task\nrelatedness. These enhanced prompts not only improve the parameter efficiency\nof the restoration model but also enable more accurate task modeling, as\nevidenced by t-SNE analysis. Experimental results on different restoration\ntasks demonstrate that the proposed method achieves superior performance with\nonly 2.75M parameters.",
        "url": "http://arxiv.org/abs/2508.07878v1",
        "published_date": "2025-08-11T11:51:06+00:00",
        "updated_date": "2025-08-11T11:51:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hanting Wang",
            "Shengpeng Ji",
            "Shulei Wang",
            "Hai Huang",
            "Xiao Jin",
            "Qifei Zhang",
            "Tao Jin"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a parameter-efficient image restoration framework using task-aware enhanced prompts to tackle adverse weather degradations, achieving superior performance with only 2.75M parameters.",
        "tldr_zh": "该论文提出了一种参数高效的图像恢复框架，利用任务感知增强提示来处理恶劣天气降解，仅使用2.75M参数就实现了卓越性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning",
        "summary": "Modern large vision-language models (LVLMs) convert each input image into a\nlarge set of tokens, far outnumbering the text tokens. Although this improves\nvisual perception, it introduces severe image token redundancy. Because image\ntokens carry sparse information, many add little to reasoning, yet greatly\nincrease inference cost. The emerging image token pruning methods tackle this\nissue by identifying the most important tokens and discarding the rest. These\nmethods can raise efficiency with only modest performance loss. However, most\nof them only consider single-image tasks and overlook multimodal in-context\nlearning (ICL), where redundancy is greater and efficiency is more critical.\nRedundant tokens weaken the advantage of multimodal ICL for rapid domain\nadaptation and cause unstable performance. Applying existing pruning methods in\nthis setting leads to large accuracy drops, exposing a clear gap and the need\nfor new techniques. Thus, we propose Contextually Adaptive Token Pruning\n(CATP), a training-free pruning method targeted at multimodal ICL. CATP\nconsists of two stages that perform progressive pruning to fully account for\nthe complex cross-modal interactions in the input sequence. After removing\n77.8\\% of the image tokens, CATP produces an average performance gain of 0.6\\%\nover the vanilla model on four LVLMs and eight benchmarks, exceeding all\nbaselines remarkably. Meanwhile, it effectively improves efficiency by\nachieving an average reduction of 10.78\\% in inference latency. CATP enhances\nthe practical value of multimodal ICL and lays the groundwork for future\nprogress in interleaved image-text scenarios.",
        "url": "http://arxiv.org/abs/2508.07871v1",
        "published_date": "2025-08-11T11:41:51+00:00",
        "updated_date": "2025-08-11T11:41:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanshu Li",
            "Jianjiang Yang",
            "Zhennan Shen",
            "Ligong Han",
            "Haoyan Xu",
            "Ruixiang Tang"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes Contextually Adaptive Token Pruning (CATP) for efficient multimodal in-context learning, achieving a performance gain of 0.6% and a reduction in inference latency by 10.78% on four LVLMs and eight benchmarks.",
        "tldr_zh": "本文提出了一种面向高效多模态上下文学习的上下文自适应标记剪枝（CATP），在四个LVLM和8个基准测试上实现了0.6%的性能提升和推理延迟减少了10.78%。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model",
        "summary": "Human motion generation has emerged as a critical technology with\ntransformative potential for real-world applications. However, existing\nvision-language-motion models (VLMMs) face significant limitations that hinder\ntheir practical deployment. We identify controllability as a main bottleneck,\nmanifesting in five key aspects: inadequate response to diverse human commands,\nlimited pose initialization capabilities, poor performance on long-term\nsequences, insufficient handling of unseen scenarios, and lack of fine-grained\ncontrol over individual body parts. To overcome these limitations, we present\nBeing-M0.5, the first real-time, controllable VLMM that achieves\nstate-of-the-art performance across multiple motion generation tasks. Our\napproach is built upon HuMo100M, the largest and most comprehensive human\nmotion dataset to date, comprising over 5 million self-collected motion\nsequences, 100 million multi-task instructional instances, and detailed\npart-level annotations that address a critical gap in existing datasets. We\nintroduce a novel part-aware residual quantization technique for motion\ntokenization that enables precise, granular control over individual body parts\nduring generation. Extensive experimental validation demonstrates Being-M0.5's\nsuperior performance across diverse motion benchmarks, while comprehensive\nefficiency analysis confirms its real-time capabilities. Our contributions\ninclude design insights and detailed computational analysis to guide future\ndevelopment of practical motion generators. We believe that HuMo100M and\nBeing-M0.5 represent significant advances that will accelerate the adoption of\nmotion generation technologies in real-world applications. The project page is\navailable at https://beingbeyond.github.io/Being-M0.5.",
        "url": "http://arxiv.org/abs/2508.07863v1",
        "published_date": "2025-08-11T11:26:10+00:00",
        "updated_date": "2025-08-11T11:26:10+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Bin Cao",
            "Sipeng Zheng",
            "Ye Wang",
            "Lujie Xia",
            "Qianshan Wei",
            "Qin Jin",
            "Jing Liu",
            "Zongqing Lu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a real-time controllable vision-language-motion model called Being-M0.5 that aims to overcome limitations in existing models for human motion generation.",
        "tldr_zh": "这篇论文介绍了一种名为Being-M0.5的实时可控视觉语言动作模型，旨在克服现有人类运动生成模型的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Deep Space Weather Model: Long-Range Solar Flare Prediction from Multi-Wavelength Images",
        "summary": "Accurate, reliable solar flare prediction is crucial for mitigating potential\ndisruptions to critical infrastructure, while predicting solar flares remains a\nsignificant challenge. Existing methods based on heuristic physical features\noften lack representation learning from solar images. On the other hand,\nend-to-end learning approaches struggle to model long-range temporal\ndependencies in solar images. In this study, we propose Deep Space Weather\nModel (Deep SWM), which is based on multiple deep state space models for\nhandling both ten-channel solar images and long-range spatio-temporal\ndependencies. Deep SWM also features a sparse masked autoencoder, a novel\npretraining strategy that employs a two-phase masking approach to preserve\ncrucial regions such as sunspots while compressing spatial information.\nFurthermore, we built FlareBench, a new public benchmark for solar flare\nprediction covering a full 11-year solar activity cycle, to validate our\nmethod. Our method outperformed baseline methods and even human expert\nperformance on standard metrics in terms of performance and reliability. The\nproject page can be found at https://keio-smilab25.github.io/DeepSWM.",
        "url": "http://arxiv.org/abs/2508.07847v1",
        "published_date": "2025-08-11T11:06:56+00:00",
        "updated_date": "2025-08-11T11:06:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shunya Nagashima",
            "Komei Sugiura"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a Deep Space Weather Model for long-range solar flare prediction from multi-wavelength images, outperforming baseline methods and even human experts.",
        "tldr_zh": "本文提出了一个深度空间天气模型，用于从多波长图像中进行长程太阳耀斑预测，优于基准方法甚至人类专家。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Effortless Vision-Language Model Specialization in Histopathology without Annotation",
        "summary": "Recent advances in Vision-Language Models (VLMs) in histopathology, such as\nCONCH and QuiltNet, have demonstrated impressive zero-shot classification\ncapabilities across various tasks. However, their general-purpose design may\nlead to suboptimal performance in specific downstream applications. While\nsupervised fine-tuning methods address this issue, they require manually\nlabeled samples for adaptation. This paper investigates annotation-free\nadaptation of VLMs through continued pretraining on domain- and task-relevant\nimage-caption pairs extracted from existing databases. Our experiments on two\nVLMs, CONCH and QuiltNet, across three downstream tasks reveal that these pairs\nsubstantially enhance both zero-shot and few-shot performance. Notably, with\nlarger training sizes, continued pretraining matches the performance of\nfew-shot methods while eliminating manual labeling. Its effectiveness,\ntask-agnostic design, and annotation-free workflow make it a promising pathway\nfor adapting VLMs to new histopathology tasks. Code is available at\nhttps://github.com/DeepMicroscopy/Annotation-free-VLM-specialization.",
        "url": "http://arxiv.org/abs/2508.07835v1",
        "published_date": "2025-08-11T10:39:27+00:00",
        "updated_date": "2025-08-11T10:39:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingna Qiu",
            "Nishanth Jain",
            "Jonas Ammeling",
            "Marc Aubreville",
            "Katharina Breininger"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "This paper explores adapting Vision-Language Models (VLMs) in histopathology without manual annotation, showing improved performance across tasks with domain-specific image-caption pairs.",
        "tldr_zh": "本文探讨了在组织病理学中调整视觉语言模型（VLMs）而不需要手动注释，通过特定领域的图像-标题对显示出跨任务的性能改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MIMIC: Multimodal Inversion for Model Interpretation and Conceptualization",
        "summary": "Vision Language Models (VLMs) encode multimodal inputs over large, complex,\nand difficult-to-interpret architectures, which limit transparency and trust.\nWe propose a Multimodal Inversion for Model Interpretation and\nConceptualization (MIMIC) framework to visualize the internal representations\nof VLMs by synthesizing visual concepts corresponding to internal encodings.\nMIMIC uses a joint VLM-based inversion and a feature alignment objective to\naccount for VLM's autoregressive processing. It additionally includes a triplet\nof regularizers for spatial alignment, natural image smoothness, and semantic\nrealism. We quantitatively and qualitatively evaluate MIMIC by inverting visual\nconcepts over a range of varying-length free-form VLM output texts. Reported\nresults include both standard visual quality metrics as well as semantic\ntext-based metrics. To the best of our knowledge, this is the first model\ninversion approach addressing visual interpretations of VLM concepts.",
        "url": "http://arxiv.org/abs/2508.07833v1",
        "published_date": "2025-08-11T10:36:58+00:00",
        "updated_date": "2025-08-11T10:36:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Animesh Jain",
            "Alexandros Stergiou"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer"
        ],
        "tldr": "The paper proposes a framework called MIMIC to visualize the internal representations of vision language models by synthesizing visual concepts corresponding to internal encodings, with a focus on model interpretation and conceptualization.",
        "tldr_zh": "该论文提出了一个名为MIMIC的框架，通过合成与内部编码相对应的视觉概念，重点关注模型解释和概念化。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Segmenting and Understanding: Region-aware Semantic Attention for Fine-grained Image Quality Assessment with Large Language Models",
        "summary": "No-reference image quality assessment (NR-IQA) aims to simulate the process\nof perceiving image quality aligned with subjective human perception. However,\nexisting NR-IQA methods either focus on global representations that leads to\nlimited insights into the semantically salient regions or employ a uniform\nweighting for region features that weakens the sensitivity to local quality\nvariations. In this paper, we propose a fine-grained image quality assessment\nmodel, named RSFIQA, which integrates region-level distortion information to\nperceive multi-dimensional quality discrepancies. To enhance regional quality\nawareness, we first utilize the Segment Anything Model (SAM) to dynamically\npartition the input image into non-overlapping semantic regions. For each\nregion, we teach a powerful Multi-modal Large Language Model (MLLM) to extract\ndescriptive content and perceive multi-dimensional distortions, enabling a\ncomprehensive understanding of both local semantics and quality degradations.\nTo effectively leverage this information, we introduce Region-Aware Semantic\nAttention (RSA) mechanism, which generates a global attention map by\naggregating fine-grained representations from local regions. In addition,\nRSFIQA is backbone-agnostic and can be seamlessly integrated into various deep\nneural network architectures. Extensive experiments demonstrate the robustness\nand effectiveness of the proposed method, which achieves competitive quality\nprediction performance across multiple benchmark datasets.",
        "url": "http://arxiv.org/abs/2508.07818v1",
        "published_date": "2025-08-11T10:03:00+00:00",
        "updated_date": "2025-08-11T10:03:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chenyue Song",
            "Chen Hui",
            "Haiqi Zhu",
            "Feng Jiang",
            "Yachun Mi",
            "Wei Zhang",
            "Shaohui Liu"
        ],
        "ai_categories": [
            "Transformer",
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper presents a fine-grained image quality assessment model called RSFIQA that integrates region-level distortion information for better quality prediction performance.",
        "tldr_zh": "该论文提出了一种名为RSFIQA的细粒度图像质量评估模型，它整合了区域级的失真信息，以提升质量预测性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MIND: A Noise-Adaptive Denoising Framework for Medical Images Integrating Multi-Scale Transformer",
        "summary": "The core role of medical images in disease diagnosis makes their quality\ndirectly affect the accuracy of clinical judgment. However, due to factors such\nas low-dose scanning, equipment limitations and imaging artifacts, medical\nimages are often accompanied by non-uniform noise interference, which seriously\naffects structure recognition and lesion detection. This paper proposes a\nmedical image adaptive denoising model (MI-ND) that integrates multi-scale\nconvolutional and Transformer architecture, introduces a noise level estimator\n(NLE) and a noise adaptive attention module (NAAB), and realizes\nchannel-spatial attention regulation and cross-modal feature fusion driven by\nnoise perception. Systematic testing is carried out on multimodal public\ndatasets. Experiments show that this method significantly outperforms the\ncomparative methods in image quality indicators such as PSNR, SSIM, and LPIPS,\nand improves the F1 score and ROC-AUC in downstream diagnostic tasks, showing\nstrong prac-tical value and promotional potential. The model has outstanding\nbenefits in structural recovery, diagnostic sensitivity, and cross-modal\nrobustness, and provides an effective solution for medical image enhancement\nand AI-assisted diagnosis and treatment.",
        "url": "http://arxiv.org/abs/2508.07817v1",
        "published_date": "2025-08-11T10:00:51+00:00",
        "updated_date": "2025-08-11T10:00:51+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.MM"
        ],
        "authors": [
            "Tao Tang",
            "Chengxu Yang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a noise-adaptive denoising framework for medical images that significantly outperforms existing methods in image quality indicators and diagnostic tasks.",
        "tldr_zh": "本文提出了一种适应性去噪框架，用于医学图像，明显优于现有方法在图像质量指标和诊断任务中的表现。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Pose-RFT: Enhancing MLLMs for 3D Pose Generation via Hybrid Action Reinforcement Fine-Tuning",
        "summary": "Generating 3D human poses from multimodal inputs such as images or text\nrequires models to capture both rich spatial and semantic correspondences.\nWhile pose-specific multimodal large language models (MLLMs) have shown promise\nin this task, they are typically trained with supervised objectives such as\nSMPL parameter regression or token-level prediction, which struggle to model\nthe inherent ambiguity and achieve task-specific alignment required for\naccurate 3D pose generation. To address these limitations, we propose Pose-RFT,\na reinforcement fine-tuning framework tailored for 3D human pose generation in\nMLLMs. We formulate the task as a hybrid action reinforcement learning problem\nthat jointly optimizes discrete language prediction and continuous pose\ngeneration. To this end, we introduce HyGRPO, a hybrid reinforcement learning\nalgorithm that performs group-wise reward normalization over sampled responses\nto guide joint optimization of discrete and continuous actions. Pose-RFT\nfurther incorporates task-specific reward functions to guide optimization\ntowards spatial alignment in image-to-pose generation and semantic consistency\nin text-to-pose generation. Extensive experiments on multiple pose generation\nbenchmarks demonstrate that Pose-RFT significantly improves performance over\nexisting pose-specific MLLMs, validating the effectiveness of hybrid action\nreinforcement fine-tuning for 3D pose generation.",
        "url": "http://arxiv.org/abs/2508.07804v1",
        "published_date": "2025-08-11T09:44:58+00:00",
        "updated_date": "2025-08-11T09:44:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bao Li",
            "Xiaomei Zhang",
            "Miao Xu",
            "Zhaoxin Fan",
            "Xiangyu Zhu",
            "Zhen Lei"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "Pose-RFT proposes a reinforcement fine-tuning framework for 3D human pose generation in MLLMs, outperforming existing models.",
        "tldr_zh": "Pose-RFT提出了一个针对MLLMs中3D人体姿态生成的强化微调框架，优于现有模型。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MambaTrans: Multimodal Fusion Image Translation via Large Language Model Priors for Downstream Visual Tasks",
        "summary": "The goal of multimodal image fusion is to integrate complementary information\nfrom infrared and visible images, generating multimodal fused images for\ndownstream tasks. Existing downstream pre-training models are typically trained\non visible images. However, the significant pixel distribution differences\nbetween visible and multimodal fusion images can degrade downstream task\nperformance, sometimes even below that of using only visible images. This paper\nexplores adapting multimodal fused images with significant modality differences\nto object detection and semantic segmentation models trained on visible images.\nTo address this, we propose MambaTrans, a novel multimodal fusion image\nmodality translator. MambaTrans uses descriptions from a multimodal large\nlanguage model and masks from semantic segmentation models as input. Its core\ncomponent, the Multi-Model State Space Block, combines mask-image-text\ncross-attention and a 3D-Selective Scan Module, enhancing pure visual\ncapabilities. By leveraging object detection prior knowledge, MambaTrans\nminimizes detection loss during training and captures long-term dependencies\namong text, masks, and images. This enables favorable results in pre-trained\nmodels without adjusting their parameters. Experiments on public datasets show\nthat MambaTrans effectively improves multimodal image performance in downstream\ntasks.",
        "url": "http://arxiv.org/abs/2508.07803v1",
        "published_date": "2025-08-11T09:39:16+00:00",
        "updated_date": "2025-08-11T09:39:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yushen Xu",
            "Xiaosong Li",
            "Zhenyu Kuang",
            "Xiaoqi Cheng",
            "Haishu Tan",
            "Huafeng Li"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces MambaTrans, a multimodal fusion image modality translator that uses large language model descriptions and semantic segmentation masks to improve downstream tasks performance.",
        "tldr_zh": "本文介绍了MambaTrans，一种多模态融合图像模态转换器，利用大型语言模型描述和语义分割蒙版来提高下游任务的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniSVG: A Unified Dataset for Vector Graphic Understanding and Generation with Multimodal Large Language Models",
        "summary": "Unlike bitmap images, scalable vector graphics (SVG) maintain quality when\nscaled, frequently employed in computer vision and artistic design in the\nrepresentation of SVG code. In this era of proliferating AI-powered systems,\nenabling AI to understand and generate SVG has become increasingly urgent.\nHowever, AI-driven SVG understanding and generation (U&G) remain significant\nchallenges. SVG code, equivalent to a set of curves and lines controlled by\nfloating-point parameters, demands high precision in SVG U&G. Besides, SVG\ngeneration operates under diverse conditional constraints, including textual\nprompts and visual references, which requires powerful multi-modal processing\nfor condition-to-SVG transformation. Recently, the rapid growth of Multi-modal\nLarge Language Models (MLLMs) have demonstrated capabilities to process\nmulti-modal inputs and generate complex vector controlling parameters,\nsuggesting the potential to address SVG U&G tasks within a unified model. To\nunlock MLLM's capabilities in the SVG area, we propose an SVG-centric dataset\ncalled UniSVG, comprising 525k data items, tailored for MLLM training and\nevaluation. To our best knowledge, it is the first comprehensive dataset\ndesigned for unified SVG generation (from textual prompts and images) and SVG\nunderstanding (color, category, usage, etc.). As expected, learning on the\nproposed dataset boosts open-source MLLMs' performance on various SVG U&G\ntasks, surpassing SOTA close-source MLLMs like GPT-4V. We release dataset,\nbenchmark, weights, codes and experiment details on\nhttps://ryanlijinke.github.io/.",
        "url": "http://arxiv.org/abs/2508.07766v1",
        "published_date": "2025-08-11T08:50:14+00:00",
        "updated_date": "2025-08-11T08:50:14+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jinke Li",
            "Jiarui Yu",
            "Chenxing Wei",
            "Hande Dong",
            "Qiang Lin",
            "Liangjing Yang",
            "Zhicai Wang",
            "Yanbin Hao"
        ],
        "ai_categories": [
            "Dataset",
            "Multimodality"
        ],
        "tldr": "The paper introduces UniSVG, a dataset for AI systems to understand and generate scalable vector graphics using multi-modal large language models, showing improved performance over existing models.",
        "tldr_zh": "本文介绍了UniSVG，一个用于AI系统理解和生成可伸缩矢量图形的数据集，利用多模态大型语言模型，表现出比现有模型更好的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Sea-Undistort: A Dataset for Through-Water Image Restoration in High Resolution Airborne Bathymetric Mapping",
        "summary": "Accurate image-based bathymetric mapping in shallow waters remains\nchallenging due to the complex optical distortions such as wave induced\npatterns, scattering and sunglint, introduced by the dynamic water surface, the\nwater column properties, and solar illumination. In this work, we introduce\nSea-Undistort, a comprehensive synthetic dataset of 1200 paired 512x512\nthrough-water scenes rendered in Blender. Each pair comprises a distortion-free\nand a distorted view, featuring realistic water effects such as sun glint,\nwaves, and scattering over diverse seabeds. Accompanied by per-image metadata\nsuch as camera parameters, sun position, and average depth, Sea-Undistort\nenables supervised training that is otherwise infeasible in real environments.\nWe use Sea-Undistort to benchmark two state-of-the-art image restoration\nmethods alongside an enhanced lightweight diffusion-based framework with an\nearly-fusion sun-glint mask. When applied to real aerial data, the enhanced\ndiffusion model delivers more complete Digital Surface Models (DSMs) of the\nseabed, especially in deeper areas, reduces bathymetric errors, suppresses\nglint and scattering, and crisply restores fine seabed details. Dataset,\nweights, and code are publicly available at\nhttps://www.magicbathy.eu/Sea-Undistort.html.",
        "url": "http://arxiv.org/abs/2508.07760v1",
        "published_date": "2025-08-11T08:43:29+00:00",
        "updated_date": "2025-08-11T08:43:29+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Maximilian Kromer",
            "Panagiotis Agrafiotis",
            "Begüm Demir"
        ],
        "ai_categories": [
            "Dataset",
            "Diffusion"
        ],
        "tldr": "Sea-Undistort introduces a synthetic dataset for through-water image restoration in high-resolution airborne bathymetric mapping, improving accuracy in shallow waters by addressing optical distortions.",
        "tldr_zh": "Sea-Undistort引入了一个合成数据集，用于高分辨率空中水下测深图像恢复，通过解决光学失真问题，提高了浅水区准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Comparison Reveals Commonality: Customized Image Generation through Contrastive Inversion",
        "summary": "The recent demand for customized image generation raises a need for\ntechniques that effectively extract the common concept from small sets of\nimages. Existing methods typically rely on additional guidance, such as text\nprompts or spatial masks, to capture the common target concept. Unfortunately,\nrelying on manually provided guidance can lead to incomplete separation of\nauxiliary features, which degrades generation quality.In this paper, we propose\nContrastive Inversion, a novel approach that identifies the common concept by\ncomparing the input images without relying on additional information. We train\nthe target token along with the image-wise auxiliary text tokens via\ncontrastive learning, which extracts the well-disentangled true semantics of\nthe target. Then we apply disentangled cross-attention fine-tuning to improve\nconcept fidelity without overfitting. Experimental results and analysis\ndemonstrate that our method achieves a balanced, high-level performance in both\nconcept representation and editing, outperforming existing techniques.",
        "url": "http://arxiv.org/abs/2508.07755v1",
        "published_date": "2025-08-11T08:36:29+00:00",
        "updated_date": "2025-08-11T08:36:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Minseo Kim",
            "Minchan Kwon",
            "Dongyeun Lee",
            "Yunho Jeon",
            "Junmo Kim"
        ],
        "ai_categories": [
            "AIGC",
            "GAN"
        ],
        "tldr": "The paper introduces Contrastive Inversion, a method for generating customized images without the need for additional guidance like text prompts or spatial masks. It outperforms existing techniques in concept representation and editing.",
        "tldr_zh": "这篇论文提出了对比反演，一种生成定制图像的方法，无需额外的指导，如文本提示或空间掩模。它在概念表示和编辑方面优于现有技术。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing Small-Scale Dataset Expansion with Triplet-Connection-based Sample Re-Weighting",
        "summary": "The performance of computer vision models in certain real-world applications,\nsuch as medical diagnosis, is often limited by the scarcity of available\nimages. Expanding datasets using pre-trained generative models is an effective\nsolution. However, due to the uncontrollable generation process and the\nambiguity of natural language, noisy images may be generated. Re-weighting is\nan effective way to address this issue by assigning low weights to such noisy\nimages. We first theoretically analyze three types of supervision for the\ngenerated images. Based on the theoretical analysis, we develop TriReWeight, a\ntriplet-connection-based sample re-weighting method to enhance generative data\naugmentation. Theoretically, TriReWeight can be integrated with any generative\ndata augmentation methods and never downgrade their performance. Moreover, its\ngeneralization approaches the optimal in the order $O(\\sqrt{d\\ln (n)/n})$. Our\nexperiments validate the correctness of the theoretical analysis and\ndemonstrate that our method outperforms the existing SOTA methods by $7.9\\%$ on\naverage over six natural image datasets and by $3.4\\%$ on average over three\nmedical datasets. We also experimentally validate that our method can enhance\nthe performance of different generative data augmentation methods.",
        "url": "http://arxiv.org/abs/2508.07723v1",
        "published_date": "2025-08-11T07:50:47+00:00",
        "updated_date": "2025-08-11T07:50:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ting Xiang",
            "Changjian Chen",
            "Zhuo Tang",
            "Qifeng Zhang",
            "Fei Lyu",
            "Li Yang",
            "Jiapeng Zhang",
            "Kenli Li"
        ],
        "ai_categories": [
            "Dataset",
            "GAN"
        ],
        "tldr": "The paper introduces TriReWeight, a method for re-weighting noisy images generated by pre-trained generative models to enhance dataset expansion in computer vision applications, outperforming existing methods on natural and medical image datasets.",
        "tldr_zh": "该论文介绍了 TriReWeight，一种重新加权嘈杂图像的方法，通过增强计算机视觉应用中数据集扩展，优于现有方法对自然和医学图像数据集进行了验证。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multi-view Normal and Distance Guidance Gaussian Splatting for Surface Reconstruction",
        "summary": "3D Gaussian Splatting (3DGS) achieves remarkable results in the field of\nsurface reconstruction. However, when Gaussian normal vectors are aligned\nwithin the single-view projection plane, while the geometry appears reasonable\nin the current view, biases may emerge upon switching to nearby views. To\naddress the distance and global matching challenges in multi-view scenes, we\ndesign multi-view normal and distance-guided Gaussian splatting. This method\nachieves geometric depth unification and high-accuracy reconstruction by\nconstraining nearby depth maps and aligning 3D normals. Specifically, for the\nreconstruction of small indoor and outdoor scenes, we propose a multi-view\ndistance reprojection regularization module that achieves multi-view Gaussian\nalignment by computing the distance loss between two nearby views and the same\nGaussian surface. Additionally, we develop a multi-view normal enhancement\nmodule, which ensures consistency across views by matching the normals of pixel\npoints in nearby views and calculating the loss. Extensive experimental results\ndemonstrate that our method outperforms the baseline in both quantitative and\nqualitative evaluations, significantly enhancing the surface reconstruction\ncapability of 3DGS.",
        "url": "http://arxiv.org/abs/2508.07701v1",
        "published_date": "2025-08-11T07:25:13+00:00",
        "updated_date": "2025-08-11T07:25:13+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Bo Jia",
            "Yanan Guo",
            "Ying Chang",
            "Benkui Zhang",
            "Ying Xie",
            "Kangning Du",
            "Lin Cao"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a method called multi-view normal and distance-guided Gaussian splatting for surface reconstruction, improving the accuracy and consistency of 3D surface reconstructions in multi-view scenes.",
        "tldr_zh": "本文提出了一种称为多视图正常和距离引导的高斯分割的方法，用于表面重建，在多视图场景中提高3D表面重建的准确性和一致性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Make Your MoVe: Make Your 3D Contents by Adapting Multi-View Diffusion Models to External Editing",
        "summary": "As 3D generation techniques continue to flourish, the demand for generating\npersonalized content is rapidly rising. Users increasingly seek to apply\nvarious editing methods to polish generated 3D content, aiming to enhance its\ncolor, style, and lighting without compromising the underlying geometry.\nHowever, most existing editing tools focus on the 2D domain, and directly\nfeeding their results into 3D generation methods (like multi-view diffusion\nmodels) will introduce information loss, degrading the quality of the final 3D\nassets. In this paper, we propose a tuning-free, plug-and-play scheme that\naligns edited assets with their original geometry in a single inference run.\nCentral to our approach is a geometry preservation module that guides the\nedited multi-view generation with original input normal latents. Besides, an\ninjection switcher is proposed to deliberately control the supervision extent\nof the original normals, ensuring the alignment between the edited color and\nnormal views. Extensive experiments show that our method consistently improves\nboth the multi-view consistency and mesh quality of edited 3D assets, across\nmultiple combinations of multi-view diffusion models and editing methods.",
        "url": "http://arxiv.org/abs/2508.07700v1",
        "published_date": "2025-08-11T07:23:39+00:00",
        "updated_date": "2025-08-11T07:23:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weitao Wang",
            "Haoran Xu",
            "Jun Meng",
            "Haoqian Wang"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer"
        ],
        "tldr": "The paper presents a method for aligning edited 3D assets with their original geometry to improve multi-view consistency and mesh quality.",
        "tldr_zh": "该论文提出了一种方法，可以将编辑后的3D资产与其原始几何形状对齐，以提高多视图一致性和网格质量。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding",
        "summary": "Temporal Video Grounding (TVG) aims to precisely localize video segments\ncorresponding to natural language queries, which is a critical capability for\nlong-form video understanding. Although existing reinforcement learning\napproaches encourage models to generate reasoning chains before predictions,\nthey fail to explicitly constrain the reasoning process to ensure the quality\nof the final temporal predictions. To address this limitation, we propose\nTimestamp Anchor-constrained Reasoning for Temporal Video Grounding (TAR-TVG),\na novel framework that introduces timestamp anchors within the reasoning\nprocess to enforce explicit supervision to the thought content. These anchors\nserve as intermediate verification points. More importantly, we require each\nreasoning step to produce increasingly accurate temporal estimations, thereby\nensuring that the reasoning process contributes meaningfully to the final\nprediction. To address the challenge of low-probability anchor generation in\nmodels (e.g., Qwen2.5-VL-3B), we develop an efficient self-distillation\ntraining strategy: (1) initial GRPO training to collect 30K high-quality\nreasoning traces containing multiple timestamp anchors, (2) supervised\nfine-tuning (SFT) on distilled data, and (3) final GRPO optimization on the\nSFT-enhanced model. This three-stage training strategy enables robust anchor\ngeneration while maintaining reasoning quality. Experiments show that our model\nachieves state-of-the-art performance while producing interpretable, verifiable\nreasoning chains with progressively refined temporal estimations.",
        "url": "http://arxiv.org/abs/2508.07683v1",
        "published_date": "2025-08-11T06:59:32+00:00",
        "updated_date": "2025-08-11T06:59:32+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Chaohong Guo",
            "Xun Mo",
            "Yongwei Nie",
            "Xuemiao Xu",
            "Chao Xu",
            "Fei Yu",
            "Chengjiang Long"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "TAR-TVG is a new framework for enhancing VLMs for temporal video grounding by introducing timestamp anchors to ensure the quality of temporal predictions.",
        "tldr_zh": "TAR-TVG是一个新的框架，通过引入时间戳锚点来提高VLMs的性能，用于视频时间定位。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DiffVC-OSD: One-Step Diffusion-based Perceptual Neural Video Compression Framework",
        "summary": "In this work, we first propose DiffVC-OSD, a One-Step Diffusion-based\nPerceptual Neural Video Compression framework. Unlike conventional multi-step\ndiffusion-based methods, DiffVC-OSD feeds the reconstructed latent\nrepresentation directly into a One-Step Diffusion Model, enhancing perceptual\nquality through a single diffusion step guided by both temporal context and the\nlatent itself. To better leverage temporal dependencies, we design a Temporal\nContext Adapter that encodes conditional inputs into multi-level features,\noffering more fine-grained guidance for the Denoising Unet. Additionally, we\nemploy an End-to-End Finetuning strategy to improve overall compression\nperformance. Extensive experiments demonstrate that DiffVC-OSD achieves\nstate-of-the-art perceptual compression performance, offers about 20$\\times$\nfaster decoding and a 86.92\\% bitrate reduction compared to the corresponding\nmulti-step diffusion-based variant.",
        "url": "http://arxiv.org/abs/2508.07682v1",
        "published_date": "2025-08-11T06:59:23+00:00",
        "updated_date": "2025-08-11T06:59:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenzhuo Ma",
            "Zhenzhong Chen"
        ],
        "ai_categories": [
            "Diffusion"
        ],
        "tldr": "The paper proposes DiffVC-OSD, a one-step diffusion-based neural video compression framework with improved perceptual quality and faster decoding.",
        "tldr_zh": "本文提出了DiffVC-OSD，一种一步扩散式神经视频压缩框架，提高了感知质量，解码速度更快。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Undress to Redress: A Training-Free Framework for Virtual Try-On",
        "summary": "Virtual try-on (VTON) is a crucial task for enhancing user experience in\nonline shopping by generating realistic garment previews on personal photos.\nAlthough existing methods have achieved impressive results, they struggle with\nlong-sleeve-to-short-sleeve conversions-a common and practical scenario-often\nproducing unrealistic outputs when exposed skin is underrepresented in the\noriginal image. We argue that this challenge arises from the ''majority''\ncompletion rule in current VTON models, which leads to inaccurate skin\nrestoration in such cases. To address this, we propose UR-VTON (Undress-Redress\nVirtual Try-ON), a novel, training-free framework that can be seamlessly\nintegrated with any existing VTON method. UR-VTON introduces an\n''undress-to-redress'' mechanism: it first reveals the user's torso by\nvirtually ''undressing,'' then applies the target short-sleeve garment,\neffectively decomposing the conversion into two more manageable steps.\nAdditionally, we incorporate Dynamic Classifier-Free Guidance scheduling to\nbalance diversity and image quality during DDPM sampling, and employ Structural\nRefiner to enhance detail fidelity using high-frequency cues. Finally, we\npresent LS-TON, a new benchmark for long-sleeve-to-short-sleeve try-on.\nExtensive experiments demonstrate that UR-VTON outperforms state-of-the-art\nmethods in both detail preservation and image quality. Code will be released\nupon acceptance.",
        "url": "http://arxiv.org/abs/2508.07680v1",
        "published_date": "2025-08-11T06:55:49+00:00",
        "updated_date": "2025-08-11T06:55:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhiying Li",
            "Junhao Wu",
            "Yeying Jin",
            "Daiheng Gao",
            "Yun Ji",
            "Kaichuan Kong",
            "Lei Yu",
            "Hao Xu",
            "Kai Chen",
            "Bruce Gu",
            "Nana Wang",
            "Zhaoxin Fan"
        ],
        "ai_categories": [
            "GAN",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel framework for virtual try-on that improves long-sleeve-to-short-sleeve conversions by introducing an 'undress-to-redress' mechanism.",
        "tldr_zh": "本文提出了一种新颖的虚拟试穿框架，通过引入“脱衣-穿衣”机制改善长袖转短袖的效果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AR-VRM: Imitating Human Motions for Visual Robot Manipulation with Analogical Reasoning",
        "summary": "Visual Robot Manipulation (VRM) aims to enable a robot to follow natural\nlanguage instructions based on robot states and visual observations, and\ntherefore requires costly multi-modal data. To compensate for the deficiency of\nrobot data, existing approaches have employed vision-language pretraining with\nlarge-scale data. However, they either utilize web data that differs from\nrobotic tasks, or train the model in an implicit way (e.g., predicting future\nframes at the pixel level), thus showing limited generalization ability under\ninsufficient robot data. In this paper, we propose to learn from large-scale\nhuman action video datasets in an explicit way (i.e., imitating human actions\nfrom hand keypoints), introducing Visual Robot Manipulation with Analogical\nReasoning (AR-VRM). To acquire action knowledge explicitly from human action\nvideos, we propose a keypoint Vision-Language Model (VLM) pretraining scheme,\nenabling the VLM to learn human action knowledge and directly predict human\nhand keypoints. During fine-tuning on robot data, to facilitate the robotic arm\nin imitating the action patterns of human motions, we first retrieve human\naction videos that perform similar manipulation tasks and have similar\nhistorical observations , and then learn the Analogical Reasoning (AR) map\nbetween human hand keypoints and robot components. Taking advantage of focusing\non action keypoints instead of irrelevant visual cues, our method achieves\nleading performance on the CALVIN benchmark {and real-world experiments}. In\nfew-shot scenarios, our AR-VRM outperforms previous methods by large margins ,\nunderscoring the effectiveness of explicitly imitating human actions under data\nscarcity.",
        "url": "http://arxiv.org/abs/2508.07626v1",
        "published_date": "2025-08-11T05:09:58+00:00",
        "updated_date": "2025-08-11T05:09:58+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Dejie Yang",
            "Zijing Zhao",
            "Yang Liu"
        ],
        "ai_categories": [
            "Multimodality",
            "Other"
        ],
        "tldr": "The paper introduces AR-VRM, a method that imitates human actions from videos to improve robot manipulation tasks, showing superior performance in data-scarce scenarios.",
        "tldr_zh": "本文介绍了AR-VRM，一种从视频中模仿人类动作以提高机器人操作任务性能的方法，在数据匮乏情况下表现优越。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Trustworthy Method for Multimodal Emotion Recognition",
        "summary": "Existing emotion recognition methods mainly focus on enhancing performance by\nemploying complex deep models, typically resulting in significantly higher\nmodel complexity. Although effective, it is also crucial to ensure the\nreliability of the final decision, especially for noisy, corrupted and\nout-of-distribution data. To this end, we propose a novel emotion recognition\nmethod called trusted emotion recognition (TER), which utilizes uncertainty\nestimation to calculate the confidence value of predictions. TER combines the\nresults from multiple modalities based on their confidence values to output the\ntrusted predictions. We also provide a new evaluation criterion to assess the\nreliability of predictions. Specifically, we incorporate trusted precision and\ntrusted recall to determine the trusted threshold and formulate the trusted\nAcc. and trusted F1 score to evaluate the model's trusted performance. The\nproposed framework combines the confidence module that accordingly endows the\nmodel with reliability and robustness against possible noise or corruption. The\nextensive experimental results validate the effectiveness of our proposed\nmodel. The TER achieves state-of-the-art performance on the Music-video,\nachieving 82.40% Acc. In terms of trusted performance, TER outperforms other\nmethods on the IEMOCAP and Music-video, achieving trusted F1 scores of 0.7511\nand 0.9035, respectively.",
        "url": "http://arxiv.org/abs/2508.07625v1",
        "published_date": "2025-08-11T05:08:31+00:00",
        "updated_date": "2025-08-11T05:08:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junxiao Xue",
            "Xiaozhen Liu",
            "Jie Wang",
            "Xuecheng Wu",
            "Bin Wu"
        ],
        "ai_categories": [
            "Multimodality",
            "AIGC"
        ],
        "tldr": "The paper introduces a trustworthy method for multimodal emotion recognition named TER, which uses uncertainty estimation to calculate confidence values and combines results from multiple modalities based on these values. It achieves state-of-the-art performance on Music-video datasets.",
        "tldr_zh": "本文介绍了一种可靠的多模态情感识别方法TER，利用不确定性估计计算置信度值，并基于这些值结合多个模态的结果。它在音乐视频数据集上取得了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing Egocentric Object Detection in Static Environments using Graph-based Spatial Anomaly Detection and Correction",
        "summary": "In many real-world applications involving static environments, the spatial\nlayout of objects remains consistent across instances. However,\nstate-of-the-art object detection models often fail to leverage this spatial\nprior, resulting in inconsistent predictions, missed detections, or\nmisclassifications, particularly in cluttered or occluded scenes. In this work,\nwe propose a graph-based post-processing pipeline that explicitly models the\nspatial relationships between objects to correct detection anomalies in\negocentric frames. Using a graph neural network (GNN) trained on manually\nannotated data, our model identifies invalid object class labels and predicts\ncorrected class labels based on their neighbourhood context. We evaluate our\napproach both as a standalone anomaly detection and correction framework and as\na post-processing module for standard object detectors such as YOLOv7 and\nRT-DETR. Experiments demonstrate that incorporating this spatial reasoning\nsignificantly improves detection performance, with mAP@50 gains of up to 4%.\nThis method highlights the potential of leveraging the environment's spatial\nstructure to improve reliability in object detection systems.",
        "url": "http://arxiv.org/abs/2508.07624v1",
        "published_date": "2025-08-11T05:08:02+00:00",
        "updated_date": "2025-08-11T05:08:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Vishakha Lall",
            "Yisi Liu"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper proposes a graph-based method to improve object detection in static environments by leveraging spatial relationships between objects.",
        "tldr_zh": "该论文提出了一种基于图的方法，通过利用物体之间的空间关系，改进静态环境中的对象检测。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "X2Edit: Revisiting Arbitrary-Instruction Image Editing through Self-Constructed Data and Task-Aware Representation Learning",
        "summary": "Existing open-source datasets for arbitrary-instruction image editing remain\nsuboptimal, while a plug-and-play editing module compatible with\ncommunity-prevalent generative models is notably absent. In this paper, we\nfirst introduce the X2Edit Dataset, a comprehensive dataset covering 14 diverse\nediting tasks, including subject-driven generation. We utilize the\nindustry-leading unified image generation models and expert models to construct\nthe data. Meanwhile, we design reasonable editing instructions with the VLM and\nimplement various scoring mechanisms to filter the data. As a result, we\nconstruct 3.7 million high-quality data with balanced categories. Second, to\nbetter integrate seamlessly with community image generation models, we design\ntask-aware MoE-LoRA training based on FLUX.1, with only 8\\% of the parameters\nof the full model. To further improve the final performance, we utilize the\ninternal representations of the diffusion model and define positive/negative\nsamples based on image editing types to introduce contrastive learning.\nExtensive experiments demonstrate that the model's editing performance is\ncompetitive among many excellent models. Additionally, the constructed dataset\nexhibits substantial advantages over existing open-source datasets. The\nopen-source code, checkpoints, and datasets for X2Edit can be found at the\nfollowing link: https://github.com/OPPO-Mente-Lab/X2Edit.",
        "url": "http://arxiv.org/abs/2508.07607v1",
        "published_date": "2025-08-11T04:22:49+00:00",
        "updated_date": "2025-08-11T04:22:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jian Ma",
            "Xujie Zhu",
            "Zihao Pan",
            "Qirong Peng",
            "Xu Guo",
            "Chen Chen",
            "Haonan Lu"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer",
            "LoRA"
        ],
        "tldr": "The paper introduces the X2Edit Dataset and a task-aware MoE-LoRA training method for image editing, showing competitive performance among existing models.",
        "tldr_zh": "本文介绍了X2Edit数据集和一种面向任务的MoE-LoRA训练方法，表现出与现有模型竞争力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ShoulderShot: Generating Over-the-Shoulder Dialogue Videos",
        "summary": "Over-the-shoulder dialogue videos are essential in films, short dramas, and\nadvertisements, providing visual variety and enhancing viewers' emotional\nconnection. Despite their importance, such dialogue scenes remain largely\nunderexplored in video generation research. The main challenges include\nmaintaining character consistency across different shots, creating a sense of\nspatial continuity, and generating long, multi-turn dialogues within limited\ncomputational budgets. Here, we present ShoulderShot, a framework that combines\ndual-shot generation with looping video, enabling extended dialogues while\npreserving character consistency. Our results demonstrate capabilities that\nsurpass existing methods in terms of shot-reverse-shot layout, spatial\ncontinuity, and flexibility in dialogue length, thereby opening up new\npossibilities for practical dialogue video generation. Videos and comparisons\nare available at https://shouldershot.github.io.",
        "url": "http://arxiv.org/abs/2508.07597v1",
        "published_date": "2025-08-11T03:56:23+00:00",
        "updated_date": "2025-08-11T03:56:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuang Zhang",
            "Junqi Cheng",
            "Haoyu Zhao",
            "Jiaxi Gu",
            "Fangyuan Zou",
            "Zenghui Lu",
            "Peng Shu"
        ],
        "ai_categories": [
            "GAN",
            "Multimodality",
            "Transformer"
        ],
        "tldr": "ShoulderShot is a framework for generating over-the-shoulder dialogue videos, addressing challenges like character consistency and dialogue length within limited computational budgets.",
        "tldr_zh": "ShoulderShot 是一个用于生成肩头对话视频的框架，解决了角色一致性和对话长度等挑战，在有限的计算预算内。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MSPT: A Lightweight Face Image Quality Assessment Method with Multi-stage Progressive Training",
        "summary": "Accurately assessing the perceptual quality of face images is crucial,\nespecially with the rapid progress in face restoration and generation.\nTraditional quality assessment methods often struggle with the unique\ncharacteristics of face images, limiting their generalizability. While\nlearning-based approaches demonstrate superior performance due to their strong\nfitting capabilities, their high complexity typically incurs significant\ncomputational and storage costs, hindering practical deployment. To address\nthis, we propose a lightweight face quality assessment network with Multi-Stage\nProgressive Training (MSPT). Our network employs a three-stage progressive\ntraining strategy that gradually introduces more diverse data samples and\nincreases input image resolution. This novel approach enables lightweight\nnetworks to achieve high performance by effectively learning complex quality\nfeatures while significantly mitigating catastrophic forgetting. Our MSPT\nachieved the second highest score on the VQualA 2025 face image quality\nassessment benchmark dataset, demonstrating that MSPT achieves comparable or\nbetter performance than state-of-the-art methods while maintaining efficient\ninference.",
        "url": "http://arxiv.org/abs/2508.07590v1",
        "published_date": "2025-08-11T03:37:09+00:00",
        "updated_date": "2025-08-11T03:37:09+00:00",
        "categories": [
            "cs.MM",
            "cs.CV"
        ],
        "authors": [
            "Xiongwei Xiao",
            "Baoying Chen",
            "Jishen Zeng",
            "Jianquan Yang"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper proposes a lightweight face image quality assessment method using Multi-Stage Progressive Training to achieve high performance while being efficient in terms of computational resources.",
        "tldr_zh": "本文提出了一种使用多阶段渐进训练的轻量级人脸图像质量评估方法，可以在高性能的同时高效利用计算资源。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GAPNet: A Lightweight Framework for Image and Video Salient Object Detection via Granularity-Aware Paradigm",
        "summary": "Recent salient object detection (SOD) models predominantly rely on\nheavyweight backbones, incurring substantial computational cost and hindering\ntheir practical application in various real-world settings, particularly on\nedge devices. This paper presents GAPNet, a lightweight network built on the\ngranularity-aware paradigm for both image and video SOD. We assign saliency\nmaps of different granularities to supervise the multi-scale decoder\nside-outputs: coarse object locations for high-level outputs and fine-grained\nobject boundaries for low-level outputs. Specifically, our decoder is built\nwith granularity-aware connections which fuse high-level features of low\ngranularity and low-level features of high granularity, respectively. To\nsupport these connections, we design granular pyramid convolution (GPC) and\ncross-scale attention (CSA) modules for efficient fusion of low-scale and\nhigh-scale features, respectively. On top of the encoder, a self-attention\nmodule is built to learn global information, enabling accurate object\nlocalization with negligible computational cost. Unlike traditional U-Net-based\napproaches, our proposed method optimizes feature utilization and semantic\ninterpretation while applying appropriate supervision at each processing stage.\nExtensive experiments show that the proposed method achieves a new\nstate-of-the-art performance among lightweight image and video SOD models. Code\nis available at https://github.com/yuhuan-wu/GAPNet.",
        "url": "http://arxiv.org/abs/2508.07585v1",
        "published_date": "2025-08-11T03:30:59+00:00",
        "updated_date": "2025-08-11T03:30:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yu-Huan Wu",
            "Wei Liu",
            "Zi-Xuan Zhu",
            "Zizhou Wang",
            "Yong Liu",
            "Liangli Zhen"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "GAPNet is a lightweight framework for image and video salient object detection that outperforms traditional methods by utilizing granularity-aware connections and efficient fusion modules.",
        "tldr_zh": "GAPNet是一个轻量级的框架，用于图像和视频显著目标检测，通过利用粒度感知连接和高效融合模块，优于传统方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Exploiting Layer Normalization Fine-tuning in Visual Transformer Foundation Models for Classification",
        "summary": "LayerNorm is pivotal in Vision Transformers (ViTs), yet its fine-tuning\ndynamics under data scarcity and domain shifts remain underexplored. This paper\nshows that shifts in LayerNorm parameters after fine-tuning (LayerNorm shifts)\nare indicative of the transitions between source and target domains; its\nefficacy is contingent upon the degree to which the target training samples\naccurately represent the target domain, as quantified by our proposed\nFine-tuning Shift Ratio ($FSR$). Building on this, we propose a simple yet\neffective rescaling mechanism using a scalar $\\lambda$ that is negatively\ncorrelated to $FSR$ to align learned LayerNorm shifts with those ideal shifts\nachieved under fully representative data, combined with a cyclic framework that\nfurther enhances the LayerNorm fine-tuning. Extensive experiments across\nnatural and pathological images, in both in-distribution (ID) and\nout-of-distribution (OOD) settings, and various target training sample regimes\nvalidate our framework. Notably, OOD tasks tend to yield lower $FSR$ and higher\n$\\lambda$ in comparison to ID cases, especially with scarce data, indicating\nunder-represented target training samples. Moreover, ViTFs fine-tuned on\npathological data behave more like ID settings, favoring conservative LayerNorm\nupdates. Our findings illuminate the underexplored dynamics of LayerNorm in\ntransfer learning and provide practical strategies for LayerNorm fine-tuning.",
        "url": "http://arxiv.org/abs/2508.07577v1",
        "published_date": "2025-08-11T03:18:47+00:00",
        "updated_date": "2025-08-11T03:18:47+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Zhaorui Tan",
            "Tan Pan",
            "Kaizhu Huang",
            "Weimiao Yu",
            "Kai Yao",
            "Chen Jiang",
            "Qiufeng Wang",
            "Anh Nguyen",
            "Xin Guo",
            "Yuan Cheng",
            "Xi Yang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper explores the impact of fine-tuning LayerNorm in visual transformer models for classification tasks, proposing a rescaling mechanism to align shifts in LayerNorm parameters with ideal shifts for better performance in transfer learning scenarios.",
        "tldr_zh": "本文探讨了在视觉转换模型中微调LayerNorm对分类任务的影响，提出了一种重新缩放机制，以使LayerNorm参数的变化与理想变化对齐，以实现更好的迁移学习性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Splat4D: Diffusion-Enhanced 4D Gaussian Splatting for Temporally and Spatially Consistent Content Creation",
        "summary": "Generating high-quality 4D content from monocular videos for applications\nsuch as digital humans and AR/VR poses challenges in ensuring temporal and\nspatial consistency, preserving intricate details, and incorporating user\nguidance effectively. To overcome these challenges, we introduce Splat4D, a\nnovel framework enabling high-fidelity 4D content generation from a monocular\nvideo. Splat4D achieves superior performance while maintaining faithful\nspatial-temporal coherence by leveraging multi-view rendering, inconsistency\nidentification, a video diffusion model, and an asymmetric U-Net for\nrefinement. Through extensive evaluations on public benchmarks, Splat4D\nconsistently demonstrates state-of-the-art performance across various metrics,\nunderscoring the efficacy of our approach. Additionally, the versatility of\nSplat4D is validated in various applications such as text/image conditioned 4D\ngeneration, 4D human generation, and text-guided content editing, producing\ncoherent outcomes following user instructions.",
        "url": "http://arxiv.org/abs/2508.07557v1",
        "published_date": "2025-08-11T02:35:53+00:00",
        "updated_date": "2025-08-11T02:35:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Minghao Yin",
            "Yukang Cao",
            "Songyou Peng",
            "Kai Han"
        ],
        "ai_categories": [
            "Diffusion"
        ],
        "tldr": "Splat4D is a novel framework for generating high-fidelity 4D content from monocular videos with superior performance and spatial-temporal coherence.",
        "tldr_zh": "Splat4D是一个新颖的框架，可以从单眼视频中生成高保真度的4D内容，具有出色的性能和时空一致性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CoT-Pose: Chain-of-Thought Reasoning for 3D Pose Generation from Abstract Prompts",
        "summary": "Recent advances in multi-modal large language models (MLLMs) and\nchain-of-thought (CoT) reasoning have led to significant progress in image and\ntext generation tasks. However, the field of 3D human pose generation still\nfaces critical limitations. Most existing text-to-pose models rely heavily on\ndetailed (low-level) prompts that explicitly describe joint configurations. In\ncontrast, humans tend to communicate actions and intentions using abstract\n(high-level) language. This mismatch results in a practical challenge for\ndeploying pose generation systems in real-world scenarios. To bridge this gap,\nwe introduce a novel framework that incorporates CoT reasoning into the pose\ngeneration process, enabling the interpretation of abstract prompts into\naccurate 3D human poses. We further propose a data synthesis pipeline that\nautomatically generates triplets of abstract prompts, detailed prompts, and\ncorresponding 3D poses for training process. Experimental results demonstrate\nthat our reasoning-enhanced model, CoT-Pose, can effectively generate plausible\nand semantically aligned poses from abstract textual inputs. This work\nhighlights the importance of high-level understanding in pose generation and\nopens new directions for reasoning-enhanced approach for human pose generation.",
        "url": "http://arxiv.org/abs/2508.07540v1",
        "published_date": "2025-08-11T01:43:41+00:00",
        "updated_date": "2025-08-11T01:43:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junuk Cha",
            "Jihyeon Kim"
        ],
        "ai_categories": [
            "Multimodality",
            "LoRA",
            "Dataset"
        ],
        "tldr": "The paper introduces a framework called CoT-Pose that uses Chain-of-Thought reasoning to generate 3D human poses from abstract textual inputs.",
        "tldr_zh": "该论文介绍了一个名为CoT-Pose的框架，利用链式推理从抽象文本输入生成3D人体姿势。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Exploring Multimodal Diffusion Transformers for Enhanced Prompt-based Image Editing",
        "summary": "Transformer-based diffusion models have recently superseded traditional U-Net\narchitectures, with multimodal diffusion transformers (MM-DiT) emerging as the\ndominant approach in state-of-the-art models like Stable Diffusion 3 and\nFlux.1. Previous approaches have relied on unidirectional cross-attention\nmechanisms, with information flowing from text embeddings to image latents. In\ncontrast, MMDiT introduces a unified attention mechanism that concatenates\ninput projections from both modalities and performs a single full attention\noperation, allowing bidirectional information flow between text and image\nbranches. This architectural shift presents significant challenges for existing\nediting techniques. In this paper, we systematically analyze MM-DiT's attention\nmechanism by decomposing attention matrices into four distinct blocks,\nrevealing their inherent characteristics. Through these analyses, we propose a\nrobust, prompt-based image editing method for MM-DiT that supports global to\nlocal edits across various MM-DiT variants, including few-step models. We\nbelieve our findings bridge the gap between existing U-Net-based methods and\nemerging architectures, offering deeper insights into MMDiT's behavioral\npatterns.",
        "url": "http://arxiv.org/abs/2508.07519v1",
        "published_date": "2025-08-11T00:40:12+00:00",
        "updated_date": "2025-08-11T00:40:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Joonghyuk Shin",
            "Alchan Hwang",
            "Yujin Kim",
            "Daneul Kim",
            "Jaesik Park"
        ],
        "ai_categories": [
            "Multimodality",
            "Diffusion",
            "Transformer"
        ],
        "tldr": "The paper introduces multimodal diffusion transformers for image editing, proposing a prompt-based method that bridges traditional U-Net methods and new architectures.",
        "tldr_zh": "本文介绍了用于图像编辑的多模态扩散变压器，提出了一种基于提示的方法，可以弥合传统 U-Net 方法和新架构之间的差距。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From Field to Drone: Domain Drift Tolerant Automated Multi-Species and Damage Plant Semantic Segmentation for Herbicide Trials",
        "summary": "Field trials are vital in herbicide research and development to assess\neffects on crops and weeds under varied conditions. Traditionally, evaluations\nrely on manual visual assessments, which are time-consuming, labor-intensive,\nand subjective. Automating species and damage identification is challenging due\nto subtle visual differences, but it can greatly enhance efficiency and\nconsistency.\n  We present an improved segmentation model combining a general-purpose\nself-supervised visual model with hierarchical inference based on botanical\ntaxonomy. Trained on a multi-year dataset (2018-2020) from Germany and Spain\nusing digital and mobile cameras, the model was tested on digital camera data\n(year 2023) and drone imagery from the United States, Germany, and Spain (year\n2024) to evaluate robustness under domain shift. This cross-device evaluation\nmarks a key step in assessing generalization across platforms of the model.\n  Our model significantly improved species identification (F1-score: 0.52 to\n0.85, R-squared: 0.75 to 0.98) and damage classification (F1-score: 0.28 to\n0.44, R-squared: 0.71 to 0.87) over prior methods. Under domain shift (drone\nimages), it maintained strong performance with moderate degradation (species:\nF1-score 0.60, R-squared 0.80; damage: F1-score 0.41, R-squared 0.62), where\nearlier models failed.\n  These results confirm the model's robustness and real-world applicability. It\nis now deployed in BASF's phenotyping pipeline, enabling large-scale, automated\ncrop and weed monitoring across diverse geographies.",
        "url": "http://arxiv.org/abs/2508.07514v1",
        "published_date": "2025-08-11T00:08:42+00:00",
        "updated_date": "2025-08-11T00:08:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Artzai Picon",
            "Itziar Eguskiza",
            "Daniel Mugica",
            "Javier Romero",
            "Carlos Javier Jimenez",
            "Eric White",
            "Gabriel Do-Lago-Junqueira",
            "Christian Klukas",
            "Ramon Navarra-Mestre"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper presents an automated plant segmentation model for herbicide trials, showing improved performance in species and damage identification across different datasets and domains.",
        "tldr_zh": "本文提出了一种用于除草剂试验的自动化植物分割模型，展示了在不同数据集和领域中对物种和损伤识别性能的改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FormCoach: Lift Smarter, Not Harder",
        "summary": "Good form is the difference between strength and strain, yet for the\nfast-growing community of at-home fitness enthusiasts, expert feedback is often\nout of reach. FormCoach transforms a simple camera into an always-on,\ninteractive AI training partner, capable of spotting subtle form errors and\ndelivering tailored corrections in real time, leveraging vision-language models\n(VLMs). We showcase this capability through a web interface and benchmark\nstate-of-the-art VLMs on a dataset of 1,700 expert-annotated user-reference\nvideo pairs spanning 22 strength and mobility exercises. To accelerate research\nin AI-driven coaching, we release both the dataset and an automated,\nrubric-based evaluation pipeline, enabling standardized comparison across\nmodels. Our benchmarks reveal substantial gaps compared to human-level\ncoaching, underscoring both the challenges and opportunities in integrating\nnuanced, context-aware movement analysis into interactive AI systems. By\nframing form correction as a collaborative and creative process between humans\nand machines, FormCoach opens a new frontier in embodied AI.",
        "url": "http://arxiv.org/abs/2508.07501v1",
        "published_date": "2025-08-10T22:33:43+00:00",
        "updated_date": "2025-08-10T22:33:43+00:00",
        "categories": [
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Xiaoye Zuo",
            "Nikos Athanasiou",
            "Ginger Delmas",
            "Yiming Huang",
            "Xingyu Fu",
            "Lingjie Liu"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "FormCoach is an interactive AI training partner that helps users improve their form during strength and mobility exercises using vision-language models. The system aims to bridge the gap between human-level coaching and AI-driven form correction.",
        "tldr_zh": "FormCoach是一个交互式AI训练伙伴，利用视觉语言模型帮助用户在力量和灵活性练习中改善姿势。该系统旨在弥合人类级别辅导与AI驱动的形式纠正之间的差距。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding",
        "summary": "Most organizational data in this world are stored as documents, and visual\nretrieval plays a crucial role in unlocking the collective intelligence from\nall these documents. However, existing benchmarks focus on English-only\ndocument retrieval or only consider multilingual question-answering on a\nsingle-page image. To bridge this gap, we introduce VisR-Bench, a multilingual\nbenchmark designed for question-driven multimodal retrieval in long documents.\nOur benchmark comprises over 35K high-quality QA pairs across 1.2K documents,\nenabling fine-grained evaluation of multimodal retrieval. VisR-Bench spans\nsixteen languages with three question types (figures, text, and tables),\noffering diverse linguistic and question coverage. Unlike prior datasets, we\ninclude queries without explicit answers, preventing models from relying on\nsuperficial keyword matching. We evaluate various retrieval models, including\ntext-based methods, multimodal encoders, and MLLMs, providing insights into\ntheir strengths and limitations. Our results show that while MLLMs\nsignificantly outperform text-based and multimodal encoder models, they still\nstruggle with structured tables and low-resource languages, highlighting key\nchallenges in multilingual visual retrieval.",
        "url": "http://arxiv.org/abs/2508.07493v1",
        "published_date": "2025-08-10T21:44:43+00:00",
        "updated_date": "2025-08-10T21:44:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jian Chen",
            "Ming Li",
            "Jihyung Kil",
            "Chenguang Wang",
            "Tong Yu",
            "Ryan Rossi",
            "Tianyi Zhou",
            "Changyou Chen",
            "Ruiyi Zhang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "VisR-Bench is a multilingual benchmark for question-driven multimodal retrieval in long documents, evaluating retrieval models across different languages and question types.",
        "tldr_zh": "VisR-Bench是一个多语言基准数据集，用于长文档的问答式多模态检索，评估不同语言和问题类型下的检索模型。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Novel View Synthesis with Gaussian Splatting: Impact on Photogrammetry Model Accuracy and Resolution",
        "summary": "In this paper, I present a comprehensive study comparing Photogrammetry and\nGaussian Splatting techniques for 3D model reconstruction and view synthesis. I\ncreated a dataset of images from a real-world scene and constructed 3D models\nusing both methods. To evaluate the performance, I compared the models using\nstructural similarity index (SSIM), peak signal-to-noise ratio (PSNR), learned\nperceptual image patch similarity (LPIPS), and lp/mm resolution based on the\nUSAF resolution chart. A significant contribution of this work is the\ndevelopment of a modified Gaussian Splatting repository, which I forked and\nenhanced to enable rendering images from novel camera poses generated in the\nBlender environment. This innovation allows for the synthesis of high-quality\nnovel views, showcasing the flexibility and potential of Gaussian Splatting. My\ninvestigation extends to an augmented dataset that includes both original\nground images and novel views synthesized via Gaussian Splatting. This\naugmented dataset was employed to generate a new photogrammetry model, which\nwas then compared against the original photogrammetry model created using only\nthe original images. The results demonstrate the efficacy of using Gaussian\nSplatting to generate novel high-quality views and its potential to improve\nphotogrammetry-based 3D reconstructions. The comparative analysis highlights\nthe strengths and limitations of both approaches, providing valuable\ninformation for applications in extended reality (XR), photogrammetry, and\nautonomous vehicle simulations. Code is available at\nhttps://github.com/pranavc2255/gaussian-splatting-novel-view-render.git.",
        "url": "http://arxiv.org/abs/2508.07483v1",
        "published_date": "2025-08-10T20:57:36+00:00",
        "updated_date": "2025-08-10T20:57:36+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Pranav Chougule"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper compares Photogrammetry and Gaussian Splatting for 3D model reconstruction and view synthesis, with a focus on generating novel high-quality views.",
        "tldr_zh": "本文比较了摄影测量和高斯喷溅技术在三维模型重建和视角合成上的应用，重点在于生成高质量的新视角。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GRASPTrack: Geometry-Reasoned Association via Segmentation and Projection for Multi-Object Tracking",
        "summary": "Multi-object tracking (MOT) in monocular videos is fundamentally challenged\nby occlusions and depth ambiguity, issues that conventional\ntracking-by-detection (TBD) methods struggle to resolve owing to a lack of\ngeometric awareness. To address these limitations, we introduce GRASPTrack, a\nnovel depth-aware MOT framework that integrates monocular depth estimation and\ninstance segmentation into a standard TBD pipeline to generate high-fidelity 3D\npoint clouds from 2D detections, thereby enabling explicit 3D geometric\nreasoning. These 3D point clouds are then voxelized to enable a precise and\nrobust Voxel-Based 3D Intersection-over-Union (IoU) for spatial association. To\nfurther enhance tracking robustness, our approach incorporates Depth-aware\nAdaptive Noise Compensation, which dynamically adjusts the Kalman filter\nprocess noise based on occlusion severity for more reliable state estimation.\nAdditionally, we propose a Depth-enhanced Observation-Centric Momentum, which\nextends the motion direction consistency from the image plane into 3D space to\nimprove motion-based association cues, particularly for objects with complex\ntrajectories. Extensive experiments on the MOT17, MOT20, and DanceTrack\nbenchmarks demonstrate that our method achieves competitive performance,\nsignificantly improving tracking robustness in complex scenes with frequent\nocclusions and intricate motion patterns.",
        "url": "http://arxiv.org/abs/2508.08117v1",
        "published_date": "2025-08-11T15:56:21+00:00",
        "updated_date": "2025-08-11T15:56:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xudong Han",
            "Pengcheng Fang",
            "Yueying Tian",
            "Jianhui Yu",
            "Xiaohao Cai",
            "Daniel Roggen",
            "Philip Birch"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "GRASPTrack is a depth-aware MOT framework that integrates monocular depth estimation and instance segmentation to improve tracking robustness in complex scenes.",
        "tldr_zh": "GRASPTrack是一个深度感知的MOT框架，通过整合单眼深度估计和实例分割来提高复杂场景中的跟踪稳健性。",
        "relevance_score": 6,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7.75
    },
    {
        "title": "Information Bottleneck-based Causal Attention for Multi-label Medical Image Recognition",
        "summary": "Multi-label classification (MLC) of medical images aims to identify multiple\ndiseases and holds significant clinical potential. A critical step is to learn\nclass-specific features for accurate diagnosis and improved interpretability\neffectively. However, current works focus primarily on causal attention to\nlearn class-specific features, yet they struggle to interpret the true cause\ndue to the inadvertent attention to class-irrelevant features. To address this\nchallenge, we propose a new structural causal model (SCM) that treats\nclass-specific attention as a mixture of causal, spurious, and noisy factors,\nand a novel Information Bottleneck-based Causal Attention (IBCA) that is\ncapable of learning the discriminative class-specific attention for MLC of\nmedical images. Specifically, we propose learning Gaussian mixture multi-label\nspatial attention to filter out class-irrelevant information and capture each\nclass-specific attention pattern. Then a contrastive enhancement-based causal\nintervention is proposed to gradually mitigate the spurious attention and\nreduce noise information by aligning multi-head attention with the Gaussian\nmixture multi-label spatial. Quantitative and ablation results on Endo and\nMuReD show that IBCA outperforms all methods. Compared to the second-best\nresults for each metric, IBCA achieves improvements of 6.35\\% in CR, 7.72\\% in\nOR, and 5.02\\% in mAP for MuReD, 1.47\\% in CR, and 1.65\\% in CF1, and 1.42\\% in\nmAP for Endo.",
        "url": "http://arxiv.org/abs/2508.08069v1",
        "published_date": "2025-08-11T15:12:54+00:00",
        "updated_date": "2025-08-11T15:12:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoxiao Cui",
            "Yiran Li",
            "Kai He",
            "Shanzhi Jiang",
            "Mengli Xue",
            "Wentao Li",
            "Junhong Leng",
            "Zhi Liu",
            "Lizhen Cui",
            "Shuo Li"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes Information Bottleneck-based Causal Attention for multi-label medical image recognition, achieving significant performance improvements over existing methods.",
        "tldr_zh": "本文提出了基于信息瓶颈的因果注意力模型，用于多标签医学图像识别，相较于现有方法取得了显著的性能改进。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7.75
    },
    {
        "title": "Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models",
        "summary": "Despite significant progress in generative modelling, existing diffusion\nmodels often struggle to produce anatomically precise female pelvic images,\nlimiting their application in gynaecological imaging, where data scarcity and\npatient privacy concerns are critical. To overcome these barriers, we introduce\na novel diffusion-based framework for uterine MRI synthesis, integrating both\nunconditional and conditioned Denoising Diffusion Probabilistic Models (DDPMs)\nand Latent Diffusion Models (LDMs) in 2D and 3D. Our approach generates\nanatomically coherent, high fidelity synthetic images that closely mimic real\nscans and provide valuable resources for training robust diagnostic models. We\nevaluate generative quality using advanced perceptual and distributional\nmetrics, benchmarking against standard reconstruction methods, and demonstrate\nsubstantial gains in diagnostic accuracy on a key classification task. A\nblinded expert evaluation further validates the clinical realism of our\nsynthetic images. We release our models with privacy safeguards and a\ncomprehensive synthetic uterine MRI dataset to support reproducible research\nand advance equitable AI in gynaecology.",
        "url": "http://arxiv.org/abs/2508.07903v1",
        "published_date": "2025-08-11T12:18:23+00:00",
        "updated_date": "2025-08-11T12:18:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Johanna P. Müller",
            "Anika Knupfer",
            "Pedro Blöss",
            "Edoardo Berardi Vittur",
            "Bernhard Kainz",
            "Jana Hutter"
        ],
        "ai_categories": [
            "Dataset",
            "Diffusion",
            "GAN"
        ],
        "tldr": "The paper presents a novel diffusion-based framework for uterine MRI synthesis, generating anatomically coherent synthetic images that can improve diagnostic accuracy.",
        "tldr_zh": "该论文提出了一种新颖的扩散模型框架用于子宫MRI合成，生成解剖学连贯的合成图像，可以提高诊断准确性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.75
    },
    {
        "title": "Adaptive Pseudo Label Selection for Individual Unlabeled Data by Positive and Unlabeled Learning",
        "summary": "This paper proposes a novel pseudo-labeling method for medical image\nsegmentation that can perform learning on ``individual images'' to select\neffective pseudo-labels. We introduce Positive and Unlabeled Learning (PU\nlearning), which uses only positive and unlabeled data for binary\nclassification problems, to obtain the appropriate metric for discriminating\nforeground and background regions on each unlabeled image. Our PU learning\nmakes us easy to select pseudo-labels for various background regions. The\nexperimental results show the effectiveness of our method.",
        "url": "http://arxiv.org/abs/2508.07548v1",
        "published_date": "2025-08-11T02:11:49+00:00",
        "updated_date": "2025-08-11T02:11:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Takehiro Yamane",
            "Itaru Tsuge",
            "Susumu Saito",
            "Ryoma Bise"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper introduces a new pseudo-labeling method for medical image segmentation using Positive and Unlabeled Learning, showing effectiveness in selecting pseudo-labels for background regions.",
        "tldr_zh": "本文提出一种新的伪标签方法，利用正样本和未标记样本学习，在选择背景区域的伪标签方面表现出有效性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.75
    },
    {
        "title": "ReferSplat: Referring Segmentation in 3D Gaussian Splatting",
        "summary": "We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task\nthat aims to segment target objects in a 3D Gaussian scene based on natural\nlanguage descriptions, which often contain spatial relationships or object\nattributes. This task requires the model to identify newly described objects\nthat may be occluded or not directly visible in a novel view, posing a\nsignificant challenge for 3D multi-modal understanding. Developing this\ncapability is crucial for advancing embodied AI. To support research in this\narea, we construct the first R3DGS dataset, Ref-LERF. Our analysis reveals that\n3D multi-modal understanding and spatial relationship modeling are key\nchallenges for R3DGS. To address these challenges, we propose ReferSplat, a\nframework that explicitly models 3D Gaussian points with natural language\nexpressions in a spatially aware paradigm. ReferSplat achieves state-of-the-art\nperformance on both the newly proposed R3DGS task and 3D open-vocabulary\nsegmentation benchmarks. Dataset and code are available at\nhttps://github.com/heshuting555/ReferSplat.",
        "url": "http://arxiv.org/abs/2508.08252v1",
        "published_date": "2025-08-11T17:59:30+00:00",
        "updated_date": "2025-08-11T17:59:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuting He",
            "Guangquan Jie",
            "Changshuo Wang",
            "Yun Zhou",
            "Shuming Hu",
            "Guanbin Li",
            "Henghui Ding"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a new task called Referring 3D Gaussian Splatting Segmentation (R3DGS) that aims to segment objects in 3D scenes based on natural language descriptions. They propose ReferSplat, a framework that achieves state-of-the-art performance on this task.",
        "tldr_zh": "本文介绍了一项名为参考三维高斯点喷射分割（R3DGS）的新任务，旨在根据自然语言描述在三维场景中对物体进行分割。他们提出了ReferSplat框架，在该任务上取得了最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold Representation Learning",
        "summary": "Semantic segmentation of structural defects in civil infrastructure remains\nchallenging due to variable defect appearances, harsh imaging conditions, and\nsignificant class imbalance. Current deep learning methods, despite their\neffectiveness, typically require millions of parameters, rendering them\nimpractical for real-time inspection systems. We introduce KARMA\n(Kolmogorov-Arnold Representation Mapping Architecture), a highly efficient\nsemantic segmentation framework that models complex defect patterns through\ncompositions of one-dimensional functions rather than conventional\nconvolutions. KARMA features three technical innovations: (1) a\nparameter-efficient Tiny Kolmogorov-Arnold Network (TiKAN) module leveraging\nlow-rank factorization for KAN-based feature transformation; (2) an optimized\nfeature pyramid structure with separable convolutions for multi-scale defect\nanalysis; and (3) a static-dynamic prototype mechanism that enhances feature\nrepresentation for imbalanced classes. Extensive experiments on benchmark\ninfrastructure inspection datasets demonstrate that KARMA achieves competitive\nor superior mean IoU performance compared to state-of-the-art approaches, while\nusing significantly fewer parameters (0.959M vs. 31.04M, a 97% reduction).\nOperating at 0.264 GFLOPS, KARMA maintains inference speeds suitable for\nreal-time deployment, enabling practical automated infrastructure inspection\nsystems without compromising accuracy. The source code can be accessed at the\nfollowing URL: https://github.com/faeyelab/karma.",
        "url": "http://arxiv.org/abs/2508.08186v1",
        "published_date": "2025-08-11T17:06:55+00:00",
        "updated_date": "2025-08-11T17:06:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Md Meftahul Ferdaus",
            "Mahdi Abdelguerfi",
            "Elias Ioup",
            "Steven Sloan",
            "Kendall N. Niles",
            "Ken Pathak"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces KARMA, a highly efficient semantic segmentation framework for structural defect analysis in civil infrastructure, achieving competitive performance with significantly fewer parameters.",
        "tldr_zh": "本文介绍了KARMA，一种高效的语义分割框架，可用于民用基础设施的结构缺陷分析，在参数显著减少的情况下实现竞争性能。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Learned Regularization for Microwave Tomography",
        "summary": "Microwave Tomography (MWT) aims to reconstruct the dielectric properties of\ntissues from measured scattered electromagnetic fields. This inverse problem is\nhighly nonlinear and ill-posed, posing significant challenges for conventional\noptimization-based methods, which, despite being grounded in physical models,\noften fail to recover fine structural details. Recent deep learning strategies,\nincluding end-to-end and post-processing networks, have improved reconstruction\nquality but typically require large paired training datasets and may struggle\nto generalize. To overcome these limitations, we propose a physics-informed\nhybrid framework that integrates diffusion models as learned regularization\nwithin a data-consistency-driven variational scheme. Specifically, we introduce\nSingle-Step Diffusion Regularization (SSD-Reg), a novel approach that embeds\ndiffusion priors into the iterative reconstruction process, enabling the\nrecovery of complex anatomical structures without the need for paired data.\nSSD-Reg maintains fidelity to both the governing physics and learned structural\ndistributions, improving accuracy, stability, and robustness. Extensive\nexperiments demonstrate that SSD-Reg, implemented as a Plug-and-Play (PnP)\nmodule, provides a flexible and effective solution for tackling the\nill-posedness inherent in functional image reconstruction.",
        "url": "http://arxiv.org/abs/2508.08114v1",
        "published_date": "2025-08-11T15:54:58+00:00",
        "updated_date": "2025-08-11T15:54:58+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Bowen Tong",
            "Hao Chen",
            "Shaorui Guo",
            "Dong Liu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a physics-informed hybrid framework called SSD-Reg for Microwave Tomography, which integrates diffusion models as learned regularization to improve reconstruction quality without the need for large training datasets.",
        "tldr_zh": "该论文引入了一种名为SSD-Reg的物理信息混合框架，用于微波断层成像，通过将扩散模型作为学习正则化来提高重建质量，无需大规模训练数据集。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Hyperspectral Imaging",
        "summary": "Hyperspectral imaging (HSI) is an advanced sensing modality that\nsimultaneously captures spatial and spectral information, enabling\nnon-invasive, label-free analysis of material, chemical, and biological\nproperties. This Primer presents a comprehensive overview of HSI, from the\nunderlying physical principles and sensor architectures to key steps in data\nacquisition, calibration, and correction. We summarize common data structures\nand highlight classical and modern analysis methods, including dimensionality\nreduction, classification, spectral unmixing, and AI-driven techniques such as\ndeep learning. Representative applications across Earth observation, precision\nagriculture, biomedicine, industrial inspection, cultural heritage, and\nsecurity are also discussed, emphasizing HSI's ability to uncover sub-visual\nfeatures for advanced monitoring, diagnostics, and decision-making. Persistent\nchallenges, such as hardware trade-offs, acquisition variability, and the\ncomplexity of high-dimensional data, are examined alongside emerging solutions,\nincluding computational imaging, physics-informed modeling, cross-modal fusion,\nand self-supervised learning. Best practices for dataset sharing,\nreproducibility, and metadata documentation are further highlighted to support\ntransparency and reuse. Looking ahead, we explore future directions toward\nscalable, real-time, and embedded HSI systems, driven by sensor\nminiaturization, self-supervised learning, and foundation models. As HSI\nevolves into a general-purpose, cross-disciplinary platform, it holds promise\nfor transformative applications in science, technology, and society.",
        "url": "http://arxiv.org/abs/2508.08107v1",
        "published_date": "2025-08-11T15:47:24+00:00",
        "updated_date": "2025-08-11T15:47:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Danfeng Hong",
            "Chenyu Li",
            "Naoto Yokoya",
            "Bing Zhang",
            "Xiuping Jia",
            "Antonio Plaza",
            "Paolo Gamba",
            "Jon Atli Benediktsson",
            "Jocelyn Chanussot"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper provides an overview of hyperspectral imaging, discussing its applications in various fields and highlighting challenges and emerging solutions.",
        "tldr_zh": "本文概述了高光谱成像技术，在各个领域的应用以及挑战和新兴解决方案。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "TrackOR: Towards Personalized Intelligent Operating Rooms Through Robust Tracking",
        "summary": "Providing intelligent support to surgical teams is a key frontier in\nautomated surgical scene understanding, with the long-term goal of improving\npatient outcomes. Developing personalized intelligence for all staff members\nrequires maintaining a consistent state of who is located where for long\nsurgical procedures, which still poses numerous computational challenges. We\npropose TrackOR, a framework for tackling long-term multi-person tracking and\nre-identification in the operating room. TrackOR uses 3D geometric signatures\nto achieve state-of-the-art online tracking performance (+11% Association\nAccuracy over the strongest baseline), while also enabling an effective offline\nrecovery process to create analysis-ready trajectories. Our work shows that by\nleveraging 3D geometric information, persistent identity tracking becomes\nattainable, enabling a critical shift towards the more granular, staff-centric\nanalyses required for personalized intelligent systems in the operating room.\nThis new capability opens up various applications, including our proposed\ntemporal pathway imprints that translate raw tracking data into actionable\ninsights for improving team efficiency and safety and ultimately providing\npersonalized support.",
        "url": "http://arxiv.org/abs/2508.07968v1",
        "published_date": "2025-08-11T13:28:50+00:00",
        "updated_date": "2025-08-11T13:28:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tony Danjun Wang",
            "Christian Heiliger",
            "Nassir Navab",
            "Lennart Bastian"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "TrackOR proposes a framework for personalized intelligent operating rooms through robust tracking, leveraging 3D geometric signatures to achieve high tracking performance and enable staff-centric analyses for improved efficiency and safety.",
        "tldr_zh": "TrackOR提出了一个框架，通过稳健的跟踪利用3D几何特征，实现高效跟踪表现，为改进效率和安全性提供了员工为中心的分析。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "VOIDFace: A Privacy-Preserving Multi-Network Face Recognition With Enhanced Security",
        "summary": "Advancement of machine learning techniques, combined with the availability of\nlarge-scale datasets, has significantly improved the accuracy and efficiency of\nfacial recognition. Modern facial recognition systems are trained using large\nface datasets collected from diverse individuals or public repositories.\nHowever, for training, these datasets are often replicated and stored in\nmultiple workstations, resulting in data replication, which complicates\ndatabase management and oversight. Currently, once a user submits their face\nfor dataset preparation, they lose control over how their data is used, raising\nsignificant privacy and ethical concerns. This paper introduces VOIDFace, a\nnovel framework for facial recognition systems that addresses two major issues.\nFirst, it eliminates the need of data replication and improves data control to\nsecurely store training face data by using visual secret sharing. Second, it\nproposes a patch-based multi-training network that uses this novel training\ndata storage mechanism to develop a robust, privacy-preserving facial\nrecognition system. By integrating these advancements, VOIDFace aims to improve\nthe privacy, security, and efficiency of facial recognition training, while\nensuring greater control over sensitive personal face data. VOIDFace also\nenables users to exercise their Right-To-Be-Forgotten property to control their\npersonal data. Experimental evaluations on the VGGFace2 dataset show that\nVOIDFace provides Right-To-Be-Forgotten, improved data control, security, and\nprivacy while maintaining competitive facial recognition performance. Code is\navailable at: https://github.com/ajnasmuhammed89/VOIDFace",
        "url": "http://arxiv.org/abs/2508.07960v1",
        "published_date": "2025-08-11T13:15:36+00:00",
        "updated_date": "2025-08-11T13:15:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ajnas Muhammed",
            "Iurri Medvedev",
            "Nuno Gonçalves"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC"
        ],
        "tldr": "VOIDFace is a privacy-preserving multi-network face recognition system that eliminates data replication and improves data control using visual secret sharing, providing better security and privacy.",
        "tldr_zh": "VOIDFace是一个隐私保护的多网络人脸识别系统，通过视觉秘密共享消除数据复制并改善数据控制，提供更好的安全性和隐私保护。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Tracking Any Point Methods for Markerless 3D Tissue Tracking in Endoscopic Stereo Images",
        "summary": "Minimally invasive surgery presents challenges such as dynamic tissue motion\nand a limited field of view. Accurate tissue tracking has the potential to\nsupport surgical guidance, improve safety by helping avoid damage to sensitive\nstructures, and enable context-aware robotic assistance during complex\nprocedures. In this work, we propose a novel method for markerless 3D tissue\ntracking by leveraging 2D Tracking Any Point (TAP) networks. Our method\ncombines two CoTracker models, one for temporal tracking and one for stereo\nmatching, to estimate 3D motion from stereo endoscopic images. We evaluate the\nsystem using a clinical laparoscopic setup and a robotic arm simulating tissue\nmotion, with experiments conducted on a synthetic 3D-printed phantom and a\nchicken tissue phantom. Tracking on the chicken tissue phantom yielded more\nreliable results, with Euclidean distance errors as low as 1.1 mm at a velocity\nof 10 mm/s. These findings highlight the potential of TAP-based models for\naccurate, markerless 3D tracking in challenging surgical scenarios.",
        "url": "http://arxiv.org/abs/2508.07851v1",
        "published_date": "2025-08-11T11:10:16+00:00",
        "updated_date": "2025-08-11T11:10:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Konrad Reuter",
            "Suresh Guttikonda",
            "Sarah Latus",
            "Lennart Maack",
            "Christian Betz",
            "Tobias Maurer",
            "Alexander Schlaefer"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel method for markerless 3D tissue tracking in endoscopic stereo images using Tracking Any Point networks. The method shows promising results for accurate tracking in surgical scenarios.",
        "tldr_zh": "该论文提出了一种在内窥镜立体图像中使用跟踪任意点网络进行无标记3D组织跟踪的新方法。该方法在手术场景中展现出了良好的跟踪结果。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP",
        "summary": "Pre-trained Vision-Language Models (VLMs) face a significant adaptation gap\nwhen applied to Zero-Shot Anomaly Detection (ZSAD), stemming from their lack of\nlocal inductive biases for dense prediction and their reliance on inflexible\nfeature fusion paradigms. We address these limitations through an Architectural\nCo-Design framework that jointly refines feature representation and cross-modal\nfusion. Our method integrates a parameter-efficient Convolutional Low-Rank\nAdaptation (Conv-LoRA) adapter to inject local inductive biases for\nfine-grained representation, and introduces a Dynamic Fusion Gateway (DFG) that\nleverages visual context to adaptively modulate text prompts, enabling a\npowerful bidirectional fusion. Extensive experiments on diverse industrial and\nmedical benchmarks demonstrate superior accuracy and robustness, validating\nthat this synergistic co-design is critical for robustly adapting foundation\nmodels to dense perception tasks.",
        "url": "http://arxiv.org/abs/2508.07819v1",
        "published_date": "2025-08-11T10:03:45+00:00",
        "updated_date": "2025-08-11T10:03:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Ke Ma",
            "Jun Long",
            "Hongxiao Fei",
            "Liujie Hua",
            "Yueyi Luo"
        ],
        "ai_categories": [
            "Transformer",
            "LoRA"
        ],
        "tldr": "The paper introduces an Architectural Co-Design framework for Zero-Shot Anomaly Detection by refining feature representation and cross-modal fusion through Conv-LoRA and DFG modules, showing superior results in industrial and medical benchmarks.",
        "tldr_zh": "本文介绍了一种建筑合作设计框架，通过Conv-LoRA和DFG模块对特征表示和跨模态融合进行细化，显示在工业和医学基准测试中的优越结果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "Semi-supervised Multiscale Matching for SAR-Optical Image",
        "summary": "Driven by the complementary nature of optical and synthetic aperture radar\n(SAR) images, SAR-optical image matching has garnered significant interest.\nMost existing SAR-optical image matching methods aim to capture effective\nmatching features by employing the supervision of pixel-level matched\ncorrespondences within SAR-optical image pairs, which, however, suffers from\ntime-consuming and complex manual annotation, making it difficult to collect\nsufficient labeled SAR-optical image pairs. To handle this, we design a\nsemi-supervised SAR-optical image matching pipeline that leverages both scarce\nlabeled and abundant unlabeled image pairs and propose a semi-supervised\nmultiscale matching for SAR-optical image matching (S2M2-SAR). Specifically, we\npseudo-label those unlabeled SAR-optical image pairs with pseudo ground-truth\nsimilarity heatmaps by combining both deep and shallow level matching results,\nand train the matching model by employing labeled and pseudo-labeled similarity\nheatmaps. In addition, we introduce a cross-modal feature enhancement module\ntrained using a cross-modality mutual independence loss, which requires no\nground-truth labels. This unsupervised objective promotes the separation of\nmodality-shared and modality-specific features by encouraging statistical\nindependence between them, enabling effective feature disentanglement across\noptical and SAR modalities. To evaluate the effectiveness of S2M2-SAR, we\ncompare it with existing competitors on benchmark datasets. Experimental\nresults demonstrate that S2M2-SAR not only surpasses existing semi-supervised\nmethods but also achieves performance competitive with fully supervised SOTA\nmethods, demonstrating its efficiency and practical potential.",
        "url": "http://arxiv.org/abs/2508.07812v1",
        "published_date": "2025-08-11T09:55:39+00:00",
        "updated_date": "2025-08-11T09:55:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingze Gai",
            "Changchun Li"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes a semi-supervised approach for matching SAR-optical images, achieving competitive performance with fully supervised methods.",
        "tldr_zh": "本文提出了一种半监督的方法，用于匹配SAR-光学图像，在实验结果中表现出与全监督方法竞争力。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Power Battery Detection",
        "summary": "Power batteries are essential components in electric vehicles, where internal\nstructural defects can pose serious safety risks. We conduct a comprehensive\nstudy on a new task, power battery detection (PBD), which aims to localize the\ndense endpoints of cathode and anode plates from industrial X-ray images for\nquality inspection. Manual inspection is inefficient and error-prone, while\ntraditional vision algorithms struggle with densely packed plates, low\ncontrast, scale variation, and imaging artifacts. To address this issue and\ndrive more attention into this meaningful task, we present PBD5K, the first\nlarge-scale benchmark for this task, consisting of 5,000 X-ray images from nine\nbattery types with fine-grained annotations and eight types of real-world\nvisual interference. To support scalable and consistent labeling, we develop an\nintelligent annotation pipeline that combines image filtering, model-assisted\npre-labeling, cross-verification, and layered quality evaluation. We formulate\nPBD as a point-level segmentation problem and propose MDCNeXt, a model designed\nto extract and integrate multi-dimensional structure clues including point,\nline, and count information from the plate itself. To improve discrimination\nbetween plates and suppress visual interference, MDCNeXt incorporates two state\nspace modules. The first is a prompt-filtered module that learns contrastive\nrelationships guided by task-specific prompts. The second is a density-aware\nreordering module that refines segmentation in regions with high plate density.\nIn addition, we propose a distance-adaptive mask generation strategy to provide\nrobust supervision under varying spatial distributions of anode and cathode\npositions. The source code and datasets will be publicly available at\n\\href{https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBD}{PBD5K}.",
        "url": "http://arxiv.org/abs/2508.07797v1",
        "published_date": "2025-08-11T09:35:25+00:00",
        "updated_date": "2025-08-11T09:35:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoqi Zhao",
            "Peiqian Cao",
            "Lihe Zhang",
            "Zonglei Feng",
            "Hanqi Liu",
            "Jiaming Zuo",
            "Youwei Pang",
            "Weisi Lin",
            "Georges El Fakhri",
            "Huchuan Lu",
            "Xiaofeng Liu"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a new task called power battery detection (PBD) to locate cathode and anode plates in electric vehicle batteries using X-ray images. A benchmark dataset, intelligent annotation pipeline, and a model called MDCNeXt are proposed to address challenges in plate localization.",
        "tldr_zh": "本文介绍一项名为电池电量检测（PBD）的新任务，旨在使用X射线图像定位电动汽车电池中的阴极和阳极板。提出了基准数据集、智能标注流程和名为MDCNeXt的模型，以解决板块定位中的挑战。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 7.5
    },
    {
        "title": "Boosting Active Defense Persistence: A Two-Stage Defense Framework Combining Interruption and Poisoning Against Deepfake",
        "summary": "Active defense strategies have been developed to counter the threat of\ndeepfake technology. However, a primary challenge is their lack of persistence,\nas their effectiveness is often short-lived. Attackers can bypass these\ndefenses by simply collecting protected samples and retraining their models.\nThis means that static defenses inevitably fail when attackers retrain their\nmodels, which severely limits practical use. We argue that an effective defense\nnot only distorts forged content but also blocks the model's ability to adapt,\nwhich occurs when attackers retrain their models on protected images. To\nachieve this, we propose an innovative Two-Stage Defense Framework (TSDF).\nBenefiting from the intensity separation mechanism designed in this paper, the\nframework uses dual-function adversarial perturbations to perform two roles.\nFirst, it can directly distort the forged results. Second, it acts as a\npoisoning vehicle that disrupts the data preparation process essential for an\nattacker's retraining pipeline. By poisoning the data source, TSDF aims to\nprevent the attacker's model from adapting to the defensive perturbations, thus\nensuring the defense remains effective long-term. Comprehensive experiments\nshow that the performance of traditional interruption methods degrades sharply\nwhen it is subjected to adversarial retraining. However, our framework shows a\nstrong dual defense capability, which can improve the persistence of active\ndefense. Our code will be available at https://github.com/vpsg-research/TSDF.",
        "url": "http://arxiv.org/abs/2508.07795v1",
        "published_date": "2025-08-11T09:26:48+00:00",
        "updated_date": "2025-08-11T09:26:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hongrui Zheng",
            "Yuezun Li",
            "Liejun Wang",
            "Yunfeng Diao",
            "Zhiqing Guo"
        ],
        "ai_categories": [
            "GAN",
            "Transformer"
        ],
        "tldr": "The paper introduces a Two-Stage Defense Framework to enhance active defense against deepfake attacks by disrupting attackers' model retraining process.",
        "tldr_zh": "本文引入了一个两阶段防御框架，通过干扰攻击者的模型重新训练过程来增强对deepfake攻击的主动防御。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7.5
    },
    {
        "title": "Correspondence as Video: Test-Time Adaption on SAM2 for Reference Segmentation in the Wild",
        "summary": "Large vision models like the Segment Anything Model (SAM) exhibit significant\nlimitations when applied to downstream tasks in the wild. Consequently,\nreference segmentation, which leverages reference images and their\ncorresponding masks to impart novel knowledge to the model, emerges as a\npromising new direction for adapting vision models. However, existing reference\nsegmentation approaches predominantly rely on meta-learning, which still\nnecessitates an extensive meta-training process and brings massive data and\ncomputational cost. In this study, we propose a novel approach by representing\nthe inherent correspondence between reference-target image pairs as a pseudo\nvideo. This perspective allows the latest version of SAM, known as SAM2, which\nis equipped with interactive video object segmentation (iVOS) capabilities, to\nbe adapted to downstream tasks in a lightweight manner. We term this approach\nCorrespondence As Video for SAM (CAV-SAM). CAV-SAM comprises two key modules:\nthe Diffusion-Based Semantic Transition (DBST) module employs a diffusion model\nto construct a semantic transformation sequence, while the Test-Time Geometric\nAlignment (TTGA) module aligns the geometric changes within this sequence\nthrough test-time fine-tuning. We evaluated CAVSAM on widely-used datasets,\nachieving segmentation performance improvements exceeding 5% over SOTA methods.\nImplementation is provided in the supplementary materials.",
        "url": "http://arxiv.org/abs/2508.07759v1",
        "published_date": "2025-08-11T08:42:49+00:00",
        "updated_date": "2025-08-11T08:42:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoran Wang",
            "Zekun Li",
            "Jian Zhang",
            "Lei Qi",
            "Yinghuan Shi"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel approach called CAV-SAM for adapting vision models to downstream tasks more efficiently using reference segmentation as video.",
        "tldr_zh": "本文提出了一种名为CAV-SAM的新方法，通过将参考分割作为视频，更有效地将视觉模型适应到下游任务中。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Voice Pathology Detection Using Phonation",
        "summary": "Voice disorders significantly affect communication and quality of life,\nrequiring an early and accurate diagnosis. Traditional methods like\nlaryngoscopy are invasive, subjective, and often inaccessible. This research\nproposes a noninvasive, machine learning-based framework for detecting voice\npathologies using phonation data.\n  Phonation data from the Saarbr\\\"ucken Voice Database are analyzed using\nacoustic features such as Mel Frequency Cepstral Coefficients (MFCCs), chroma\nfeatures, and Mel spectrograms. Recurrent Neural Networks (RNNs), including\nLSTM and attention mechanisms, classify samples into normal and pathological\ncategories. Data augmentation techniques, including pitch shifting and Gaussian\nnoise addition, enhance model generalizability, while preprocessing ensures\nsignal quality. Scale-based features, such as H\\\"older and Hurst exponents,\nfurther capture signal irregularities and long-term dependencies.\n  The proposed framework offers a noninvasive, automated diagnostic tool for\nearly detection of voice pathologies, supporting AI-driven healthcare, and\nimproving patient outcomes.",
        "url": "http://arxiv.org/abs/2508.07587v1",
        "published_date": "2025-08-11T03:33:18+00:00",
        "updated_date": "2025-08-11T03:33:18+00:00",
        "categories": [
            "cs.CV",
            "cs.SD"
        ],
        "authors": [
            "Sri Raksha Siva",
            "Nived Suthahar",
            "Prakash Boominathan",
            "Uma Ranjan"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Transformer"
        ],
        "tldr": "This paper proposes a noninvasive machine learning framework to detect voice pathologies using phonation data, enhancing early diagnosis and patient outcomes.",
        "tldr_zh": "本文提出了一种利用语音数据检测声音病变的非侵入式机器学习框架，增强早期诊断和患者结局。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language Models",
        "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot generalization but\nsuffer performance degradation under distribution shifts in downstream tasks,\nparticularly in the absence of labeled data. Test-Time Adaptation (TTA)\naddresses this challenge by enabling online optimization of VLMs during\ninference, eliminating the need for annotated data. Cache-based TTA methods\nexploit historical knowledge by maintaining a dynamic memory cache of\nlow-entropy or high-confidence samples, promoting efficient adaptation to\nout-of-distribution data. Nevertheless, these methods face two critical\nchallenges: (1) unreliable confidence metrics under significant distribution\nshifts, resulting in error accumulation within the cache and degraded\nadaptation performance; and (2) rigid decision boundaries that fail to\naccommodate substantial distributional variations, leading to suboptimal\npredictions. To overcome these limitations, we introduce the Adaptive Cache\nEnhancement (ACE) framework, which constructs a robust cache by selectively\nstoring high-confidence or low-entropy image embeddings per class, guided by\ndynamic, class-specific thresholds initialized from zero-shot statistics and\niteratively refined using an exponential moving average and\nexploration-augmented updates. This approach enables adaptive, class-wise\ndecision boundaries, ensuring robust and accurate predictions across diverse\nvisual distributions. Extensive experiments on 15 diverse benchmark datasets\ndemonstrate that ACE achieves state-of-the-art performance, delivering superior\nrobustness and generalization compared to existing TTA methods in challenging\nout-of-distribution scenarios.",
        "url": "http://arxiv.org/abs/2508.07570v1",
        "published_date": "2025-08-11T03:03:34+00:00",
        "updated_date": "2025-08-11T03:03:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Khanh-Binh Nguyen",
            "Phuoc-Nguyen Bui",
            "Hyunseung Choo",
            "Duc Thanh Nguyen"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces the Adaptive Cache Enhancement (ACE) framework to improve Test-Time Adaptation of Vision-Language Models by constructing a robust cache for adaptive decision-making, achieving state-of-the-art performance in out-of-distribution scenarios.",
        "tldr_zh": "本文引入了自适应缓存增强（ACE）框架，通过构建强大的缓存进行自适应决策，实现了处理分布偏移场景下的最先进性能。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "PP-Motion: Physical-Perceptual Fidelity Evaluation for Human Motion Generation",
        "summary": "Human motion generation has found widespread applications in AR/VR, film,\nsports, and medical rehabilitation, offering a cost-effective alternative to\ntraditional motion capture systems. However, evaluating the fidelity of such\ngenerated motions is a crucial, multifaceted task. Although previous approaches\nhave attempted at motion fidelity evaluation using human perception or physical\nconstraints, there remains an inherent gap between human-perceived fidelity and\nphysical feasibility. Moreover, the subjective and coarse binary labeling of\nhuman perception further undermines the development of a robust data-driven\nmetric. We address these issues by introducing a physical labeling method. This\nmethod evaluates motion fidelity by calculating the minimum modifications\nneeded for a motion to align with physical laws. With this approach, we are\nable to produce fine-grained, continuous physical alignment annotations that\nserve as objective ground truth. With these annotations, we propose PP-Motion,\na novel data-driven metric to evaluate both physical and perceptual fidelity of\nhuman motion. To effectively capture underlying physical priors, we employ\nPearson's correlation loss for the training of our metric. Additionally, by\nincorporating a human-based perceptual fidelity loss, our metric can capture\nfidelity that simultaneously considers both human perception and physical\nalignment. Experimental results demonstrate that our metric, PP-Motion, not\nonly aligns with physical laws but also aligns better with human perception of\nmotion fidelity than previous work.",
        "url": "http://arxiv.org/abs/2508.08179v1",
        "published_date": "2025-08-11T16:59:15+00:00",
        "updated_date": "2025-08-11T16:59:15+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Sihan Zhao",
            "Zixuan Wang",
            "Tianyu Luan",
            "Jia Jia",
            "Wentao Zhu",
            "Jiebo Luo",
            "Junsong Yuan",
            "Nan Xi"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel method, PP-Motion, for evaluating both physical and perceptual fidelity of human motion generation using physical alignment annotations and Pearson's correlation loss.",
        "tldr_zh": "该论文介绍了一种新方法PP-Motion，用于评估人体动作生成的物理和感知逼真度，使用物理对齐标注和Pearson相关损失。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "CBDES MoE: Hierarchically Decoupled Mixture-of-Experts for Functional Modules in Autonomous Driving",
        "summary": "Bird's Eye View (BEV) perception systems based on multi-sensor feature fusion\nhave become a fundamental cornerstone for end-to-end autonomous driving.\nHowever, existing multi-modal BEV methods commonly suffer from limited input\nadaptability, constrained modeling capacity, and suboptimal generalization. To\naddress these challenges, we propose a hierarchically decoupled\nMixture-of-Experts architecture at the functional module level, termed\nComputing Brain DEvelopment System Mixture-of-Experts (CBDES MoE). CBDES MoE\nintegrates multiple structurally heterogeneous expert networks with a\nlightweight Self-Attention Router (SAR) gating mechanism, enabling dynamic\nexpert path selection and sparse, input-aware efficient inference. To the best\nof our knowledge, this is the first modular Mixture-of-Experts framework\nconstructed at the functional module granularity within the autonomous driving\ndomain. Extensive evaluations on the real-world nuScenes dataset demonstrate\nthat CBDES MoE consistently outperforms fixed single-expert baselines in 3D\nobject detection. Compared to the strongest single-expert model, CBDES MoE\nachieves a 1.6-point increase in mAP and a 4.1-point improvement in NDS,\ndemonstrating the effectiveness and practical advantages of the proposed\napproach.",
        "url": "http://arxiv.org/abs/2508.07838v1",
        "published_date": "2025-08-11T10:44:25+00:00",
        "updated_date": "2025-08-11T10:44:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qi Xiang",
            "Kunsong Shi",
            "Zhigui Lin",
            "Lei He"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "Proposes a hierarchically decoupled Mixture-of-Experts architecture for autonomous driving, outperforming single-expert baselines in 3D object detection.",
        "tldr_zh": "提出了一种适用于自动驾驶的分层解耦的专家混合体系结构，在3D物体检测方面优于单一专家基线。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Anatomy-Aware Low-Dose CT Denoising via Pretrained Vision Models and Semantic-Guided Contrastive Learning",
        "summary": "To reduce radiation exposure and improve the diagnostic efficacy of low-dose\ncomputed tomography (LDCT), numerous deep learning-based denoising methods have\nbeen developed to mitigate noise and artifacts. However, most of these\napproaches ignore the anatomical semantics of human tissues, which may\npotentially result in suboptimal denoising outcomes. To address this problem,\nwe propose ALDEN, an anatomy-aware LDCT denoising method that integrates\nsemantic features of pretrained vision models (PVMs) with adversarial and\ncontrastive learning. Specifically, we introduce an anatomy-aware discriminator\nthat dynamically fuses hierarchical semantic features from reference\nnormal-dose CT (NDCT) via cross-attention mechanisms, enabling tissue-specific\nrealism evaluation in the discriminator. In addition, we propose a\nsemantic-guided contrastive learning module that enforces anatomical\nconsistency by contrasting PVM-derived features from LDCT, denoised CT and\nNDCT, preserving tissue-specific patterns through positive pairs and\nsuppressing artifacts via dual negative pairs. Extensive experiments conducted\non two LDCT denoising datasets reveal that ALDEN achieves the state-of-the-art\nperformance, offering superior anatomy preservation and substantially reducing\nover-smoothing issue of previous work. Further validation on a downstream\nmulti-organ segmentation task (encompassing 117 anatomical structures) affirms\nthe model's ability to maintain anatomical awareness.",
        "url": "http://arxiv.org/abs/2508.07788v1",
        "published_date": "2025-08-11T09:17:12+00:00",
        "updated_date": "2025-08-11T09:17:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Runze Wang",
            "Zeli Chen",
            "Zhiyun Song",
            "Wei Fang",
            "Jiajin Zhang",
            "Danyang Tu",
            "Yuxing Tang",
            "Minfeng Xu",
            "Xianghua Ye",
            "Le Lu",
            "Dakai Jin"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes an anatomy-aware low-dose CT denoising method using pretrained vision models and semantic-guided contrastive learning, achieving state-of-the-art performance in preserving anatomy and reducing over-smoothing.",
        "tldr_zh": "该论文提出了一种利用预训练视觉模型和语义引导对比学习的解剖感知低剂量CT去噪方法，在保持解剖结构和减少过度平滑方面取得了最先进的性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Forecasting Continuous Non-Conservative Dynamical Systems in SO(3)",
        "summary": "Modeling the rotation of moving objects is a fundamental task in computer\nvision, yet $SO(3)$ extrapolation still presents numerous challenges: (1)\nunknown quantities such as the moment of inertia complicate dynamics, (2) the\npresence of external forces and torques can lead to non-conservative\nkinematics, and (3) estimating evolving state trajectories under sparse, noisy\nobservations requires robustness. We propose modeling trajectories of noisy\npose estimates on the manifold of 3D rotations in a physically and\ngeometrically meaningful way by leveraging Neural Controlled Differential\nEquations guided with $SO(3)$ Savitzky-Golay paths. Existing extrapolation\nmethods often rely on energy conservation or constant velocity assumptions,\nlimiting their applicability in real-world scenarios involving non-conservative\nforces. In contrast, our approach is agnostic to energy and momentum\nconservation while being robust to input noise, making it applicable to\ncomplex, non-inertial systems. Our approach is easily integrated as a module in\nexisting pipelines and generalizes well to trajectories with unknown physical\nparameters. By learning to approximate object dynamics from noisy states during\ntraining, our model attains robust extrapolation capabilities in simulation and\nvarious real-world settings. Code is available at\nhttps://github.com/bastianlb/forecasting-rotational-dynamics",
        "url": "http://arxiv.org/abs/2508.07775v1",
        "published_date": "2025-08-11T09:03:10+00:00",
        "updated_date": "2025-08-11T09:03:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lennart Bastian",
            "Mohammad Rashed",
            "Nassir Navab",
            "Tolga Birdal"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes a method for modeling noisy pose estimates on the manifold of 3D rotations using Neural Controlled Differential Equations and SO(3) Savitzky-Golay paths.",
        "tldr_zh": "本文提出了一种利用神经控制微分方程和SO(3) Savitzky-Golay路径对3D旋转的噪声姿态估计进行建模的方法。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "PCA-Guided Autoencoding for Structured Dimensionality Reduction in Active Infrared Thermography",
        "summary": "Active Infrared thermography (AIRT) is a widely adopted non-destructive\ntesting (NDT) technique for detecting subsurface anomalies in industrial\ncomponents. Due to the high dimensionality of AIRT data, current approaches\nemploy non-linear autoencoders (AEs) for dimensionality reduction. However, the\nlatent space learned by AIRT AEs lacks structure, limiting their effectiveness\nin downstream defect characterization tasks. To address this limitation, this\npaper proposes a principal component analysis guided (PCA-guided) autoencoding\nframework for structured dimensionality reduction to capture intricate,\nnon-linear features in thermographic signals while enforcing a structured\nlatent space. A novel loss function, PCA distillation loss, is introduced to\nguide AIRT AEs to align the latent representation with structured PCA\ncomponents while capturing the intricate, non-linear patterns in thermographic\nsignals. To evaluate the utility of the learned, structured latent space, we\npropose a neural network-based evaluation metric that assesses its suitability\nfor defect characterization. Experimental results show that the proposed\nPCA-guided AE outperforms state-of-the-art dimensionality reduction methods on\nPVC, CFRP, and PLA samples in terms of contrast, signal-to-noise ratio (SNR),\nand neural network-based metrics.",
        "url": "http://arxiv.org/abs/2508.07773v1",
        "published_date": "2025-08-11T08:58:13+00:00",
        "updated_date": "2025-08-11T08:58:13+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Mohammed Salah",
            "Numan Saeed",
            "Davor Svetinovic",
            "Stefano Sfarra",
            "Mohammed Omar",
            "Yusra Abdulrahman"
        ],
        "ai_categories": [
            "Other"
        ],
        "tldr": "The paper introduces a PCA-guided autoencoding framework for structured dimensionality reduction in Active Infrared Thermography, outperforming state-of-the-art methods in defect characterization tasks.",
        "tldr_zh": "本文介绍了一种PCA引导的自动编码框架，用于结构化维度缩减在主动红外热成像中，优于现有方法在缺陷表征任务中。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "DoorDet: Semi-Automated Multi-Class Door Detection Dataset via Object Detection and Large Language Models",
        "summary": "Accurate detection and classification of diverse door types in floor plans\ndrawings is critical for multiple applications, such as building compliance\nchecking, and indoor scene understanding. Despite their importance, publicly\navailable datasets specifically designed for fine-grained multi-class door\ndetection remain scarce. In this work, we present a semi-automated pipeline\nthat leverages a state-of-the-art object detector and a large language model\n(LLM) to construct a multi-class door detection dataset with minimal manual\neffort. Doors are first detected as a unified category using a deep object\ndetection model. Next, an LLM classifies each detected instance based on its\nvisual and contextual features. Finally, a human-in-the-loop stage ensures\nhigh-quality labels and bounding boxes. Our method significantly reduces\nannotation cost while producing a dataset suitable for benchmarking neural\nmodels in floor plan analysis. This work demonstrates the potential of\ncombining deep learning and multimodal reasoning for efficient dataset\nconstruction in complex real-world domains.",
        "url": "http://arxiv.org/abs/2508.07714v1",
        "published_date": "2025-08-11T07:41:09+00:00",
        "updated_date": "2025-08-11T07:41:09+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.ET"
        ],
        "authors": [
            "Licheng Zhang",
            "Bach Le",
            "Naveed Akhtar",
            "Tuan Ngo"
        ],
        "ai_categories": [
            "Dataset",
            "Multimodality",
            "Transformer"
        ],
        "tldr": "The paper presents a method to create a door detection dataset using deep learning models and language models with minimal manual effort, useful for building compliance checking and indoor scene understanding.",
        "tldr_zh": "该论文提出了一种利用深度学习模型和语言模型来创建门检测数据集的方法，可用于建筑合规检查和室内场景理解。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7.25
    },
    {
        "title": "Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents",
        "summary": "Vision-and-Language Navigation (VLN) poses significant challenges in enabling\nagents to interpret natural language instructions and navigate complex 3D\nenvironments. While recent progress has been driven by large-scale pre-training\nand data augmentation, current methods still struggle to generalize to unseen\nscenarios, particularly when complex spatial and temporal reasoning is\nrequired. In this work, we propose SkillNav, a modular framework that\nintroduces structured, skill-based reasoning into Transformer-based VLN agents.\nOur method decomposes navigation into a set of interpretable atomic skills\n(e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each\nhandled by a specialized agent. We then introduce a novel zero-shot\nVision-Language Model (VLM)-based router, which dynamically selects the most\nsuitable agent at each time step by aligning sub-goals with visual observations\nand historical actions. SkillNav achieves a new state-of-the-art performance on\nthe R2R benchmark and demonstrates strong generalization to the GSA-R2R\nbenchmark that includes novel instruction styles and unseen environments.",
        "url": "http://arxiv.org/abs/2508.07642v1",
        "published_date": "2025-08-11T05:50:30+00:00",
        "updated_date": "2025-08-11T05:50:30+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Tianyi Ma",
            "Yue Zhang",
            "Zehao Wang",
            "Parisa Kordjamshidi"
        ],
        "ai_categories": [
            "Transformer",
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces SkillNav, a framework that breaks down navigation tasks into interpretable atomic skills handled by specialized agents, achieving state-of-the-art performance on benchmarks.",
        "tldr_zh": "该论文介绍了SkillNav，一个将导航任务分解为可解释原子技能的框架，由专门的代理处理，在基准测试中取得了最先进的性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "AD-AVSR: Asymmetric Dual-stream Enhancement for Robust Audio-Visual Speech Recognition",
        "summary": "Audio-visual speech recognition (AVSR) combines audio-visual modalities to\nimprove speech recognition, especially in noisy environments. However, most\nexisting methods deploy the unidirectional enhancement or symmetric fusion\nmanner, which limits their capability to capture heterogeneous and\ncomplementary correlations of audio-visual data-especially under asymmetric\ninformation conditions. To tackle these gaps, we introduce a new AVSR framework\ntermed AD-AVSR based on bidirectional modality enhancement. Specifically, we\nfirst introduce the audio dual-stream encoding strategy to enrich audio\nrepresentations from multiple perspectives and intentionally establish\nasymmetry to support subsequent cross-modal interactions. The enhancement\nprocess involves two key components, Audio-aware Visual Refinement Module for\nenhanced visual representations under audio guidance, and Cross-modal Noise\nSuppression Masking Module which refines audio representations using visual\ncues, collaboratively leading to the closed-loop and bidirectional information\nflow. To further enhance correlation robustness, we adopt a threshold-based\nselection mechanism to filter out irrelevant or weakly correlated audio-visual\npairs. Extensive experimental results on the LRS2 and LRS3 datasets indicate\nthat our AD-AVSR consistently surpasses SOTA methods in both performance and\nnoise robustness, highlighting the effectiveness of our model design.",
        "url": "http://arxiv.org/abs/2508.07608v1",
        "published_date": "2025-08-11T04:23:08+00:00",
        "updated_date": "2025-08-11T04:23:08+00:00",
        "categories": [
            "cs.MM",
            "cs.CV",
            "cs.SD"
        ],
        "authors": [
            "Junxiao Xue",
            "Xiaozhen Liu",
            "Xuecheng Wu",
            "Xinyi Yin",
            "Danlei Huang",
            "Fei Yu"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a new framework, AD-AVSR, for audio-visual speech recognition that outperforms existing methods in both performance and noise robustness.",
        "tldr_zh": "这篇论文介绍了一种新的音视频语音识别框架AD-AVSR，在性能和噪声鲁棒性方面优于现有方法。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Decoupled Functional Evaluation of Autonomous Driving Models via Feature Map Quality Scoring",
        "summary": "End-to-end models are emerging as the mainstream in autonomous driving\nperception and planning. However, the lack of explicit supervision signals for\nintermediate functional modules leads to opaque operational mechanisms and\nlimited interpretability, making it challenging for traditional methods to\nindependently evaluate and train these modules. Pioneering in the issue, this\nstudy builds upon the feature map-truth representation similarity-based\nevaluation framework and proposes an independent evaluation method based on\nFeature Map Convergence Score (FMCS). A Dual-Granularity Dynamic Weighted\nScoring System (DG-DWSS) is constructed, formulating a unified quantitative\nmetric - Feature Map Quality Score - to enable comprehensive evaluation of the\nquality of feature maps generated by functional modules. A CLIP-based Feature\nMap Quality Evaluation Network (CLIP-FMQE-Net) is further developed, combining\nfeature-truth encoders and quality score prediction heads to enable real-time\nquality analysis of feature maps generated by functional modules. Experimental\nresults on the NuScenes dataset demonstrate that integrating our evaluation\nmodule into the training improves 3D object detection performance, achieving a\n3.89 percent gain in NDS. These results verify the effectiveness of our method\nin enhancing feature representation quality and overall model performance.",
        "url": "http://arxiv.org/abs/2508.07552v1",
        "published_date": "2025-08-11T02:24:08+00:00",
        "updated_date": "2025-08-11T02:24:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ludan Zhang",
            "Sihan Wang",
            "Yuqi Dai",
            "Shuofei Qiao",
            "Lei He"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a method for evaluating the quality of feature maps generated by functional modules in autonomous driving models, leading to improved object detection performance.",
        "tldr_zh": "该论文提出了一种评估自动驾驶模型中功能模块生成的特征图质量的方法，从而提高了目标检测性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks",
        "summary": "Language-guided long-horizon mobile manipulation has long been a grand\nchallenge in embodied semantic reasoning, generalizable manipulation, and\nadaptive locomotion. Three fundamental limitations hinder progress: First,\nalthough large language models have improved spatial reasoning and task\nplanning through semantic priors, existing implementations remain confined to\ntabletop scenarios, failing to address the constrained perception and limited\nactuation ranges of mobile platforms. Second, current manipulation strategies\nexhibit insufficient generalization when confronted with the diverse object\nconfigurations encountered in open-world environments. Third, while crucial for\npractical deployment, the dual requirement of maintaining high platform\nmaneuverability alongside precise end-effector control in unstructured settings\nremains understudied.\n  In this work, we present ODYSSEY, a unified mobile manipulation framework for\nagile quadruped robots equipped with manipulators, which seamlessly integrates\nhigh-level task planning with low-level whole-body control. To address the\nchallenge of egocentric perception in language-conditioned tasks, we introduce\na hierarchical planner powered by a vision-language model, enabling\nlong-horizon instruction decomposition and precise action execution. At the\ncontrol level, our novel whole-body policy achieves robust coordination across\nchallenging terrains. We further present the first benchmark for long-horizon\nmobile manipulation, evaluating diverse indoor and outdoor scenarios. Through\nsuccessful sim-to-real transfer, we demonstrate the system's generalization and\nrobustness in real-world deployments, underscoring the practicality of legged\nmanipulators in unstructured environments. Our work advances the feasibility of\ngeneralized robotic assistants capable of complex, dynamic tasks. Our project\npage: https://kaijwang.github.io/odyssey.github.io/",
        "url": "http://arxiv.org/abs/2508.08240v1",
        "published_date": "2025-08-11T17:54:31+00:00",
        "updated_date": "2025-08-11T17:54:31+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Kaijun Wang",
            "Liqin Lu",
            "Mingyu Liu",
            "Jianuo Jiang",
            "Zeju Li",
            "Bolin Zhang",
            "Wancai Zheng",
            "Xinyi Yu",
            "Hao Chen",
            "Chunhua Shen"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "ODYSSEY presents a framework for mobile manipulation in open-world environments with agile quadruped robots, integrating task planning and control, showcasing generalization and robustness through sim-to-real transfer.",
        "tldr_zh": "ODYSSEY提出了一个框架，用于移动机器人在开放环境中的操作，通过敏捷四足机器人整合任务规划和控制，在模拟到真实环境中展示了泛化和稳健性。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "THAT: Token-wise High-frequency Augmentation Transformer for Hyperspectral Pansharpening",
        "summary": "Transformer-based methods have demonstrated strong potential in hyperspectral\npansharpening by modeling long-range dependencies. However, their effectiveness\nis often limited by redundant token representations and a lack of multi-scale\nfeature modeling. Hyperspectral images exhibit intrinsic spectral priors (e.g.,\nabundance sparsity) and spatial priors (e.g., non-local similarity), which are\ncritical for accurate reconstruction. From a spectral-spatial perspective,\nVision Transformers (ViTs) face two major limitations: they struggle to\npreserve high-frequency components--such as material edges and texture\ntransitions--and suffer from attention dispersion across redundant tokens.\nThese issues stem from the global self-attention mechanism, which tends to\ndilute high-frequency signals and overlook localized details. To address these\nchallenges, we propose the Token-wise High-frequency Augmentation Transformer\n(THAT), a novel framework designed to enhance hyperspectral pansharpening\nthrough improved high-frequency feature representation and token selection.\nSpecifically, THAT introduces: (1) Pivotal Token Selective Attention (PTSA) to\nprioritize informative tokens and suppress redundancy; (2) a Multi-level\nVariance-aware Feed-forward Network (MVFN) to enhance high-frequency detail\nlearning. Experiments on standard benchmarks show that THAT achieves\nstate-of-the-art performance with improved reconstruction quality and\nefficiency. The source code is available at https://github.com/kailuo93/THAT.",
        "url": "http://arxiv.org/abs/2508.08183v1",
        "published_date": "2025-08-11T17:03:10+00:00",
        "updated_date": "2025-08-11T17:03:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hongkun Jin",
            "Hongcheng Jiang",
            "Zejun Zhang",
            "Yuan Zhang",
            "Jia Fu",
            "Tingfeng Li",
            "Kai Luo"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a Token-wise High-frequency Augmentation Transformer (THAT) for hyperspectral pansharpening, addressing limitations of existing methods to enhance feature representation and token selection.",
        "tldr_zh": "本文介绍了一种用于高光谱图像融合的基于Token的高频增强Transformer（THAT），以解决现有方法的局限性，增强特征表示和Token选择。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "RedDino: A foundation model for red blood cell analysis",
        "summary": "Red blood cells (RBCs) are essential to human health, and their precise\nmorphological analysis is important for diagnosing hematological disorders.\nDespite the promise of foundation models in medical diagnostics, comprehensive\nAI solutions for RBC analysis remain scarce. We present RedDino, a\nself-supervised foundation model designed for RBC image analysis. RedDino uses\nan RBC-specific adaptation of the DINOv2 self-supervised learning framework and\nis trained on a curated dataset of 1.25 million RBC images from diverse\nacquisition modalities and sources. Extensive evaluations show that RedDino\noutperforms existing state-of-the-art models on RBC shape classification.\nThrough assessments including linear probing and nearest neighbor\nclassification, we confirm its strong feature representations and\ngeneralization ability. Our main contributions are: (1) a foundation model\ntailored for RBC analysis, (2) ablation studies exploring DINOv2 configurations\nfor RBC modeling, and (3) a detailed evaluation of generalization performance.\nRedDino addresses key challenges in computational hematology by capturing\nnuanced morphological features, advancing the development of reliable\ndiagnostic tools. The source code and pretrained models for RedDino are\navailable at https://github.com/Snarci/RedDino, and the pretrained models can\nbe downloaded from our Hugging Face collection at\nhttps://huggingface.co/collections/Snarcy/reddino-689a13e29241d2e5690202fc",
        "url": "http://arxiv.org/abs/2508.08180v1",
        "published_date": "2025-08-11T16:59:31+00:00",
        "updated_date": "2025-08-11T16:59:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Luca Zedda",
            "Andrea Loddo",
            "Cecilia Di Ruberto",
            "Carsten Marr"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "RedDino is a self-supervised foundation model designed for analyzing red blood cells, outperforming existing models in shape classification.",
        "tldr_zh": "RedDino是一个自监督基础模型，专门设计用于分析红细胞，在形状分类方面胜过现有模型。",
        "relevance_score": 1,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based Class-Incremental Learning",
        "summary": "Class-Incremental Learning (CIL) requires a learning system to continually\nlearn new classes without forgetting. Existing pre-trained model-based CIL\nmethods often freeze the pre-trained network and adapt to incremental tasks\nusing additional lightweight modules such as adapters. However, incorrect\nmodule selection during inference hurts performance, and task-specific modules\noften overlook shared general knowledge, leading to errors on distinguishing\nbetween similar classes across tasks. To address the aforementioned challenges,\nwe propose integrating Task-Specific and Universal Adapters (TUNA) in this\npaper. Specifically, we train task-specific adapters to capture the most\ncrucial features relevant to their respective tasks and introduce an\nentropy-based selection mechanism to choose the most suitable adapter.\nFurthermore, we leverage an adapter fusion strategy to construct a universal\nadapter, which encodes the most discriminative features shared across tasks. We\ncombine task-specific and universal adapter predictions to harness both\nspecialized and general knowledge during inference. Extensive experiments on\nvarious benchmark datasets demonstrate the state-of-the-art performance of our\napproach. Code is available at: https://github.com/LAMDA-CL/ICCV2025-TUNA",
        "url": "http://arxiv.org/abs/2508.08165v1",
        "published_date": "2025-08-11T16:41:04+00:00",
        "updated_date": "2025-08-11T16:41:04+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yan Wang",
            "Da-Wei Zhou",
            "Han-Jia Ye"
        ],
        "ai_categories": [
            "AIGC"
        ],
        "tldr": "The paper introduces a method called Task-Specific and Universal Adapters for Class-Incremental Learning, which combines task-specific and universal adapters to improve performance on incremental learning tasks.",
        "tldr_zh": "本文介绍了一种称为任务特定和通用适配器的方法，用于类增量学习，在增量学习任务中结合任务特定和通用适配器以提高性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "3D Plant Root Skeleton Detection and Extraction",
        "summary": "Plant roots typically exhibit a highly complex and dense architecture,\nincorporating numerous slender lateral roots and branches, which significantly\nhinders the precise capture and modeling of the entire root system.\nAdditionally, roots often lack sufficient texture and color information, making\nit difficult to identify and track root traits using visual methods. Previous\nresearch on roots has been largely confined to 2D studies; however, exploring\nthe 3D architecture of roots is crucial in botany. Since roots grow in real 3D\nspace, 3D phenotypic information is more critical for studying genetic traits\nand their impact on root development. We have introduced a 3D root skeleton\nextraction method that efficiently derives the 3D architecture of plant roots\nfrom a few images. This method includes the detection and matching of lateral\nroots, triangulation to extract the skeletal structure of lateral roots, and\nthe integration of lateral and primary roots. We developed a highly complex\nroot dataset and tested our method on it. The extracted 3D root skeletons\nshowed considerable similarity to the ground truth, validating the\neffectiveness of the model. This method can play a significant role in\nautomated breeding robots. Through precise 3D root structure analysis, breeding\nrobots can better identify plant phenotypic traits, especially root structure\nand growth patterns, helping practitioners select seeds with superior root\nsystems. This automated approach not only improves breeding efficiency but also\nreduces manual intervention, making the breeding process more intelligent and\nefficient, thus advancing modern agriculture.",
        "url": "http://arxiv.org/abs/2508.08094v1",
        "published_date": "2025-08-11T15:33:10+00:00",
        "updated_date": "2025-08-11T15:33:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiakai Lin",
            "Jinchang Zhang",
            "Ge Jin",
            "Wenzhan Song",
            "Tianming Liu",
            "Guoyu Lu"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper presents a method for efficiently extracting 3D plant root skeletons from images, with potential applications in automated breeding robots.",
        "tldr_zh": "该论文提出了一种有效从图像中提取3D植物根骨架的方法，具有在自动育种机器人中的潜在应用。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "ME-TST+: Micro-expression Analysis via Temporal State Transition with ROI Relationship Awareness",
        "summary": "Micro-expressions (MEs) are regarded as important indicators of an\nindividual's intrinsic emotions, preferences, and tendencies. ME analysis\nrequires spotting of ME intervals within long video sequences and recognition\nof their corresponding emotional categories. Previous deep learning approaches\ncommonly employ sliding-window classification networks. However, the use of\nfixed window lengths and hard classification presents notable limitations in\npractice. Furthermore, these methods typically treat ME spotting and\nrecognition as two separate tasks, overlooking the essential relationship\nbetween them. To address these challenges, this paper proposes two state space\nmodel-based architectures, namely ME-TST and ME-TST+, which utilize temporal\nstate transition mechanisms to replace conventional window-level classification\nwith video-level regression. This enables a more precise characterization of\nthe temporal dynamics of MEs and supports the modeling of MEs with varying\ndurations. In ME-TST+, we further introduce multi-granularity ROI modeling and\nthe slowfast Mamba framework to alleviate information loss associated with\ntreating ME analysis as a time-series task. Additionally, we propose a synergy\nstrategy for spotting and recognition at both the feature and result levels,\nleveraging their intrinsic connection to enhance overall analysis performance.\nExtensive experiments demonstrate that the proposed methods achieve\nstate-of-the-art performance. The codes are available at\nhttps://github.com/zizheng-guo/ME-TST.",
        "url": "http://arxiv.org/abs/2508.08082v1",
        "published_date": "2025-08-11T15:28:32+00:00",
        "updated_date": "2025-08-11T15:28:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zizheng Guo",
            "Bochao Zou",
            "Junbao Zhuo",
            "Huimin Ma"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes ME-TST and ME-TST+ models for analyzing micro-expressions in videos, using state transition mechanisms and ROI modeling to improve accuracy and performance.",
        "tldr_zh": "本文提出了ME-TST和ME-TST+模型，用于分析视频中的微表情，利用状态转移机制和ROI建模提高准确性和性能。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "IPBA: Imperceptible Perturbation Backdoor Attack in Federated Self-Supervised Learning",
        "summary": "Federated self-supervised learning (FSSL) combines the advantages of\ndecentralized modeling and unlabeled representation learning, serving as a\ncutting-edge paradigm with strong potential for scalability and privacy\npreservation. Although FSSL has garnered increasing attention, research\nindicates that it remains vulnerable to backdoor attacks. Existing methods\ngenerally rely on visually obvious triggers, which makes it difficult to meet\nthe requirements for stealth and practicality in real-world deployment. In this\npaper, we propose an imperceptible and effective backdoor attack method against\nFSSL, called IPBA. Our empirical study reveals that existing imperceptible\ntriggers face a series of challenges in FSSL, particularly limited\ntransferability, feature entanglement with augmented samples, and\nout-of-distribution properties. These issues collectively undermine the\neffectiveness and stealthiness of traditional backdoor attacks in FSSL. To\novercome these challenges, IPBA decouples the feature distributions of backdoor\nand augmented samples, and introduces Sliced-Wasserstein distance to mitigate\nthe out-of-distribution properties of backdoor samples, thereby optimizing the\ntrigger generation process. Our experimental results on several FSSL scenarios\nand datasets show that IPBA significantly outperforms existing backdoor attack\nmethods in performance and exhibits strong robustness under various defense\nmechanisms.",
        "url": "http://arxiv.org/abs/2508.08031v1",
        "published_date": "2025-08-11T14:36:11+00:00",
        "updated_date": "2025-08-11T14:36:11+00:00",
        "categories": [
            "cs.CR",
            "cs.CV"
        ],
        "authors": [
            "Jiayao Wang",
            "Yang Song",
            "Zhendong Zhao",
            "Jiale Zhang",
            "Qilin Wu",
            "Junwu Zhu",
            "Dongfang Zhao"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a new imperceptible backdoor attack method, IPBA, to target vulnerabilities in Federated Self-Supervised Learning (FSSL), outperforming existing methods in performance and robustness under defense mechanisms.",
        "tldr_zh": "本文提出一种新的不可察觉的后门攻击方法IPBA，针对联邦自监督学习（FSSL）中的漏洞，表现出在性能和抵御机制方面优于现有方法。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning",
        "summary": "This paper introduces an advanced AI-driven perception system for autonomous\nquadcopter navigation in GPS-denied indoor environments. The proposed framework\nleverages cloud computing to offload computationally intensive tasks and\nincorporates a custom-designed printed circuit board (PCB) for efficient sensor\ndata acquisition, enabling robust navigation in confined spaces. The system\nintegrates YOLOv11 for object detection, Depth Anything V2 for monocular depth\nestimation, a PCB equipped with Time-of-Flight (ToF) sensors and an Inertial\nMeasurement Unit (IMU), and a cloud-based Large Language Model (LLM) for\ncontext-aware decision-making. A virtual safety envelope, enforced by\ncalibrated sensor offsets, ensures collision avoidance, while a multithreaded\narchitecture achieves low-latency processing. Enhanced spatial awareness is\nfacilitated by 3D bounding box estimation with Kalman filtering. Experimental\nresults in an indoor testbed demonstrate strong performance, with object\ndetection achieving a mean Average Precision (mAP50) of 0.6, depth estimation\nMean Absolute Error (MAE) of 7.2 cm, only 16 safety envelope breaches across 42\ntrials over approximately 11 minutes, and end-to-end system latency below 1\nsecond. This cloud-supported, high-intelligence framework serves as an\nauxiliary perception and navigation system, complementing state-of-the-art\ndrone autonomy for GPS-denied confined spaces.",
        "url": "http://arxiv.org/abs/2508.07885v1",
        "published_date": "2025-08-11T12:00:03+00:00",
        "updated_date": "2025-08-11T12:00:03+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.SY",
            "eess.SY"
        ],
        "authors": [
            "Shoaib Ahmmad",
            "Zubayer Ahmed Aditto",
            "Md Mehrab Hossain",
            "Noushin Yeasmin",
            "Shorower Hossain"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "Paper presents an AI-driven system for quadcopter navigation in indoor environments using cloud computing and custom sensors. Achieved strong performance in object detection and depth estimation with low-latency processing.",
        "tldr_zh": "本文介绍了一种利用云计算和定制传感器在室内环境中进行四轴飞行器导航的人工智能驱动系统。在目标检测和深度估计方面表现出色，处理延迟低。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Towards Human-AI Collaboration System for the Detection of Invasive Ductal Carcinoma in Histopathology Images",
        "summary": "Invasive ductal carcinoma (IDC) is the most prevalent form of breast cancer,\nand early, accurate diagnosis is critical to improving patient survival rates\nby guiding treatment decisions. Combining medical expertise with artificial\nintelligence (AI) holds significant promise for enhancing the precision and\nefficiency of IDC detection. In this work, we propose a human-in-the-loop\n(HITL) deep learning system designed to detect IDC in histopathology images.\nThe system begins with an initial diagnosis provided by a high-performance\nEfficientNetV2S model, offering feedback from AI to the human expert. Medical\nprofessionals then review the AI-generated results, correct any misclassified\nimages, and integrate the revised labels into the training dataset, forming a\nfeedback loop from the human back to the AI. This iterative process refines the\nmodel's performance over time. The EfficientNetV2S model itself achieves\nstate-of-the-art performance compared to existing methods in the literature,\nwith an overall accuracy of 93.65\\%. Incorporating the human-in-the-loop system\nfurther improves the model's accuracy using four experimental groups with\nmisclassified images. These results demonstrate the potential of this\ncollaborative approach to enhance AI performance in diagnostic systems. This\nwork contributes to advancing automated, efficient, and highly accurate methods\nfor IDC detection through human-AI collaboration, offering a promising\ndirection for future AI-assisted medical diagnostics.",
        "url": "http://arxiv.org/abs/2508.07875v1",
        "published_date": "2025-08-11T11:45:57+00:00",
        "updated_date": "2025-08-11T11:45:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.HC",
            "cs.LG"
        ],
        "authors": [
            "Shuo Han",
            "Ahmed Karam Eldaly",
            "Solomon Sunday Oyelere"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces a human-AI collaboration system for detecting invasive ductal carcinoma in histopathology images, showing promising results in improving diagnostic accuracy.",
        "tldr_zh": "本论文介绍了一种用于检测浸润性导管癌的人工智能协作系统，在提高诊断准确性方面显示出了良好的效果。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Collaborative Learning of Scattering and Deep Features for SAR Target Recognition with Noisy Labels",
        "summary": "The acquisition of high-quality labeled synthetic aperture radar (SAR) data\nis challenging due to the demanding requirement for expert knowledge.\nConsequently, the presence of unreliable noisy labels is unavoidable, which\nresults in performance degradation of SAR automatic target recognition (ATR).\nExisting research on learning with noisy labels mainly focuses on image data.\nHowever, the non-intuitive visual characteristics of SAR data are insufficient\nto achieve noise-robust learning. To address this problem, we propose\ncollaborative learning of scattering and deep features (CLSDF) for SAR ATR with\nnoisy labels. Specifically, a multi-model feature fusion framework is designed\nto integrate scattering and deep features. The attributed scattering centers\n(ASCs) are treated as dynamic graph structure data, and the extracted physical\ncharacteristics effectively enrich the representation of deep image features.\nThen, the samples with clean and noisy labels are divided by modeling the loss\ndistribution with multiple class-wise Gaussian Mixture Models (GMMs).\nAfterward, the semi-supervised learning of two divergent branches is conducted\nbased on the data divided by each other. Moreover, a joint distribution\nalignment strategy is introduced to enhance the reliability of co-guessed\nlabels. Extensive experiments have been done on the Moving and Stationary\nTarget Acquisition and Recognition (MSTAR) dataset, and the results show that\nthe proposed method can achieve state-of-the-art performance under different\noperating conditions with various label noises.",
        "url": "http://arxiv.org/abs/2508.07656v1",
        "published_date": "2025-08-11T06:10:23+00:00",
        "updated_date": "2025-08-11T06:10:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yimin Fu",
            "Zhunga Liu",
            "Dongxiu Guo",
            "Longfei Wang"
        ],
        "ai_categories": [
            "Dataset",
            "GAN",
            "Transformer",
            "AIGC"
        ],
        "tldr": "The paper proposes a collaborative learning method that combines scattering and deep features for SAR target recognition, specifically addressing noisy labels. Extensive experiments on the MSTAR dataset show state-of-the-art performance.",
        "tldr_zh": "该论文提出了一种结合散射和深度特征的合作学习方法，用于合成孔径雷达目标识别，特别是解决嘈杂标签问题。对MSTAR数据集进行的大量实验表明，该方法在不同操作条件和标签嘈杂度下实现了领先水平。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Commentary Generation for Soccer Highlights",
        "summary": "Automated soccer commentary generation has evolved from template-based\nsystems to advanced neural architectures, aiming to produce real-time\ndescriptions of sports events. While frameworks like SoccerNet-Caption laid\nfoundational work, their inability to achieve fine-grained alignment between\nvideo content and commentary remains a significant challenge. Recent efforts\nsuch as MatchTime, with its MatchVoice model, address this issue through coarse\nand fine-grained alignment techniques, achieving improved temporal\nsynchronization. In this paper, we extend MatchVoice to commentary generation\nfor soccer highlights using the GOAL dataset, which emphasizes short clips over\nentire games. We conduct extensive experiments to reproduce the original\nMatchTime results and evaluate our setup, highlighting the impact of different\ntraining configurations and hardware limitations. Furthermore, we explore the\neffect of varying window sizes on zero-shot performance. While MatchVoice\nexhibits promising generalization capabilities, our findings suggest the need\nfor integrating techniques from broader video-language domains to further\nenhance performance. Our code is available at\nhttps://github.com/chidaksh/SoccerCommentary.",
        "url": "http://arxiv.org/abs/2508.07543v1",
        "published_date": "2025-08-11T01:48:37+00:00",
        "updated_date": "2025-08-11T01:48:37+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Chidaksh Ravuru"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper extends an existing model to generate soccer commentary for highlights, emphasizing alignment between video and commentary. It explores different training configurations and suggests the need for integrating techniques from broader video-language domains for performance enhancement.",
        "tldr_zh": "该论文扩展了现有模型，用于生成足球精彩瞬间的评论，强调视频和评论之间的对齐。它探讨了不同的培训配置，并建议整合更广泛的视频语言领域的技术以提高性能。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "A DICOM Image De-identification Algorithm in the MIDI-B Challenge",
        "summary": "Image de-identification is essential for the public sharing of medical\nimages, particularly in the widely used Digital Imaging and Communications in\nMedicine (DICOM) format as required by various regulations and standards,\nincluding Health Insurance Portability and Accountability Act (HIPAA) privacy\nrules, the DICOM PS3.15 standard, and best practices recommended by the Cancer\nImaging Archive (TCIA). The Medical Image De-Identification Benchmark (MIDI-B)\nChallenge at the 27th International Conference on Medical Image Computing and\nComputer Assisted Intervention (MICCAI 2024) was organized to evaluate\nrule-based DICOM image de-identification algorithms with a large dataset of\nclinical DICOM images. In this report, we explore the critical challenges of\nde-identifying DICOM images, emphasize the importance of removing personally\nidentifiable information (PII) to protect patient privacy while ensuring the\ncontinued utility of medical data for research, diagnostics, and treatment, and\nprovide a comprehensive overview of the standards and regulations that govern\nthis process. Additionally, we detail the de-identification methods we applied\n- such as pixel masking, date shifting, date hashing, text recognition, text\nreplacement, and text removal - to process datasets during the test phase in\nstrict compliance with these standards. According to the final leaderboard of\nthe MIDI-B challenge, the latest version of our solution algorithm correctly\nexecuted 99.92% of the required actions and ranked 2nd out of 10 teams that\ncompleted the challenge (from a total of 22 registered teams). Finally, we\nconducted a thorough analysis of the resulting statistics and discussed the\nlimitations of current approaches and potential avenues for future improvement.",
        "url": "http://arxiv.org/abs/2508.07538v1",
        "published_date": "2025-08-11T01:38:07+00:00",
        "updated_date": "2025-08-11T01:38:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hongzhu Jiang",
            "Sihan Xie",
            "Zhiyu Wan"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper discusses a DICOM image de-identification algorithm used in a challenge to protect patient privacy while maintaining data utility for research purposes.",
        "tldr_zh": "本文讨论了一种用于挑战的DICOM图像去标识化算法，旨在保护患者隐私同时保持数据对研究目的的有效性。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.0
    },
    {
        "title": "Extracting Overlapping Microservices from Monolithic Code via Deep Semantic Embeddings and Graph Neural Network-Based Soft Clustering",
        "summary": "Modern software systems are increasingly shifting from monolithic\narchitectures to microservices to enhance scalability, maintainability, and\ndeployment flexibility. Existing microservice extraction methods typically rely\non hard clustering, assigning each software component to a single microservice.\nThis approach often increases inter-service coupling and reduces intra-service\ncohesion. We propose Mo2oM (Monolithic to Overlapping Microservices), a\nframework that formulates microservice extraction as a soft clustering problem,\nallowing components to belong probabilistically to multiple microservices. This\napproach is inspired by expert-driven decompositions, where practitioners\nintentionally replicate certain software components across services to reduce\ncommunication overhead. Mo2oM combines deep semantic embeddings with structural\ndependencies extracted from methodcall graphs to capture both functional and\narchitectural relationships. A graph neural network-based soft clustering\nalgorithm then generates the final set of microservices. We evaluate Mo2oM on\nfour open-source monolithic benchmarks and compare it against eight\nstate-of-the-art baselines. Our results demonstrate that Mo2oM achieves\nimprovements of up to 40.97% in structural modularity (balancing cohesion and\ncoupling), 58% in inter-service call percentage (communication overhead),\n26.16% in interface number (modularity and decoupling), and 38.96% in\nnon-extreme distribution (service size balance) across all benchmarks.",
        "url": "http://arxiv.org/abs/2508.07486v1",
        "published_date": "2025-08-10T21:07:20+00:00",
        "updated_date": "2025-08-10T21:07:20+00:00",
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Morteza Ziabakhsh",
            "Kiyan Rezaee",
            "Sadegh Eskandari",
            "Seyed Amir Hossein Tabatabaei",
            "Mohammad M. Ghassemi"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a framework, Mo2oM, that extracts microservices from monolithic code using soft clustering to reduce inter-service coupling and increase intra-service cohesion.",
        "tldr_zh": "该论文提出了一个名为Mo2oM的框架，通过软聚类将微服务从单体代码中提取出来，以减少服务间耦合并增加服务内聚性。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Mitigating Biases in Surgical Operating Rooms with Geometry",
        "summary": "Deep neural networks are prone to learning spurious correlations, exploiting\ndataset-specific artifacts rather than meaningful features for prediction. In\nsurgical operating rooms (OR), these manifest through the standardization of\nsmocks and gowns that obscure robust identifying landmarks, introducing model\nbias for tasks related to modeling OR personnel. Through gradient-based\nsaliency analysis on two public OR datasets, we reveal that CNN models succumb\nto such shortcuts, fixating on incidental visual cues such as footwear beneath\nsurgical gowns, distinctive eyewear, or other role-specific identifiers.\nAvoiding such biases is essential for the next generation of intelligent\nassistance systems in the OR, which should accurately recognize personalized\nworkflow traits, such as surgical skill level or coordination with other staff\nmembers. We address this problem by encoding personnel as 3D point cloud\nsequences, disentangling identity-relevant shape and motion patterns from\nappearance-based confounders. Our experiments demonstrate that while RGB and\ngeometric methods achieve comparable performance on datasets with apparent\nsimulation artifacts, RGB models suffer a 12% accuracy drop in realistic\nclinical settings with decreased visual diversity due to standardizations. This\nperformance gap confirms that geometric representations capture more meaningful\nbiometric features, providing an avenue to developing robust methods of\nmodeling humans in the OR.",
        "url": "http://arxiv.org/abs/2508.08028v1",
        "published_date": "2025-08-11T14:32:32+00:00",
        "updated_date": "2025-08-11T14:32:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tony Danjun Wang",
            "Tobias Czempiel",
            "Nassir Navab",
            "Lennart Bastian"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper explores biases in surgical operating rooms and proposes a method using 3D point cloud sequences to mitigate these biases compared to traditional RGB models.",
        "tldr_zh": "本文探讨了手术室中的偏见，并提出了一种使用3D点云序列的方法，以减轻这些偏见，与传统的RGB模型相比。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "TAG: A Simple Yet Effective Temporal-Aware Approach for Zero-Shot Video Temporal Grounding",
        "summary": "Video Temporal Grounding (VTG) aims to extract relevant video segments based\non a given natural language query. Recently, zero-shot VTG methods have gained\nattention by leveraging pretrained vision-language models (VLMs) to localize\ntarget moments without additional training. However, existing approaches suffer\nfrom semantic fragmentation, where temporally continuous frames sharing the\nsame semantics are split across multiple segments. When segments are\nfragmented, it becomes difficult to predict an accurate target moment that\naligns with the text query. Also, they rely on skewed similarity distributions\nfor localization, making it difficult to select the optimal segment.\nFurthermore, they heavily depend on the use of LLMs which require expensive\ninferences. To address these limitations, we propose a \\textit{TAG}, a simple\nyet effective Temporal-Aware approach for zero-shot video temporal Grounding,\nwhich incorporates temporal pooling, temporal coherence clustering, and\nsimilarity adjustment. Our proposed method effectively captures the temporal\ncontext of videos and addresses distorted similarity distributions without\ntraining. Our approach achieves state-of-the-art results on Charades-STA and\nActivityNet Captions benchmark datasets without rely on LLMs. Our code is\navailable at https://github.com/Nuetee/TAG",
        "url": "http://arxiv.org/abs/2508.07925v1",
        "published_date": "2025-08-11T12:38:46+00:00",
        "updated_date": "2025-08-11T12:38:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jin-Seop Lee",
            "SungJoon Lee",
            "Jaehan Ahn",
            "YunSeok Choi",
            "Jee-Hyong Lee"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces TAG, a method for zero-shot video temporal grounding that addresses issues with existing approaches by incorporating temporal pooling, coherence clustering, and similarity adjustment.",
        "tldr_zh": "该论文介绍了TAG，一种针对零阶视频时间定位的方法，通过引入时间池化、一致性聚类和相似性调整来解决现有方法存在的问题。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Selective Contrastive Learning for Weakly Supervised Affordance Grounding",
        "summary": "Facilitating an entity's interaction with objects requires accurately\nidentifying parts that afford specific actions. Weakly supervised affordance\ngrounding (WSAG) seeks to imitate human learning from third-person\ndemonstrations, where humans intuitively grasp functional parts without needing\npixel-level annotations. To achieve this, grounding is typically learned using\na shared classifier across images from different perspectives, along with\ndistillation strategies incorporating part discovery process. However, since\naffordance-relevant parts are not always easily distinguishable, models\nprimarily rely on classification, often focusing on common class-specific\npatterns that are unrelated to affordance. To address this limitation, we move\nbeyond isolated part-level learning by introducing selective prototypical and\npixel contrastive objectives that adaptively learn affordance-relevant cues at\nboth the part and object levels, depending on the granularity of the available\ninformation. Initially, we find the action-associated objects in both\negocentric (object-focused) and exocentric (third-person example) images by\nleveraging CLIP. Then, by cross-referencing the discovered objects of\ncomplementary views, we excavate the precise part-level affordance clues in\neach perspective. By consistently learning to distinguish affordance-relevant\nregions from affordance-irrelevant background context, our approach effectively\nshifts activation from irrelevant areas toward meaningful affordance cues.\nExperimental results demonstrate the effectiveness of our method. Codes are\navailable at github.com/hynnsk/SelectiveCL.",
        "url": "http://arxiv.org/abs/2508.07877v1",
        "published_date": "2025-08-11T11:49:37+00:00",
        "updated_date": "2025-08-11T11:49:37+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "WonJun Moon",
            "Hyun Seok Seong",
            "Jae-Pil Heo"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper proposes a method for weakly supervised affordance grounding using selective contrastive learning to identify parts that afford specific actions without pixel-level annotations.",
        "tldr_zh": "本文提出了一种利用选择性对比学习的方法，用于在无需像素级标注的情况下识别可实现特定动作的部分。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Prototype-Guided Curriculum Learning for Zero-Shot Learning",
        "summary": "In Zero-Shot Learning (ZSL), embedding-based methods enable knowledge\ntransfer from seen to unseen classes by learning a visual-semantic mapping from\nseen-class images to class-level semantic prototypes (e.g., attributes).\nHowever, these semantic prototypes are manually defined and may introduce noisy\nsupervision for two main reasons: (i) instance-level mismatch: variations in\nperspective, occlusion, and annotation bias will cause discrepancies between\nindividual sample and the class-level semantic prototypes; and (ii) class-level\nimprecision: the manually defined semantic prototypes may not accurately\nreflect the true semantics of the class. Consequently, the visual-semantic\nmapping will be misled, reducing the effectiveness of knowledge transfer to\nunseen classes. In this work, we propose a prototype-guided curriculum learning\nframework (dubbed as CLZSL), which mitigates instance-level mismatches through\na Prototype-Guided Curriculum Learning (PCL) module and addresses class-level\nimprecision via a Prototype Update (PUP) module. Specifically, the PCL module\nprioritizes samples with high cosine similarity between their visual mappings\nand the class-level semantic prototypes, and progressively advances to\nless-aligned samples, thereby reducing the interference of instance-level\nmismatches to achieve accurate visual-semantic mapping. Besides, the PUP module\ndynamically updates the class-level semantic prototypes by leveraging the\nvisual mappings learned from instances, thereby reducing class-level\nimprecision and further improving the visual-semantic mapping. Experiments were\nconducted on standard benchmark datasets-AWA2, SUN, and CUB-to verify the\neffectiveness of our method.",
        "url": "http://arxiv.org/abs/2508.07771v1",
        "published_date": "2025-08-11T08:56:21+00:00",
        "updated_date": "2025-08-11T08:56:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lei Wang",
            "Shiming Chen",
            "Guo-Sen Xie",
            "Ziming Hong",
            "Chaojian Yu",
            "Qinmu Peng",
            "Xinge You"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a curriculum learning framework for zero-shot learning to improve the visual-semantic mapping by addressing instance-level mismatches and class-level imprecision through two modules.",
        "tldr_zh": "本文介绍了一种课程学习框架，用于改善零样本学习中的视觉-语义映射，通过两个模块解决实例级不匹配和类别级不准确问题。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "A Registration-Based Star-Shape Segmentation Model and Fast Algorithms",
        "summary": "Image segmentation plays a crucial role in extracting objects of interest and\nidentifying their boundaries within an image. However, accurate segmentation\nbecomes challenging when dealing with occlusions, obscurities, or noise in\ncorrupted images. To tackle this challenge, prior information is often\nutilized, with recent attention on star-shape priors. In this paper, we propose\na star-shape segmentation model based on the registration framework. By\ncombining the level set representation with the registration framework and\nimposing constraints on the deformed level set function, our model enables both\nfull and partial star-shape segmentation, accommodating single or multiple\ncenters. Additionally, our approach allows for the enforcement of identified\nboundaries to pass through specified landmark locations. We tackle the proposed\nmodels using the alternating direction method of multipliers. Through numerical\nexperiments conducted on synthetic and real images, we demonstrate the efficacy\nof our approach in achieving accurate star-shape segmentation.",
        "url": "http://arxiv.org/abs/2508.07721v1",
        "published_date": "2025-08-11T07:47:46+00:00",
        "updated_date": "2025-08-11T07:47:46+00:00",
        "categories": [
            "cs.CV",
            "cs.NA",
            "math.NA",
            "65D18, 68U10, 94A08"
        ],
        "authors": [
            "Daoping Zhang",
            "Xue-Cheng Tai",
            "Lok Ming Lui"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a registration-based star-shape segmentation model for accurate object extraction in images, showcasing effectiveness through numerical experiments.",
        "tldr_zh": "该论文提出了基于注册的星形分割模型，用于图像中对象提取，通过数值实验展示了其有效性。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Domain Generalization of Pathological Image Segmentation by Patch-Level and WSI-Level Contrastive Learning",
        "summary": "In this paper, we address domain shifts in pathological images by focusing on\nshifts within whole slide images~(WSIs), such as patient characteristics and\ntissue thickness, rather than shifts between hospitals. Traditional approaches\nrely on multi-hospital data, but data collection challenges often make this\nimpractical. Therefore, the proposed domain generalization method captures and\nleverages intra-hospital domain shifts by clustering WSI-level features from\nnon-tumor regions and treating these clusters as domains. To mitigate domain\nshift, we apply contrastive learning to reduce feature gaps between WSI pairs\nfrom different clusters. The proposed method introduces a two-stage contrastive\nlearning approach WSI-level and patch-level contrastive learning to minimize\nthese gaps effectively.",
        "url": "http://arxiv.org/abs/2508.07539v1",
        "published_date": "2025-08-11T01:38:31+00:00",
        "updated_date": "2025-08-11T01:38:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuki Shigeyasu",
            "Shota Harada",
            "Akihiko Yoshizawa",
            "Kazuhiro Terada",
            "Naoki Nakazima",
            "Mariyo Kurata",
            "Hiroyuki Abe",
            "Tetsuo Ushiku",
            "Ryoma Bise"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper proposes a method to address domain shifts in pathological images by focusing on within-WSI shifts, using contrastive learning at both WSI and patch levels.",
        "tldr_zh": "本文提出了一种方法，通过在WSI内部变化中使用对比学习在WSI和patch级别上来解决病理图像中的领域偏移。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "CTC Transcription Alignment of the Bullinger Letters: Automatic Improvement of Annotation Quality",
        "summary": "Handwritten text recognition for historical documents remains challenging due\nto handwriting variability, degraded sources, and limited layout-aware\nannotations. In this work, we address annotation errors - particularly\nhyphenation issues - in the Bullinger correspondence, a large 16th-century\nletter collection. We introduce a self-training method based on a CTC alignment\nalgorithm that matches full transcriptions to text line images using dynamic\nprogramming and model output probabilities trained with the CTC loss. Our\napproach improves performance (e.g., by 1.1 percentage points CER with PyLaia)\nand increases alignment accuracy. Interestingly, we find that weaker models\nyield more accurate alignments, enabling an iterative training strategy. We\nrelease a new manually corrected subset of 100 pages from the Bullinger\ndataset, along with our code and benchmarks. Our approach can be applied\niteratively to further improve the CER as well as the alignment quality for\ntext recognition pipelines. Code and data are available via\nhttps://github.com/andreas-fischer-unifr/nntp.",
        "url": "http://arxiv.org/abs/2508.07904v1",
        "published_date": "2025-08-11T12:18:41+00:00",
        "updated_date": "2025-08-11T12:18:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Marco Peer",
            "Anna Scius-Bertrand",
            "Andreas Fischer"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a method to improve annotation quality in historical handwritten documents using a self-training approach based on a CTC alignment algorithm.",
        "tldr_zh": "本文介绍了一种使用基于CTC对齐算法的自训练方法，以改进历史手写文件中的注释质量。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "Morphological Analysis of Semiconductor Microstructures using Skeleton Graphs",
        "summary": "In this paper, electron microscopy images of microstructures formed on Ge\nsurfaces by ion beam irradiation were processed to extract topological features\nas skeleton graphs, which were then embedded using a graph convolutional\nnetwork. The resulting embeddings were analyzed using principal component\nanalysis, and cluster separability in the resulting PCA space was evaluated\nusing the Davies-Bouldin index. The results indicate that variations in\nirradiation angle have a more significant impact on the morphological\nproperties of Ge surfaces than variations in irradiation fluence.",
        "url": "http://arxiv.org/abs/2508.07850v1",
        "published_date": "2025-08-11T11:10:07+00:00",
        "updated_date": "2025-08-11T11:10:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Noriko Nitta",
            "Rei Miyata",
            "Naoto Oishi"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper utilizes skeleton graphs and graph convolutional networks to analyze semiconductor microstructures, showing that irradiation angle has a more significant impact on surface properties than irradiation fluence.",
        "tldr_zh": "本文利用骨架图和图卷积网络分析半导体微结构，结果表明辐照角度对表面性质的影响比辐照通量更显著。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.0
    },
    {
        "title": "GaitSnippet: Gait Recognition Beyond Unordered Sets and Ordered Sequences",
        "summary": "Recent advancements in gait recognition have significantly enhanced\nperformance by treating silhouettes as either an unordered set or an ordered\nsequence. However, both set-based and sequence-based approaches exhibit notable\nlimitations. Specifically, set-based methods tend to overlook short-range\ntemporal context for individual frames, while sequence-based methods struggle\nto capture long-range temporal dependencies effectively. To address these\nchallenges, we draw inspiration from human identification and propose a new\nperspective that conceptualizes human gait as a composition of individualized\nactions. Each action is represented by a series of frames, randomly selected\nfrom a continuous segment of the sequence, which we term a snippet.\nFundamentally, the collection of snippets for a given sequence enables the\nincorporation of multi-scale temporal context, facilitating more comprehensive\ngait feature learning. Moreover, we introduce a non-trivial solution for\nsnippet-based gait recognition, focusing on Snippet Sampling and Snippet\nModeling as key components. Extensive experiments on four widely-used gait\ndatasets validate the effectiveness of our proposed approach and, more\nimportantly, highlight the potential of gait snippets. For instance, our method\nachieves the rank-1 accuracy of 77.5% on Gait3D and 81.7% on GREW using a 2D\nconvolution-based backbone.",
        "url": "http://arxiv.org/abs/2508.07782v1",
        "published_date": "2025-08-11T09:13:38+00:00",
        "updated_date": "2025-08-11T09:13:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Saihui Hou",
            "Chenye Wang",
            "Wenpeng Lang",
            "Zhengxiang Lan",
            "Yongzhen Huang"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper proposes a new approach for gait recognition by representing human gait as a collection of 'snippets' that capture multi-scale temporal context, showing promising results on widely-used datasets.",
        "tldr_zh": "该论文提出了一种新的步态识别方法，通过将人类步态表示为一系列'片段'来捕捉多尺度时间上下文，对广泛使用的数据集展现出了良好的结果。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information",
        "summary": "We introduce InterChart, a diagnostic benchmark that evaluates how well\nvision-language models (VLMs) reason across multiple related charts, a task\ncentral to real-world applications such as scientific reporting, financial\nanalysis, and public policy dashboards. Unlike prior benchmarks focusing on\nisolated, visually uniform charts, InterChart challenges models with diverse\nquestion types ranging from entity inference and trend correlation to numerical\nestimation and abstract multi-step reasoning grounded in 2-3 thematically or\nstructurally related charts. We organize the benchmark into three tiers of\nincreasing difficulty: (1) factual reasoning over individual charts, (2)\nintegrative analysis across synthetically aligned chart sets, and (3) semantic\ninference over visually complex, real-world chart pairs. Our evaluation of\nstate-of-the-art open and closed-source VLMs reveals consistent and steep\naccuracy declines as chart complexity increases. We find that models perform\nbetter when we decompose multi-entity charts into simpler visual units,\nunderscoring their struggles with cross-chart integration. By exposing these\nsystematic limitations, InterChart provides a rigorous framework for advancing\nmultimodal reasoning in complex, multi-visual environments.",
        "url": "http://arxiv.org/abs/2508.07630v1",
        "published_date": "2025-08-11T05:19:23+00:00",
        "updated_date": "2025-08-11T05:19:23+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "I.2.7; I.2.10; I.4.10; I.7.5"
        ],
        "authors": [
            "Anirudh Iyengar Kaniyar Narayana Iyengar",
            "Srija Mukhopadhyay",
            "Adnan Qidwai",
            "Shubhankar Singh",
            "Dan Roth",
            "Vivek Gupta"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces InterChart, a benchmark for evaluating how well vision-language models reason across multiple related charts, with a focus on complex, real-world chart pairs.",
        "tldr_zh": "该论文引入了InterChart，这是一个评估视觉-语言模型在多个相关图表上推理能力的基准。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6.0
    },
    {
        "title": "An Iterative Reconstruction Method for Dental Cone-Beam Computed Tomography with a Truncated Field of View",
        "summary": "In dental cone-beam computed tomography (CBCT), compact and cost-effective\nsystem designs often use small detectors, resulting in a truncated field of\nview (FOV) that does not fully encompass the patient's head. In iterative\nreconstruction approaches, the discrepancy between the actual projection and\nthe forward projection within the truncated FOV accumulates over iterations,\nleading to significant degradation in the reconstructed image quality. In this\nstudy, we propose a two-stage approach to mitigate truncation artifacts in\ndental CBCT. In the first stage, we employ Implicit Neural Representation\n(INR), leveraging its superior representation power, to generate a prior image\nover an extended region so that its forward projection fully covers the\npatient's head. To reduce computational and memory burdens, INR reconstruction\nis performed with a coarse voxel size. The forward projection of this prior\nimage is then used to estimate the discrepancy due to truncated FOV in the\nmeasured projection data. In the second stage, the discrepancy-corrected\nprojection data is utilized in a conventional iterative reconstruction process\nwithin the truncated region. Our numerical results demonstrate that the\nproposed two-grid approach effectively suppresses truncation artifacts, leading\nto improved CBCT image quality.",
        "url": "http://arxiv.org/abs/2508.07618v1",
        "published_date": "2025-08-11T04:54:18+00:00",
        "updated_date": "2025-08-11T04:54:18+00:00",
        "categories": [
            "cs.CV",
            "68Wxx"
        ],
        "authors": [
            "Hyoung Suk Park",
            "Kiwan Jeon"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a method to improve dental cone-beam computed tomography image quality by mitigating truncation artifacts through an iterative reconstruction process.",
        "tldr_zh": "该论文提出了一种通过迭代重建过程来减轻截断伪影从而改善牙科锥束计算机体层摄影图像质量的方法。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "Progressive Bird's Eye View Perception for Safety-Critical Autonomous Driving: A Comprehensive Survey",
        "summary": "Bird's-Eye-View (BEV) perception has become a foundational paradigm in\nautonomous driving, enabling unified spatial representations that support\nrobust multi-sensor fusion and multi-agent collaboration. As autonomous\nvehicles transition from controlled environments to real-world deployment,\nensuring the safety and reliability of BEV perception in complex scenarios -\nsuch as occlusions, adverse weather, and dynamic traffic - remains a critical\nchallenge. This survey provides the first comprehensive review of BEV\nperception from a safety-critical perspective, systematically analyzing\nstate-of-the-art frameworks and implementation strategies across three\nprogressive stages: single-modality vehicle-side, multimodal vehicle-side, and\nmulti-agent collaborative perception. Furthermore, we examine public datasets\nencompassing vehicle-side, roadside, and collaborative settings, evaluating\ntheir relevance to safety and robustness. We also identify key open-world\nchallenges - including open-set recognition, large-scale unlabeled data, sensor\ndegradation, and inter-agent communication latency - and outline future\nresearch directions, such as integration with end-to-end autonomous driving\nsystems, embodied intelligence, and large language models.",
        "url": "http://arxiv.org/abs/2508.07560v1",
        "published_date": "2025-08-11T02:40:46+00:00",
        "updated_date": "2025-08-11T02:40:46+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Yan Gong",
            "Naibang Wang",
            "Jianli Lu",
            "Xinyu Zhang",
            "Yongsheng Gao",
            "Jie Zhao",
            "Zifan Huang",
            "Haozhi Bai",
            "Nanxin Zeng",
            "Nayu Su",
            "Lei Yang",
            "Ziying Song",
            "Xiaoxi Hu",
            "Xinmin Jiang",
            "Xiaojuan Zhang",
            "Susanto Rahardja"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "This paper provides a comprehensive survey on Bird's-Eye-View perception for safety-critical autonomous driving, analyzing state-of-the-art frameworks and future research directions.",
        "tldr_zh": "本文对鸟瞰视角感知在安全关键自动驾驶中的应用进行了全面调研，分析了最新的框架和未来研究方向。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6.0
    },
    {
        "title": "Enhancing Reliability of Medical Image Diagnosis through Top-rank Learning with Rejection Module",
        "summary": "In medical image processing, accurate diagnosis is of paramount importance.\nLeveraging machine learning techniques, particularly top-rank learning, shows\nsignificant promise by focusing on the most crucial instances. However,\nchallenges arise from noisy labels and class-ambiguous instances, which can\nseverely hinder the top-rank objective, as they may be erroneously placed among\nthe top-ranked instances. To address these, we propose a novel approach that\nenhances toprank learning by integrating a rejection module. Cooptimized with\nthe top-rank loss, this module identifies and mitigates the impact of outliers\nthat hinder training effectiveness. The rejection module functions as an\nadditional branch, assessing instances based on a rejection function that\nmeasures their deviation from the norm. Through experimental validation on a\nmedical dataset, our methodology demonstrates its efficacy in detecting and\nmitigating outliers, improving the reliability and accuracy of medical image\ndiagnoses.",
        "url": "http://arxiv.org/abs/2508.07528v1",
        "published_date": "2025-08-11T01:08:53+00:00",
        "updated_date": "2025-08-11T01:08:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaotong Ji",
            "Ryoma Bise",
            "Seiichi Uchida"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel approach to enhance top-rank learning in medical image diagnosis by integrating a rejection module to mitigate outliers and improve reliability.",
        "tldr_zh": "本文提出了一种新方法，通过整合拒绝模块来强化在医学图像诊断中的高排名学习，以减少异常值并提高可靠性。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "Health Care Waste Classification Using Deep Learning Aligned with Nepal's Bin Color Guidelines",
        "summary": "The increasing number of Health Care facilities in Nepal has also added up\nthe challenges on managing health care waste (HCW). Improper segregation and\ndisposal of HCW leads to the contamination, spreading of infectious diseases\nand puts a risk of waste handlers. This study benchmarks the state of the art\nwaste classification models: ResNeXt-50, EfficientNet-B0, MobileNetV3-S,\nYOLOv8-n and YOLOv5-s using Stratified K-fold techniques where we use 5 folds\non combined HCW data, and found that the YOLOv5-s achieved higher of 95.06%\naccuracy but fell short few milliseconds in inference speed with YOLOv8-n\nmodel. The EfficientNet-B0 showed promising results of 93.22% accuracy but took\nthe highest inference time. A repetitive ANOVA was performed to see statistical\nsignificance and the best performing model (YOLOv5-s) was deployed to the web\nwith mapped bin color using Nepal's HCW management standards for public usage.\nFurther work on the data was suggested along with localized context.",
        "url": "http://arxiv.org/abs/2508.07450v1",
        "published_date": "2025-08-10T18:25:01+00:00",
        "updated_date": "2025-08-10T18:25:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Suman Kunwar",
            "Prabesh Rai"
        ],
        "ai_categories": [
            "Dataset",
            "GAN"
        ],
        "tldr": "The paper evaluates various deep learning models for classifying healthcare waste in Nepal, with the best model achieving 95.06% accuracy aligned with Nepal's guidelines.",
        "tldr_zh": "本文评估了各种深度学习模型用于分类尼泊尔的医疗废物，最佳模型达到了95.06%的准确率，符合尼泊尔的指南。",
        "relevance_score": 3,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]