[
    {
        "title": "Uncertainty-Aware Deep Learning for Wildfire Danger Forecasting",
        "summary": "Wildfires are among the most severe natural hazards, posing a significant\nthreat to both humans and natural ecosystems. The growing risk of wildfires\nincreases the demand for forecasting models that are not only accurate but also\nreliable. Deep Learning (DL) has shown promise in predicting wildfire danger;\nhowever, its adoption is hindered by concerns over the reliability of its\npredictions, some of which stem from the lack of uncertainty quantification. To\naddress this challenge, we present an uncertainty-aware DL framework that\njointly captures epistemic (model) and aleatoric (data) uncertainty to enhance\nshort-term wildfire danger forecasting. In the next-day forecasting, our\nbest-performing model improves the F1 Score by 2.3% and reduces the Expected\nCalibration Error by 2.1% compared to a deterministic baseline, enhancing both\npredictive skill and calibration. Our experiments confirm the reliability of\nthe uncertainty estimates and illustrate their practical utility for decision\nsupport, including the identification of uncertainty thresholds for rejecting\nlow-confidence predictions and the generation of well-calibrated wildfire\ndanger maps with accompanying uncertainty layers. Extending the forecast\nhorizon up to ten days, we observe that aleatoric uncertainty increases with\ntime, showing greater variability in environmental conditions, while epistemic\nuncertainty remains stable. Finally, we show that although the two uncertainty\ntypes may be redundant in low-uncertainty cases, they provide complementary\ninsights under more challenging conditions, underscoring the value of their\njoint modeling for robust wildfire danger prediction. In summary, our approach\nsignificantly improves the accuracy and reliability of wildfire danger\nforecasting, advancing the development of trustworthy wildfire DL systems.",
        "url": "http://arxiv.org/abs/2509.25017v1",
        "published_date": "2025-09-29T16:43:17+00:00",
        "updated_date": "2025-09-29T16:43:17+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Spyros Kondylatos",
            "Gustau Camps-Valls",
            "Ioannis Papoutsis"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces an uncertainty-aware deep learning framework for wildfire danger forecasting that improves accuracy and reliability by quantifying both model and data uncertainty.",
        "tldr_zh": "本文提出了一种考虑不确定性的深度学习框架，用于野火危险预测，通过量化模型和数据的不确定性，提高了准确性和可靠性。",
        "relevance_score": 1,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation",
        "summary": "High-fidelity 3D asset generation is crucial for various industries. While\nrecent 3D pretrained models show strong capability in producing realistic\ncontent, most are built upon diffusion models and follow a two-stage pipeline\nthat first generates geometry and then synthesizes appearance. Such a decoupled\ndesign tends to produce geometry-texture misalignment and non-negligible cost.\nIn this paper, we propose UniLat3D, a unified framework that encodes geometry\nand appearance in a single latent space, enabling direct single-stage\ngeneration. Our key contribution is a geometry-appearance Unified VAE, which\ncompresses high-resolution sparse features into a compact latent representation\n-- UniLat. UniLat integrates structural and visual information into a dense\nlow-resolution latent, which can be efficiently decoded into diverse 3D\nformats, e.g., 3D Gaussians and meshes. Based on this unified representation,\nwe train a single flow-matching model to map Gaussian noise directly into\nUniLat, eliminating redundant stages. Trained solely on public datasets,\nUniLat3D produces high-quality 3D assets in seconds from a single image,\nachieving superior appearance fidelity and geometric quality. More demos \\&\ncode are available at https://unilat3d.github.io/",
        "url": "http://arxiv.org/abs/2509.25079v1",
        "published_date": "2025-09-29T17:21:23+00:00",
        "updated_date": "2025-09-29T17:21:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.GR"
        ],
        "authors": [
            "Guanjun Wu",
            "Jiemin Fang",
            "Chen Yang",
            "Sikuang Li",
            "Taoran Yi",
            "Jia Lu",
            "Zanwei Zhou",
            "Jiazhong Cen",
            "Lingxi Xie",
            "Xiaopeng Zhang",
            "Wei Wei",
            "Wenyu Liu",
            "Xinggang Wang",
            "Qi Tian"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces UniLat3D, a unified framework for single-stage 3D asset generation that integrates geometry and appearance in a single latent space, achieving high-quality results in seconds.",
        "tldr_zh": "该论文介绍了UniLat3D，一个统一的框架，用于单阶段3D资产生成，在单个潜在空间中整合几何和外观，可以在几秒钟内实现高质量结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "Environment-Aware Satellite Image Generation with Diffusion Models",
        "summary": "Diffusion-based foundation models have recently garnered much attention in\nthe field of generative modeling due to their ability to generate images of\nhigh quality and fidelity. Although not straightforward, their recent\napplication to the field of remote sensing signaled the first successful trials\ntowards harnessing the large volume of publicly available datasets containing\nmultimodal information. Despite their success, existing methods face\nconsiderable limitations: they rely on limited environmental context, struggle\nwith missing or corrupted data, and often fail to reliably reflect user\nintentions in generated outputs. In this work, we propose a novel diffusion\nmodel conditioned on environmental context, that is able to generate satellite\nimages by conditioning from any combination of three different control signals:\na) text, b) metadata, and c) visual data. In contrast to previous works, the\nproposed method is i) to our knowledge, the first of its kind to condition\nsatellite image generation on dynamic environmental conditions as part of its\ncontrol signals, and ii) incorporating a metadata fusion strategy that models\nattribute embedding interactions to account for partially corrupt and/or\nmissing observations. Our method outperforms previous methods both\nqualitatively (robustness to missing metadata, higher responsiveness to control\ninputs) and quantitatively (higher fidelity, accuracy, and quality of\ngenerations measured using 6 different metrics) in the trials of single-image\nand temporal generation. The reported results support our hypothesis that\nconditioning on environmental context can improve the performance of foundation\nmodels for satellite imagery, and render our model a promising candidate for\nusage in downstream tasks. The collected 3-modal dataset is to our knowledge,\nthe first publicly-available dataset to combine data from these three different\nmediums.",
        "url": "http://arxiv.org/abs/2509.24875v1",
        "published_date": "2025-09-29T14:54:53+00:00",
        "updated_date": "2025-09-29T14:54:53+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Nikos Kostagiolas",
            "Pantelis Georgiades",
            "Yannis Panagakis",
            "Mihalis A. Nicolaou"
        ],
        "ai_categories": [
            "Diffusion",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel diffusion model conditioned on environmental context to generate satellite images with higher quality and fidelity, outperforming existing methods by incorporating dynamic environmental conditions and metadata fusion strategy.",
        "tldr_zh": "本文提出了一种新颖的扩散模型，根据环境背景条件生成高质量和高保真度的卫星图像，通过动态环境条件和元数据融合策略，性能优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "StreamForest: Efficient Online Video Understanding with Persistent Event Memory",
        "summary": "Multimodal Large Language Models (MLLMs) have recently achieved remarkable\nprogress in video understanding. However, their effectiveness in real-time\nstreaming scenarios remains limited due to storage constraints of historical\nvisual features and insufficient real-time spatiotemporal reasoning. To address\nthese challenges, we propose StreamForest, a novel architecture specifically\ndesigned for streaming video understanding. Central to StreamForest is the\nPersistent Event Memory Forest, a memory mechanism that adaptively organizes\nvideo frames into multiple event-level tree structures. This process is guided\nby penalty functions based on temporal distance, content similarity, and merge\nfrequency, enabling efficient long-term memory retention under limited\ncomputational resources. To enhance real-time perception, we introduce a\nFine-grained Spatiotemporal Window, which captures detailed short-term visual\ncues to improve current scene perception. Additionally, we present OnlineIT, an\ninstruction-tuning dataset tailored for streaming video tasks. OnlineIT\nsignificantly boosts MLLM performance in both real-time perception and future\nprediction. To evaluate generalization in practical applications, we introduce\nODV-Bench, a new benchmark focused on real-time streaming video understanding\nin autonomous driving scenarios. Experimental results demonstrate that\nStreamForest achieves the state-of-the-art performance, with accuracies of\n77.3% on StreamingBench, 60.5% on OVBench, and 55.6% on OVO-Bench. In\nparticular, even under extreme visual token compression (limited to 1024\ntokens), the model retains 96.8% of its average accuracy in eight benchmarks\nrelative to the default setting. These results underscore the robustness,\nefficiency, and generalizability of StreamForest for streaming video\nunderstanding.",
        "url": "http://arxiv.org/abs/2509.24871v1",
        "published_date": "2025-09-29T14:53:57+00:00",
        "updated_date": "2025-09-29T14:53:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiangyu Zeng",
            "Kefan Qiu",
            "Qingyu Zhang",
            "Xinhao Li",
            "Jing Wang",
            "Jiaxin Li",
            "Ziang Yan",
            "Kun Tian",
            "Meng Tian",
            "Xinhai Zhao",
            "Yi Wang",
            "Limin Wang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "StreamForest proposes a novel architecture for streaming video understanding, achieving state-of-the-art performance with efficient long-term memory retention and real-time perception enhancement.",
        "tldr_zh": "StreamForest 提出了一个新颖的架构，用于流媒体视频理解，在长期记忆保留和实时感知增强方面取得了最新的成果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "Uni-X: Mitigating Modality Conflict with a Two-End-Separated Architecture for Unified Multimodal Models",
        "summary": "Unified Multimodal Models (UMMs) built on shared autoregressive (AR)\ntransformers are attractive for their architectural simplicity. However, we\nidentify a critical limitation: when trained on multimodal inputs,\nmodality-shared transformers suffer from severe gradient conflicts between\nvision and text, particularly in shallow and deep layers. We trace this issue\nto the fundamentally different low-level statistical properties of images and\ntext, while noting that conflicts diminish in middle layers where\nrepresentations become more abstract and semantically aligned. To overcome this\nchallenge, we propose Uni-X, a two-end-separated, middle-shared architecture.\nUni-X dedicates its initial and final layers to modality-specific processing,\nwhile maintaining shared parameters in the middle layers for high-level\nsemantic fusion. This X-shaped design not only eliminates gradient conflicts at\nboth ends but also further alleviates residual conflicts in the shared layers.\nExtensive experiments validate the effectiveness of Uni-X. Under identical\ntraining conditions, Uni-X achieves superior training efficiency compared to\nstrong baselines. When scaled to 3B parameters with larger training data, Uni-X\nmatches or surpasses 7B AR-based UMMs, achieving a GenEval score of 82 for\nimage generation alongside strong performance in text and vision understanding\ntasks. These results establish Uni-X as a parameter-efficient and scalable\nfoundation for future unified multimodal modeling. Our code is available at\nhttps://github.com/CURRENTF/Uni-X",
        "url": "http://arxiv.org/abs/2509.24365v1",
        "published_date": "2025-09-29T07:05:10+00:00",
        "updated_date": "2025-09-29T07:05:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jitai Hao",
            "Hao Liu",
            "Xinyan Xiao",
            "Qiang Huang",
            "Jun Yu"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer"
        ],
        "tldr": "Uni-X proposes a new architecture to mitigate modality conflict in unified multimodal models, achieving superior training efficiency and performance in image generation and understanding tasks.",
        "tldr_zh": "Uni-X提出了一种新的架构来减轻统一多模态模型中的模态冲突，实现了优越的训练效率和在图像生成和理解任务中的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "Triangle Splatting+: Differentiable Rendering with Opaque Triangles",
        "summary": "Reconstructing 3D scenes and synthesizing novel views has seen rapid progress\nin recent years. Neural Radiance Fields demonstrated that continuous volumetric\nradiance fields can achieve high-quality image synthesis, but their long\ntraining and rendering times limit practicality. 3D Gaussian Splatting (3DGS)\naddressed these issues by representing scenes with millions of Gaussians,\nenabling real-time rendering and fast optimization. However, Gaussian\nprimitives are not natively compatible with the mesh-based pipelines used in VR\nheadsets, and real-time graphics applications. Existing solutions attempt to\nconvert Gaussians into meshes through post-processing or two-stage pipelines,\nwhich increases complexity and degrades visual quality. In this work, we\nintroduce Triangle Splatting+, which directly optimizes triangles, the\nfundamental primitive of computer graphics, within a differentiable splatting\nframework. We formulate triangle parametrization to enable connectivity through\nshared vertices, and we design a training strategy that enforces opaque\ntriangles. The final output is immediately usable in standard graphics engines\nwithout post-processing. Experiments on the Mip-NeRF360 and Tanks & Temples\ndatasets show that Triangle Splatting+achieves state-of-the-art performance in\nmesh-based novel view synthesis. Our method surpasses prior splatting\napproaches in visual fidelity while remaining efficient and fast to training.\nMoreover, the resulting semi-connected meshes support downstream applications\nsuch as physics-based simulation or interactive walkthroughs. The project page\nis https://trianglesplatting2.github.io/trianglesplatting2/.",
        "url": "http://arxiv.org/abs/2509.25122v1",
        "published_date": "2025-09-29T17:43:46+00:00",
        "updated_date": "2025-09-29T17:43:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jan Held",
            "Renaud Vandeghen",
            "Sanghyun Son",
            "Daniel Rebain",
            "Matheus Gadelha",
            "Yi Zhou",
            "Ming C. Lin",
            "Marc Van Droogenbroeck",
            "Andrea Tagliasacchi"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "Triangle Splatting+ introduces a differentiable rendering method for opaque triangles, enabling state-of-the-art performance in mesh-based novel view synthesis with high visual fidelity.",
        "tldr_zh": "Triangle Splatting+引入了一种透明三角形的可微渲染方法，实现了在基于网格的新视角合成中具有行业领先水平的高视觉保真度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "CharGen: Fast and Fluent Portrait Modification",
        "summary": "Interactive editing of character images with diffusion models remains\nchallenging due to the inherent trade-off between fine-grained control,\ngeneration speed, and visual fidelity. We introduce CharGen, a\ncharacter-focused editor that combines attribute-specific Concept Sliders,\ntrained to isolate and manipulate attributes such as facial feature size,\nexpression, and decoration with the StreamDiffusion sampling pipeline for more\ninteractive performance. To counteract the loss of detail that often\naccompanies accelerated sampling, we propose a lightweight Repair Step that\nreinstates fine textures without compromising structural consistency.\nThroughout extensive ablation studies and in comparison to open-source\nInstructPix2Pix and closed-source Google Gemini, and a comprehensive user\nstudy, CharGen achieves two-to-four-fold faster edit turnaround with precise\nediting control and identity-consistent results. Project page:\nhttps://chargen.jdihlmann.com/",
        "url": "http://arxiv.org/abs/2509.25058v1",
        "published_date": "2025-09-29T17:09:30+00:00",
        "updated_date": "2025-09-29T17:09:30+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Jan-Niklas Dihlmann",
            "Arnela Killguss",
            "Hendrik P. A. Lensch"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "CharGen introduces a character-focused editor using Concept Sliders and StreamDiffusion sampling for faster and more precise portrait modification with a lightweight Repair Step. It outperforms existing methods in edit speed and consistency.",
        "tldr_zh": "CharGen通过概念滑块和StreamDiffusion采样引入了一个以角色为中心的编辑器，用于更快速和更精确的肖像修改，提供了一个轻量级的修复步骤。它在编辑速度和一致性方面优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "PanoWorld-X: Generating Explorable Panoramic Worlds via Sphere-Aware Video Diffusion",
        "summary": "Generating a complete and explorable 360-degree visual world enables a wide\nrange of downstream applications. While prior works have advanced the field,\nthey remain constrained by either narrow field-of-view limitations, which\nhinder the synthesis of continuous and holistic scenes, or insufficient camera\ncontrollability that restricts free exploration by users or autonomous agents.\nTo address this, we propose PanoWorld-X, a novel framework for high-fidelity\nand controllable panoramic video generation with diverse camera trajectories.\nSpecifically, we first construct a large-scale dataset of panoramic\nvideo-exploration route pairs by simulating camera trajectories in virtual 3D\nenvironments via Unreal Engine. As the spherical geometry of panoramic data\nmisaligns with the inductive priors from conventional video diffusion, we then\nintroduce a Sphere-Aware Diffusion Transformer architecture that reprojects\nequirectangular features onto the spherical surface to model geometric\nadjacency in latent space, significantly enhancing visual fidelity and\nspatiotemporal continuity. Extensive experiments demonstrate that our\nPanoWorld-X achieves superior performance in various aspects, including motion\nrange, control precision, and visual quality, underscoring its potential for\nreal-world applications.",
        "url": "http://arxiv.org/abs/2509.24997v1",
        "published_date": "2025-09-29T16:22:00+00:00",
        "updated_date": "2025-09-29T16:22:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuyang Yin",
            "HaoXiang Guo",
            "Fangfu Liu",
            "Mengyu Wang",
            "Hanwen Liang",
            "Eric Li",
            "Yikai Wang",
            "Xiaojie Jin",
            "Yao Zhao",
            "Yunchao Wei"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces PanoWorld-X, a framework for generating controllable panoramic videos with high visual fidelity, utilizing a Sphere-Aware Diffusion Transformer architecture for better geometric alignment and visual quality.",
        "tldr_zh": "本文介绍了PanoWorld-X，一个用于生成具有高视觉保真度的可控全景视频的框架，利用Sphere-Aware扩散变换器架构以实现更好的几何对齐和视觉质量。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning",
        "summary": "Video-conditioned sound and speech generation, encompassing video-to-sound\n(V2S) and visual text-to-speech (VisualTTS) tasks, are conventionally addressed\nas separate tasks, with limited exploration to unify them within a signle\nframework. Recent attempts to unify V2S and VisualTTS face challenges in\nhandling distinct condition types (e.g., heterogeneous video and transcript\nconditions) and require complex training stages. Unifying these two tasks\nremains an open problem. To bridge this gap, we present VSSFlow, which\nseamlessly integrates both V2S and VisualTTS tasks into a unified flow-matching\nframework. VSSFlow uses a novel condition aggregation mechanism to handle\ndistinct input signals. We find that cross-attention and self-attention layer\nexhibit different inductive biases in the process of introducing condition.\nTherefore, VSSFlow leverages these inductive biases to effectively handle\ndifferent representations: cross-attention for ambiguous video conditions and\nself-attention for more deterministic speech transcripts. Furthermore, contrary\nto the prevailing belief that joint training on the two tasks requires complex\ntraining strategies and may degrade performance, we find that VSSFlow benefits\nfrom the end-to-end joint learning process for sound and speech generation\nwithout extra designs on training stages. Detailed analysis attributes it to\nthe learned general audio prior shared between tasks, which accelerates\nconvergence, enhances conditional generation, and stabilizes the\nclassifier-free guidance process. Extensive experiments demonstrate that\nVSSFlow surpasses the state-of-the-art domain-specific baselines on both V2S\nand VisualTTS benchmarks, underscoring the critical potential of unified\ngenerative models.",
        "url": "http://arxiv.org/abs/2509.24773v1",
        "published_date": "2025-09-29T13:38:24+00:00",
        "updated_date": "2025-09-29T13:38:24+00:00",
        "categories": [
            "eess.AS",
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.SD"
        ],
        "authors": [
            "Xin Cheng",
            "Yuyue Wang",
            "Xihua Wang",
            "Yihan Wu",
            "Kaisi Guan",
            "Yijing Chen",
            "Peng Zhang",
            "Xiaojiang Liu",
            "Meng Cao",
            "Ruihua Song"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces VSSFlow, a framework that combines video-to-sound and visual text-to-speech tasks into a unified flow-matching framework. It leverages cross-attention and self-attention mechanisms to handle different input signals, showing superior performance over existing methods.",
        "tldr_zh": "本文介绍了VSSFlow框架，将视频到声音和视觉文本到语音任务结合到一个统一的流匹配框架中。它利用交叉注意力和自注意力机制处理不同的输入信号，在现有方法的基础上展现出优越的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Collaborating Vision, Depth, and Thermal Signals for Multi-Modal Tracking: Dataset and Algorithm",
        "summary": "Existing multi-modal object tracking approaches primarily focus on dual-modal\nparadigms, such as RGB-Depth or RGB-Thermal, yet remain challenged in complex\nscenarios due to limited input modalities. To address this gap, this work\nintroduces a novel multi-modal tracking task that leverages three complementary\nmodalities, including visible RGB, Depth (D), and Thermal Infrared (TIR),\naiming to enhance robustness in complex scenarios. To support this task, we\nconstruct a new multi-modal tracking dataset, coined RGBDT500, which consists\nof 500 videos with synchronised frames across the three modalities. Each frame\nprovides spatially aligned RGB, depth, and thermal infrared images with precise\nobject bounding box annotations. Furthermore, we propose a novel multi-modal\ntracker, dubbed RDTTrack. RDTTrack integrates tri-modal information for robust\ntracking by leveraging a pretrained RGB-only tracking model and prompt learning\ntechniques. In specific, RDTTrack fuses thermal infrared and depth modalities\nunder a proposed orthogonal projection constraint, then integrates them with\nRGB signals as prompts for the pre-trained foundation tracking model,\neffectively harmonising tri-modal complementary cues. The experimental results\ndemonstrate the effectiveness and advantages of the proposed method, showing\nsignificant improvements over existing dual-modal approaches in terms of\ntracking accuracy and robustness in complex scenarios.",
        "url": "http://arxiv.org/abs/2509.24741v1",
        "published_date": "2025-09-29T13:05:15+00:00",
        "updated_date": "2025-09-29T13:05:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xue-Feng Zhu",
            "Tianyang Xu",
            "Yifan Pan",
            "Jinjie Gu",
            "Xi Li",
            "Jiwen Lu",
            "Xiao-Jun Wu",
            "Josef Kittler"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a novel multi-modal object tracking task using RGB, Depth, and Thermal Infrared signals, along with a new dataset and tracker. It shows significant improvements over existing dual-modal approaches in complex scenarios.",
        "tldr_zh": "该论文介绍了一种新的多模态物体跟踪任务，利用RGB、深度和红外热像信号，以及新的数据集和跟踪器。在复杂场景中明显优于现有的双模态方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Enhancing Physical Plausibility in Video Generation by Reasoning the Implausibility",
        "summary": "Diffusion models can generate realistic videos, but existing methods rely on\nimplicitly learning physical reasoning from large-scale text-video datasets,\nwhich is costly, difficult to scale, and still prone to producing implausible\nmotions that violate fundamental physical laws. We introduce a training-free\nframework that improves physical plausibility at inference time by explicitly\nreasoning about implausibility and guiding the generation away from it.\nSpecifically, we employ a lightweight physics-aware reasoning pipeline to\nconstruct counterfactual prompts that deliberately encode physics-violating\nbehaviors. Then, we propose a novel Synchronized Decoupled Guidance (SDG)\nstrategy, which leverages these prompts through synchronized directional\nnormalization to counteract lagged suppression and trajectory-decoupled\ndenoising to mitigate cumulative trajectory bias, ensuring that implausible\ncontent is suppressed immediately and consistently throughout denoising.\nExperiments across different physical domains show that our approach\nsubstantially enhances physical fidelity while maintaining photorealism,\ndespite requiring no additional training. Ablation studies confirm the\ncomplementary effectiveness of both the physics-aware reasoning component and\nSDG. In particular, the aforementioned two designs of SDG are also individually\nvalidated to contribute critically to the suppression of implausible content\nand the overall gains in physical plausibility. This establishes a new and\nplug-and-play physics-aware paradigm for video generation.",
        "url": "http://arxiv.org/abs/2509.24702v1",
        "published_date": "2025-09-29T12:32:54+00:00",
        "updated_date": "2025-09-29T12:32:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yutong Hao",
            "Chen Chen",
            "Ajmal Saeed Mian",
            "Chang Xu",
            "Daochang Liu"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Other"
        ],
        "tldr": "The paper introduces a training-free framework to improve physical plausibility in video generation by explicitly reasoning about implausibility and guiding the generation away from it.",
        "tldr_zh": "本文介绍了一种无需训练的框架，通过明确理解不合理性并引导生成避免不合理性，从而改善视频生成中的物理可信度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Learning Object-Centric Representations Based on Slots in Real World Scenarios",
        "summary": "A central goal in AI is to represent scenes as compositions of discrete\nobjects, enabling fine-grained, controllable image and video generation. Yet\nleading diffusion models treat images holistically and rely on text\nconditioning, creating a mismatch for object-level editing. This thesis\nintroduces a framework that adapts powerful pretrained diffusion models for\nobject-centric synthesis while retaining their generative capacity.\n  We identify a core challenge: balancing global scene coherence with\ndisentangled object control. Our method integrates lightweight, slot-based\nconditioning into pretrained models, preserving their visual priors while\nproviding object-specific manipulation. For images, SlotAdapt augments\ndiffusion models with a register token for background/style and\nslot-conditioned modules for objects, reducing text-conditioning bias and\nachieving state-of-the-art results in object discovery, segmentation,\ncompositional editing, and controllable image generation.\n  We further extend the framework to video. Using Invariant Slot Attention\n(ISA) to separate object identity from pose and a Transformer-based temporal\naggregator, our approach maintains consistent object representations and\ndynamics across frames. This yields new benchmarks in unsupervised video object\nsegmentation and reconstruction, and supports advanced editing tasks such as\nobject removal, replacement, and insertion without explicit supervision.\n  Overall, this work establishes a general and scalable approach to\nobject-centric generative modeling for images and videos. By bridging human\nobject-based perception and machine learning, it expands the design space for\ninteractive, structured, and user-driven generative tools in creative,\nscientific, and practical domains.",
        "url": "http://arxiv.org/abs/2509.24652v1",
        "published_date": "2025-09-29T12:01:49+00:00",
        "updated_date": "2025-09-29T12:01:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Adil Kaan Akan"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a framework for generating object-centric representations in images and videos by adapting pretrained diffusion models using slot-based conditioning.",
        "tldr_zh": "该论文介绍了一种通过使用基于槽的条件化来调整预训练扩散模型以生成图像和视频中的物体中心表示的框架。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "FreeRet: MLLMs as Training-Free Retrievers",
        "summary": "Multimodal large language models (MLLMs) are emerging as versatile\nfoundations for mixed-modality retrieval. Yet, they often require heavy\npost-hoc training to convert them into contrastive encoders for retrieval. This\nwork asks: Can off-the-shelf MLLMs serve as powerful retrievers without\nadditional training? We present FreeRet, a plug-and-play framework that turns\nany MLLM into a two-stage retriever. FreeRet first derives semantically\ngrounded embeddings directly from the model for fast candidate search, and then\nexploits its reasoning ability for precise reranking. The framework contributes\nthree advances: bypassing lexical alignment layers to obtain semantically\nfaithful embeddings, conditioning representation generation with explicit\npriors, and mitigating framing effect in reranking via neutral choice framing.\nOn the MMEB and MMEB-V2 benchmarks spanning 46 datasets, FreeRet substantially\noutperforms models trained on millions of pairs. Beyond benchmarks, FreeRet is\nmodel-agnostic and scales seamlessly across MLLM families and sizes, preserves\ntheir generative abilities, supports arbitrary modality combinations, and\nunifies retrieval, reranking, and generation into end-to-end RAG within a\nsingle model. Our findings demonstrate that pretrained MLLMs, when carefully\nharnessed, can serve as strong retrieval engines without training, closing a\ncritical gap in their role as generalists.",
        "url": "http://arxiv.org/abs/2509.24621v1",
        "published_date": "2025-09-29T11:28:42+00:00",
        "updated_date": "2025-09-29T11:28:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuhan Zhu",
            "Xiangyu Zeng",
            "Chenting Wang",
            "Xinhao Li",
            "Yicheng Xu",
            "Ziang Yan",
            "Yi Wang",
            "Limin Wang"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces FreeRet, a framework that allows off-the-shelf MLLMs to be used as powerful retrievers without additional training, closing a critical gap in their role as generalists.",
        "tldr_zh": "本文介绍了FreeRet，一个框架，可以使现成的MLLMs无需额外训练即可成为强大的检索器，填补了它们作为通用工具的重要缺口。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Instruction Guided Multi Object Image Editing with Quantity and Layout Consistency",
        "summary": "Instruction driven image editing with standard CLIP text encoders often fails\nin complex scenes with many objects. We present QL-Adapter, a framework for\nmultiple object editing that tackles two challenges: enforcing object counts\nand spatial layouts, and accommodating diverse categories. QL-Adapter consists\nof two core modules: the Image-Layout Fusion Module (ILFM) and the Cross-Modal\nAugmentation Module (CMAM). ILFM fuses layout priors with ViT patch tokens from\nthe CLIP image encoder to strengthen spatial structure understanding. CMAM\ninjects image features into the text branch to enrich textual embeddings and\nimprove instruction following. We further build QL-Dataset, a benchmark that\nspans broad category, layout, and count variations, and define the task of\nquantity and layout consistent image editing (QL-Edit). Extensive experiments\nshow that QL-Adapter achieves state of the art performance on QL-Edit and\nsignificantly outperforms existing models.",
        "url": "http://arxiv.org/abs/2509.24514v1",
        "published_date": "2025-09-29T09:33:51+00:00",
        "updated_date": "2025-09-29T09:33:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaqi Tan",
            "Fangyu Li",
            "Yang Liu"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces QL-Adapter, a framework for editing multiple objects in images that enforces object counts and layouts while accommodating diverse categories. It outperforms existing models on quantity and layout consistent image editing.",
        "tldr_zh": "本文介绍了QL-Adapter，一个用于编辑图像中多个对象的框架，强调对象计数和布局，并适应不同类别。它在数量和布局一致的图像编辑上胜过现有模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Mitigating Visual Hallucinations via Semantic Curriculum Preference Optimization in MLLMs",
        "summary": "Multimodal Large Language Models (MLLMs) have significantly improved the\nperformance of various tasks, but continue to suffer from visual\nhallucinations, a critical issue where generated responses contradict visual\nevidence. While Direct Preference Optimization(DPO) is widely used for\nalignment, its application to MLLMs often fails to capture fine-grained\nsemantic differences and encourages shortcut learning. To address these\nchallenges, we propose Semantic Curriculum Preference Optimization (SCPO), a\nnovel framework for MLLM alignment. SCPO employs a progressive, easy-to-hard\ncurriculum built upon our Semantic Curriculum Preference Pairs dataset, which\nprovides fine-grained semantic contrasts sorted by difficulty. This curriculum\nis trained with a dynamic reference model and a novel symmetric, bidirectional\nobjective to facilitate simultaneous learning from both textual and visual\npreferences. To our knowledge, SCPO is the first framework to unify semantics,\nsymmetry, and curriculum for MLLMs alignment, effectively mitigating visual\nhallucinations. Extensive experiments on LLaVA models across various scales and\nversions validate that SCPO demonstrates superior performance compared to\nbaseline models on multiple hallucination benchmarks, reducing the\nhallucination rate by up to 62.9%. Moreover, evaluations on generalized\nbenchmarks show that SCPO improves factuality while preserving general\ncapabilities, with its performance remaining stable across general\nvision-language benchmarks.",
        "url": "http://arxiv.org/abs/2509.24491v1",
        "published_date": "2025-09-29T09:03:36+00:00",
        "updated_date": "2025-09-29T09:03:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuanshuai Li",
            "Yuping Yan",
            "Junfeng Tang",
            "Yunxuan Li",
            "Zeqi Zheng",
            "Yaochu Jin"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes a framework called SCPO to address visual hallucinations in Multimodal Large Language Models (MLLMs) by optimizing semantic curriculum preferences, demonstrating up to 62.9% reduction in hallucination rate.",
        "tldr_zh": "该论文提出了一个名为SCPO的框架，通过优化语义课程首选项来解决多模态大语言模型（MLLMs）中的视觉幻觉问题，最多减少62.9%的幻觉率。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "UI2V-Bench: An Understanding-based Image-to-video Generation Benchmark",
        "summary": "Generative diffusion models are developing rapidly and attracting increasing\nattention due to their wide range of applications. Image-to-Video (I2V)\ngeneration has become a major focus in the field of video synthesis. However,\nexisting evaluation benchmarks primarily focus on aspects such as video quality\nand temporal consistency, while largely overlooking the model's ability to\nunderstand the semantics of specific subjects in the input image or to ensure\nthat the generated video aligns with physical laws and human commonsense. To\naddress this gap, we propose UI2V-Bench, a novel benchmark for evaluating I2V\nmodels with a focus on semantic understanding and reasoning. It introduces four\nprimary evaluation dimensions: spatial understanding, attribute binding,\ncategory understanding, and reasoning. To assess these dimensions, we design\ntwo evaluation methods based on Multimodal Large Language Models (MLLMs): an\ninstance-level pipeline for fine-grained semantic understanding, and a\nfeedback-based reasoning pipeline that enables step-by-step causal assessment\nfor more accurate evaluation. UI2V-Bench includes approximately 500 carefully\nconstructed text-image pairs and evaluates a range of both open source and\nclosed-source I2V models across all defined dimensions. We further incorporate\nhuman evaluations, which show strong alignment with the proposed MLLM-based\nmetrics. Overall, UI2V-Bench fills a critical gap in I2V evaluation by\nemphasizing semantic comprehension and reasoning ability, offering a robust\nframework and dataset to support future research and model development in the\nfield.",
        "url": "http://arxiv.org/abs/2509.24427v1",
        "published_date": "2025-09-29T08:14:26+00:00",
        "updated_date": "2025-09-29T08:14:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ailing Zhang",
            "Lina Lei",
            "Dehong Kong",
            "Zhixin Wang",
            "Jiaqi Xu",
            "Fenglong Song",
            "Chun-Le Guo",
            "Chang Liu",
            "Fan Li",
            "Jie Chen"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "UI2V-Bench introduces a new benchmark for evaluating Image-to-Video models focusing on semantic understanding and reasoning.",
        "tldr_zh": "UI2V-Bench 提出了一个新的基准，用于评估图片到视频模型，重点关注语义理解和推理能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "From Satellite to Street: A Hybrid Framework Integrating Stable Diffusion and PanoGAN for Consistent Cross-View Synthesis",
        "summary": "Street view imagery has become an essential source for geospatial data\ncollection and urban analytics, enabling the extraction of valuable insights\nthat support informed decision-making. However, synthesizing street-view images\nfrom corresponding satellite imagery presents significant challenges due to\nsubstantial differences in appearance and viewing perspective between these two\ndomains. This paper presents a hybrid framework that integrates diffusion-based\nmodels and conditional generative adversarial networks to generate\ngeographically consistent street-view images from satellite imagery. Our\napproach uses a multi-stage training strategy that incorporates Stable\nDiffusion as the core component within a dual-branch architecture. To enhance\nthe framework's capabilities, we integrate a conditional Generative Adversarial\nNetwork (GAN) that enables the generation of geographically consistent\npanoramic street views. Furthermore, we implement a fusion strategy that\nleverages the strengths of both models to create robust representations,\nthereby improving the geometric consistency and visual quality of the generated\nstreet-view images. The proposed framework is evaluated on the challenging\nCross-View USA (CVUSA) dataset, a standard benchmark for cross-view image\nsynthesis. Experimental results demonstrate that our hybrid approach\noutperforms diffusion-only methods across multiple evaluation metrics and\nachieves competitive performance compared to state-of-the-art GAN-based\nmethods. The framework successfully generates realistic and geometrically\nconsistent street-view images while preserving fine-grained local details,\nincluding street markings, secondary roads, and atmospheric elements such as\nclouds.",
        "url": "http://arxiv.org/abs/2509.24369v1",
        "published_date": "2025-09-29T07:14:49+00:00",
        "updated_date": "2025-09-29T07:14:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "authors": [
            "Khawlah Bajbaa",
            "Abbas Anwar",
            "Muhammad Saqib",
            "Hafeez Anwar",
            "Nabin Sharma",
            "Muhammad Usman"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper presents a hybrid framework combining diffusion-based models and GANs to generate consistent street-view images from satellite imagery, outperforming diffusion-only methods and competing with state-of-the-art GAN-based methods.",
        "tldr_zh": "本论文提出了一种混合框架，结合了基于扩散的模型和GAN，从卫星图像生成一致的街景图像，胜过了仅使用扩散的方法，并与最先进的基于GAN的方法竞争。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "BRIDGE - Building Reinforcement-Learning Depth-to-Image Data Generation Engine for Monocular Depth Estimation",
        "summary": "Monocular Depth Estimation (MDE) is a foundational task for computer vision.\nTraditional methods are limited by data scarcity and quality, hindering their\nrobustness. To overcome this, we propose BRIDGE, an RL-optimized depth-to-image\n(D2I) generation framework that synthesizes over 20M realistic and\ngeometrically accurate RGB images, each intrinsically paired with its ground\ntruth depth, from diverse source depth maps. Then we train our depth estimation\nmodel on this dataset, employing a hybrid supervision strategy that integrates\nteacher pseudo-labels with ground truth depth for comprehensive and robust\ntraining. This innovative data generation and training paradigm enables BRIDGE\nto achieve breakthroughs in scale and domain diversity, consistently\noutperforming existing state-of-the-art approaches quantitatively and in\ncomplex scene detail capture, thereby fostering general and robust depth\nfeatures. Code and models are available at\nhttps://dingning-liu.github.io/bridge.github.io/.",
        "url": "http://arxiv.org/abs/2509.25077v1",
        "published_date": "2025-09-29T17:19:45+00:00",
        "updated_date": "2025-09-29T17:19:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Dingning Liu",
            "Haoyu Guo",
            "Jingyi Zhou",
            "Tong He"
        ],
        "ai_categories": [
            "Dataset",
            "GAN"
        ],
        "tldr": "The paper introduces BRIDGE, an RL-optimized depth-to-image generation engine for monocular depth estimation. It outperforms existing methods in scale and domain diversity.",
        "tldr_zh": "本文介绍了BRIDGE，一种用于单眼深度估计的RL优化的深度到图像生成引擎。在规模和领域多样性方面优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "A Scalable Distributed Framework for Multimodal GigaVoxel Image Registration",
        "summary": "In this work, we propose FFDP, a set of IO-aware non-GEMM fused kernels\nsupplemented with a distributed framework for image registration at\nunprecedented scales. Image registration is an inverse problem fundamental to\nbiomedical and life sciences, but algorithms have not scaled in tandem with\nimage acquisition capabilities. Our framework complements existing model\nparallelism techniques proposed for large-scale transformer training by\noptimizing non-GEMM bottlenecks and enabling convolution-aware tensor sharding.\nWe demonstrate unprecedented capabilities by performing multimodal registration\nof a 100 micron ex-vivo human brain MRI volume at native resolution - an\ninverse problem more than 570x larger than a standard clinical datum in about a\nminute using only 8 A6000 GPUs. FFDP accelerates existing state-of-the-art\noptimization and deep learning registration pipelines by upto 6 - 7x while\nreducing peak memory consumption by 20 - 59%. Comparative analysis on a 250\nmicron dataset shows that FFDP can fit upto 64x larger problems than existing\nSOTA on a single GPU, and highlights both the performance and efficiency gains\nof FFDP compared to SOTA image registration methods.",
        "url": "http://arxiv.org/abs/2509.25044v1",
        "published_date": "2025-09-29T16:58:40+00:00",
        "updated_date": "2025-09-29T16:58:40+00:00",
        "categories": [
            "cs.CV",
            "cs.DC"
        ],
        "authors": [
            "Rohit Jena",
            "Vedant Zope",
            "Pratik Chaudhari",
            "James C. Gee"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "FFDP is a distributed framework for multimodal image registration that accelerates optimization and deep learning pipelines while reducing memory consumption.",
        "tldr_zh": "FFDP是一个用于多模态图像配准的分布式框架，可以加速优化和深度学习管线，同时减少内存消耗。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning",
        "summary": "Few-shot learning (FSL) aims to recognize novel concepts from only a few\nlabeled support samples. Recent studies enhance support features by\nincorporating additional semantic information or designing complex semantic\nfusion modules. However, they still suffer from hallucinating semantics that\ncontradict the visual evidence due to the lack of grounding in actual\ninstances, resulting in noisy guidance and costly corrections. To address these\nissues, we propose a novel framework, bridging Vision and Text with LLMs for\nFew-Shot Learning (VT-FSL), which constructs precise cross-modal prompts\nconditioned on Large Language Models (LLMs) and support images, seamlessly\nintegrating them through a geometry-aware alignment. It mainly consists of\nCross-modal Iterative Prompting (CIP) and Cross-modal Geometric Alignment\n(CGA). Specifically, the CIP conditions an LLM on both class names and support\nimages to generate precise class descriptions iteratively in a single\nstructured reasoning pass. These descriptions not only enrich the semantic\nunderstanding of novel classes but also enable the zero-shot synthesis of\nsemantically consistent images. The descriptions and synthetic images act\nrespectively as complementary textual and visual prompts, providing high-level\nclass semantics and low-level intra-class diversity to compensate for limited\nsupport data. Furthermore, the CGA jointly aligns the fused textual, support,\nand synthetic visual representations by minimizing the kernelized volume of the\n3-dimensional parallelotope they span. It captures global and nonlinear\nrelationships among all representations, enabling structured and consistent\nmultimodal integration. The proposed VT-FSL method establishes new\nstate-of-the-art performance across ten diverse benchmarks, including standard,\ncross-domain, and fine-grained few-shot learning scenarios. Code is available\nat https://github.com/peacelwh/VT-FSL.",
        "url": "http://arxiv.org/abs/2509.25033v1",
        "published_date": "2025-09-29T16:52:47+00:00",
        "updated_date": "2025-09-29T16:52:47+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "I.4.9"
        ],
        "authors": [
            "Wenhao Li",
            "Qiangchang Wang",
            "Xianjing Meng",
            "Zhibin Wu",
            "Yilong Yin"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel framework (VT-FSL) for few-shot learning that integrates vision and text with Large Language Models, achieving state-of-the-art performance on various benchmarks.",
        "tldr_zh": "该论文提出了一个新颖的框架（VT-FSL）用于少样本学习，将视觉和文本与大型语言模型结合，实现了各种基准测试的最先进性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "SDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose Estimation",
        "summary": "Pre-trained diffusion models provide rich multi-scale latent features and are\nemerging as powerful vision backbones. While recent works such as\nMarigold~\\citep{ke2024repurposing} and Lotus~\\citep{he2024lotus} adapt\ndiffusion priors for dense prediction with strong cross-domain generalization,\ntheir potential for structured outputs (e.g., human pose estimation) remains\nunderexplored. In this paper, we propose \\textbf{SDPose}, a fine-tuning\nframework built upon Stable Diffusion to fully exploit pre-trained diffusion\npriors for human pose estimation. First, rather than modifying cross-attention\nmodules or introducing learnable embeddings, we directly predict keypoint\nheatmaps in the SD U-Net's image latent space to preserve the original\ngenerative priors. Second, we map these latent features into keypoint heatmaps\nthrough a lightweight convolutional pose head, which avoids disrupting the\npre-trained backbone. Finally, to prevent overfitting and enhance\nout-of-distribution robustness, we incorporate an auxiliary RGB reconstruction\nbranch that preserves domain-transferable generative semantics. To evaluate\nrobustness under domain shift, we further construct \\textbf{COCO-OOD}, a\nstyle-transferred variant of COCO with preserved annotations. With just\none-fifth of the training schedule used by Sapiens on COCO, SDPose attains\nparity with Sapiens-1B/2B on the COCO validation set and establishes a new\nstate of the art on the cross-domain benchmarks HumanArt and COCO-OOD.\nFurthermore, we showcase SDPose as a zero-shot pose annotator for downstream\ncontrollable generation tasks, including ControlNet-based image synthesis and\nvideo generation, where it delivers qualitatively superior pose guidance.",
        "url": "http://arxiv.org/abs/2509.24980v1",
        "published_date": "2025-09-29T16:09:03+00:00",
        "updated_date": "2025-09-29T16:09:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuang Liang",
            "Jing He",
            "Chuanmeizhi Wang",
            "Lejun Liao",
            "Guo Zhang",
            "Yingcong Chen",
            "Yuan Yuan"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces SDPose, a framework that leverages pre-trained diffusion models for human pose estimation, achieving state-of-the-art results on cross-domain benchmarks and enabling zero-shot pose annotation for image and video generation tasks.",
        "tldr_zh": "该论文引入了SDPose框架，利用预训练扩散模型进行人体姿势估计，在跨领域基准测试上取得了最先进的结果，并实现了零样本姿势注释用于图像和视频生成任务。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "VAGUEGAN: Stealthy Poisoning and Backdoor Attacks on Image Generative Pipelines",
        "summary": "Generative models such as GANs and diffusion models are widely used to\nsynthesize photorealistic images and to support downstream creative and editing\ntasks. While adversarial attacks on discriminative models are well studied,\nattacks targeting generative pipelines where small, stealthy perturbations in\ninputs lead to controlled changes in outputs are less explored. This study\nintroduces VagueGAN, an attack pipeline combining a modular perturbation\nnetwork PoisonerNet with a Generator Discriminator pair to craft stealthy\ntriggers that cause targeted changes in generated images. Attack efficacy is\nevaluated using a custom proxy metric, while stealth is analyzed through\nperceptual and frequency domain measures. The transferability of the method to\na modern diffusion based pipeline is further examined through ControlNet guided\nediting. Interestingly, the experiments show that poisoned outputs can display\nhigher visual quality compared to clean counterparts, challenging the\nassumption that poisoning necessarily reduces fidelity. Unlike conventional\npixel level perturbations, latent space poisoning in GANs and diffusion\npipelines can retain or even enhance output aesthetics, exposing a blind spot\nin pixel level defenses. Moreover, carefully optimized perturbations can\nproduce consistent, stealthy effects on generator outputs while remaining\nvisually inconspicuous, raising concerns for the integrity of image generation\npipelines.",
        "url": "http://arxiv.org/abs/2509.24891v1",
        "published_date": "2025-09-29T15:02:31+00:00",
        "updated_date": "2025-09-29T15:02:31+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Mostafa Mohaimen Akand Faisal",
            "Rabeya Amin Jhuma"
        ],
        "ai_categories": [
            "GAN",
            "Diffusion",
            "Transformer"
        ],
        "tldr": "VagueGAN introduces a stealthy poisoning attack on image generative pipelines that can cause controlled changes in outputs without affecting visual quality.",
        "tldr_zh": "VagueGAN引入了一种对图像生成管道的隐蔽毒化攻击，可以在不影响视觉质量的情况下引起控制性的输出变化。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "A TRIANGLE Enables Multimodal Alignment Beyond Cosine Similarity",
        "summary": "Multimodal learning plays a pivotal role in advancing artificial intelligence\nsystems by incorporating information from multiple modalities to build a more\ncomprehensive representation. Despite its importance, current state-of-the-art\nmodels still suffer from severe limitations that prevent the successful\ndevelopment of a fully multimodal model. Such methods may not provide\nindicators that all the involved modalities are effectively aligned. As a\nresult, some modalities may not be aligned, undermining the effectiveness of\nthe model in downstream tasks where multiple modalities should provide\nadditional information that the model fails to exploit. In this paper, we\npresent TRIANGLE: TRI-modAl Neural Geometric LEarning, the novel proposed\nsimilarity measure that is directly computed in the higher-dimensional space\nspanned by the modality embeddings. TRIANGLE improves the joint alignment of\nthree modalities via a triangle-area similarity, avoiding additional fusion\nlayers or pairwise similarities. When incorporated in contrastive losses\nreplacing cosine similarity, TRIANGLE significantly boosts the performance of\nmultimodal modeling, while yielding interpretable alignment rationales.\nExtensive evaluation in three-modal tasks such as video-text and audio-text\nretrieval or audio-video classification, demonstrates that TRIANGLE achieves\nstate-of-the-art results across different datasets improving the performance of\ncosine-based methods up to 9 points of Recall@1.",
        "url": "http://arxiv.org/abs/2509.24734v1",
        "published_date": "2025-09-29T12:58:46+00:00",
        "updated_date": "2025-09-29T12:58:46+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Giordano Cicchetti",
            "Eleonora Grassucci",
            "Danilo Comminiello"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer"
        ],
        "tldr": "The paper introduces TRIANGLE, a new similarity measure for aligning three modalities in multimodal learning models, improving performance and interpretability.",
        "tldr_zh": "本文介绍了TRIANGLE，一种新的相似度度量，用于在多模态学习模型中对三种模态进行对齐，提高性能和解释性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer",
        "summary": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration.",
        "url": "http://arxiv.org/abs/2509.24695v1",
        "published_date": "2025-09-29T12:28:09+00:00",
        "updated_date": "2025-09-29T12:28:09+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Junsong Chen",
            "Yuyang Zhao",
            "Jincheng Yu",
            "Ruihang Chu",
            "Junyu Chen",
            "Shuai Yang",
            "Xianbang Wang",
            "Yicheng Pan",
            "Daquan Zhou",
            "Huan Ling",
            "Haozhe Liu",
            "Hongwei Yi",
            "Hao Zhang",
            "Muyang Li",
            "Yukang Chen",
            "Han Cai",
            "Sanja Fidler",
            "Ping Luo",
            "Song Han",
            "Enze Xie"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer",
            "Dataset"
        ],
        "tldr": "SANA-Video is an efficient video generation model that can produce high-resolution, high-quality videos at a fast speed, surpassing other models in both cost and performance.",
        "tldr_zh": "SANA-Video是一种高效的视频生成模型，可以以更快的速度生成高分辨率、高质量的视频，在成本和性能方面超越其他模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "CMT: Mid-Training for Efficient Learning of Consistency, Mean Flow, and Flow Map Models",
        "summary": "Flow map models such as Consistency Models (CM) and Mean Flow (MF) enable\nfew-step generation by learning the long jump of the ODE solution of diffusion\nmodels, yet training remains unstable, sensitive to hyperparameters, and\ncostly. Initializing from a pre-trained diffusion model helps, but still\nrequires converting infinitesimal steps into a long-jump map, leaving\ninstability unresolved. We introduce mid-training, the first concept and\npractical method that inserts a lightweight intermediate stage between the\n(diffusion) pre-training and the final flow map training (i.e., post-training)\nfor vision generation. Concretely, Consistency Mid-Training (CMT) is a compact\nand principled stage that trains a model to map points along a solver\ntrajectory from a pre-trained model, starting from a prior sample, directly to\nthe solver-generated clean sample. It yields a trajectory-consistent and stable\ninitialization. This initializer outperforms random and diffusion-based\nbaselines and enables fast, robust convergence without heuristics. Initializing\npost-training with CMT weights further simplifies flow map learning.\nEmpirically, CMT achieves state of the art two step FIDs: 1.97 on CIFAR-10,\n1.32 on ImageNet 64x64, and 1.84 on ImageNet 512x512, while using up to 98%\nless training data and GPU time, compared to CMs. On ImageNet 256x256, CMT\nreaches 1-step FID 3.34 while cutting total training time by about 50% compared\nto MF from scratch (FID 3.43). This establishes CMT as a principled, efficient,\nand general framework for training flow map models.",
        "url": "http://arxiv.org/abs/2509.24526v1",
        "published_date": "2025-09-29T09:42:08+00:00",
        "updated_date": "2025-09-29T09:42:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Zheyuan Hu",
            "Chieh-Hsin Lai",
            "Yuki Mitsufuji",
            "Stefano Ermon"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "Dataset"
        ],
        "tldr": "This paper introduces a new method called Consistency Mid-Training (CMT) to improve the training of flow map models for vision generation, achieving state-of-the-art results with less data and GPU time.",
        "tldr_zh": "本文介绍了一种名为一致性中间训练（CMT）的新方法，用于改进视觉生成的流图模型的训练，以少量数据和GPU时间实现最新的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "ReCon-GS: Continuum-Preserved Guassian Streaming for Fast and Compact Reconstruction of Dynamic Scenes",
        "summary": "Online free-viewpoint video (FVV) reconstruction is challenged by slow\nper-frame optimization, inconsistent motion estimation, and unsustainable\nstorage demands. To address these challenges, we propose the Reconfigurable\nContinuum Gaussian Stream, dubbed ReCon-GS, a novel storage-aware framework\nthat enables high fidelity online dynamic scene reconstruction and real-time\nrendering. Specifically, we dynamically allocate multi-level Anchor Gaussians\nin a density-adaptive fashion to capture inter-frame geometric deformations,\nthereby decomposing scene motion into compact coarse-to-fine representations.\nThen, we design a dynamic hierarchy reconfiguration strategy that preserves\nlocalized motion expressiveness through on-demand anchor re-hierarchization,\nwhile ensuring temporal consistency through intra-hierarchical deformation\ninheritance that confines transformation priors to their respective hierarchy\nlevels. Furthermore, we introduce a storage-aware optimization mechanism that\nflexibly adjusts the density of Anchor Gaussians at different hierarchy levels,\nenabling a controllable trade-off between reconstruction fidelity and memory\nusage. Extensive experiments on three widely used datasets demonstrate that,\ncompared to state-of-the-art methods, ReCon-GS improves training efficiency by\napproximately 15% and achieves superior FVV synthesis quality with enhanced\nrobustness and stability. Moreover, at equivalent rendering quality, ReCon-GS\nslashes memory requirements by over 50% compared to leading state-of-the-art\nmethods.",
        "url": "http://arxiv.org/abs/2509.24325v1",
        "published_date": "2025-09-29T06:23:47+00:00",
        "updated_date": "2025-09-29T06:23:47+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Jiaye Fu",
            "Qiankun Gao",
            "Chengxiang Wen",
            "Yanmin Wu",
            "Siwei Ma",
            "Jiaqi Zhang",
            "Jian Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces ReCon-GS, a framework for fast and compact reconstruction of dynamic scenes in online free-viewpoint video applications, achieving superior quality and efficiency compared to state-of-the-art methods.",
        "tldr_zh": "该论文介绍了ReCon-GS，这是一个用于在线自由视点视频动态场景快速、紧凑重构的框架，与现有方法相比，质量和效率都更高。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "EVLF-FM: Explainable Vision Language Foundation Model for Medicine",
        "summary": "Despite the promise of foundation models in medical AI, current systems\nremain limited - they are modality-specific and lack transparent reasoning\nprocesses, hindering clinical adoption. To address this gap, we present\nEVLF-FM, a multimodal vision-language foundation model (VLM) designed to unify\nbroad diagnostic capability with fine-grain explainability. The development and\ntesting of EVLF-FM encompassed over 1.3 million total samples from 23 global\ndatasets across eleven imaging modalities related to six clinical specialties:\ndermatology, hepatology, ophthalmology, pathology, pulmonology, and radiology.\nExternal validation employed 8,884 independent test samples from 10 additional\ndatasets across five imaging modalities. Technically, EVLF-FM is developed to\nassist with multiple disease diagnosis and visual question answering with\npixel-level visual grounding and reasoning capabilities. In internal validation\nfor disease diagnostics, EVLF-FM achieved the highest average accuracy (0.858)\nand F1-score (0.797), outperforming leading generalist and specialist models.\nIn medical visual grounding, EVLF-FM also achieved stellar performance across\nnine modalities with average mIOU of 0.743 and Acc@0.5 of 0.837. External\nvalidations further confirmed strong zero-shot and few-shot performance, with\ncompetitive F1-scores despite a smaller model size. Through a hybrid training\nstrategy combining supervised and visual reinforcement fine-tuning, EVLF-FM not\nonly achieves state-of-the-art accuracy but also exhibits step-by-step\nreasoning, aligning outputs with visual evidence. EVLF-FM is an early\nmulti-disease VLM model with explainability and reasoning capabilities that\ncould advance adoption of and trust in foundation models for real-world\nclinical deployment.",
        "url": "http://arxiv.org/abs/2509.24231v1",
        "published_date": "2025-09-29T03:15:57+00:00",
        "updated_date": "2025-09-29T03:15:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yang Bai",
            "Haoran Cheng",
            "Yang Zhou",
            "Jun Zhou",
            "Arun Thirunavukarasu",
            "Yuhe Ke",
            "Jie Yao",
            "Kanae Fukutsu",
            "Chrystie Wan Ning Quek",
            "Ashley Hong",
            "Laura Gutierrez",
            "Zhen Ling Teo",
            "Darren Shu Jeng Ting",
            "Brian T. Soetikno",
            "Christopher S. Nielsen",
            "Tobias Elze",
            "Zengxiang Li",
            "Linh Le Dinh",
            "Hiok Hong Chan",
            "Victor Koh",
            "Marcus Tan",
            "Kelvin Z. Li",
            "Leonard Yip",
            "Ching Yu Cheng",
            "Yih Chung Tham",
            "Gavin Siew Wei Tan",
            "Leopold Schmetterer",
            "Marcus Ang",
            "Rahat Hussain",
            "Jod Mehta",
            "Tin Aung",
            "Lionel Tim-Ee Cheng",
            "Tran Nguyen Tuan Anh",
            "Chee Leong Cheng",
            "Tien Yin Wong",
            "Nan Liu",
            "Iain Beehuat Tan",
            "Soon Thye Lim",
            "Eyal Klang",
            "Tony Kiat Hon Lim",
            "Rick Siow Mong Goh",
            "Yong Liu",
            "Daniel Shu Wei Ting"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper presents EVLF-FM, a multimodal vision-language foundation model for medical diagnostics with explainability and reasoning capabilities.",
        "tldr_zh": "该论文提出EVLF-FM，这是一个用于医学诊断的多模态视觉语言基础模型，具有解释性和推理能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Forge4D: Feed-Forward 4D Human Reconstruction and Interpolation from Uncalibrated Sparse-view Videos",
        "summary": "Instant reconstruction of dynamic 3D humans from uncalibrated sparse-view\nvideos is critical for numerous downstream applications. Existing methods,\nhowever, are either limited by the slow reconstruction speeds or incapable of\ngenerating novel-time representations. To address these challenges, we propose\nForge4D, a feed-forward 4D human reconstruction and interpolation model that\nefficiently reconstructs temporally aligned representations from uncalibrated\nsparse-view videos, enabling both novel view and novel time synthesis. Our\nmodel simplifies the 4D reconstruction and interpolation problem as a joint\ntask of streaming 3D Gaussian reconstruction and dense motion prediction. For\nthe task of streaming 3D Gaussian reconstruction, we first reconstruct static\n3D Gaussians from uncalibrated sparse-view images and then introduce learnable\nstate tokens to enforce temporal consistency in a memory-friendly manner by\ninteractively updating shared information across different timestamps. For\nnovel time synthesis, we design a novel motion prediction module to predict\ndense motions for each 3D Gaussian between two adjacent frames, coupled with an\nocclusion-aware Gaussian fusion process to interpolate 3D Gaussians at\narbitrary timestamps. To overcome the lack of the ground truth for dense motion\nsupervision, we formulate dense motion prediction as a dense point matching\ntask and introduce a self-supervised retargeting loss to optimize this module.\nAn additional occlusion-aware optical flow loss is introduced to ensure motion\nconsistency with plausible human movement, providing stronger regularization.\nExtensive experiments demonstrate the effectiveness of our model on both\nin-domain and out-of-domain datasets. Project page and code at:\nhttps://zhenliuzju.github.io/huyingdong/Forge4D.",
        "url": "http://arxiv.org/abs/2509.24209v1",
        "published_date": "2025-09-29T02:47:14+00:00",
        "updated_date": "2025-09-29T02:47:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yingdong Hu",
            "Yisheng He",
            "Jinnan Chen",
            "Weihao Yuan",
            "Kejie Qiu",
            "Zehong Lin",
            "Siyu Zhu",
            "Zilong Dong",
            "Jun Zhang"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces Forge4D, a model for reconstructing dynamic 3D humans from sparse-view videos, enabling novel view and time synthesis.",
        "tldr_zh": "本文介绍了Forge4D，一个用于从稀疏视角视频中重建动态3D人物的模型，实现新视角和时间综合。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "An Efficient 3D Latent Diffusion Model for T1-contrast Enhanced MRI Generation",
        "summary": "Objective: Gadolinium-based contrast agents (GBCAs) are commonly employed\nwith T1w MRI to enhance lesion visualization but are restricted in patients at\nrisk of nephrogenic systemic fibrosis and variations in GBCA administration can\nintroduce imaging inconsistencies. This study develops an efficient 3D\ndeep-learning framework to generate T1-contrast enhanced images (T1C) from\npre-contrast multiparametric MRI. Approach: We propose the 3D latent rectified\nflow (T1C-RFlow) model for generating high-quality T1C images. First, T1w and\nT2-FLAIR images are input into a pretrained autoencoder to acquire an efficient\nlatent space representation. A rectified flow diffusion model is then trained\nin this latent space representation. The T1C-RFlow model was trained on a\ncurated dataset comprised of the BraTS 2024 glioma (GLI; 1480 patients),\nmeningioma (MEN; 1141 patients), and metastases (MET; 1475 patients) datasets.\nSelected patients were split into train (N=2860), validation (N=612), and test\n(N=614) sets. Results: Both qualitative and quantitative results demonstrate\nthat the T1C-RFlow model outperforms benchmark 3D models (pix2pix, DDPM,\nDiffusion Transformers (DiT-3D)) trained in the same latent space. T1C-RFlow\nachieved the following metrics - GLI: NMSE 0.044 +/- 0.047, SSIM 0.935 +/-\n0.025; MEN: NMSE 0.046 +/- 0.029, SSIM 0.937 +/- 0.021; MET: NMSE 0.098 +/-\n0.088, SSIM 0.905 +/- 0.082. T1C-RFlow had the best tumor reconstruction\nperformance and significantly faster denoising times (6.9 s/volume, 200 steps)\nthan conventional DDPM models in both latent space (37.7s, 1000 steps) and\npatch-based in image space (4.3 hr/volume). Significance: Our proposed method\ngenerates synthetic T1C images that closely resemble ground truth T1C in much\nless time than previous diffusion models. Further development may permit a\npractical method for contrast-agent-free MRI for brain tumors.",
        "url": "http://arxiv.org/abs/2509.24194v1",
        "published_date": "2025-09-29T02:22:55+00:00",
        "updated_date": "2025-09-29T02:22:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zach Eidex",
            "Mojtaba Safari",
            "Jie Ding",
            "Richard Qiu",
            "Justin Roper",
            "David Yu",
            "Hui-Kuo Shu",
            "Zhen Tian",
            "Hui Mao",
            "Xiaofeng Yang"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "A new 3D deep-learning model, T1C-RFlow, is proposed to generate T1-contrast enhanced images from pre-contrast MRI, outperforming existing models in tumor reconstruction and denoising speeds.",
        "tldr_zh": "提出了一种新的3D深度学习模型T1C-RFlow，用于从预对比MRI生成T1-增强图像，在肿瘤重建和去噪速度方面优于现有模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "LatXGen: Towards Radiation-Free and Accurate Quantitative Analysis of Sagittal Spinal Alignment Via Cross-Modal Radiographic View Synthesis",
        "summary": "Adolescent Idiopathic Scoliosis (AIS) is a complex three-dimensional spinal\ndeformity, and accurate morphological assessment requires evaluating both\ncoronal and sagittal alignment. While previous research has made significant\nprogress in developing radiation-free methods for coronal plane assessment,\nreliable and accurate evaluation of sagittal alignment without ionizing\nradiation remains largely underexplored. To address this gap, we propose\nLatXGen, a novel generative framework that synthesizes realistic lateral spinal\nradiographs from posterior Red-Green-Blue and Depth (RGBD) images of unclothed\nbacks. This enables accurate, radiation-free estimation of sagittal spinal\nalignment. LatXGen tackles two core challenges: (1) inferring sagittal spinal\nmorphology changes from a lateral perspective based on posteroanterior surface\ngeometry, and (2) performing cross-modality translation from RGBD input to the\nradiographic domain. The framework adopts a dual-stage architecture that\nprogressively estimates lateral spinal structure and synthesizes corresponding\nradiographs. To enhance anatomical consistency, we introduce an attention-based\nFast Fourier Convolution (FFC) module for integrating anatomical features from\nRGBD images and 3D landmarks, and a Spatial Deformation Network (SDN) to model\nmorphological variations in the lateral view. Additionally, we construct the\nfirst large-scale paired dataset for this task, comprising 3,264 RGBD and\nlateral radiograph pairs. Experimental results demonstrate that LatXGen\nproduces anatomically accurate radiographs and outperforms existing GAN-based\nmethods in both visual fidelity and quantitative metrics. This study offers a\npromising, radiation-free solution for sagittal spine assessment and advances\ncomprehensive AIS evaluation.",
        "url": "http://arxiv.org/abs/2509.24165v1",
        "published_date": "2025-09-29T01:29:53+00:00",
        "updated_date": "2025-09-29T01:29:53+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Moxin Zhao",
            "Nan Meng",
            "Jason Pui Yin Cheung",
            "Chris Yuk Kwan Tang",
            "Chenxi Yu",
            "Wenting Zhong",
            "Pengyu Lu",
            "Chang Shi",
            "Yipeng Zhuang",
            "Teng Zhang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "GAN"
        ],
        "tldr": "The paper introduces LatXGen, a framework for generating realistic lateral spinal radiographs from RGBD images to accurately evaluate sagittal spinal alignment without radiation.",
        "tldr_zh": "该论文介绍了LatXGen，一种从RGBD图像生成逼真的侧面脊柱X射线照片的框架，以无辐射地准确评估冠状脊柱对准。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Fast Feature Field ($\\text{F}^3$): A Predictive Representation of Events",
        "summary": "This paper develops a mathematical argument and algorithms for building\nrepresentations of data from event-based cameras, that we call Fast Feature\nField ($\\text{F}^3$). We learn this representation by predicting future events\nfrom past events and show that it preserves scene structure and motion\ninformation. $\\text{F}^3$ exploits the sparsity of event data and is robust to\nnoise and variations in event rates. It can be computed efficiently using ideas\nfrom multi-resolution hash encoding and deep sets - achieving 120 Hz at HD and\n440 Hz at VGA resolutions. $\\text{F}^3$ represents events within a contiguous\nspatiotemporal volume as a multi-channel image, enabling a range of downstream\ntasks. We obtain state-of-the-art performance on optical flow estimation,\nsemantic segmentation, and monocular metric depth estimation, on data from\nthree robotic platforms (a car, a quadruped robot and a flying platform),\nacross different lighting conditions (daytime, nighttime), environments\n(indoors, outdoors, urban, as well as off-road) and dynamic vision sensors\n(resolutions and event rates). Our implementations can predict these tasks at\n25-75 Hz at HD resolution.",
        "url": "http://arxiv.org/abs/2509.25146v1",
        "published_date": "2025-09-29T17:52:31+00:00",
        "updated_date": "2025-09-29T17:52:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Richeek Das",
            "Kostas Daniilidis",
            "Pratik Chaudhari"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces Fast Feature Field (F^3), a predictive representation of events from event-based cameras. F^3 preserves scene structure and motion information, achieving state-of-the-art performance on various computer vision tasks.",
        "tldr_zh": "本文介绍了一种名为Fast Feature Field (F^3)的预测事件表示方法，该方法可以有效保留场景结构和运动信息，并在各种计算机视觉任务上取得了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TemMed-Bench: Evaluating Temporal Medical Image Reasoning in Vision-Language Models",
        "summary": "Existing medical reasoning benchmarks for vision-language models primarily\nfocus on analyzing a patient's condition based on an image from a single visit.\nHowever, this setting deviates significantly from real-world clinical practice,\nwhere doctors typically refer to a patient's historical conditions to provide a\ncomprehensive assessment by tracking their changes over time. In this paper, we\nintroduce TemMed-Bench, the first benchmark designed for analyzing changes in\npatients' conditions between different clinical visits, which challenges large\nvision-language models (LVLMs) to reason over temporal medical images.\nTemMed-Bench consists of a test set comprising three tasks - visual\nquestion-answering (VQA), report generation, and image-pair selection - and a\nsupplementary knowledge corpus of over 17,000 instances. With TemMed-Bench, we\nconduct an evaluation of six proprietary and six open-source LVLMs. Our results\nshow that most LVLMs lack the ability to analyze patients' condition changes\nover temporal medical images, and a large proportion perform only at a\nrandom-guessing level in the closed-book setting. In contrast, GPT o3, o4-mini\nand Claude 3.5 Sonnet demonstrate comparatively decent performance, though they\nhave yet to reach the desired level. Furthermore, we explore augmenting the\ninput with both retrieved visual and textual modalities in the medical domain.\nWe also show that multi-modal retrieval augmentation yields notably higher\nperformance gains than no retrieval and textual retrieval alone across most\nmodels on our benchmark, with the VQA task showing an average improvement of\n2.59%. Overall, we compose a benchmark grounded on real-world clinical\npractice, and it reveals LVLMs' limitations in temporal medical image\nreasoning, as well as highlighting the use of multi-modal retrieval\naugmentation as a potentially promising direction worth exploring to address\nthis challenge.",
        "url": "http://arxiv.org/abs/2509.25143v1",
        "published_date": "2025-09-29T17:51:26+00:00",
        "updated_date": "2025-09-29T17:51:26+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Junyi Zhang",
            "Jia-Chen Gu",
            "Wenbo Hu",
            "Yu Zhou",
            "Robinson Piramuthu",
            "Nanyun Peng"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a benchmark for analyzing changes in patients' conditions between clinical visits using vision-language models and highlights the limitations of current models in this area.",
        "tldr_zh": "本文引入一个基准测试，用于使用视觉语言模型分析患者在临床访问之间状况的变化，并强调当前模型在这一领域的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LayerD: Decomposing Raster Graphic Designs into Layers",
        "summary": "Designers craft and edit graphic designs in a layer representation, but\nlayer-based editing becomes impossible once composited into a raster image. In\nthis work, we propose LayerD, a method to decompose raster graphic designs into\nlayers for re-editable creative workflow. LayerD addresses the decomposition\ntask by iteratively extracting unoccluded foreground layers. We propose a\nsimple yet effective refinement approach taking advantage of the assumption\nthat layers often exhibit uniform appearance in graphic designs. As\ndecomposition is ill-posed and the ground-truth layer structure may not be\nreliable, we develop a quality metric that addresses the difficulty. In\nexperiments, we show that LayerD successfully achieves high-quality\ndecomposition and outperforms baselines. We also demonstrate the use of LayerD\nwith state-of-the-art image generators and layer-based editing.",
        "url": "http://arxiv.org/abs/2509.25134v1",
        "published_date": "2025-09-29T17:50:12+00:00",
        "updated_date": "2025-09-29T17:50:12+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Tomoyuki Suzuki",
            "Kang-Jun Liu",
            "Naoto Inoue",
            "Kota Yamaguchi"
        ],
        "ai_categories": [
            "Multimodality",
            "GAN",
            "Transformer"
        ],
        "tldr": "LayerD introduces a method to decompose raster graphic designs into layers for re-editable creative workflow, outperforming baselines and enabling layer-based editing.",
        "tldr_zh": "LayerD 提出了一种将光栅图形设计分解为图层的方法，优于基线方法，实现了基于图层的编辑。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech",
        "summary": "We present MGM-Omni, a unified Omni LLM for omni-modal understanding and\nexpressive, long-horizon speech generation. Unlike cascaded pipelines that\nisolate speech synthesis, MGM-Omni adopts a \"brain-mouth\" design with a\ndual-track, token-based architecture that cleanly decouples multimodal\nreasoning from real-time speech generation. This design enables efficient\ncross-modal interaction and low-latency, streaming speech generation. For\nunderstanding, a unified training strategy coupled with a dual audio encoder\ndesign enables long-form audio perception across diverse acoustic conditions.\nFor generation, a chunk-based parallel decoding scheme narrows the text speech\ntoken-rate gap, accelerating inference and supporting streaming zero-shot voice\ncloning with stable timbre over extended durations. Compared to concurrent\nwork, MGM-Omni achieves these capabilities with markedly data-efficient\ntraining. Extensive experiments demonstrate that MGM-Omni outperforms existing\nopen source models in preserving timbre identity across extended sequences,\nproducing natural and context-aware speech, and achieving superior long-form\naudio and omnimodal understanding. MGM-Omni establishes an efficient,\nend-to-end paradigm for omnimodal understanding and controllable, personalised\nlong-horizon speech generation.",
        "url": "http://arxiv.org/abs/2509.25131v1",
        "published_date": "2025-09-29T17:48:28+00:00",
        "updated_date": "2025-09-29T17:48:28+00:00",
        "categories": [
            "cs.SD",
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Chengyao Wang",
            "Zhisheng Zhong",
            "Bohao Peng",
            "Senqiao Yang",
            "Yuqi Liu",
            "Haokun Gui",
            "Bin Xia",
            "Jingyao Li",
            "Bei Yu",
            "Jiaya Jia"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality"
        ],
        "tldr": "MGM-Omni is a unified model for understanding and generating speech with a dual-track architecture, enabling efficient cross-modal interaction and real-time speech generation.",
        "tldr_zh": "MGM-Omni是一个统一的模型，用于理解和生成语音，具有双轨架构，能够实现跨模态交互和实时语音生成。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Score Distillation of Flow Matching Models",
        "summary": "Diffusion models achieve high-quality image generation but are limited by\nslow iterative sampling. Distillation methods alleviate this by enabling one-\nor few-step generation. Flow matching, originally introduced as a distinct\nframework, has since been shown to be theoretically equivalent to diffusion\nunder Gaussian assumptions, raising the question of whether distillation\ntechniques such as score distillation transfer directly. We provide a simple\nderivation -- based on Bayes' rule and conditional expectations -- that unifies\nGaussian diffusion and flow matching without relying on ODE/SDE formulations.\nBuilding on this view, we extend Score identity Distillation (SiD) to\npretrained text-to-image flow-matching models, including SANA, SD3-Medium,\nSD3.5-Medium/Large, and FLUX.1-dev, all with DiT backbones. Experiments show\nthat, with only modest flow-matching- and DiT-specific adjustments, SiD works\nout of the box across these models, in both data-free and data-aided settings,\nwithout requiring teacher finetuning or architectural changes. This provides\nthe first systematic evidence that score distillation applies broadly to\ntext-to-image flow matching models, resolving prior concerns about stability\nand soundness and unifying acceleration techniques across diffusion- and\nflow-based generators. We will make the PyTorch implementation publicly\navailable.",
        "url": "http://arxiv.org/abs/2509.25127v1",
        "published_date": "2025-09-29T17:45:48+00:00",
        "updated_date": "2025-09-29T17:45:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Mingyuan Zhou",
            "Yi Gu",
            "Huangjie Zheng",
            "Liangchen Song",
            "Guande He",
            "Yizhe Zhang",
            "Wenze Hu",
            "Yinfei Yang"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces Score identity Distillation (SiD) to pretrained text-to-image flow-matching models, showing that it applies broadly and provides acceleration techniques across diffusion- and flow-based generators.",
        "tldr_zh": "本文将评分识别蒸馏（SiD）引入预训练的文本到图像流匹配模型，表明它广泛适用，并提供扩散和流动生成器之间的加速技术。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial Purification",
        "summary": "Adversarial purification with diffusion models has emerged as a promising\ndefense strategy, but existing methods typically rely on uniform noise\ninjection, which indiscriminately perturbs all frequencies, corrupting semantic\nstructures and undermining robustness. Our empirical study reveals that\nadversarial perturbations are not uniformly distributed: they are predominantly\nconcentrated in high-frequency regions, with heterogeneous magnitude intensity\npatterns that vary across frequencies and attack types. Motivated by this\nobservation, we introduce MANI-Pure, a magnitude-adaptive purification\nframework that leverages the magnitude spectrum of inputs to guide the\npurification process. Instead of injecting homogeneous noise, MANI-Pure\nadaptively applies heterogeneous, frequency-targeted noise, effectively\nsuppressing adversarial perturbations in fragile high-frequency, low-magnitude\nbands while preserving semantically critical low-frequency content. Extensive\nexperiments on CIFAR-10 and ImageNet-1K validate the effectiveness of\nMANI-Pure. It narrows the clean accuracy gap to within 0.59 of the original\nclassifier, while boosting robust accuracy by 2.15, and achieves the top-1\nrobust accuracy on the RobustBench leaderboard, surpassing the previous\nstate-of-the-art method.",
        "url": "http://arxiv.org/abs/2509.25082v1",
        "published_date": "2025-09-29T17:22:40+00:00",
        "updated_date": "2025-09-29T17:22:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoyi Huang",
            "Junwei Wu",
            "Kejia Zhang",
            "Carl Yang",
            "Zhiming Luo"
        ],
        "ai_categories": [
            "GAN"
        ],
        "tldr": "MANI-Pure introduces a magnitude-adaptive noise injection method to purify adversarial perturbations, achieving top-1 robust accuracy on the RobustBench leaderboard.",
        "tldr_zh": "MANI-Pure引入一种幅度自适应的噪声注入方法，用于净化敌对扰动，在RobustBench排行榜上获得了前1的稳健准确性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM Reconstruction",
        "summary": "Cryo-electron microscopy (cryo-EM) has become a central tool for\nhigh-resolution structural biology, yet the massive scale of datasets (often\nexceeding 100k particle images) renders 3D reconstruction both computationally\nexpensive and memory intensive. Traditional Fourier-space methods are efficient\nbut lose fidelity due to repeated transforms, while recent real-space\napproaches based on neural radiance fields (NeRFs) improve accuracy but incur\ncubic memory and computation overhead. Therefore, we introduce GEM, a novel\ncryo-EM reconstruction framework built on 3D Gaussian Splatting (3DGS) that\noperates directly in real-space while maintaining high efficiency. Instead of\nmodeling the entire density volume, GEM represents proteins with compact 3D\nGaussians, each parameterized by only 11 values. To further improve the\ntraining efficiency, we designed a novel gradient computation to 3D Gaussians\nthat contribute to each voxel. This design substantially reduced both memory\nfootprint and training cost. On standard cryo-EM benchmarks, GEM achieves up to\n48% faster training and 12% lower memory usage compared to state-of-the-art\nmethods, while improving local resolution by as much as 38.8%. These results\nestablish GEM as a practical and scalable paradigm for cryo-EM reconstruction,\nunifying speed, efficiency, and high-resolution accuracy. Our code is available\nat https://github.com/UNITES-Lab/GEM.",
        "url": "http://arxiv.org/abs/2509.25075v1",
        "published_date": "2025-09-29T17:17:53+00:00",
        "updated_date": "2025-09-29T17:17:53+00:00",
        "categories": [
            "cs.CV",
            "cs.CE"
        ],
        "authors": [
            "Huaizhi Qu",
            "Xiao Wang",
            "Gengwei Zhang",
            "Jie Peng",
            "Tianlong Chen"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces GEM, a reconstruction framework for cryo-EM using efficient 3D Gaussian splatting, achieving faster training, lower memory usage, and improved resolution.",
        "tldr_zh": "本文介绍了一种用于冷冻电子显微镜的重建框架-GEM，采用高效的3D高斯喷溅，实现更快的训练、更低的内存使用和改善的分辨率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "STAGE: Stable and Generalizable GRPO for Autoregressive Image Generation",
        "summary": "Reinforcement learning has recently been explored to improve text-to-image\ngeneration, yet applying existing GRPO algorithms to autoregressive (AR) image\nmodels remains challenging. The instability of the training process easily\ndisrupts the pretrained model capability during long runs, resulting in\nmarginal gains, degraded image quality, and poor generalization. In this work,\nwe revisit GRPO for AR image generation and identify two key issues:\ncontradictory gradients from unnecessary tokens and unstable policy entropy\ndynamics. To address these, we introduce STAGE, a stable and generalizable\nframework that leverages two targeted solutions: 1) Advantage/KL reweighting.\nSimilarity-aware reweighting to alleviate conflicting updates; and 2) Entropy\nreward. An entropy-based reward corresponding to reference model to stabilize\nlearning. With the help of alleviating conflicts between tokens and an entropy\nreward for stabilizing training, we reduce disruption of the pretrained\ndistribution and mitigate reward hacking, which in turn improves generalization\nand transfer better to other benchmarks. Experiments across multiple benchmarks\nshow that STAGE consistently improves visual quality, stability, and cross-task\ngeneralization compared to baseline GRPO.",
        "url": "http://arxiv.org/abs/2509.25027v1",
        "published_date": "2025-09-29T16:50:21+00:00",
        "updated_date": "2025-09-29T16:50:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoxiao Ma",
            "Haibo Qiu",
            "Guohui Zhang",
            "Zhixiong Zeng",
            "Siqi Yang",
            "Lin Ma",
            "Feng Zhao"
        ],
        "ai_categories": [
            "AIGC"
        ],
        "tldr": "The paper introduces STAGE, a stable and generalizable framework for autoregressive image generation that improves visual quality, stability, and cross-task generalization compared to baseline methods.",
        "tldr_zh": "本文介绍了一种稳定且通用的框架STAGE，用于自回归图像生成，相对于基线方法，它改善了视觉质量、稳定性和跨任务泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GeoVLM-R1: Reinforcement Fine-Tuning for Improved Remote Sensing Reasoning",
        "summary": "Recent advances in reinforcement learning (RL) have delivered strong\nreasoning capabilities in natural image domains, yet their potential for Earth\nObservation (EO) remains largely unexplored. EO tasks introduce unique\nchallenges, spanning referred object detection, image or region captioning,\nchange detection, grounding, and temporal analysis, that demand task aware\nreasoning. We propose a novel post training framework that incorporates task\naware rewards to enable effective adaptation of reasoning based RL models to\ndiverse EO tasks. This training strategy enhances reasoning capabilities for\nremote sensing images, stabilizes optimization, and improves robustness.\nExtensive experiments across multiple EO benchmarks show consistent performance\ngains over state of the art generic and specialized vision language models.\nCode and models will be released publicly at\nhttps://mustansarfiaz.github.io/GeoVLM-R1/ .",
        "url": "http://arxiv.org/abs/2509.25026v1",
        "published_date": "2025-09-29T16:48:54+00:00",
        "updated_date": "2025-09-29T16:48:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mustansar Fiaz",
            "Hiyam Debary",
            "Paolo Fraccaro",
            "Danda Paudel",
            "Luc Van Gool",
            "Fahad Khan",
            "Salman Khan"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a reinforcement fine-tuning framework for improving reasoning capabilities in Earth Observation tasks, showing consistent performance gains over existing models.",
        "tldr_zh": "该论文提出了一种强化微调框架，用于改善地球观测任务中的推理能力，在现有模型上表现出一贯的性能增益。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LVT: Large-Scale Scene Reconstruction via Local View Transformers",
        "summary": "Large transformer models are proving to be a powerful tool for 3D vision and\nnovel view synthesis. However, the standard Transformer's well-known quadratic\ncomplexity makes it difficult to scale these methods to large scenes. To\naddress this challenge, we propose the Local View Transformer (LVT), a\nlarge-scale scene reconstruction and novel view synthesis architecture that\ncircumvents the need for the quadratic attention operation. Motivated by the\ninsight that spatially nearby views provide more useful signal about the local\nscene composition than distant views, our model processes all information in a\nlocal neighborhood around each view. To attend to tokens in nearby views, we\nleverage a novel positional encoding that conditions on the relative geometric\ntransformation between the query and nearby views. We decode the output of our\nmodel into a 3D Gaussian Splat scene representation that includes both color\nand opacity view-dependence. Taken together, the Local View Transformer enables\nreconstruction of arbitrarily large, high-resolution scenes in a single forward\npass. See our project page for results and interactive demos\nhttps://toobaimt.github.io/lvt/.",
        "url": "http://arxiv.org/abs/2509.25001v1",
        "published_date": "2025-09-29T16:24:34+00:00",
        "updated_date": "2025-09-29T16:24:34+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Tooba Imtiaz",
            "Lucy Chai",
            "Kathryn Heal",
            "Xuan Luo",
            "Jungyeon Park",
            "Jennifer Dy",
            "John Flynn"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces the Local View Transformer (LVT) for large-scale scene reconstruction and novel view synthesis without the quadratic complexity of standard Transformers.",
        "tldr_zh": "本文介绍了一种名为本地视图变换器（LVT）的方法，用于大规模场景重建和新视图合成，避免了标准Transformer的二次复杂度。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Light-SQ: Structure-aware Shape Abstraction with Superquadrics for Generated Meshes",
        "summary": "In user-generated-content (UGC) applications, non-expert users often rely on\nimage-to-3D generative models to create 3D assets. In this context,\nprimitive-based shape abstraction offers a promising solution for UGC scenarios\nby compressing high-resolution meshes into compact, editable representations.\nTowards this end, effective shape abstraction must therefore be\nstructure-aware, characterized by low overlap between primitives, part-aware\nalignment, and primitive compactness. We present Light-SQ, a novel\nsuperquadric-based optimization framework that explicitly emphasizes\nstructure-awareness from three aspects. (a) We introduce SDF carving to\niteratively udpate the target signed distance field, discouraging overlap\nbetween primitives. (b) We propose a block-regrow-fill strategy guided by\nstructure-aware volumetric decomposition, enabling structural partitioning to\ndrive primitive placement. (c) We implement adaptive residual pruning based on\nSDF update history to surpress over-segmentation and ensure compact results. In\naddition, Light-SQ supports multiscale fitting, enabling localized refinement\nto preserve fine geometric details. To evaluate our method, we introduce\n3DGen-Prim, a benchmark extending 3DGen-Bench with new metrics for both\nreconstruction quality and primitive-level editability. Extensive experiments\ndemonstrate that Light-SQ enables efficient, high-fidelity, and editable shape\nabstraction with superquadrics for complex generated geometry, advancing the\nfeasibility of 3D UGC creation.",
        "url": "http://arxiv.org/abs/2509.24986v1",
        "published_date": "2025-09-29T16:18:32+00:00",
        "updated_date": "2025-09-29T16:18:32+00:00",
        "categories": [
            "cs.GR",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Yuhan Wang",
            "Weikai Chen",
            "Zeyu Hu",
            "Runze Zhang",
            "Yingda Yin",
            "Ruoyu Wu",
            "Keyang Luo",
            "Shengju Qian",
            "Yiyan Ma",
            "Hongyi Li",
            "Yuan Gao",
            "Yuhuan Zhou",
            "Hao Luo",
            "Wan Wang",
            "Xiaobin Shen",
            "Zhaowei Li",
            "Kuixin Zhu",
            "Chuanlang Hong",
            "Yueyue Wang",
            "Lijie Feng",
            "Xin Wang",
            "Chen Change Loy"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces Light-SQ, a superquadric-based optimization framework for structure-aware shape abstraction, enabling efficient and editable creation of 3D assets from user-generated content.",
        "tldr_zh": "该论文引入了Light-SQ，这是一个基于超椭球体的优化框架，用于结构感知的形状抽象，有效且可编辑地创建用户生成内容的3D资产。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Wan-Alpha: High-Quality Text-to-Video Generation with Alpha Channel",
        "summary": "RGBA video generation, which includes an alpha channel to represent\ntransparency, is gaining increasing attention across a wide range of\napplications. However, existing methods often neglect visual quality, limiting\ntheir practical usability. In this paper, we propose \\textit{Wan-Alpha}, a new\nframework that generates transparent videos by learning both RGB and alpha\nchannels jointly. We design an effective variational autoencoder (VAE) that\nencodes the alpha channel into the RGB latent space. Then, to support the\ntraining of our diffusion transformer, we construct a high-quality and diverse\nRGBA video dataset. Compared with state-of-the-art methods, our model\ndemonstrates superior performance in visual quality, motion realism, and\ntransparency rendering. Notably, our model can generate a wide variety of\nsemi-transparent objects, glowing effects, and fine-grained details such as\nhair strands. The released model is available on our website:\n\\href{https://donghaotian123.github.io/Wan-Alpha/}{https://donghaotian123.github.io/Wan-Alpha/}.",
        "url": "http://arxiv.org/abs/2509.24979v1",
        "published_date": "2025-09-29T16:08:21+00:00",
        "updated_date": "2025-09-29T16:08:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haotian Dong",
            "Wenjing Wang",
            "Chen Li",
            "Di Lin"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces Wan-Alpha, a framework for generating high-quality transparent videos by learning RGB and alpha channels jointly.",
        "tldr_zh": "本文介绍了Wan-Alpha，一种通过联合学习RGB和alpha通道生成高质量透明视频的框架。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "On-the-Fly Data Augmentation for Brain Tumor Segmentation",
        "summary": "Robust segmentation across both pre-treatment and post-treatment glioma scans\ncan be helpful for consistent tumor monitoring and treatment planning. BraTS\n2025 Task 1 addresses this by challenging models to generalize across varying\ntumor appearances throughout the treatment timeline. However, training such\ngeneralized models requires access to diverse, high-quality annotated data,\nwhich is often limited. While data augmentation can alleviate this, storing\nlarge volumes of augmented 3D data is computationally expensive. To address\nthese challenges, we propose an on-the-fly augmentation strategy that\ndynamically inserts synthetic tumors using pretrained generative adversarial\nnetworks (GliGANs) during training. We evaluate three nnU-Net-based models and\ntheir ensembles: (1) a baseline without external augmentation, (2) a regular\non-the-fly augmented model, and (3) a model with customized on-the-fly\naugmentation. Built upon the nnU-Net framework, our pipeline leverages\npretrained GliGAN weights and tumor insertion methods from prior\nchallenge-winning solutions. An ensemble of the three models achieves\nlesion-wise Dice scores of 0.79 (ET), 0.749 (NETC), 0.872 (RC), 0.825 (SNFH),\n0.79 (TC), and 0.88 (WT) on the online BraTS 2025 validation platform. This\nwork ranked first in the BraTS Lighthouse Challenge 2025 Task 1- Adult Glioma\nSegmentation.",
        "url": "http://arxiv.org/abs/2509.24973v1",
        "published_date": "2025-09-29T16:02:36+00:00",
        "updated_date": "2025-09-29T16:02:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ishika Jain",
            "Siri Willems",
            "Steven Latre",
            "Tom De Schepper"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces an on-the-fly data augmentation strategy using GliGANs for brain tumor segmentation, achieving high Dice scores on the BraTS 2025 validation platform.",
        "tldr_zh": "本文介绍了一种使用GliGANs进行现场数据增强的方法，用于脑肿瘤分割，在BraTS 2025验证平台上取得了高的Dice得分。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Event-based Facial Keypoint Alignment via Cross-Modal Fusion Attention and Self-Supervised Multi-Event Representation Learning",
        "summary": "Event cameras offer unique advantages for facial keypoint alignment under\nchallenging conditions, such as low light and rapid motion, due to their high\ntemporal resolution and robustness to varying illumination. However, existing\nRGB facial keypoint alignment methods do not perform well on event data, and\ntraining solely on event data often leads to suboptimal performance because of\nits limited spatial information. Moreover, the lack of comprehensive labeled\nevent datasets further hinders progress in this area. To address these issues,\nwe propose a novel framework based on cross-modal fusion attention (CMFA) and\nself-supervised multi-event representation learning (SSMER) for event-based\nfacial keypoint alignment. Our framework employs CMFA to integrate\ncorresponding RGB data, guiding the model to extract robust facial features\nfrom event input images. In parallel, SSMER enables effective feature learning\nfrom unlabeled event data, overcoming spatial limitations. Extensive\nexperiments on our real-event E-SIE dataset and a synthetic-event version of\nthe public WFLW-V benchmark show that our approach consistently surpasses\nstate-of-the-art methods across multiple evaluation metrics.",
        "url": "http://arxiv.org/abs/2509.24968v1",
        "published_date": "2025-09-29T16:00:50+00:00",
        "updated_date": "2025-09-29T16:00:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Donghwa Kang",
            "Junho Kim",
            "Dongwoo Kang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel framework for event-based facial keypoint alignment using cross-modal fusion attention and self-supervised multi-event representation learning, outperforming existing methods.",
        "tldr_zh": "本文提出了一种基于事件的人脸关键点对齐框架，使用跨模态融合注意力和自监督多事件表示学习，在性能上胜过现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Social 3D Scene Graphs: Modeling Human Actions and Relations for Interactive Service Robots",
        "summary": "Understanding how people interact with their surroundings and each other is\nessential for enabling robots to act in socially compliant and context-aware\nways. While 3D Scene Graphs have emerged as a powerful semantic representation\nfor scene understanding, existing approaches largely ignore humans in the\nscene, also due to the lack of annotated human-environment relationships.\nMoreover, existing methods typically capture only open-vocabulary relations\nfrom single image frames, which limits their ability to model long-range\ninteractions beyond the observed content. We introduce Social 3D Scene Graphs,\nan augmented 3D Scene Graph representation that captures humans, their\nattributes, activities and relationships in the environment, both local and\nremote, using an open-vocabulary framework. Furthermore, we introduce a new\nbenchmark consisting of synthetic environments with comprehensive human-scene\nrelationship annotations and diverse types of queries for evaluating social\nscene understanding in 3D. The experiments demonstrate that our representation\nimproves human activity prediction and reasoning about human-environment\nrelations, paving the way toward socially intelligent robots.",
        "url": "http://arxiv.org/abs/2509.24966v1",
        "published_date": "2025-09-29T16:00:40+00:00",
        "updated_date": "2025-09-29T16:00:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ermanno Bartoli",
            "Dennis Rotondi",
            "Buwei He",
            "Patric Jensfelt",
            "Kai O. Arras",
            "Iolanda Leite"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces Social 3D Scene Graphs, a representation that captures human actions and relations in 3D scenes to enable socially intelligent robots. They also present a benchmark for evaluating social scene understanding in 3D environments.",
        "tldr_zh": "该论文介绍了社交3D场景图，这是一种捕捉3D场景中人类行为和关系的表示方法，可用于实现社交智能机器人。他们还提出了一个用于评估3D环境中社交场景理解的基准。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Scalable GANs with Transformers",
        "summary": "Scalability has driven recent advances in generative modeling, yet its\nprinciples remain underexplored for adversarial learning. We investigate the\nscalability of Generative Adversarial Networks (GANs) through two design\nchoices that have proven to be effective in other types of generative models:\ntraining in a compact Variational Autoencoder latent space and adopting purely\ntransformer-based generators and discriminators. Training in latent space\nenables efficient computation while preserving perceptual fidelity, and this\nefficiency pairs naturally with plain transformers, whose performance scales\nwith computational budget. Building on these choices, we analyze failure modes\nthat emerge when naively scaling GANs. Specifically, we find issues as\nunderutilization of early layers in the generator and optimization instability\nas the network scales. Accordingly, we provide simple and scale-friendly\nsolutions as lightweight intermediate supervision and width-aware learning-rate\nadjustment. Our experiments show that GAT, a purely transformer-based and\nlatent-space GANs, can be easily trained reliably across a wide range of\ncapacities (S through XL). Moreover, GAT-XL/2 achieves state-of-the-art\nsingle-step, class-conditional generation performance (FID of 2.96) on\nImageNet-256 in just 40 epochs, 6x fewer epochs than strong baselines.",
        "url": "http://arxiv.org/abs/2509.24935v1",
        "published_date": "2025-09-29T15:36:15+00:00",
        "updated_date": "2025-09-29T15:36:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Sangeek Hyun",
            "MinKyu Lee",
            "Jae-Pil Heo"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper explores scalable Generative Adversarial Networks (GANs) using compact Variational Autoencoder latent space and transformer-based models, addressing issues like underutilization of early layers and optimization instability.",
        "tldr_zh": "本文探讨了使用紧凑的变分自动编码器潜空间和基于变压器的模型的可扩展生成对抗网络（GANs），解决了早期层利用不足和优化不稳定性等问题。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Segmentor-Guided Counterfactual Fine-Tuning for Image Synthesis",
        "summary": "Counterfactual image generation is a powerful tool for augmenting training\ndata, de-biasing datasets, and modeling disease. Current approaches rely on\nexternal classifiers or regressors to increase the effectiveness of\nsubject-level interventions (e.g., changing the patient's age). For\nstructure-specific interventions (e.g., changing the area of the left lung in a\nchest radiograph), we show that this is insufficient, and can result in\nundesirable global effects across the image domain. Previous work used\npixel-level label maps as guidance, requiring a user to provide hypothetical\nsegmentations which are tedious and difficult to obtain. We propose\nSegmentor-guided Counterfactual Fine-Tuning (Seg-CFT), which preserves the\nsimplicity of intervening on scalar-valued, structure-specific variables while\nproducing locally coherent and effective counterfactuals. We demonstrate the\ncapability of generating realistic chest radiographs, and we show promising\nresults for modeling coronary artery disease. Code:\nhttps://github.com/biomedia-mira/seg-cft.",
        "url": "http://arxiv.org/abs/2509.24913v1",
        "published_date": "2025-09-29T15:19:09+00:00",
        "updated_date": "2025-09-29T15:19:09+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Tian Xia",
            "Matthew Sinclair",
            "Andreas Schuh",
            "Fabio De Sousa Ribeiro",
            "Raghav Mehta",
            "Rajat Rasal",
            "Esther Puyol-Antón",
            "Samuel Gerber",
            "Kersten Petersen",
            "Michiel Schaap",
            "Ben Glocker"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a method called Seg-CFT for generating counterfactual images by intervening on structure-specific variables while producing locally coherent results. It demonstrates the capability to generate realistic chest radiographs and model coronary artery disease.",
        "tldr_zh": "本文提出了一种称为Seg-CFT的方法，通过对结构特定变量进行干预，同时产生局部连贯的结果，生成逼真的胸部放射照片并模拟冠状动脉疾病。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing",
        "summary": "The performance of unified multimodal models for image generation and editing\nis fundamentally constrained by the quality and comprehensiveness of their\ntraining data. While existing datasets have covered basic tasks like style\ntransfer and simple object manipulation, they often lack the systematic\nstructure and challenging scenarios required for real-world applications. To\naddress this bottleneck, we introduce OpenGPT-4o-Image, a large-scale dataset\nconstructed using a novel methodology that combines hierarchical task taxonomy\nwith automated data generation. Our taxonomy not only includes fundamental\ncapabilities such as text rendering and style control but also introduces\nhighly practical yet challenging categories like scientific imagery for\nchemistry illustrations and complex instruction editing requiring simultaneous\nexecution of multiple operations. Through an automated pipeline leveraging\nstructured resource pools and GPT-4o, we generate 80k high-quality\ninstruction-image pairs with controlled diversity, covering 11 major domains\nand 51 subtasks. Extensive experiments show that fine-tuning leading models on\nour dataset achieves significant performance gains across multiple benchmarks,\nwith improvements of up to 18\\% on editing tasks (UniWorld-V1 on ImgEdit-Bench)\nand 13% on generation tasks (Harmon on GenEval). Our work demonstrates that\nsystematic data construction is key to advancing multimodal AI capabilities.",
        "url": "http://arxiv.org/abs/2509.24900v1",
        "published_date": "2025-09-29T15:11:09+00:00",
        "updated_date": "2025-09-29T15:11:09+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhihong Chen",
            "Xuehai Bai",
            "Yang Shi",
            "Chaoyou Fu",
            "Huanyu Zhang",
            "Haotian Wang",
            "Xiaoyan Sun",
            "Zhang Zhang",
            "Liang Wang",
            "Yuanxing Zhang",
            "Pengfei Wan",
            "Yi-Fan Zhang"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer",
            "Multimodality"
        ],
        "tldr": "The paper introduces a new dataset, OpenGPT-4o-Image, for advanced image generation and editing tasks, showing significant performance gains on various benchmarks through fine-tuning leading models.",
        "tldr_zh": "本文介绍了一个新的数据集OpenGPT-4o-Image，用于高级图像生成和编辑任务，通过在多个基准测试上微调领先模型，取得了显著的性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Attention Surgery: An Efficient Recipe to Linearize Your Video Diffusion Transformer",
        "summary": "Transformer-based video diffusion models (VDMs) deliver state-of-the-art\nvideo generation quality but are constrained by the quadratic cost of\nself-attention, making long sequences and high resolutions computationally\nexpensive. While linear attention offers sub-quadratic complexity, prior\nattempts fail to match the expressiveness of softmax attention without costly\nretraining. We introduce \\textit{Attention Surgery}, an efficient framework for\n\\textit{linearizing} or \\textit{hybridizing} attention in pretrained VDMs\nwithout training from scratch. Inspired by recent advances in language models,\nour method combines a novel hybrid attention mechanism-mixing softmax and\nlinear tokens-with a lightweight distillation and fine-tuning pipeline\nrequiring only a few GPU-days. Additionally, we incorporate a cost-aware\nblock-rate strategy to balance expressiveness and efficiency across layers.\nApplied to Wan2.1 1.3B, a state-of-the-art DiT-based VDM, Attention Surgery\nachieves the first competitive sub-quadratic attention video diffusion models,\nreducing attention cost by up to 40\\% in terms of FLOPs, while maintaining\ngeneration quality as measured on the standard VBench and VBench-2.0\nbenchmarks.",
        "url": "http://arxiv.org/abs/2509.24899v1",
        "published_date": "2025-09-29T15:09:51+00:00",
        "updated_date": "2025-09-29T15:09:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mohsen Ghafoorian",
            "Denis Korzhenkov",
            "Amirhossein Habibian"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion"
        ],
        "tldr": "The paper introduces Attention Surgery, a framework for linearizing attention in video diffusion models to reduce computational costs while maintaining generation quality.",
        "tldr_zh": "本文介绍了注意力手术，一种线性化视频扩散模型的框架，以减少计算成本同时保持生成质量。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DAM: Dual Active Learning with Multimodal Foundation Model for Source-Free Domain Adaptation",
        "summary": "Source-free active domain adaptation (SFADA) enhances knowledge transfer from\na source model to an unlabeled target domain using limited manual labels\nselected via active learning. While recent domain adaptation studies have\nintroduced Vision-and-Language (ViL) models to improve pseudo-label quality or\nfeature alignment, they often treat ViL-based and data supervision as separate\nsources, lacking effective fusion. To overcome this limitation, we propose Dual\nActive learning with Multimodal (DAM) foundation model, a novel framework that\nintegrates multimodal supervision from a ViL model to complement sparse human\nannotations, thereby forming a dual supervisory signal. DAM initializes stable\nViL-guided targets and employs a bidirectional distillation mechanism to foster\nmutual knowledge exchange between the target model and the dual supervisions\nduring iterative adaptation. Extensive experiments demonstrate that DAM\nconsistently outperforms existing methods and sets a new state-of-the-art\nacross multiple SFADA benchmarks and active learning strategies.",
        "url": "http://arxiv.org/abs/2509.24896v1",
        "published_date": "2025-09-29T15:06:56+00:00",
        "updated_date": "2025-09-29T15:06:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xi Chen",
            "Hongxun Yao",
            "Zhaopan Xu",
            "Kui Jiang"
        ],
        "ai_categories": [
            "Multimodality",
            "AIGC"
        ],
        "tldr": "The paper proposes a Dual Active Learning with Multimodal foundation model for Source-Free Domain Adaptation, which outperforms existing methods in knowledge transfer between domains.",
        "tldr_zh": "本文提出了一种双活跃学习与多模态基础模型，用于无源域自适应，优于现有方法在领域之间的知识传递。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DWGS: Enhancing Sparse-View Gaussian Splatting with Hybrid-Loss Depth Estimation and Bidirectional Warping",
        "summary": "Novel View Synthesis (NVS) from sparse views remains a core challenge in 3D\nreconstruction, typically suffering from overfitting, geometric distortion, and\nincomplete scene recovery due to limited multi-view constraints. Although 3D\nGaussian Splatting (3DGS) enables real-time, high-fidelity rendering, it\nsuffers from floating artifacts and structural inconsistencies under\nsparse-input settings. To address these issues, we propose DWGS, a novel\nunified framework that enhances 3DGS for sparse-view synthesis by integrating\nrobust structural cues, virtual view constraints, and occluded region\ncompletion. Our approach introduces three principal contributions: a\nHybrid-Loss Depth Estimation module that leverages dense matching priors with\nreprojection, point propagation, and smoothness constraints to enforce\nmulti-view consistency; a Bidirectional Warping Virtual View Synthesis method\ngenerates virtual training views to impose stronger geometric and photometric\nconstraints; and an Occlusion-Aware Reconstruction component that utilizes\ndepth-difference mask and a learning-based inpainting model to recover obscured\nregions. Extensive experiments on standard benchmarks (LLFF, Blender, and DTU)\nshow that DWGS achieves a new state-of-the-art, achieving up to 21.13 dB PSNR\nand 0.189 LPIPS, while retaining real-time inference capabilities.",
        "url": "http://arxiv.org/abs/2509.24893v1",
        "published_date": "2025-09-29T15:03:31+00:00",
        "updated_date": "2025-09-29T15:03:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yu Ma",
            "Guoliang Wei",
            "Yue Cheng"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "DWGS proposes a new framework to enhance 3D Gaussian Splatting for sparse-view synthesis, achieving state-of-the-art results on various benchmarks.",
        "tldr_zh": "DWGS提出了一个新的框架，用于增强3D高斯散点在稀疏视图合成中的应用，在各种基准测试上取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MMRQA: Signal-Enhanced Multimodal Large Language Models for MRI Quality Assessment",
        "summary": "Magnetic resonance imaging (MRI) quality assessment is crucial for clinical\ndecision-making, yet remains challenging due to data scarcity and protocol\nvariability. Traditional approaches face fundamental trade-offs: signal-based\nmethods like MRIQC provide quantitative metrics but lack semantic\nunderstanding, while deep learning approaches achieve high accuracy but\nsacrifice interpretability. To address these limitations, we introduce the\nMultimodal MRI Quality Assessment (MMRQA) framework, pioneering the integration\nof multimodal large language models (MLLMs) with acquisition-aware signal\nprocessing. MMRQA combines three key innovations: robust metric extraction via\nMRQy augmented with simulated artifacts, structured transformation of metrics\ninto question-answer pairs using Qwen, and parameter-efficient fusion through\nLow-Rank Adaptation (LoRA) of LLaVA-OneVision. Evaluated on MR-ART, FastMRI,\nand MyConnectome benchmarks, MMRQA achieves state-of-the-art performance with\nstrong zero-shot generalization, as validated by comprehensive ablation\nstudies. By bridging quantitative analysis with semantic reasoning, our\nframework generates clinically interpretable outputs that enhance quality\ncontrol in dynamic medical settings.",
        "url": "http://arxiv.org/abs/2509.24888v1",
        "published_date": "2025-09-29T15:00:19+00:00",
        "updated_date": "2025-09-29T15:00:19+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Fankai Jia",
            "Daisong Gan",
            "Zhe Zhang",
            "Zhaochi Wen",
            "Chenchen Dan",
            "Dong Liang",
            "Haifeng Wang"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a framework, MMRQA, that integrates multimodal large language models with signal processing for MRI quality assessment, achieving state-of-the-art performance and interpretability.",
        "tldr_zh": "本文介绍了一种框架MMRQA，将多模态大型语言模型与信号处理相结合，用于MRI质量评估，实现了最先进的性能和可解释性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ThermalGen: Style-Disentangled Flow-Based Generative Models for RGB-to-Thermal Image Translation",
        "summary": "Paired RGB-thermal data is crucial for visual-thermal sensor fusion and\ncross-modality tasks, including important applications such as multi-modal\nimage alignment and retrieval. However, the scarcity of synchronized and\ncalibrated RGB-thermal image pairs presents a major obstacle to progress in\nthese areas. To overcome this challenge, RGB-to-Thermal (RGB-T) image\ntranslation has emerged as a promising solution, enabling the synthesis of\nthermal images from abundant RGB datasets for training purposes. In this study,\nwe propose ThermalGen, an adaptive flow-based generative model for RGB-T image\ntranslation, incorporating an RGB image conditioning architecture and a\nstyle-disentangled mechanism. To support large-scale training, we curated eight\npublic satellite-aerial, aerial, and ground RGB-T paired datasets, and\nintroduced three new large-scale satellite-aerial RGB-T datasets--DJI-day,\nBosonplus-day, and Bosonplus-night--captured across diverse times, sensor\ntypes, and geographic regions. Extensive evaluations across multiple RGB-T\nbenchmarks demonstrate that ThermalGen achieves comparable or superior\ntranslation performance compared to existing GAN-based and diffusion-based\nmethods. To our knowledge, ThermalGen is the first RGB-T image translation\nmodel capable of synthesizing thermal images that reflect significant\nvariations in viewpoints, sensor characteristics, and environmental conditions.\nProject page: http://xjh19971.github.io/ThermalGen",
        "url": "http://arxiv.org/abs/2509.24878v1",
        "published_date": "2025-09-29T14:55:51+00:00",
        "updated_date": "2025-09-29T14:55:51+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Jiuhong Xiao",
            "Roshan Nayak",
            "Ning Zhang",
            "Daniel Tortei",
            "Giuseppe Loianno"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper presents ThermalGen, a model for translating RGB images to thermal images with improved performance compared to existing methods.",
        "tldr_zh": "本文提出了ThermalGen模型，用于将RGB图像转换为热像，性能优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Training-Free Token Pruning via Zeroth-Order Gradient Estimation in Vision-Language Models",
        "summary": "Large Vision-Language Models (VLMs) enable strong multimodal reasoning but\nincur heavy inference costs from redundant visual tokens. Token pruning\nalleviates this issue, yet existing approaches face limitations.\nAttention-based methods rely on raw attention scores, which are often unstable\nacross layers and heads and can lead to redundant selections. Diversity-based\nmethods improve robustness by selecting tokens far apart in feature space but\nrisk dropping regions needed for accurate prediction. We propose \\ours, a\ntraining-free framework built on a simple intuition: tokens with higher\nsensitivity are more likely to influence the model's output, and they should\nalso capture complementary visual cues rather than overlapping information. To\nachieve this, we estimate token sensitivity using zeroth-order perturbations at\nthe projection layer, a shallow and computationally light component of the\nmodel. This approach measures how small random perturbations affect the\nprojection outputs, allowing us to approximate each token's influence through\nlightweight forward passes without backpropagation. Extensive experiments\nacross multiple VLMs and benchmarks show that \\ours consistently outperforms\nprior methods, pruning up to 94.4\\% of tokens while maintaining accuracy and\nsignificantly improving efficiency, achieving up to 2.30x faster end-to-end\ninference over the baseline.",
        "url": "http://arxiv.org/abs/2509.24837v1",
        "published_date": "2025-09-29T14:20:05+00:00",
        "updated_date": "2025-09-29T14:20:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Youngeun Kim",
            "Youjia Zhang",
            "Huiling Liu",
            "Aecheon Jung",
            "Sunwoo Lee",
            "Sungeun Hong"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a training-free token pruning method for Vision-Language Models, achieving significant efficiency improvements with minimal loss of accuracy.",
        "tldr_zh": "本文提出了一种针对视觉语言模型的零训练标记修剪方法，实现了显著的效率提升，几乎没有准确度损失。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Of-SemWat: High-payload text embedding for semantic watermarking of AI-generated images with arbitrary size",
        "summary": "We propose a high-payload image watermarking method for textual embedding,\nwhere a semantic description of the image - which may also correspond to the\ninput text prompt-, is embedded inside the image. In order to be able to\nrobustly embed high payloads in large-scale images - such as those produced by\nmodern AI generators - the proposed approach builds upon a traditional\nwatermarking scheme that exploits orthogonal and turbo codes for improved\nrobustness, and integrates frequency-domain embedding and perceptual masking\ntechniques to enhance watermark imperceptibility. Experiments show that the\nproposed method is extremely robust against a wide variety of image processing,\nand the embedded text can be retrieved also after traditional and AI\ninpainting, permitting to unveil the semantic modification the image has\nundergone via image-text mismatch analysis.",
        "url": "http://arxiv.org/abs/2509.24823v1",
        "published_date": "2025-09-29T14:10:15+00:00",
        "updated_date": "2025-09-29T14:10:15+00:00",
        "categories": [
            "cs.CR",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Benedetta Tondi",
            "Andrea Costanzo",
            "Mauro Barni"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "A high-payload text embedding method for semantic watermarking of AI-generated images is proposed, with robustness and imperceptibility enhancements.",
        "tldr_zh": "提出了一种用于AI生成图像语义水印的高负载文本嵌入方法，具有鲁棒性和隐蔽性增强。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UP2You: Fast Reconstruction of Yourself from Unconstrained Photo Collections",
        "summary": "We present UP2You, the first tuning-free solution for reconstructing\nhigh-fidelity 3D clothed portraits from extremely unconstrained in-the-wild 2D\nphotos. Unlike previous approaches that require \"clean\" inputs (e.g., full-body\nimages with minimal occlusions, or well-calibrated cross-view captures), UP2You\ndirectly processes raw, unstructured photographs, which may vary significantly\nin pose, viewpoint, cropping, and occlusion. Instead of compressing data into\ntokens for slow online text-to-3D optimization, we introduce a data rectifier\nparadigm that efficiently converts unconstrained inputs into clean, orthogonal\nmulti-view images in a single forward pass within seconds, simplifying the 3D\nreconstruction. Central to UP2You is a pose-correlated feature aggregation\nmodule (PCFA), that selectively fuses information from multiple reference\nimages w.r.t. target poses, enabling better identity preservation and nearly\nconstant memory footprint, with more observations. We also introduce a\nperceiver-based multi-reference shape predictor, removing the need for\npre-captured body templates. Extensive experiments on 4D-Dress, PuzzleIOI, and\nin-the-wild captures demonstrate that UP2You consistently surpasses previous\nmethods in both geometric accuracy (Chamfer-15%, P2S-18% on PuzzleIOI) and\ntexture fidelity (PSNR-21%, LPIPS-46% on 4D-Dress). UP2You is efficient (1.5\nminutes per person), and versatile (supports arbitrary pose control, and\ntraining-free multi-garment 3D virtual try-on), making it practical for\nreal-world scenarios where humans are casually captured. Both models and code\nwill be released to facilitate future research on this underexplored task.\nProject Page: https://zcai0612.github.io/UP2You",
        "url": "http://arxiv.org/abs/2509.24817v1",
        "published_date": "2025-09-29T14:06:00+00:00",
        "updated_date": "2025-09-29T14:06:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zeyu Cai",
            "Ziyang Li",
            "Xiaoben Li",
            "Boqian Li",
            "Zeyu Wang",
            "Zhenyu Zhang",
            "Yuliang Xiu"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "UP2You presents a fast and tuning-free method for reconstructing 3D clothed portraits from unstructured 2D photos, outperforming previous methods in geometric accuracy and texture fidelity.",
        "tldr_zh": "UP2You 提出了一种快速且无需调整的方法，可以从非结构化的2D照片中重建3D服装画像，表现优异于先前的方法在几何精度和纹理保真度上。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TACO-Net: Topological Signatures Triumph in 3D Object Classification",
        "summary": "3D object classification is a crucial problem due to its significant\npractical relevance in many fields, including computer vision, robotics, and\nautonomous driving. Although deep learning methods applied to point clouds\nsampled on CAD models of the objects and/or captured by LiDAR or RGBD cameras\nhave achieved remarkable success in recent years, achieving high classification\naccuracy remains a challenging problem due to the unordered point clouds and\ntheir irregularity and noise. To this end, we propose a novel state-of-the-art\n(SOTA) 3D object classification technique that combines topological data\nanalysis with various image filtration techniques to classify objects when they\nare represented using point clouds. We transform every point cloud into a\nvoxelized binary 3D image to extract distinguishing topological features. Next,\nwe train a lightweight one-dimensional Convolutional Neural Network (1D CNN)\nusing the extracted feature set from the training dataset. Our framework,\nTACO-Net, sets a new state-of-the-art by achieving $99.05\\%$ and $99.52\\%$\naccuracy on the widely used synthetic benchmarks ModelNet40 and ModelNet10, and\nfurther demonstrates its robustness on the large-scale real-world OmniObject3D\ndataset. When tested with ten different kinds of corrupted ModelNet40 inputs,\nthe proposed TACO-Net demonstrates strong resiliency overall.",
        "url": "http://arxiv.org/abs/2509.24802v1",
        "published_date": "2025-09-29T13:52:53+00:00",
        "updated_date": "2025-09-29T13:52:53+00:00",
        "categories": [
            "cs.CV",
            "cs.CG",
            "cs.LG"
        ],
        "authors": [
            "Anirban Ghosh",
            "Ayan Dutta"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces TACO-Net, a novel 3D object classification technique that combines topological data analysis with image filtration techniques to achieve high accuracy on various datasets.",
        "tldr_zh": "该论文介绍了TACO-Net，一种新颖的3D物体分类技术，将拓扑数据分析与图像滤波技术结合，实现在各种数据集上的高准确率。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Causal-Adapter: Taming Text-to-Image Diffusion for Faithful Counterfactual Generation",
        "summary": "We present Causal-Adapter, a modular framework that adapts frozen\ntext-to-image diffusion backbones for counterfactual image generation. Our\nmethod enables causal interventions on target attributes, consistently\npropagating their effects to causal dependents without altering the core\nidentity of the image. In contrast to prior approaches that rely on prompt\nengineering without explicit causal structure, Causal-Adapter leverages\nstructural causal modeling augmented with two attribute regularization\nstrategies: prompt-aligned injection, which aligns causal attributes with\ntextual embeddings for precise semantic control, and a conditioned token\ncontrastive loss to disentangle attribute factors and reduce spurious\ncorrelations. Causal-Adapter achieves state-of-the-art performance on both\nsynthetic and real-world datasets, with up to 91\\% MAE reduction on Pendulum\nfor accurate attribute control and 87\\% FID reduction on ADNI for high-fidelity\nMRI image generation. These results show that our approach enables robust,\ngeneralizable counterfactual editing with faithful attribute modification and\nstrong identity preservation.",
        "url": "http://arxiv.org/abs/2509.24798v1",
        "published_date": "2025-09-29T13:49:28+00:00",
        "updated_date": "2025-09-29T13:49:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Lei Tong",
            "Zhihua Liu",
            "Chaochao Lu",
            "Dino Oglic",
            "Tom Diethe",
            "Philip Teare",
            "Sotirios A. Tsaftaris",
            "Chen Jin"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces Causal-Adapter, a framework for generating counterfactual images by adapting text-to-image diffusion models with causal interventions and attribute regularization strategies.",
        "tldr_zh": "该论文介绍了Causal-Adapter，一个通过对文本到图像扩散模型进行因果干预和属性规范化策略来生成反事实图像的框架。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Vision Function Layer in Multimodal LLMs",
        "summary": "This study identifies that visual-related functional decoding is distributed\nacross different decoder layers in Multimodal Large Language Models (MLLMs).\nTypically, each function, such as counting, grounding, or OCR recognition,\nnarrows down to two or three layers, which we define as Vision Function Layers\n(VFL). Additionally, the depth and its order of different VFLs exhibits a\nconsistent pattern across different MLLMs, which is well-aligned with human\nbehaviors (e.g., recognition occurs first, followed by counting, and then\ngrounding). These findings are derived from Visual Token Swapping, our novel\nanalytical framework that modifies targeted KV cache entries to precisely\nelucidate layer-specific functions during decoding. Furthermore, these insights\noffer substantial utility in tailoring MLLMs for real-world downstream\napplications. For instance, when LoRA training is selectively applied to VFLs\nwhose functions align with the training data, VFL-LoRA not only outperform\nfull-LoRA but also prevent out-of-domain function forgetting. Moreover, by\nanalyzing the performance differential on training data when particular VFLs\nare ablated, VFL-select automatically classifies data by function, enabling\nhighly efficient data selection to directly bolster corresponding capabilities.\nConsequently, VFL-select surpasses human experts in data selection, and\nachieves 98% of full-data performance with only 20% of the original dataset.\nThis study delivers deeper comprehension of MLLM visual processing, fostering\nthe creation of more efficient, interpretable, and robust models.",
        "url": "http://arxiv.org/abs/2509.24791v1",
        "published_date": "2025-09-29T13:45:35+00:00",
        "updated_date": "2025-09-29T13:45:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Cheng Shi",
            "Yizhou Yu",
            "Sibei Yang"
        ],
        "ai_categories": [
            "Multimodality",
            "LoRA",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces the concept of Vision Function Layers in Multimodal Large Language Models (MLLMs) and their consistent patterns across different models. It demonstrates the utility of these layers in tailoring MLLMs for real-world applications.",
        "tldr_zh": "该论文介绍了多模型大语言模型（MLLMs）中的视觉功能层概念及其在不同模型中的一致模式。它展示了这些层在定制MLLMs以适用于现实应用中的实用性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LOVE-R1: Advancing Long Video Understanding with an Adaptive Zoom-in Mechanism via Multi-Step Reasoning",
        "summary": "Long video understanding is still challenging for recent Large Video-Language\nModels (LVLMs) due to the conflict between long-form temporal understanding and\ndetailed spatial perception. LVLMs with a uniform frame sampling mechanism,\nwhich samples frames with an equal frame size and fixed sampling rate,\ninevitably sacrifice either temporal clues or spatial details, resulting in\nsuboptimal solutions. To mitigate this dilemma, we propose LOVE-R1, a model\nthat can adaptively zoom in on a video clip. The model is first provided with\ndensely sampled frames but in a small resolution. If some spatial details are\nneeded, the model can zoom in on a clip of interest with a large frame\nresolution based on its reasoning until key visual information is obtained. The\nwhole process is implemented as a multi-step reasoning process. To train the\nreasoning ability, we first finetune the model on our collected 38k\nhigh-quality CoT data and enhance it with decoupled reinforcement finetuning.\nAs outcome rewards can not provide fine-grained process supervision, we\ndecouple multi-step reasoning into multiple single-step reasoning and optimize\nthe internal zoom-in ability explicitly. Experiments on long video\nunderstanding benchmarks show that our model with the slow-fast adaptive frame\nsampling mechanism achieves a great trade-off between sampling density and\nframe resolutions, and LOVE-R1 outperforms our baseline Qwen2.5-VL by an\naverage of 3.1% points across 4 common long video understanding benchmarks.",
        "url": "http://arxiv.org/abs/2509.24786v1",
        "published_date": "2025-09-29T13:43:55+00:00",
        "updated_date": "2025-09-29T13:43:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shenghao Fu",
            "Qize Yang",
            "Yuan-Ming Li",
            "Xihan Wei",
            "Xiaohua Xie",
            "Wei-Shi Zheng"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "LOVE-R1 proposes an adaptive zoom-in mechanism for long video understanding to balance temporal clues and spatial details, outperforming baseline models by 3.1% on common benchmarks.",
        "tldr_zh": "LOVE-R1提出了一种自适应缩放机制，用于长视频理解，平衡时间线索和空间细节，比常见基准模型提高3.1%。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VTPerception-R1: Enhancing Multimodal Reasoning via Explicit Visual and Textual Perceptual Grounding",
        "summary": "Multimodal large language models (MLLMs) often struggle to ground reasoning\nin perceptual evidence. We present a systematic study of perception\nstrategies-explicit, implicit, visual, and textual-across four multimodal\nbenchmarks and two MLLMs. Our findings show that explicit perception,\nespecially when paired with textual cues, consistently yields the best\nimprovements, particularly for smaller models. Based on this insight, we\npropose VTPerception-R1, a unified two-stage framework that decouples\nperception from reasoning. Stage 1 introduces perception-augmented fine-tuning,\nand Stage 2 applies perception-aware reinforcement learning with novel visual,\ntextual, and consistency rewards. Experiments demonstrate that VTPerception-R1\nsignificantly improves reasoning accuracy and robustness across diverse tasks,\noffering a scalable and auditable solution for perception-grounded multimodal\nreasoning. Our code is available at:\nhttps://github.com/yizhuoDi/VTPerceprion-R1.",
        "url": "http://arxiv.org/abs/2509.24776v1",
        "published_date": "2025-09-29T13:40:34+00:00",
        "updated_date": "2025-09-29T13:40:34+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yizhuo Ding",
            "Mingkang Chen",
            "Zhibang Feng",
            "Tong Xiao",
            "Wanying Qu",
            "Wenqi Shao",
            "Yanwei Fu"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a framework, VTPerception-R1, that improves reasoning accuracy and robustness in multimodal models by decoupling perception from reasoning through two stages of fine-tuning and reinforcement learning.",
        "tldr_zh": "本文提出了一个框架，VTPerception-R1，通过两个阶段的微调和强化学习，将感知与推理分离，从而提高多模态模型的推理准确性和稳健性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ExGS: Extreme 3D Gaussian Compression with Diffusion Priors",
        "summary": "Neural scene representations, such as 3D Gaussian Splatting (3DGS), have\nenabled high-quality neural rendering; however, their large storage and\ntransmission costs hinder deployment in resource-constrained environments.\nExisting compression methods either rely on costly optimization, which is slow\nand scene-specific, or adopt training-free pruning and quantization, which\ndegrade rendering quality under high compression ratios. In contrast, recent\ndata-driven approaches provide a promising direction to overcome this\ntrade-off, enabling efficient compression while preserving high rendering\nquality. We introduce \\textbf{ExGS}, a novel feed-forward framework that\nunifies \\textbf{Universal Gaussian Compression} (UGC) with\n\\textbf{GaussPainter} for \\textbf{Ex}treme 3D\\textbf{GS} compression.\n\\textbf{UGC} performs re-optimization-free pruning to aggressively reduce\nGaussian primitives while retaining only essential information, whereas\n\\textbf{GaussPainter} leverages powerful diffusion priors with mask-guided\nrefinement to restore high-quality renderings from heavily pruned Gaussian\nscenes. Unlike conventional inpainting, GaussPainter not only fills in missing\nregions but also enhances visible pixels, yielding substantial improvements in\ndegraded renderings. To ensure practicality, it adopts a lightweight VAE and a\none-step diffusion design, enabling real-time restoration. Our framework can\neven achieve over $100\\times$ compression (reducing a typical 354.77 MB model\nto about 3.31 MB) while preserving fidelity and significantly improving image\nquality under challenging conditions. These results highlight the central role\nof diffusion priors in bridging the gap between extreme compression and\nhigh-quality neural rendering. Our code repository will be released at\n\\href{https://github.com/chenttt2001/ExGS}{here}.",
        "url": "http://arxiv.org/abs/2509.24758v1",
        "published_date": "2025-09-29T13:23:06+00:00",
        "updated_date": "2025-09-29T13:23:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaqi Chen",
            "Xinhao Ji",
            "Yuanyuan Gao",
            "Hao Li",
            "Yuning Gong",
            "Yifei Liu",
            "Dan Xu",
            "Zhihang Zhong",
            "Dingwen Zhang",
            "Xiao Sun"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper introduces ExGS, a novel framework that combines Universal Gaussian Compression and GaussPainter for extreme 3D Gaussian compression, achieving high compression ratios without compromising rendering quality.",
        "tldr_zh": "本文介绍了ExGS，一种将Universal Gaussian Compression和GaussPainter结合起来进行极端3D高斯压缩的新框架，实现高压缩比而不影响渲染质量。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Toward a Vision-Language Foundation Model for Medical Data: Multimodal Dataset and Benchmarks for Vietnamese PET/CT Report Generation",
        "summary": "Vision-Language Foundation Models (VLMs), trained on large-scale multimodal\ndatasets, have driven significant advances in Artificial Intelligence by\nenabling rich cross-modal reasoning. Despite their success in general domains,\napplying these models to medical imaging remains challenging due to the limited\navailability of diverse imaging modalities and multilingual clinical data. Most\nexisting medical VLMs are trained on a subset of imaging modalities and focus\nprimarily on high-resource languages, thus limiting their generalizability and\nclinical utility. To address these limitations, we introduce a novel\nVietnamese-language multimodal medical dataset comprising 1,567,062 paired\nCT-PET images and corresponding 2,757 full-length clinical reports. This\ndataset is designed to fill two pressing gaps in medical AI development: (1)\nthe lack of PET/CT imaging data in existing VLMs training corpora, which\nhinders the development of models capable of handling functional imaging tasks;\nand (2) the underrepresentation of low-resource languages, particularly the\nVietnamese language, in medical vision-language research. To the best of our\nknowledge, this is the first dataset to provide comprehensive PET/CT-report\npairs in Vietnamese. We further introduce a training framework to enhance VLMs'\nlearning, including data augmentation and expert-validated test sets. We\nconduct comprehensive experiments benchmarking state-of-the-art VLMs on\ndownstream tasks, including medical report generation and visual question\nanswering. The experimental results show that incorporating our dataset\nsignificantly improves the performance of existing VLMs. We believe this\ndataset and benchmark will serve as a pivotal step in advancing the development\nof more robust VLMs for medical imaging, particularly in low-resource\nlanguages, and improving their clinical relevance in Vietnamese healthcare.",
        "url": "http://arxiv.org/abs/2509.24739v1",
        "published_date": "2025-09-29T13:03:57+00:00",
        "updated_date": "2025-09-29T13:03:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Huu Tien Nguyen",
            "Dac Thai Nguyen",
            "The Minh Duc Nguyen",
            "Trung Thanh Nguyen",
            "Thao Nguyen Truong",
            "Huy Hieu Pham",
            "Johan Barthelemy",
            "Minh Quan Tran",
            "Thanh Tam Nguyen",
            "Quoc Viet Hung Nguyen",
            "Quynh Anh Chau",
            "Hong Son Mai",
            "Thanh Trung Nguyen",
            "Phi Le Nguyen"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "Introduction of a new Vietnamese-language multimodal medical dataset for PET/CT imaging paired with clinical reports, enhancing VLMs' learning and improving performance on medical imaging tasks.",
        "tldr_zh": "引入一种新的越南语多模式医学数据集，用于PET/CT成像与临床报告配对，增强VLMs的学习并提高医学影像任务的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Evaluation of Polarimetric Fusion for Semantic Segmentation in Aquatic Environments",
        "summary": "Accurate segmentation of floating debris on water is often compromised by\nsurface glare and changing outdoor illumination. Polarimetric imaging offers a\nsingle-sensor route to mitigate water-surface glare that disrupts semantic\nsegmentation of floating objects. We benchmark state-of-the-art fusion networks\non PoTATO, a public dataset of polarimetric images of plastic bottles in inland\nwaterways, and compare their performance with single-image baselines using\ntraditional models. Our results indicate that polarimetric cues help recover\nlow-contrast objects and suppress reflection-induced false positives, raising\nmean IoU and lowering contour error relative to RGB inputs. These sharper masks\ncome at a cost: the additional channels enlarge the models increasing the\ncomputational load and introducing the risk of new false positives. By\nproviding a reproducible, diagnostic benchmark and publicly available code, we\nhope to help researchers choose if polarized cameras are suitable for their\napplications and to accelerate related research.",
        "url": "http://arxiv.org/abs/2509.24731v1",
        "published_date": "2025-09-29T12:57:20+00:00",
        "updated_date": "2025-09-29T12:57:20+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Luis F. W. Batista",
            "Tom Bourbon",
            "Cedric Pradalier"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper evaluates the use of polarimetric fusion for semantic segmentation in aquatic environments, showing improved performance in recovering low-contrast objects and suppressing false positives.",
        "tldr_zh": "本文评估了极化融合在水域语义分割中的应用，展示了在恢复低对比度物体和抑制假阳性方面的表现性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "IWR-Bench: Can LVLMs reconstruct interactive webpage from a user interaction video?",
        "summary": "The webpage-to-code task requires models to understand visual representations\nof webpages and generate corresponding code. However, existing benchmarks\nprimarily focus on static screenshot-to-code tasks, thereby overlooking the\ndynamic interactions fundamental to real-world web applications. To address\nthis limitation, this paper introduces IWR-Bench, a novel benchmark for\nevaluating the capabilities of Large Vision-Language Models (LVLMs) in\ninteractive webpage reconstruction from video. IWR-Bench comprises 113\nmeticulously curated tasks from 100 real-world websites, with 1,001 actions and\nfeaturing diverse interaction complexities (e.g., web games), visual styles,\nand domains. Aligning with standard web development practices, each task\nincludes not only user interaction videos but also all crawled static assets\n(e.g., images, videos). This benchmark evaluates models on two fundamental\nchallenges: comprehensive multi-modal reasoning to infer interaction logic from\nvideo and assets, and advanced code generation to translate this logic into\nfunctional code. An agent-as-a-judge framework with a comprehensive metric\nsystem automatically assesses the functional correctness and visual fidelity of\ngenerated webpages. Extensive experiments on 28 LVLMs reveal a significant\nchallenge: the best model achieves an overall score of only 36.35%, as\nfunctional correctness (24.39% IFS) lags significantly behind visual fidelity\n(64.25% VFS). These results highlight critical limitations in current models'\nability to reason about temporal dynamics and synthesize event-driven logic,\nestablishing IWR-Bench as a challenging frontier for vision-language research.\nThe benchmark and evaluation code will be made publicly available. Code is\navailable at https://github.com/L-O-I/IWR-Bench.",
        "url": "http://arxiv.org/abs/2509.24709v1",
        "published_date": "2025-09-29T12:38:06+00:00",
        "updated_date": "2025-09-29T12:38:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yang Chen",
            "Minghao Liu",
            "Yufan Shen",
            "Yunwen Li",
            "Tianyuan Huang",
            "Xinyu Fang",
            "Tianyu Zheng",
            "Wenxuan Huang",
            "Cheng Yang",
            "Daocheng Fu",
            "Jianbiao Mei",
            "Rong Wu",
            "Licheng Wen",
            "Xuemeng Yang",
            "Song Mao",
            "Qunshu Lin",
            "Zhi Yu",
            "Yongliang Shen",
            "Yu Qiao",
            "Botian Shi"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces IWR-Bench, a benchmark for evaluating Large Vision-Language Models in reconstructing interactive webpages from videos. Results show current models struggle with reasoning temporal dynamics and event-driven logic.",
        "tldr_zh": "本文介绍了IWR-Bench，这是一个评估大规模视觉-语言模型在从视频中重建交互式网页方面的基准。结果显示当前模型在推理时间动态和事件驱动逻辑方面存在困难。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "VNODE: A Piecewise Continuous Volterra Neural Network",
        "summary": "This paper introduces Volterra Neural Ordinary Differential Equations\n(VNODE), a piecewise continuous Volterra Neural Network that integrates\nnonlinear Volterra filtering with continuous time neural ordinary differential\nequations for image classification. Drawing inspiration from the visual cortex,\nwhere discrete event processing is interleaved with continuous integration,\nVNODE alternates between discrete Volterra feature extraction and ODE driven\nstate evolution. This hybrid formulation captures complex patterns while\nrequiring substantially fewer parameters than conventional deep architectures.\nVNODE consistently outperforms state of the art models with improved\ncomputational complexity as exemplified on benchmark datasets like CIFAR10 and\nImagenet1K.",
        "url": "http://arxiv.org/abs/2509.24659v1",
        "published_date": "2025-09-29T12:05:57+00:00",
        "updated_date": "2025-09-29T12:05:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Siddharth Roheda",
            "Aniruddha Bala",
            "Rohit Chowdhury",
            "Rohan Jaiswal"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces VNODE, a hybrid neural network combining Volterra filtering and neural ODE for image classification with improved performance and fewer parameters.",
        "tldr_zh": "本文介绍了VNODE，一种将Volterra滤波器和神经ODE结合的混合神经网络，用于图像分类，性能更好且参数更少。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RIFLE: Removal of Image Flicker-Banding via Latent Diffusion Enhancement",
        "summary": "Capturing screens is now routine in our everyday lives. But the photographs\nof emissive displays are often influenced by the flicker-banding (FB), which is\nalternating bright%u2013dark stripes that arise from temporal aliasing between\na camera's rolling-shutter readout and the display's brightness modulation.\nUnlike moire degradation, which has been extensively studied, the FB remains\nunderexplored despite its frequent and severe impact on readability and\nperceived quality. We formulate FB removal as a dedicated restoration task and\nintroduce Removal of Image Flicker-Banding via Latent Diffusion Enhancement,\nRIFLE, a diffusion-based framework designed to remove FB while preserving fine\ndetails. We propose the flicker-banding prior estimator (FPE) that predicts key\nbanding attributes and injects it into the restoration network. Additionally,\nMasked Loss (ML) is proposed to concentrate supervision on banded regions\nwithout sacrificing global fidelity. To overcome data scarcity, we provide a\nsimulation pipeline that synthesizes FB in the luminance domain with stochastic\njitter in banding angle, banding spacing, and banding width. Feathered\nboundaries and sensor noise are also applied for a more realistic simulation.\nFor evaluation, we collect a paired real-world FB dataset with pixel-aligned\nbanding-free references captured via long exposure. Across quantitative metrics\nand visual comparisons on our real-world dataset, RIFLE consistently\noutperforms recent image reconstruction baselines from mild to severe\nflicker-banding. To the best of our knowledge, it is the first work to research\nthe simulation and removal of FB. Our work establishes a great foundation for\nsubsequent research in both the dataset construction and the removal model\ndesign. Our dataset and code will be released soon.",
        "url": "http://arxiv.org/abs/2509.24644v1",
        "published_date": "2025-09-29T11:53:13+00:00",
        "updated_date": "2025-09-29T11:53:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhu",
            "Libo",
            "Zhou",
            "Zihan",
            "Liu",
            "Xiaoyang",
            "Zhang",
            "Weihang",
            "Shi",
            "Keyu",
            "Fu",
            "Yifan",
            "Zhang",
            "Yulun"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper introduces RIFLE, a framework to remove flicker-banding in images taken from emissive displays, improving image quality and readability.",
        "tldr_zh": "该论文介绍了RIFLE框架，用于消除从发光显示器拍摄的图像中的闪烁带，提高图像质量和可读性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Discovering \"Words\" in Music: Unsupervised Learning of Compositional Sparse Code for Symbolic Music",
        "summary": "This paper presents an unsupervised machine learning algorithm that\nidentifies recurring patterns -- referred to as ``music-words'' -- from\nsymbolic music data. These patterns are fundamental to musical structure and\nreflect the cognitive processes involved in composition. However, extracting\nthese patterns remains challenging because of the inherent semantic ambiguity\nin musical interpretation. We formulate the task of music-word discovery as a\nstatistical optimization problem and propose a two-stage\nExpectation-Maximization (EM)-based learning framework: 1. Developing a\nmusic-word dictionary; 2. Reconstructing the music data. When evaluated against\nhuman expert annotations, the algorithm achieved an Intersection over Union\n(IoU) score of 0.61. Our findings indicate that minimizing code length\neffectively addresses semantic ambiguity, suggesting that human optimization of\nencoding systems shapes musical semantics. This approach enables computers to\nextract ``basic building blocks'' from music data, facilitating structural\nanalysis and sparse encoding. The method has two primary applications. First,\nin AI music, it supports downstream tasks such as music generation,\nclassification, style transfer, and improvisation. Second, in musicology, it\nprovides a tool for analyzing compositional patterns and offers insights into\nthe principle of minimal encoding across diverse musical styles and composers.",
        "url": "http://arxiv.org/abs/2509.24603v1",
        "published_date": "2025-09-29T11:10:57+00:00",
        "updated_date": "2025-09-29T11:10:57+00:00",
        "categories": [
            "cs.SD",
            "cs.CV"
        ],
        "authors": [
            "Tianle Wang",
            "Sirui Zhang",
            "Xinyi Tong",
            "Peiyang Yu",
            "Jishang Chen",
            "Liangke Zhao",
            "Xinpu Gao",
            "Yves Zhu",
            "Tiezheng Ge",
            "Bo Zheng",
            "Duo Xu",
            "Yang Liu",
            "Xin Jin",
            "Feng Yu",
            "Songchun Zhu"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces an unsupervised learning algorithm to discover recurring patterns in symbolic music, called 'music-words'. It has applications in AI music for tasks like generation and classification, as well as in musicology for analyzing compositional patterns.",
        "tldr_zh": "本文介绍了一种无监督学习算法，用于发现象征音乐中的重复模式，称为'音乐词'。它在AI音乐中有生成和分类等任务上的应用，同时也可被应用于音乐学分析作曲模式。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SAIP: A Plug-and-Play Scale-adaptive Module in Diffusion-based Inverse Problems",
        "summary": "Solving inverse problems with diffusion models has shown promise in tasks\nsuch as image restoration. A common approach is to formulate the problem in a\nBayesian framework and sample from the posterior by combining the prior score\nwith the likelihood score. Since the likelihood term is often intractable,\nestimators like DPS, DMPS, and $\\pi$GDM are widely adopted. However, these\nmethods rely on a fixed, manually tuned scale to balance prior and likelihood\ncontributions. Such a static design is suboptimal, as the ideal balance varies\nacross timesteps and tasks, limiting performance and generalization. To address\nthis issue, we propose SAIP, a plug-and-play module that adaptively refines the\nscale at each timestep without retraining or altering the diffusion backbone.\nSAIP integrates seamlessly into existing samplers and consistently improves\nreconstruction quality across diverse image restoration tasks, including\nchallenging scenarios.",
        "url": "http://arxiv.org/abs/2509.24580v1",
        "published_date": "2025-09-29T10:41:20+00:00",
        "updated_date": "2025-09-29T10:41:20+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Lingyu Wang",
            "Xiangming Meng"
        ],
        "ai_categories": [
            "Diffusion"
        ],
        "tldr": "SAIP proposes a scale-adaptive module for solving diffusion-based inverse problems in image restoration tasks, improving reconstruction quality across diverse scenarios.",
        "tldr_zh": "SAIP提出了一个自适应模块，用于解决基于扩散的反问题，提高各种场景下的重建质量。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "BFSM: 3D Bidirectional Face-Skull Morphable Model",
        "summary": "Building a joint face-skull morphable model holds great potential for\napplications such as remote diagnostics, surgical planning, medical education,\nand physically based facial simulation. However, realizing this vision is\nconstrained by the scarcity of paired face-skull data, insufficient\nregistration accuracy, and limited exploration of reconstruction and clinical\napplications. Moreover, individuals with craniofacial deformities are often\noverlooked, resulting in underrepresentation and limited inclusivity. To\naddress these challenges, we first construct a dataset comprising over 200\nsamples, including both normal cases and rare craniofacial conditions. Each\ncase contains a CT-based skull, a CT-based face, and a high-fidelity textured\nface scan. Secondly, we propose a novel dense ray matching registration method\nthat ensures topological consistency across face, skull, and their tissue\ncorrespondences. Based on this, we introduce the 3D Bidirectional Face-Skull\nMorphable Model (BFSM), which enables shape inference between the face and\nskull through a shared coefficient space, while also modeling tissue thickness\nvariation to support one-to-many facial reconstructions from the same skull,\nreflecting individual changes such as fat over time. Finally, we demonstrate\nthe potential of BFSM in medical applications, including 3D face-skull\nreconstruction from a single image and surgical planning prediction. Extensive\nexperiments confirm the robustness and accuracy of our method. BFSM is\navailable at https://github.com/wang-zidu/BFSM",
        "url": "http://arxiv.org/abs/2509.24577v1",
        "published_date": "2025-09-29T10:34:13+00:00",
        "updated_date": "2025-09-29T10:34:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zidu Wang",
            "Meng Xu",
            "Miao Xu",
            "Hengyuan Ma",
            "Jiankuo Zhao",
            "Xutao Li",
            "Xiangyu Zhu",
            "Zhen Lei"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer",
            "Diffusion"
        ],
        "tldr": "The paper introduces a 3D Bidirectional Face-Skull Morphable Model (BFSM) for applications in medical fields, such as 3D face-skull reconstruction and surgical planning.",
        "tldr_zh": "本论文介绍了一种用于医学领域的3D双向面颅可塑模型（BFSM），可用于3D面颅重建和外科规划。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "SCOPE: Semantic Conditioning for Sim2Real Category-Level Object Pose Estimation in Robotics",
        "summary": "Object manipulation requires accurate object pose estimation. In open\nenvironments, robots encounter unknown objects, which requires semantic\nunderstanding in order to generalize both to known categories and beyond. To\nresolve this challenge, we present SCOPE, a diffusion-based category-level\nobject pose estimation model that eliminates the need for discrete category\nlabels by leveraging DINOv2 features as continuous semantic priors. By\ncombining these DINOv2 features with photorealistic training data and a noise\nmodel for point normals, we reduce the Sim2Real gap in category-level object\npose estimation. Furthermore, injecting the continuous semantic priors via\ncross-attention enables SCOPE to learn canonicalized object coordinate systems\nacross object instances beyond the distribution of known categories. SCOPE\noutperforms the current state of the art in synthetically trained\ncategory-level object pose estimation, achieving a relative improvement of\n31.9\\% on the 5$^\\circ$5cm metric. Additional experiments on two instance-level\ndatasets demonstrate generalization beyond known object categories, enabling\ngrasping of unseen objects from unknown categories with a success rate of up to\n100\\%. Code available: https://github.com/hoenigpeter/scope.",
        "url": "http://arxiv.org/abs/2509.24572v1",
        "published_date": "2025-09-29T10:27:59+00:00",
        "updated_date": "2025-09-29T10:27:59+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Peter Hönig",
            "Stefan Thalhammer",
            "Jean-Baptiste Weibel",
            "Matthias Hirschmanner",
            "Markus Vincze"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "SCOPE is a novel object pose estimation model for robots that uses semantic priors and photorealistic training data to generalize to unknown object categories.",
        "tldr_zh": "SCOPE是一种新颖的对象姿态估计模型，使用语义先验和逼真的训练数据来对未知对象类别进行泛化。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TokenSwap: Backdoor Attack on the Compositional Understanding of Large Vision-Language Models",
        "summary": "Large vision-language models (LVLMs) have achieved impressive performance\nacross a wide range of vision-language tasks, while they remain vulnerable to\nbackdoor attacks. Existing backdoor attacks on LVLMs aim to force the victim\nmodel to generate a predefined target pattern, which is either inserted into or\nreplaces the original content. We find that these fixed-pattern attacks are\nrelatively easy to detect, because the attacked LVLM tends to memorize such\nfrequent patterns in the training dataset, thereby exhibiting overconfidence on\nthese targets given poisoned inputs. To address these limitations, we introduce\nTokenSwap, a more evasive and stealthy backdoor attack that focuses on the\ncompositional understanding capabilities of LVLMs. Instead of enforcing a fixed\ntargeted content, TokenSwap subtly disrupts the understanding of object\nrelationships in text. Specifically, it causes the backdoored model to generate\noutputs that mention the correct objects in the image but misrepresent their\nrelationships (i.e., bags-of-words behavior). During training, TokenSwap\ninjects a visual trigger into selected samples and simultaneously swaps the\ngrammatical roles of key tokens in the corresponding textual answers. However,\nthe poisoned samples exhibit only subtle differences from the original ones,\nmaking it challenging for the model to learn the backdoor behavior. To address\nthis, TokenSwap employs an adaptive token-weighted loss that explicitly\nemphasizes the learning of swapped tokens, such that the visual triggers and\nbags-of-words behavior are associated. Extensive experiments demonstrate that\nTokenSwap achieves high attack success rates while maintaining superior\nevasiveness and stealthiness across multiple benchmarks and various LVLM\narchitectures.",
        "url": "http://arxiv.org/abs/2509.24566v1",
        "published_date": "2025-09-29T10:19:22+00:00",
        "updated_date": "2025-09-29T10:19:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhifang Zhang",
            "Qiqi Tao",
            "Jiaqi Lv",
            "Na Zhao",
            "Lei Feng",
            "Joey Tianyi Zhou"
        ],
        "ai_categories": [
            "GAN",
            "Transformer"
        ],
        "tldr": "TokenSwap is a backdoor attack that disrupts the understanding of object relationships in large vision-language models by swapping tokens in textual answers, maintaining stealthiness and achieving high attack success rates across benchmarks.",
        "tldr_zh": "TokenSwap 是一种后门攻击，通过在文本答案中交换标记，破坏大型视觉语言模型对对象关系的理解，保持隐蔽性，同时在各项基准测试中实现高攻击成功率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Diffusion Bridge or Flow Matching? A Unifying Framework and Comparative Analysis",
        "summary": "Diffusion Bridge and Flow Matching have both demonstrated compelling\nempirical performance in transformation between arbitrary distributions.\nHowever, there remains confusion about which approach is generally preferable,\nand the substantial discrepancies in their modeling assumptions and practical\nimplementations have hindered a unified theoretical account of their relative\nmerits. We have, for the first time, provided a unified theoretical and\nexperimental validation of these two models. We recast their frameworks through\nthe lens of Stochastic Optimal Control and prove that the cost function of the\nDiffusion Bridge is lower, guiding the system toward more stable and natural\ntrajectories. Simultaneously, from the perspective of Optimal Transport,\ninterpolation coefficients $t$ and $1-t$ of Flow Matching become increasingly\nineffective when the training data size is reduced. To corroborate these\ntheoretical claims, we propose a novel, powerful architecture for Diffusion\nBridge built on a latent Transformer, and implement a Flow Matching model with\nthe same structure to enable a fair performance comparison in various\nexperiments. Comprehensive experiments are conducted across Image Inpainting,\nSuper-Resolution, Deblurring, Denoising, Translation, and Style Transfer tasks,\nsystematically varying both the distributional discrepancy (different\ndifficulty) and the training data size. Extensive empirical results align\nperfectly with our theoretical predictions and allow us to delineate the\nrespective advantages and disadvantages of these two models. Our code is\navailable at https://anonymous.4open.science/r/DBFM-3E8E/.",
        "url": "http://arxiv.org/abs/2509.24531v1",
        "published_date": "2025-09-29T09:45:22+00:00",
        "updated_date": "2025-09-29T09:45:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaizhen Zhu",
            "Mokai Pan",
            "Zhechuan Yu",
            "Jingya Wang",
            "Jingyi Yu",
            "Ye Shi"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper compares Diffusion Bridge and Flow Matching models in transformation between distributions, showing Diffusion Bridge has lower cost function and Flow Matching is less effective with reduced data size.",
        "tldr_zh": "本文比较了扩散桥和流匹配模型在分布转换中的效果，结果表明扩散桥具有更低的成本函数，而流匹配在数据规模减少时效果较差。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Robust Multimodal Semantic Segmentation with Balanced Modality Contributions",
        "summary": "Multimodal semantic segmentation enhances model robustness by exploiting\ncross-modal complementarities. However, existing methods often suffer from\nimbalanced modal dependencies, where overall performance degrades significantly\nonce a dominant modality deteriorates in real-world scenarios. Thus, modality\nbalance has become acritical challenge for practical multimodal segmentation.\nTo address this issue, we propose EQUISeg, a multimodal segmentation framework\nthat balances modality contributions through equal encoding of modalities.\nBuilt upon a four-stage Cross-modal Transformer Block(CMTB), EQUISeg enables\nefficient multimodal fusion and hierarchical selection. Furthermore, we design\na Self-guided Module(SGM) that mitigates modality imbalance by introducing a\nmutual guidance mechanism, enabling each modality to adaptively adjust its\ncontribution and enhance robustness under degraded conditions. Extensive\nexperiments on multiple datasets demonstrate that EQUISeg achieves significant\nperformance gains and effectively alleviates the adverse effects of modality\nimbalance in segmentation tasks.",
        "url": "http://arxiv.org/abs/2509.24505v1",
        "published_date": "2025-09-29T09:19:10+00:00",
        "updated_date": "2025-09-29T09:19:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaqi Tan",
            "Xu Zheng",
            "Fangyu Li",
            "Yang Liu"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes EQUISeg, a framework for balancing modality contributions in multimodal semantic segmentation to enhance model robustness and performance.",
        "tldr_zh": "该论文提出了EQUISeg，一个在多模态语义分割中平衡模态贡献以增强模型鲁棒性和性能的框架。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Euclid's Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks",
        "summary": "Spatial intelligence spans a rich suite of abilities, including visualising\nand transforming shapes, mentally rotating objects, judging relational\npositions and containment, and estimating numerosity. However, it still remains\na critical unresolved challenge for Multimodal Large Language Models (MLLMs).To\nfill this gap, we propose to treat Euclidean geometry problem-solving as a\nsurrogate task. Specifically, we meticulously constructed a curated multimodal\ndataset, called Euclid30K, comprising approximately 30K plane and solid\ngeometry problems. To enable the model to acquire and apply Euclidean\nprinciples from these geometry problems, we employed Group Relative Policy\nOptimization (GRPO) to finetune the Qwen2.5VL family and RoboBrain2.0 family,\ninspiring the models to identify shapes, count, and relate entities, and\nperform multi-step deductive reasoning using Euclidean principles. Our\nexperiments demonstrate that the resulting models achieve substantial zero-shot\ngains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench,\nVSI-Bench, and MindCube) without any task-specific adaptations. Notably, after\ntraining on the Euclid30K, the mean VSI-Bench accuracy of all evaluated models\nrose from 34.5% to 40.5%, improving by 5.5 percentage points. Among them,\nRoboBrain2.0-Euclid-7B achieves 49.6\\% accuracy, surpassing the previous\nstate-of-the-art model, Spatial-MLLM.To our knowledge, this is the first\nsystematic study showing that geometry-centric fine-tuning can confer\nvision-language models with broadly transferable spatial skills. Code and\nEuclid30K dataset can be found in https://zgca-ai4edu.github.io/Euclids_Gift.",
        "url": "http://arxiv.org/abs/2509.24473v1",
        "published_date": "2025-09-29T08:49:21+00:00",
        "updated_date": "2025-09-29T08:49:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Shijie Lian",
            "Changti Wu",
            "Laurence Tianruo Yang",
            "Hang Yuan",
            "Bin Yu",
            "Lei Zhang",
            "Kai Chen"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a new dataset and method to enhance spatial reasoning in vision-language models by using Euclidean geometry problems as a surrogate task, achieving significant improvements in spatial reasoning benchmarks.",
        "tldr_zh": "该论文通过将欧几里德几何问题作为替代任务，引入了新的数据集和方法来增强视觉-语言模型中的空间推理能力，在空间推理基准测试中取得了显著的改进。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LaMoGen: Laban Movement-Guided Diffusion for Text-to-Motion Generation",
        "summary": "Diverse human motion generation is an increasingly important task, having\nvarious applications in computer vision, human-computer interaction and\nanimation. While text-to-motion synthesis using diffusion models has shown\nsuccess in generating high-quality motions, achieving fine-grained expressive\nmotion control remains a significant challenge. This is due to the lack of\nmotion style diversity in datasets and the difficulty of expressing\nquantitative characteristics in natural language. Laban movement analysis has\nbeen widely used by dance experts to express the details of motion including\nmotion quality as consistent as possible. Inspired by that, this work aims for\ninterpretable and expressive control of human motion generation by seamlessly\nintegrating the quantification methods of Laban Effort and Shape components\ninto the text-guided motion generation models. Our proposed zero-shot,\ninference-time optimization method guides the motion generation model to have\ndesired Laban Effort and Shape components without any additional motion data by\nupdating the text embedding of pretrained diffusion models during the sampling\nstep. We demonstrate that our approach yields diverse expressive motion\nqualities while preserving motion identity by successfully manipulating motion\nattributes according to target Laban tags.",
        "url": "http://arxiv.org/abs/2509.24469v1",
        "published_date": "2025-09-29T08:48:49+00:00",
        "updated_date": "2025-09-29T08:48:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Heechang Kim",
            "Gwanghyun Kim",
            "Se Young Chun"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes a method called LaMoGen that integrates Laban movement analysis into text-guided motion generation models to achieve more expressive human motion generation.",
        "tldr_zh": "本文提出了一种称为LaMoGen的方法，将拉班运动分析整合到文本引导的运动生成模型中，以实现更具表现力的人体运动生成。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Generalist Multi-Class Anomaly Detection via Distillation to Two Heterogeneous Student Networks",
        "summary": "Anomaly detection (AD) plays an important role in various real-world\napplications. Recent advancements in AD, however, are often biased towards\nindustrial inspection, struggle to generalize to broader tasks like semantic\nanomaly detection and vice versa. Although recent methods have attempted to\naddress general anomaly detection, their performance remains sensitive to\ndataset-specific settings and single-class tasks. In this paper, we propose a\nnovel dual-model ensemble approach based on knowledge distillation (KD) to\nbridge this gap. Our framework consists of a teacher and two student models: an\nEncoder-Decoder model, specialized in detecting patch-level minor defects for\nindustrial AD and an Encoder-Encoder model, optimized for semantic AD. Both\nmodels leverage a shared pre-trained encoder (DINOv2) to extract high-quality\nfeature representations. The dual models are jointly learned using the Noisy-OR\nobjective, and the final anomaly score is obtained using the joint probability\nvia local and semantic anomaly scores derived from the respective models. We\nevaluate our method on eight public benchmarks under both single-class and\nmulti-class settings: MVTec-AD, MVTec-LOCO, VisA and Real-IAD for industrial\ninspection and CIFAR-10/100, FMNIST and View for semantic anomaly detection.\nThe proposed method achieved state-of-the-art accuracies in both domains, in\nmulti-class as well as single-class settings, demonstrating generalization\nacross multiple domains of anomaly detection. Our model achieved an image-level\nAUROC of 99.7% on MVTec-AD and 97.8% on CIFAR-10, which is significantly better\nthan the prior general AD models in multi-class settings and even higher than\nthe best specialist models on individual benchmarks.",
        "url": "http://arxiv.org/abs/2509.24448v1",
        "published_date": "2025-09-29T08:31:31+00:00",
        "updated_date": "2025-09-29T08:31:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hangil Park",
            "Yongmin Seo",
            "Tae-Kyun Kim"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel dual-model ensemble approach for anomaly detection that outperforms existing methods in both industrial and semantic anomaly detection.",
        "tldr_zh": "本文提出了一种新颖的双模型集成方法，用于异常检测，在工业和语义异常检测领域表现优异。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "NeoWorld: Neural Simulation of Explorable Virtual Worlds via Progressive 3D Unfolding",
        "summary": "We introduce NeoWorld, a deep learning framework for generating interactive\n3D virtual worlds from a single input image. Inspired by the on-demand\nworldbuilding concept in the science fiction novel Simulacron-3 (1964), our\nsystem constructs expansive environments where only the regions actively\nexplored by the user are rendered with high visual realism through\nobject-centric 3D representations. Unlike previous approaches that rely on\nglobal world generation or 2D hallucination, NeoWorld models key foreground\nobjects in full 3D, while synthesizing backgrounds and non-interacted regions\nin 2D to ensure efficiency. This hybrid scene structure, implemented with\ncutting-edge representation learning and object-to-3D techniques, enables\nflexible viewpoint manipulation and physically plausible scene animation,\nallowing users to control object appearance and dynamics using natural language\ncommands. As users interact with the environment, the virtual world\nprogressively unfolds with increasing 3D detail, delivering a dynamic,\nimmersive, and visually coherent exploration experience. NeoWorld significantly\noutperforms existing 2D and depth-layered 2.5D methods on the WorldScore\nbenchmark.",
        "url": "http://arxiv.org/abs/2509.24441v1",
        "published_date": "2025-09-29T08:24:28+00:00",
        "updated_date": "2025-09-29T08:24:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanpeng Zhao",
            "Shanyan Guan",
            "Yunbo Wang",
            "Yanhao Ge",
            "Wei Li",
            "Xiaokang Yang"
        ],
        "ai_categories": [
            "GAN",
            "Multimodality",
            "Transformer"
        ],
        "tldr": "NeoWorld is a deep learning framework that generates interactive 3D virtual worlds from a single image, allowing users to explore and manipulate environments in a dynamic and immersive manner.",
        "tldr_zh": "NeoWorld是一个深度学习框架，可以从单个图像生成交互式3D虚拟世界，使用户能够以动态和身临其境的方式探索和操控环境。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Rethinking Unsupervised Cross-modal Flow Estimation: Learning from Decoupled Optimization and Consistency Constraint",
        "summary": "This work presents DCFlow, a novel unsupervised cross-modal flow estimation\nframework that integrates a decoupled optimization strategy and a cross-modal\nconsistency constraint. Unlike previous approaches that implicitly learn flow\nestimation solely from appearance similarity, we introduce a decoupled\noptimization strategy with task-specific supervision to address modality\ndiscrepancy and geometric misalignment distinctly. This is achieved by\ncollaboratively training a modality transfer network and a flow estimation\nnetwork. To enable reliable motion supervision without ground-truth flow, we\npropose a geometry-aware data synthesis pipeline combined with an\noutlier-robust loss. Additionally, we introduce a cross-modal consistency\nconstraint to jointly optimize both networks, significantly improving flow\nprediction accuracy. For evaluation, we construct a comprehensive cross-modal\nflow benchmark by repurposing public datasets. Experimental results demonstrate\nthat DCFlow can be integrated with various flow estimation networks and\nachieves state-of-the-art performance among unsupervised approaches.",
        "url": "http://arxiv.org/abs/2509.24423v1",
        "published_date": "2025-09-29T08:10:41+00:00",
        "updated_date": "2025-09-29T08:10:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Runmin Zhang",
            "Jialiang Wang",
            "Si-Yuan Cao",
            "Zhu Yu",
            "Junchen Yu",
            "Guangyi Zhang",
            "Hui-Liang Shen"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "DCFlow is a novel framework for unsupervised cross-modal flow estimation, integrating decoupled optimization and a consistency constraint to improve flow prediction accuracy.",
        "tldr_zh": "DCFlow是一种新的框架，用于无监督的跨模态流估计，整合了解耦优化和一致性约束，以提高流预测的准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Proxy-GS: Efficient 3D Gaussian Splatting via Proxy Mesh",
        "summary": "3D Gaussian Splatting (3DGS) has emerged as an efficient approach for\nachieving photorealistic rendering. Recent MLP-based variants further improve\nvisual fidelity but introduce substantial decoding overhead during rendering.\nTo alleviate computation cost, several pruning strategies and level-of-detail\n(LOD) techniques have been introduced, aiming to effectively reduce the number\nof Gaussian primitives in large-scale scenes. However, our analysis reveals\nthat significant redundancy still remains due to the lack of occlusion\nawareness. In this work, we propose Proxy-GS, a novel pipeline that exploits a\nproxy to introduce Gaussian occlusion awareness from any view. At the core of\nour approach is a fast proxy system capable of producing precise occlusion\ndepth maps at a resolution of 1000x1000 under 1ms. This proxy serves two roles:\nfirst, it guides the culling of anchors and Gaussians to accelerate rendering\nspeed. Second, it guides the densification towards surfaces during training,\navoiding inconsistencies in occluded regions, and improving the rendering\nquality. In heavily occluded scenarios, such as the MatrixCity Streets dataset,\nProxy-GS not only equips MLP-based Gaussian splatting with stronger rendering\ncapability but also achieves faster rendering speed. Specifically, it achieves\nmore than 2.5x speedup over Octree-GS, and consistently delivers substantially\nhigher rendering quality. Code will be public upon acceptance.",
        "url": "http://arxiv.org/abs/2509.24421v1",
        "published_date": "2025-09-29T08:10:07+00:00",
        "updated_date": "2025-09-29T08:10:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuanyuan Gao",
            "Yuning Gong",
            "Yifei Liu",
            "Li Jingfeng",
            "Zhihang Zhong",
            "Dingwen Zhang",
            "Yanci Zhang",
            "Dan Xu",
            "Xiao Sun"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces Proxy-GS, a novel approach for efficient 3D Gaussian splatting with occlusion awareness, improving rendering quality and speed.",
        "tldr_zh": "本文介绍了Proxy-GS，一种新颖的3D高斯喷洒方法，具有遮挡感知能力，提高了渲染质量和速度。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Data-Centric Perspective on the Influence of Image Data Quality in Machine Learning Models",
        "summary": "In machine learning, research has traditionally focused on model development,\nwith relatively less attention paid to training data. As model architectures\nhave matured and marginal gains from further refinements diminish, data quality\nhas emerged as a critical factor. However, systematic studies on evaluating and\nensuring dataset quality in the image domain remain limited.\n  This study investigates methods for systematically assessing image dataset\nquality and examines how various image quality factors influence model\nperformance. Using the publicly available and relatively clean CIFAKE dataset,\nwe identify common quality issues and quantify their impact on training.\nBuilding on these findings, we develop a pipeline that integrates two\ncommunity-developed tools, CleanVision and Fastdup. We analyze their underlying\nmechanisms and introduce several enhancements, including automatic threshold\nselection to detect problematic images without manual tuning.\n  Experimental results demonstrate that not all quality issues exert the same\nlevel of impact. While convolutional neural networks show resilience to certain\ndistortions, they are particularly vulnerable to degradations that obscure\ncritical visual features, such as blurring and severe downscaling. To assess\nthe performance of existing tools and the effectiveness of our proposed\nenhancements, we formulate the detection of low-quality images as a binary\nclassification task and use the F1 score as the evaluation metric. Our\nautomatic thresholding method improves the F1 score from 0.6794 to 0.9468 under\nsingle perturbations and from 0.7447 to 0.8557 under dual perturbations. For\nnear-duplicate detection, our deduplication strategy increases the F1 score\nfrom 0.4576 to 0.7928. These results underscore the effectiveness of our\nworkflow and provide a foundation for advancing data quality assessment in\nimage-based machine learning.",
        "url": "http://arxiv.org/abs/2509.24420v1",
        "published_date": "2025-09-29T08:09:21+00:00",
        "updated_date": "2025-09-29T08:09:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "eess.IV"
        ],
        "authors": [
            "Pei-Han Chen",
            "Szu-Chi Chung"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper explores the impact of image data quality on machine learning models and introduces tools to assess and improve dataset quality, showing enhancements in model performance.",
        "tldr_zh": "这篇论文探讨了图像数据质量对机器学习模型的影响，并介绍了用于评估和改善数据集质量的工具，展示了模型性能的改进。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CLQ: Cross-Layer Guided Orthogonal-based Quantization for Diffusion Transformers",
        "summary": "Visual generation quality has been greatly promoted with the rapid advances\nin diffusion transformers (DiTs), which is attributed to the scaling of model\nsize and complexity. However, these attributions also hinder the practical\ndeployment of DiTs on edge devices, limiting their development and application.\nServe as an efficient model compression technique, model post-training\nquantization (PTQ) can reduce the memory consumption and speed up the\ninference, with inevitable performance degradation. To alleviate the\ndegradation, we propose CLQ, a cross-layer guided orthogonal-based quantization\nmethod for DiTs. To be specific, CLQ consists of three key designs. First, we\nobserve that the calibration data used by most of the PTQ methods can not\nhonestly represent the distribution of the activations. Therefore, we propose\ncross-block calibration (CBC) to obtain accurate calibration data, with which\nthe quantization can be better guided. Second, we propose orthogonal-based\nsmoothing (OBS), which quantifies the outlier score of each channel and\nleverages block Hadamard matrix to smooth the outliers with negligible\noverhead. Third, we propose cross-layer parameter searching (CLPS) to search.\nWe evaluate CLQ with both image generation and video generation models and\nsuccessfully compress the model into W4A4 with negligible degradation in visual\nquality and metrics. CLQ achieves 3.98x memory saving and 3.95x speedup. Our\ncode is available at\n\\hyperlink{https://github.com/Kai-Liu001/CLQ}{https://github.com/Kai-Liu001/CLQ}.",
        "url": "http://arxiv.org/abs/2509.24416v1",
        "published_date": "2025-09-29T08:06:42+00:00",
        "updated_date": "2025-09-29T08:06:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kai Liu",
            "Shaoqiu Zhang",
            "Linghe Kong",
            "Yulun Zhang"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer"
        ],
        "tldr": "The paper introduces CLQ, a method for model compression in diffusion transformers, achieving significant memory saving and speedup with minimal visual quality degradation.",
        "tldr_zh": "该论文介绍了CLQ，一种在扩散变换器中进行模型压缩的方法，实现了显著的内存节省和加速，减少了视觉质量的降级。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Hybrid Layer-Wise ANN-SNN With Surrogate Spike Encoding-Decoding Structure",
        "summary": "Spiking Neural Networks (SNNs) have gained significant traction in both\ncomputational neuroscience and artificial intelligence for their potential in\nenergy-efficient computing. In contrast, artificial neural networks (ANNs)\nexcel at gradient-based optimization and high accuracy. This contrast has\nconsequently led to a growing subfield of hybrid ANN-SNN research. However,\nexisting hybrid approaches often rely on either a strict separation between ANN\nand SNN components or employ SNN-only encoders followed by ANN classifiers due\nto the constraints of non-differentiability of spike encoding functions,\ncausing prior hybrid architectures to lack deep layer-wise cooperation during\nbackpropagation. To address this gap, we propose a novel hybrid ANN-SNN\nframework that integrates layer-wise encode-decode SNN blocks within\nconventional ANN pipelines. Central to our method is the use of surrogate\ngradients for a bit-plane-based spike encoding function, enabling end-to-end\ndifferentiable training across ANN and SNN layers. This design achieves\ncompetitive accuracy with state-of-the-art pure ANN and SNN models while\nretaining the potential efficiency and temporal representation benefits of\nspiking computation. To the best of our knowledge, this is the first\nimplementation of a surrogate gradient for bit plane coding specifically and\nspike encoder interface in general to be utilized in the context of hybrid\nANN-SNN, successfully leading to a new class of hybrid models that pave new\ndirections for future research.",
        "url": "http://arxiv.org/abs/2509.24411v1",
        "published_date": "2025-09-29T07:57:58+00:00",
        "updated_date": "2025-09-29T07:57:58+00:00",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Nhan T. Luu",
            "Duong T. Luu",
            "Pham Ngoc Nam",
            "Truong Cong Thang"
        ],
        "ai_categories": [
            "Transformer",
            "Other"
        ],
        "tldr": "The paper introduces a novel hybrid framework that combines artificial neural networks (ANNs) and spiking neural networks (SNNs) to achieve competitive accuracy while maintaining energy efficiency benefits of SNNs.",
        "tldr_zh": "本文介绍了一种新颖的混合框架，将人工神经网络（ANNs）和脉冲神经网络（SNNs）结合在一起，以实现竞争性的准确性，同时保持 SNNs 的能源效率优势。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RapidMV: Leveraging Spatio-Angular Representations for Efficient and Consistent Text-to-Multi-View Synthesis",
        "summary": "Generating synthetic multi-view images from a text prompt is an essential\nbridge to generating synthetic 3D assets. In this work, we introduce RapidMV, a\nnovel text-to-multi-view generative model that can produce 32 multi-view\nsynthetic images in just around 5 seconds. In essence, we propose a novel\nspatio-angular latent space, encoding both the spatial appearance and angular\nviewpoint deviations into a single latent for improved efficiency and\nmulti-view consistency. We achieve effective training of RapidMV by\nstrategically decomposing our training process into multiple steps. We\ndemonstrate that RapidMV outperforms existing methods in terms of consistency\nand latency, with competitive quality and text-image alignment.",
        "url": "http://arxiv.org/abs/2509.24410v1",
        "published_date": "2025-09-29T07:57:11+00:00",
        "updated_date": "2025-09-29T07:57:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Seungwook Kim",
            "Yichun Shi",
            "Kejie Li",
            "Minsu Cho",
            "Peng Wang"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "RapidMV introduces a text-to-multi-view generative model that can efficiently produce 32 multi-view synthetic images in just around 5 seconds, outperforming existing methods in consistency and latency.",
        "tldr_zh": "RapidMV引入了一种文本至多视角生成模型，可以在大约5秒内高效生成32个多视角合成图像，在一致性和延迟方面优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PCICF: A Pedestrian Crossing Identification and Classification Framework",
        "summary": "We have recently observed the commercial roll-out of robotaxis in various\ncountries. They are deployed within an operational design domain (ODD) on\nspecific routes and environmental conditions, and are subject to continuous\nmonitoring to regain control in safety-critical situations. Since ODDs\ntypically cover urban areas, robotaxis must reliably detect vulnerable road\nusers (VRUs) such as pedestrians, bicyclists, or e-scooter riders. To better\nhandle such varied traffic situations, end-to-end AI, which directly compute\nvehicle control actions from multi-modal sensor data instead of only for\nperception, is on the rise. High quality data is needed for systematically\ntraining and evaluating such systems within their OOD. In this work, we propose\nPCICF, a framework to systematically identify and classify VRU situations to\nsupport ODD's incident analysis. We base our work on the existing synthetic\ndataset SMIRK, and enhance it by extending its single-pedestrian-only design\ninto the MoreSMIRK dataset, a structured dictionary of multi-pedestrian\ncrossing situations constructed systematically. We then use space-filling\ncurves (SFCs) to transform multi-dimensional features of scenarios into\ncharacteristic patterns, which we match with corresponding entries in\nMoreSMIRK. We evaluate PCICF with the large real-world dataset PIE, which\ncontains more than 150 manually annotated pedestrian crossing videos. We show\nthat PCICF can successfully identify and classify complex pedestrian crossings,\neven when groups of pedestrians merge or split. By leveraging computationally\nefficient components like SFCs, PCICF has even potential to be used onboard of\nrobotaxis for OOD detection for example. We share an open-source replication\npackage for PCICF containing its algorithms, the complete MoreSMIRK dataset and\ndictionary, as well as our experiment results presented in:\nhttps://github.com/Claud1234/PCICF",
        "url": "http://arxiv.org/abs/2509.24386v1",
        "published_date": "2025-09-29T07:35:12+00:00",
        "updated_date": "2025-09-29T07:35:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junyi Gu",
            "Beatriz Cabrero-Daniel",
            "Ali Nouri",
            "Lydia Armini",
            "Christian Berger"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a framework, PCICF, for identifying and classifying pedestrian crossings to support incident analysis in autonomous vehicles.",
        "tldr_zh": "本文介绍了一个框架PCICF，用于识别和分类行人过路情况，以支持自动驾驶车辆的事故分析。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Vid-LLM: A Compact Video-based 3D Multimodal LLM with Reconstruction-Reasoning Synergy",
        "summary": "Recent developments in Multimodal Large Language Models (MLLMs) have\nsignificantly improved Vision-Language (VL) reasoning in 2D domains. However,\nextending these capabilities to 3D scene understanding remains a major\nchallenge. Existing 3D Multimodal Large Language Models (3D-MLLMs) often depend\non 3D data inputs, which limits scalability and generalization. To address this\nlimitation, we propose Vid-LLM, a video-based 3D-MLLM that directly processes\nvideo inputs without requiring external 3D data, making it practical for\nreal-world deployment. In our method, the geometric prior are directly used to\nimprove the performance of the sceen perception. To integrate the geometric\ncues into the MLLM compactly, we design a Cross-Task Adapter (CTA) module to\nalign the 3D geometric priors with the vision-language representations. To\nensure geometric consistency and integrity, we introduce a Metric Depth Model\nthat recovers real-scale geometry from the reconstruction outputs. Finally, the\nmodel is fine-tuned with a two-stage distillation optimization strategy,\nrealizing fast convergence and stabilizes training. Extensive experiments\nacross diverse benchmarks verified the effectiveness of our method on 3D\nQuestion Answering, 3D Dense Captioning and 3D Visual Grounding tasks,\ndemonstrating the superior multi-task capabilities.",
        "url": "http://arxiv.org/abs/2509.24385v1",
        "published_date": "2025-09-29T07:34:18+00:00",
        "updated_date": "2025-09-29T07:34:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Haijier Chen",
            "Bo Xu",
            "Shoujian Zhang",
            "Haoze Liu",
            "Jiaxuan Lin",
            "Jingrong Wang"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "Proposes Vid-LLM, a video-based 3D multimodal large language model without external 3D data for 3D scene understanding, demonstrating effectiveness on various tasks.",
        "tldr_zh": "提出了 Vid-LLM，一种基于视频的 3D 多模态大型语言模型，无需外部 3D 数据即可进行 3D 场景理解，在各种任务上展现出有效性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Real-Aware Residual Model Merging for Deepfake Detection",
        "summary": "Deepfake generators evolve quickly, making exhaustive data collection and\nrepeated retraining impractical. We argue that model merging is a natural fit\nfor deepfake detection: unlike generic multi-task settings with disjoint\nlabels, deepfake specialists share the same binary decision and differ in\ngenerator-specific artifacts. Empirically, we show that simple weight averaging\npreserves Real representations while attenuating Fake-specific cues. Building\nupon these findings, we propose Real-aware Residual Model Merging (R$^2$M), a\ntraining-free parameter-space merging framework. R$^2$M estimates a shared Real\ncomponent via a low-rank factorization of task vectors, decomposes each\nspecialist into a Real-aligned part and a Fake residual, denoises residuals\nwith layerwise rank truncation, and aggregates them with per-task norm matching\nto prevent any single generator from dominating. A concise rationale explains\nwhy a simple head suffices: the Real component induces a common separation\ndirection in feature space, while truncated residuals contribute only minor\noff-axis variations. Across in-distribution, cross-dataset, and unseen-dataset,\nR$^2$M outperforms joint training and other merging baselines. Importantly,\nR$^2$M is also composable: when a new forgery family appears, we fine-tune one\nspecialist and re-merge, eliminating the need for retraining.",
        "url": "http://arxiv.org/abs/2509.24367v1",
        "published_date": "2025-09-29T07:10:03+00:00",
        "updated_date": "2025-09-29T07:10:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinhee Park",
            "Guisik Kim",
            "Choongsang Cho",
            "Junseok Kwon"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Other"
        ],
        "tldr": "The paper introduces a new method called Real-aware Residual Model Merging (R$^2$M) for detecting deepfakes without the need for retraining, outperforming other methods across various scenarios.",
        "tldr_zh": "本文介绍了一种名为Real-aware Residual Model Merging (R$^2$M)的新方法，用于检测深度伪造，无需重新训练，优于其他方法在各种情景下的表现。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UI-UG: A Unified MLLM for UI Understanding and Generation",
        "summary": "Although Multimodal Large Language Models (MLLMs) have been widely applied\nacross domains, they are still facing challenges in domain-specific tasks, such\nas User Interface (UI) understanding accuracy and UI generation quality. In\nthis paper, we introduce UI-UG (a unified MLLM for UI Understanding and\nGeneration), integrating both capabilities. For understanding tasks, we employ\nSupervised Fine-tuning (SFT) combined with Group Relative Policy Optimization\n(GRPO) to enhance fine-grained understanding on the modern complex UI data. For\ngeneration tasks, we further use Direct Preference Optimization (DPO) to make\nour model generate human-preferred UIs. In addition, we propose an industrially\neffective workflow, including the design of an LLM-friendly domain-specific\nlanguage (DSL), training strategies, rendering processes, and evaluation\nmetrics. In experiments, our model achieves state-of-the-art (SOTA) performance\non understanding tasks, outperforming both larger general-purpose MLLMs and\nsimilarly-sized UI-specialized models. Our model is also on par with these\nlarger MLLMs in UI generation performance at a fraction of the computational\ncost. We also demonstrate that integrating understanding and generation tasks\ncan improve accuracy and quality for both tasks.",
        "url": "http://arxiv.org/abs/2509.24361v1",
        "published_date": "2025-09-29T06:59:09+00:00",
        "updated_date": "2025-09-29T06:59:09+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.HC"
        ],
        "authors": [
            "Hao Yang",
            "Weijie Qiu",
            "Ru Zhang",
            "Zhou Fang",
            "Ruichao Mao",
            "Xiaoyu Lin",
            "Maji Huang",
            "Zhaosong Huang",
            "Teng Guo",
            "Shuoyang Liu",
            "Hai Rao"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Other"
        ],
        "tldr": "Introducing UI-UG, a unified Multimodal Large Language Model (MLLM) for both User Interface (UI) understanding and generation, achieving state-of-the-art performance in both tasks.",
        "tldr_zh": "引入UI-UG，一个统一的多模态大语言模型（MLLM）用于用户界面（UI）理解和生成，在两项任务中均实现了最先进的表现。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DRIFT: Divergent Response in Filtered Transformations for Robust Adversarial Defense",
        "summary": "Deep neural networks remain highly vulnerable to adversarial examples, and\nmost defenses collapse once gradients can be reliably estimated. We identify\n\\emph{gradient consensus}--the tendency of randomized transformations to yield\naligned gradients--as a key driver of adversarial transferability. Attackers\nexploit this consensus to construct perturbations that remain effective across\ntransformations. We introduce \\textbf{DRIFT} (Divergent Response in Filtered\nTransformations), a stochastic ensemble of lightweight, learnable filters\ntrained to actively disrupt gradient consensus. Unlike prior randomized\ndefenses that rely on gradient masking, DRIFT enforces \\emph{gradient\ndissonance} by maximizing divergence in Jacobian- and logit-space responses\nwhile preserving natural predictions. Our contributions are threefold: (i) we\nformalize gradient consensus and provide a theoretical analysis linking\nconsensus to transferability; (ii) we propose a consensus-divergence training\nstrategy combining prediction consistency, Jacobian separation, logit-space\nseparation, and adversarial robustness; and (iii) we show that DRIFT achieves\nsubstantial robustness gains on ImageNet across CNNs and Vision Transformers,\noutperforming state-of-the-art preprocessing, adversarial training, and\ndiffusion-based defenses under adaptive white-box, transfer-based, and\ngradient-free attacks. DRIFT delivers these improvements with negligible\nruntime and memory cost, establishing gradient divergence as a practical and\ngeneralizable principle for adversarial defense.",
        "url": "http://arxiv.org/abs/2509.24359v1",
        "published_date": "2025-09-29T06:57:47+00:00",
        "updated_date": "2025-09-29T06:57:47+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Amira Guesmi",
            "Muhammad Shafique"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Other"
        ],
        "tldr": "DRIFT introduces gradient divergence to disrupt gradient consensus, achieving robustness gains in adversarial defense.",
        "tldr_zh": "DRIFT引入梯度分歧以打破梯度共识，在对抗防御中取得鲁棒性收益。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "An Enhanced Pyramid Feature Network Based on Long-Range Dependencies for Multi-Organ Medical Image Segmentation",
        "summary": "In the field of multi-organ medical image segmentation, recent methods\nfrequently employ Transformers to capture long-range dependencies from image\nfeatures. However, these methods overlook the high computational cost of\nTransformers and their deficiencies in extracting local detailed information.\nTo address high computational costs and inadequate local detail information, we\nreassess the design of feature extraction modules and propose a new\ndeep-learning network called LamFormer for fine-grained segmentation tasks\nacross multiple organs. LamFormer is a novel U-shaped network that employs\nLinear Attention Mamba (LAM) in an enhanced pyramid encoder to capture\nmulti-scale long-range dependencies. We construct the Parallel Hierarchical\nFeature Aggregation (PHFA) module to aggregate features from different layers\nof the encoder, narrowing the semantic gap among features while filtering\ninformation. Finally, we design the Reduced Transformer (RT), which utilizes a\ndistinct computational approach to globally model up-sampled features. RRT\nenhances the extraction of detailed local information and improves the\nnetwork's capability to capture long-range dependencies. LamFormer outperforms\nexisting segmentation methods on seven complex and diverse datasets,\ndemonstrating exceptional performance. Moreover, the proposed network achieves\na balance between model performance and model complexity.",
        "url": "http://arxiv.org/abs/2509.24358v1",
        "published_date": "2025-09-29T06:57:11+00:00",
        "updated_date": "2025-09-29T06:57:11+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Dayu Tan",
            "Cheng Kong",
            "Yansen Su",
            "Hai Chen",
            "Dongliang Yang",
            "Junfeng Xia",
            "Chunhou Zheng"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a new deep-learning network, LamFormer, which addresses the challenges of high computational costs and inadequate local detail information in medical image segmentation. It outperforms existing methods on various datasets.",
        "tldr_zh": "该论文介绍了一种新的深度学习网络，LamFormer，用于解决医学图像分割中的高计算成本和不足的局部细节信息。 在各种数据集上表现优异。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "NeRV-Diffusion: Diffuse Implicit Neural Representations for Video Synthesis",
        "summary": "We present NeRV-Diffusion, an implicit latent video diffusion model that\nsynthesizes videos via generating neural network weights. The generated weights\ncan be rearranged as the parameters of a convolutional neural network, which\nforms an implicit neural representation (INR), and decodes into videos with\nframe indices as the input. Our framework consists of two stages: 1) A\nhypernetworkbased tokenizer that encodes raw videos from pixel space to neural\nparameter space, where the bottleneck latent serves as INR weights to decode.\n2) An implicit diffusion transformer that denoises on the latent INR weights.\nIn contrast to traditional video tokenizers that encode videos into frame-wise\nfeature maps, NeRV-Diffusion compresses and generates a video holistically as a\nunified neural network. This enables efficient and high-quality video synthesis\nvia obviating temporal cross-frame attentions in the denoiser and decoding\nvideo latent with dedicated decoders. To achieve Gaussian-distributed INR\nweights with high expressiveness, we reuse the bottleneck latent across all\nNeRV layers, as well as reform its weight assignment, upsampling connection and\ninput coordinates. We also introduce SNR-adaptive loss weighting and scheduled\nsampling for effective training of the implicit diffusion model. NeRV-Diffusion\nreaches superior video generation quality over previous INR-based models and\ncomparable performance to most recent state-of-the-art non-implicit models on\nreal-world video benchmarks including UCF-101 and Kinetics-600. It also brings\na smooth INR weight space that facilitates seamless interpolations between\nframes or videos.",
        "url": "http://arxiv.org/abs/2509.24353v1",
        "published_date": "2025-09-29T06:53:08+00:00",
        "updated_date": "2025-09-29T06:53:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yixuan Ren",
            "Hanyu Wang",
            "Hao Chen",
            "Bo He",
            "Abhinav Shrivastava"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer",
            "Dataset"
        ],
        "tldr": "NeRV-Diffusion is a video synthesis model based on implicit neural representations, achieving high-quality video generation and smooth interpolations between frames.",
        "tldr_zh": "NeRV-Diffusion是一种基于隐式神经表示的视频合成模型，实现了高质量的视频生成和帧间平滑插值。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Hyperspherical Latents Improve Continuous-Token Autoregressive Generation",
        "summary": "Autoregressive (AR) models are promising for image generation, yet\ncontinuous-token AR variants often trail latent diffusion and masked-generation\nmodels. The core issue is heterogeneous variance in VAE latents, which is\namplified during AR decoding, especially under classifier-free guidance (CFG),\nand can cause variance collapse. We propose SphereAR to address this issue. Its\ncore design is to constrain all AR inputs and outputs -- including after CFG --\nto lie on a fixed-radius hypersphere (constant $\\ell_2$ norm), leveraging\nhyperspherical VAEs. Our theoretical analysis shows that hyperspherical\nconstraint removes the scale component (the primary cause of variance\ncollapse), thereby stabilizing AR decoding. Empirically, on ImageNet\ngeneration, SphereAR-H (943M) sets a new state of the art for AR models,\nachieving FID 1.34. Even at smaller scales, SphereAR-L (479M) reaches FID 1.54\nand SphereAR-B (208M) reaches 1.92, matching or surpassing much larger\nbaselines such as MAR-H (943M, 1.55) and VAR-d30 (2B, 1.92). To our knowledge,\nthis is the first time a pure next-token AR image generator with raster order\nsurpasses diffusion and masked-generation models at comparable parameter\nscales.",
        "url": "http://arxiv.org/abs/2509.24335v1",
        "published_date": "2025-09-29T06:34:24+00:00",
        "updated_date": "2025-09-29T06:34:24+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Guolin Ke",
            "Hui Xue"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "SphereAR is a new autoregressive image generation model that constrains inputs and outputs to lie on a fixed-radius hypersphere, achieving state-of-the-art results on ImageNet generation.",
        "tldr_zh": "SphereAR 是一种新的自回归图像生成模型，将输入和输出约束在一个固定半径的超球面上，在 ImageNet 生成任务上取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TP-MVCC: Tri-plane Multi-view Fusion Model for Silkie Chicken Counting",
        "summary": "Accurate animal counting is essential for smart farming but remains difficult\nin crowded scenes due to occlusions and limited camera views. To address this,\nwe propose a tri-plane-based multi-view chicken counting model (TP-MVCC), which\nleverages geometric projection and tri-plane fusion to integrate features from\nmultiple cameras onto a unified ground plane. The framework extracts\nsingle-view features, aligns them via spatial transformation, and decodes a\nscene-level density map for precise chicken counting. In addition, we construct\nthe first multi-view dataset of silkie chickens under real farming conditions.\nExperiments show that TP-MVCC significantly outperforms single-view and\nconventional fusion comparisons, achieving 95.1\\% accuracy and strong\nrobustness in dense, occluded scenarios, demonstrating its practical potential\nfor intelligent agriculture.",
        "url": "http://arxiv.org/abs/2509.24329v1",
        "published_date": "2025-09-29T06:27:04+00:00",
        "updated_date": "2025-09-29T06:27:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sirui Chen",
            "Yuhong Feng",
            "Yifeng Wang",
            "Jianghai Liao",
            "Qi Zhang"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces a tri-plane multi-view fusion model for counting silkie chickens in smart farming, outperforming single-view and conventional fusion models in crowded scenarios.",
        "tldr_zh": "该论文引入了一种三平面多视角融合模型，用于智能农业中的丝绒鸡计数，在拥挤场景中比单视图和传统融合模型表现更好。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "TraitSpaces: Towards Interpretable Visual Creativity for Human-AI Co-Creation",
        "summary": "We introduce a psychologically grounded and artist-informed framework for\nmodeling visual creativity across four domains: Inner, Outer, Imaginative, and\nMoral Worlds. Drawing on interviews with practicing artists and theories from\npsychology, we define 12 traits that capture affective, symbolic, cultural, and\nethical dimensions of creativity.Using 20k artworks from the SemArt dataset, we\nannotate images with GPT 4.1 using detailed, theory-aligned prompts, and\nevaluate the learnability of these traits from CLIP image embeddings. Traits\nsuch as Environmental Dialogicity and Redemptive Arc are predicted with high\nreliability ($R^2 \\approx 0.64 - 0.68$), while others like Memory Imprint\nremain challenging, highlighting the limits of purely visual encoding. Beyond\ntechnical metrics, we visualize a \"creativity trait-space\" and illustrate how\nit can support interpretable, trait-aware co-creation - e.g., sliding along a\nRedemptive Arc axis to explore works of adversity and renewal. By linking\ncultural-aesthetic insights with computational modeling, our work aims not to\nreduce creativity to numbers, but to offer shared language and interpretable\ntools for artists, researchers, and AI systems to collaborate meaningfully.",
        "url": "http://arxiv.org/abs/2509.24326v1",
        "published_date": "2025-09-29T06:24:18+00:00",
        "updated_date": "2025-09-29T06:24:18+00:00",
        "categories": [
            "cs.HC",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Prerna Luthra"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset",
            "GAN"
        ],
        "tldr": "The paper introduces a framework for modeling visual creativity based on psychological and artist-informed traits. It explores the learnability of these traits from images and aims to provide interpretable tools for human-AI co-creation.",
        "tldr_zh": "该论文引入了一个基于心理学和艺术家信息的框架，用于建模视觉创造力。它探索了这些特征从图像中的可学习性，并旨在为人工智能共创提供可解释的工具。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Similarity-Aware Selective State-Space Modeling for Semantic Correspondence",
        "summary": "Establishing semantic correspondences between images is a fundamental yet\nchallenging task in computer vision. Traditional feature-metric methods enhance\nvisual features but may miss complex inter-correlation relationships, while\nrecent correlation-metric approaches are hindered by high computational costs\ndue to processing 4D correlation maps. We introduce MambaMatcher, a novel\nmethod that overcomes these limitations by efficiently modeling\nhigh-dimensional correlations using selective state-space models (SSMs). By\nimplementing a similarity-aware selective scan mechanism adapted from Mamba's\nlinear-complexity algorithm, MambaMatcher refines the 4D correlation map\neffectively without compromising feature map resolution or receptive field.\nExperiments on standard semantic correspondence benchmarks demonstrate that\nMambaMatcher achieves state-of-the-art performance.",
        "url": "http://arxiv.org/abs/2509.24318v1",
        "published_date": "2025-09-29T05:56:57+00:00",
        "updated_date": "2025-09-29T05:56:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Seungwook Kim",
            "Minsu Cho"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "MambaMatcher introduces a novel method for semantic correspondence in computer vision by efficiently modeling high-dimensional correlations using selective state-space models, achieving state-of-the-art performance on benchmarks.",
        "tldr_zh": "MambaMatcher通过使用选择性状态空间模型有效建模高维度相关性，在计算机视觉中引入了一种新方法，在基准测试中取得了最先进的表现。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Rethinking JEPA: Compute-Efficient Video SSL with Frozen Teachers",
        "summary": "Video Joint Embedding Predictive Architectures (V-JEPA) learn generalizable\noff-the-shelf video representation by predicting masked regions in latent space\nwith an exponential moving average (EMA)-updated teacher. While EMA prevents\nrepresentation collapse, it complicates scalable model selection and couples\nteacher and student architectures. We revisit masked-latent prediction and show\nthat a frozen teacher suffices. Concretely, we (i) train a target encoder with\na simple pixel-reconstruction objective under V-JEPA masking, then (ii) freeze\nit and train a student to predict the teacher's latents on masked regions. This\nleads to a two-stage, unregularized scheme that we refer to as SALT\n(Static-teacher Asymmetric Latent Training). SALT decouples optimization into\npixel reconstruction (teacher) and masked latent prediction (student),\nincreasing transparency, efficiency, and scalability while preserving the\nability of representation to generalize under frozen evaluation. Empirically,\nour student models outperform recently proposed V-JEPA 2 encoders under frozen\nbackbone evaluation across diverse benchmarks. They are also more\ncompute-optimal: at matched pretraining FLOPs, our method achieves higher\nprobing accuracy, and its scaling curves dominate V-JEPA's accuracy-FLOPs\nPareto frontier. Finally, we find that student quality is remarkably robust to\nteacher quality: high-performing students emerge even with small, sub-optimal\nteachers. This points to a compute budget allocation that should overwhelmingly\nfavor the student. These results position SALT as a simple, scalable, and\ncompute-efficient alternative to EMA-based self-distillation for video\nrepresentation learning.",
        "url": "http://arxiv.org/abs/2509.24317v1",
        "published_date": "2025-09-29T05:55:17+00:00",
        "updated_date": "2025-09-29T05:55:17+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Xianhang Li",
            "Chen Huang",
            "Chun-Liang Li",
            "Eran Malach",
            "Josh Susskind",
            "Vimal Thilak",
            "Etai Littwin"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces SALT, a new video self-supervised learning method that uses a frozen teacher and outperforms existing approaches in terms of efficiency and scalability.",
        "tldr_zh": "该论文介绍了SALT，一种新的视频自监督学习方法，使用了冻结的老师，在效率和可扩展性方面优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FrameThinker: Learning to Think with Long Videos via Multi-Turn Frame Spotlighting",
        "summary": "While Large Vision-Language Models (LVLMs) have achieved substantial progress\nin video understanding, their application to long video reasoning is hindered\nby uniform frame sampling and static textual reasoning, which are inefficient\nand struggle to handle visually intensive video tasks. To overcome these\nchallenges, in this paper, we introduce the concept of thinking with long\nvideos and propose a novel framework FrameThinker. Within this framework, LVLMs\nare able to iteratively interrogate video content. Developing such video\nreasoning capabilities in LVLMs presents notable challenges, particularly in\nadapting the model to new video actions (e.g. select frame), and designing\nreward functions to guide LVLMs to adopt the newly introduced action. To solve\nthese challenges, we propose a two-phase training strategy, first employing\nSupervised Fine-Tuning (SFT) to instill fundamental action capabilities,\nfollowed by Reinforcement Learning (RL) to optimize a strategic decision-making\npolicy.Notably, in this RL phase, we conduct an in-depth and comprehensive\nexploration of the reward design for each action and format reward. Extensive\nexperiments on reasoning benchmarks like Video-Holmes, LongVideo-Reason, and\nlong-video understanding benchmarks such as LongVideoBench, MLVU, VideoMME, and\nLVBench, demonstrate that FrameThinker achieves a significant average\nimprovement of +10.4% over baselines while drastically reducing the number of\nprocessed frames. Most notably, our 7B model, FrameThinker establishes a new\nstate-of-the-art on LongVideo-Reason, achieving 76.1% accuracy using an average\nof only 20.6 frames. This not only outperforms the competitive LongVILA-R1\n(72.0%) but does so with over 20x fewer frames (vs. 512), demonstrating\nunparalleled efficiency and effectiveness.",
        "url": "http://arxiv.org/abs/2509.24304v1",
        "published_date": "2025-09-29T05:36:58+00:00",
        "updated_date": "2025-09-29T05:36:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zefeng He",
            "Xiaoye Qu",
            "Yafu Li",
            "Siyuan Huang",
            "Daizong Liu",
            "Yu Cheng"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces FrameThinker, a framework that enables large vision-language models to think with long videos by iteratively interrogating video content. It achieves significant performance improvement on various benchmarks while using fewer frames.",
        "tldr_zh": "本文介绍了FrameThinker框架，使得大型视觉-语言模型能够通过迭代地审查视频内容来思考长视频。在各种基准测试中取得了显著的性能提升，同时使用了更少的帧数。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "SVGThinker: Instruction-Aligned and Reasoning-Driven Text-to-SVG Generation",
        "summary": "Scalable Vector Graphics (SVG) is a code-based representation for 2D visuals.\nLeveraging recent advances in large language models (LLMs), we study\ntext-to-SVG generation and address two persistent gaps: weak generalization and\npoor adherence to input instructions. We present SVGThinker, a reasoning-driven\nframework that aligns the production of SVG code with the visualization process\nand supports the full set of SVG primitives. Our pipeline first renders each\nprimitive in sequence and uses a multimodal model to annotate the image and\ncode; we then build stepwise updates that mirror the incremental addition of\nprimitives. On this data, we train an LLM with supervised fine-tuning that\nexposes its chain-of-thought as intermediate reasoning, improving robustness\nand reducing errors and hallucinations. Experiments against state-of-the-art\nbaselines show that SVGThinker produces more stable, editable, and\nhigher-quality SVGs while preserving the structural advantages of vector\ngraphics. Unlike image-based methods, our outputs enable precise and\nhierarchical editing, opening new directions for design, content creation, and\nautomated graphics generation.",
        "url": "http://arxiv.org/abs/2509.24299v1",
        "published_date": "2025-09-29T05:25:00+00:00",
        "updated_date": "2025-09-29T05:25:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hanqi Chen",
            "Zhongyin Zhao",
            "Ye Chen",
            "Zhujin Liang",
            "Bingbing Ni"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "SVGThinker is a reasoning-driven framework for generating high-quality SVGs from text, improving on weak generalization and instruction adherence.",
        "tldr_zh": "SVGThinker是一个基于推理的框架，用于从文本生成高质量的SVG图像，改善了弱泛化和指令遵循问题。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ASIA: Adaptive 3D Segmentation using Few Image Annotations",
        "summary": "We introduce ASIA (Adaptive 3D Segmentation using few Image Annotations), a\nnovel framework that enables segmentation of possibly non-semantic and\nnon-text-describable \"parts\" in 3D. Our segmentation is controllable through a\nfew user-annotated in-the-wild images, which are easier to collect than\nmulti-view images, less demanding to annotate than 3D models, and more precise\nthan potentially ambiguous text descriptions. Our method leverages the rich\npriors of text-to-image diffusion models, such as Stable Diffusion (SD), to\ntransfer segmentations from image space to 3D, even when the annotated and\ntarget objects differ significantly in geometry or structure. During training,\nwe optimize a text token for each segment and fine-tune our model with a novel\ncross-view part correspondence loss. At inference, we segment multi-view\nrenderings of the 3D mesh, fuse the labels in UV-space via voting, refine them\nwith our novel Noise Optimization technique, and finally map the UV-labels back\nonto the mesh. ASIA provides a practical and generalizable solution for both\nsemantic and non-semantic 3D segmentation tasks, outperforming existing methods\nby a noticeable margin in both quantitative and qualitative evaluations.",
        "url": "http://arxiv.org/abs/2509.24288v1",
        "published_date": "2025-09-29T05:04:11+00:00",
        "updated_date": "2025-09-29T05:04:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sai Raj Kishore Perla",
            "Aditya Vora",
            "Sauradip Nag",
            "Ali Mahdavi-Amiri",
            "Hao Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "Dataset"
        ],
        "tldr": "ASIA is a novel framework for 3D segmentation using few image annotations, leveraging text-to-image diffusion models and outperforming existing methods in both semantic and non-semantic tasks.",
        "tldr_zh": "ASIA是一种利用少量图像标注进行3D分割的新框架，利用文本到图像扩散模型，在语义和非语义任务中优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Cycle Diffusion Model for Counterfactual Image Generation",
        "summary": "Deep generative models have demonstrated remarkable success in medical image\nsynthesis. However, ensuring conditioning faithfulness and high-quality\nsynthetic images for direct or counterfactual generation remains a challenge.\nIn this work, we introduce a cycle training framework to fine-tune diffusion\nmodels for improved conditioning adherence and enhanced synthetic image\nrealism. Our approach, Cycle Diffusion Model (CDM), enforces consistency\nbetween generated and original images by incorporating cycle constraints,\nenabling more reliable direct and counterfactual generation. Experiments on a\ncombined 3D brain MRI dataset (from ABCD, HCP aging & young adults, ADNI, and\nPPMI) show that our method improves conditioning accuracy and enhances image\nquality as measured by FID and SSIM. The results suggest that the cycle\nstrategy used in CDM can be an effective method for refining diffusion-based\nmedical image generation, with applications in data augmentation,\ncounterfactual, and disease progression modeling.",
        "url": "http://arxiv.org/abs/2509.24267v1",
        "published_date": "2025-09-29T04:24:13+00:00",
        "updated_date": "2025-09-29T04:24:13+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Fangrui Huang",
            "Alan Wang",
            "Binxu Li",
            "Bailey Trang",
            "Ridvan Yesiloglu",
            "Tianyu Hua",
            "Wei Peng",
            "Ehsan Adeli"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a Cycle Diffusion Model for improved conditioning adherence and enhanced image realism in medical image generation.",
        "tldr_zh": "本文介绍了一种循环扩散模型，用于改进医学图像生成中的条件忠实度和图像逼真度。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "When MLLMs Meet Compression Distortion: A Coding Paradigm Tailored to MLLMs",
        "summary": "The increasing deployment of powerful Multimodal Large Language Models\n(MLLMs), typically hosted on cloud platforms, urgently requires effective\ncompression techniques to efficiently transmit signal inputs (e.g., images,\nvideos) from edge devices with minimal bandwidth usage. However, conventional\nimage codecs are optimized for fidelity to serve the Human Visual System (HVS)\nand ill-suited for MLLMs, in which diverse downstream tasks are jointly\nconsidered. In this paper, we first systematically analyze the impact of\ncompression artifacts on several mainstream MLLMs. We find that: Compression\ndistortion unevenly impacts different-level image features, leading to varying\neffects on MLLMs' downstream tasks depending on their feature-level reliance.\nMotivated by this discovery, we propose an image Codec TAilored to MLLMs\n(CoTAM) designed to adaptively protect multi-level features and suit different\ndemands of downstream tasks. The encoder leverages CLIP's shallow-layer\nattention to generate an importance map for bit allocation, preserving critical\nsemantic regions. Concurrently, the decoder integrates a lightweight adapter\nwith a multi-level loss function to ensure the faithful reconstruction both of\nlow-level details and high-level semantic context for robust synthesis of\ncross-level features. Extensive experiments validate that our method achieves\nup to 35.99\\% bitrate saving while maintaining the same performance on the MLLM\ntasks, outperforming previous SOTA neural codecs.",
        "url": "http://arxiv.org/abs/2509.24258v1",
        "published_date": "2025-09-29T04:07:52+00:00",
        "updated_date": "2025-09-29T04:07:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinming Liu",
            "Zhaoyang Jia",
            "Jiahao Li",
            "Bin Li",
            "Xin Jin",
            "Wenjun Zeng",
            "Yan Lu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel image codec tailored to Multimodal Large Language Models (MLLMs) to efficiently transmit signal inputs from edge devices while maintaining performance on MLLM tasks.",
        "tldr_zh": "本文介绍了一种针对多模态大型语言模型（MLLMs）定制的新型图像编解码器，可有效传输信号输入，并在MLLM任务上保持性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Latent Visual Reasoning",
        "summary": "Multimodal Large Language Models (MLLMs) have achieved notable gains in\nvarious tasks by incorporating Chain-of-Thought (CoT) reasoning in language\nspaces. Recent work extends this direction by leveraging external tools for\nvisual editing, thereby enhancing the visual signal along the reasoning\ntrajectories. Nevertheless, these approaches remain fundamentally constrained:\nreasoning is still confined to the language space, with visual information\ntreated as static preconditions. We introduce Latent Visual Reasoning (LVR), a\nnew paradigm that enables autoregressive reasoning directly in the visual\nembedding space. A visual encoder first projects images into visual tokens\nwithin a joint semantic space shared with the language model. The language\nmodel is then trained to generate latent states that reconstruct key visual\ntokens critical for answering the query, constituting the process of latent\nvisual reasoning. By interleaving LVR with standard text generation, our model\nachieves substantial gains on perception-intensive visual question answering\ntasks. In addition, we adapt the GRPO algorithm to conduct reinforcement\nlearning on latent reasoning, further balancing LVR and textual generation. We\nshow that LVR substantially improves fine-grained visual understanding and\nperception, achieving 71.67% on MMVP compared to 66.67% with Qwen2.5-VL. Code\nbase and model weights will be released later.",
        "url": "http://arxiv.org/abs/2509.24251v1",
        "published_date": "2025-09-29T03:52:01+00:00",
        "updated_date": "2025-09-29T03:52:01+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Bangzheng Li",
            "Ximeng Sun",
            "Jiang Liu",
            "Ze Wang",
            "Jialian Wu",
            "Xiaodong Yu",
            "Hao Chen",
            "Emad Barsoum",
            "Muhao Chen",
            "Zicheng Liu"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer"
        ],
        "tldr": "The paper introduces Latent Visual Reasoning (LVR) as a new approach to enable autoregressive reasoning directly in the visual embedding space, leading to improved performance on visual question answering tasks.",
        "tldr_zh": "该论文引入了潜在视觉推理（LVR）作为一种新方法，以在视觉嵌入空间中启用自回归推理，从而提高视觉问题回答任务的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FreeAction: Training-Free Techniques for Enhanced Fidelity of Trajectory-to-Video Generation",
        "summary": "Generating realistic robot videos from explicit action trajectories is a\ncritical step toward building effective world models and robotics foundation\nmodels. We introduce two training-free, inference-time techniques that fully\nexploit explicit action parameters in diffusion-based robot video generation.\nInstead of treating action vectors as passive conditioning signals, our methods\nactively incorporate them to guide both the classifier-free guidance process\nand the initialization of Gaussian latents. First, action-scaled\nclassifier-free guidance dynamically modulates guidance strength in proportion\nto action magnitude, enhancing controllability over motion intensity. Second,\naction-scaled noise truncation adjusts the distribution of initially sampled\nnoise to better align with the desired motion dynamics. Experiments on real\nrobot manipulation datasets demonstrate that these techniques significantly\nimprove action coherence and visual quality across diverse robot environments.",
        "url": "http://arxiv.org/abs/2509.24241v1",
        "published_date": "2025-09-29T03:30:40+00:00",
        "updated_date": "2025-09-29T03:30:40+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Seungwook Kim",
            "Seunghyeon Lee",
            "Minsu Cho"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces training-free techniques for improving the fidelity of generating robot videos from action trajectories by dynamically modulating guidance strength and noise truncation based on action parameters.",
        "tldr_zh": "该论文介绍了一种无需培训的技术，通过根据动作参数动态调节引导强度和噪声截断，提高了从动作轨迹生成机器人视频的逼真度。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Semantic Editing with Coupled Stochastic Differential Equations",
        "summary": "Editing the content of an image with a pretrained text-to-image model remains\nchallenging. Existing methods often distort fine details or introduce\nunintended artifacts. We propose using coupled stochastic differential\nequations (coupled SDEs) to guide the sampling process of any pre-trained\ngenerative model that can be sampled by solving an SDE, including diffusion and\nrectified flow models. By driving both the source image and the edited image\nwith the same correlated noise, our approach steers new samples toward the\ndesired semantics while preserving visual similarity to the source. The method\nworks out-of-the-box-without retraining or auxiliary networks-and achieves high\nprompt fidelity along with near-pixel-level consistency. These results position\ncoupled SDEs as a simple yet powerful tool for controlled generative AI.",
        "url": "http://arxiv.org/abs/2509.24223v1",
        "published_date": "2025-09-29T03:05:16+00:00",
        "updated_date": "2025-09-29T03:05:16+00:00",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "authors": [
            "Jianxin Zhang",
            "Clayton Scott"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper proposes using coupled stochastic differential equations to guide the sampling process of pre-trained generative models for semantic editing of images, achieving high fidelity and consistency without retraining.",
        "tldr_zh": "本文提出使用耦合随机微分方程来引导预训练生成模型的采样过程，实现对图像的语义编辑，高度保真和一致性，无需重新训练。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniVid: The Open-Source Unified Video Model",
        "summary": "Unified video modeling that combines generation and understanding\ncapabilities is increasingly important but faces two key challenges:\nmaintaining semantic faithfulness during flow-based generation due to\ntext-visual token imbalance and the limitations of uniform cross-modal\nattention across the flow trajectory, and efficiently extending image-centric\nMLLMs to video without costly retraining. We present UniVid, a unified\narchitecture that couples an MLLM with a diffusion decoder through a\nlightweight adapter, enabling both video understanding and generation. We\nintroduce Temperature Modality Alignment to improve prompt adherence and\nPyramid Reflection for efficient temporal reasoning via dynamic keyframe\nselection. Extensive experiments on standard benchmarks demonstrate\nstate-of-the-art performance, achieving a 2.2% improvement on VBench-Long total\nscore compared to EasyAnimateV5.1, and 1.0% and 3.3% accuracy gains on MSVD-QA\nand ActivityNet-QA, respectively, compared with the best prior 7B baselines.",
        "url": "http://arxiv.org/abs/2509.24200v1",
        "published_date": "2025-09-29T02:31:36+00:00",
        "updated_date": "2025-09-29T02:31:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiabin Luo",
            "Junhui Lin",
            "Zeyu Zhang",
            "Biao Wu",
            "Meng Fang",
            "Ling Chen",
            "Hao Tang"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "UniVid is a unified video model that combines generation and understanding capabilities, addressing challenges in maintaining semantic faithfulness and extending image-centric models to videos.",
        "tldr_zh": "UniVid是一个统一的视频模型，结合了生成和理解能力，解决了在流式生成中保持语义忠实性和将图像中心模型扩展到视频中的挑战。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Simulating Post-Neoadjuvant Chemotherapy Breast Cancer MRI via Diffusion Model with Prompt Tuning",
        "summary": "Neoadjuvant chemotherapy (NAC) is a common therapy option before the main\nsurgery for breast cancer. Response to NAC is monitored using follow-up dynamic\ncontrast-enhanced magnetic resonance imaging (DCE-MRI). Accurate prediction of\nNAC response helps with treatment planning. Here, we adopt maximum intensity\nprojection images from DCE-MRI to generate post-treatment images (i.e., 3 or 12\nweeks after NAC) from pre-treatment images leveraging the emerging diffusion\nmodel. We introduce prompt tuning to account for the known clinical factors\naffecting response to NAC. Our model performed better than other generative\nmodels in image quality metrics. Our model was better at generating images that\nreflected changes in tumor size according to pCR compared to other models.\nAblation study confirmed the design choices of our method. Our study has the\npotential to help with precision medicine.",
        "url": "http://arxiv.org/abs/2509.24185v1",
        "published_date": "2025-09-29T02:05:20+00:00",
        "updated_date": "2025-09-29T02:05:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jonghun Kim",
            "Hyunjin Park"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper presents a method for simulating post-neoadjuvant chemotherapy breast cancer MRI images using a diffusion model with prompt tuning, showing promising results for treatment planning.",
        "tldr_zh": "本文提出了一种使用扩散模型和快速调整生成后新辅助化疗乳腺癌MRI图像的方法，对治疗计划显示出有希望的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Tumor Synthesis conditioned on Radiomics",
        "summary": "Due to privacy concerns, obtaining large datasets is challenging in medical\nimage analysis, especially with 3D modalities like Computed Tomography (CT) and\nMagnetic Resonance Imaging (MRI). Existing generative models, developed to\naddress this issue, often face limitations in output diversity and thus cannot\naccurately represent 3D medical images. We propose a tumor-generation model\nthat utilizes radiomics features as generative conditions. Radiomics features\nare high-dimensional handcrafted semantic features that are biologically\nwell-grounded and thus are good candidates for conditioning. Our model employs\na GAN-based model to generate tumor masks and a diffusion-based approach to\ngenerate tumor texture conditioned on radiomics features. Our method allows the\nuser to generate tumor images according to user-specified radiomics features\nsuch as size, shape, and texture at an arbitrary location. This enables the\nphysicians to easily visualize tumor images to better understand tumors\naccording to changing radiomics features. Our approach allows for the removal,\nmanipulation, and repositioning of tumors, generating various tumor types in\ndifferent scenarios. The model has been tested on tumors in four different\norgans (kidney, lung, breast, and brain) across CT and MRI. The synthesized\nimages are shown to effectively aid in training for downstream tasks and their\nauthenticity was also evaluated through expert evaluations. Our method has\npotential usage in treatment planning with diverse synthesized tumors.",
        "url": "http://arxiv.org/abs/2509.24182v1",
        "published_date": "2025-09-29T02:04:12+00:00",
        "updated_date": "2025-09-29T02:04:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jonghun Kim",
            "Inye Na",
            "Eun Sook Ko",
            "Hyunjin Park"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a tumor generation model that uses radiomics features to generate tumor images for medical applications.",
        "tldr_zh": "本文提出了一种使用放射医学特征生成肿瘤图像的模型，用于医学应用。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Accelerating Cerebral Diagnostics with BrainFusion: A Comprehensive MRI Tumor Framework",
        "summary": "The early and accurate classification of brain tumors is crucial for guiding\neffective treatment strategies and improving patient outcomes. This study\npresents BrainFusion, a significant advancement in brain tumor analysis using\nmagnetic resonance imaging (MRI) by combining fine-tuned convolutional neural\nnetworks (CNNs) for tumor classification--including VGG16, ResNet50, and\nXception--with YOLOv8 for precise tumor localization with bounding boxes.\nLeveraging the Brain Tumor MRI Dataset, our experiments reveal that the\nfine-tuned VGG16 model achieves test accuracy of 99.86%, substantially\nexceeding previous benchmarks. Beyond setting a new accuracy standard, the\nintegration of bounding-box localization and explainable AI techniques further\nenhances both the clinical interpretability and trustworthiness of the system's\noutputs. Overall, this approach underscores the transformative potential of\ndeep learning in delivering faster, more reliable diagnoses, ultimately\ncontributing to improved patient care and survival rates.",
        "url": "http://arxiv.org/abs/2509.24149v1",
        "published_date": "2025-09-29T00:53:17+00:00",
        "updated_date": "2025-09-29T00:53:17+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "60G35, 62M10, 62P35, 65C20, 68T45, 68U10, 92C35, 92C40, 92C42, 93E10",
            "I.4; I.4.8; I.4.9; I.4.10; I.2; I.2.6; I.2.10; J.3; C.2.4; C.3;\n  H.2.8; H.3.4; H.3.5; I.2.4; I.5; I.5.1; I.5.4; K.6.1"
        ],
        "authors": [
            "Walid Houmaidi",
            "Youssef Sabiri",
            "Salmane El Mansour Billah",
            "Amine Abouaomar"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset",
            "Transformer"
        ],
        "tldr": "BrainFusion is an advanced MRI-based brain tumor analysis system that combines CNNs for classification and YOLOv8 for precise tumor localization, achieving high accuracy and enhancing clinical interpretability.",
        "tldr_zh": "BrainFusion是一种基于MRI的高级脑肿瘤分析系统，结合CNNs进行分类和YOLOv8进行精确定位，实现了高准确度并增强了临床解释性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Asymmetric VAE for One-Step Video Super-Resolution Acceleration",
        "summary": "Diffusion models have significant advantages in the field of real-world video\nsuper-resolution and have demonstrated strong performance in past research. In\nrecent diffusion-based video super-resolution (VSR) models, the number of\nsampling steps has been reduced to just one, yet there remains significant room\nfor further optimization in inference efficiency. In this paper, we propose\nFastVSR, which achieves substantial reductions in computational cost by\nimplementing a high compression VAE (spatial compression ratio of 16, denoted\nas f16). We design the structure of the f16 VAE and introduce a stable training\nframework. We employ pixel shuffle and channel replication to achieve\nadditional upsampling. Furthermore, we propose a lower-bound-guided training\nstrategy, which introduces a simpler training objective as a lower bound for\nthe VAE's performance. It makes the training process more stable and easier to\nconverge. Experimental results show that FastVSR achieves speedups of 111.9\ntimes compared to multi-step models and 3.92 times compared to existing\none-step models. We will release code and models at\nhttps://github.com/JianzeLi-114/FastVSR.",
        "url": "http://arxiv.org/abs/2509.24142v1",
        "published_date": "2025-09-29T00:36:14+00:00",
        "updated_date": "2025-09-29T00:36:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jianze Li",
            "Yong Guo",
            "Yulun Zhang",
            "Xiaokang Yang"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes FastVSR, a video super-resolution model using a high compression VAE to achieve faster computational speed with stable training. It outperforms existing models in terms of speed.",
        "tldr_zh": "本文提出了FastVSR，一种使用高压缩VAE的视频超分辨率模型，以实现更快的计算速度和稳定的训练。在速度方面优于现有模型。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Analysis of Bias in Deep Learning Facial Beauty Regressors",
        "summary": "Bias can be introduced to AI systems even from seemingly balanced sources,\nand AI facial beauty prediction is subject to ethnicity-based bias. This work\nsounds warnings about AI's role in shaping aesthetic norms while providing\npotential pathways toward equitable beauty technologies through comparative\nanalysis of models trained on SCUT-FBP5500 and MEBeauty datasets. Employing\nrigorous statistical validation (Kruskal-Wallis H-tests, post hoc Dunn\nanalyses). It is demonstrated that both models exhibit significant prediction\ndisparities across ethnic groups $(p < 0.001)$, even when evaluated on the\nbalanced FairFace dataset. Cross-dataset validation shows algorithmic\namplification of societal beauty biases rather than mitigation based on\nprediction and error parity. The findings underscore the inadequacy of current\nAI beauty prediction approaches, with only 4.8-9.5\\% of inter-group comparisons\nsatisfying distributional parity criteria. Mitigation strategies are proposed\nand discussed in detail.",
        "url": "http://arxiv.org/abs/2509.24138v1",
        "published_date": "2025-09-29T00:16:24+00:00",
        "updated_date": "2025-09-29T00:16:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chandon Hamel",
            "Mike Busch"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper analyzes bias in AI facial beauty prediction models, demonstrating significant prediction disparities across ethnic groups. It highlights the inadequacy of current AI beauty prediction approaches and proposes mitigation strategies.",
        "tldr_zh": "该论文分析了人工智能面部美感预测模型中的偏见，展示了不同种族群体之间存在显著的预测差异。它突出了当前人工智能美感预测方法的不足，并提出了缓解策略。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GANji: A Framework for Introductory AI Image Generation",
        "summary": "The comparative study of generative models often requires significant\ncomputational resources, creating a barrier for researchers and practitioners.\nThis paper introduces GANji, a lightweight framework for benchmarking\nfoundational AI image generation techniques using a dataset of 10,314 Japanese\nKanji characters. It systematically compares the performance of a Variational\nAutoencoder (VAE), a Generative Adversarial Network (GAN), and a Denoising\nDiffusion Probabilistic Model (DDPM). The results demonstrate that while the\nDDPM achieves the highest image fidelity, with a Fr\\'echet Inception Distance\n(FID) score of 26.2, its sampling time is over 2,000 times slower than the\nother models. The GANji framework is an effective and accessible tool for\nrevealing the fundamental trade-offs between model architecture, computational\ncost, and visual quality, making it ideal for both educational and research\npurposes.",
        "url": "http://arxiv.org/abs/2509.24128v1",
        "published_date": "2025-09-28T23:54:59+00:00",
        "updated_date": "2025-09-28T23:54:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chandon Hamel",
            "Mike Busch"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "GANji is a lightweight framework for comparing AI image generation techniques, showing trade-offs between model performance, computational cost, and visual quality.",
        "tldr_zh": "GANji是一个轻量级框架，用于比较AI图像生成技术，展示了模型性能、计算成本和视觉质量之间的权衡。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SVAC: Scaling Is All You Need For Referring Video Object Segmentation",
        "summary": "Referring Video Object Segmentation (RVOS) aims to segment target objects in\nvideo sequences based on natural language descriptions. While recent advances\nin Multi-modal Large Language Models (MLLMs) have improved RVOS performance\nthrough enhanced text-video understanding, several challenges remain, including\ninsufficient exploitation of MLLMs' prior knowledge, prohibitive computational\nand memory costs for long-duration videos, and inadequate handling of complex\ntemporal dynamics. In this work, we propose SVAC, a unified model that improves\nRVOS by scaling up input frames and segmentation tokens to enhance\nvideo-language interaction and segmentation precision. To address the resulting\ncomputational challenges, SVAC incorporates the Anchor-Based Spatio-Temporal\nCompression (ASTC) module to compress visual tokens while preserving essential\nspatio-temporal structure. Moreover, the Clip-Specific Allocation (CSA)\nstrategy is introduced to better handle dynamic object behaviors across video\nclips. Experimental results demonstrate that SVAC achieves state-of-the-art\nperformance on multiple RVOS benchmarks with competitive efficiency. Our code\nis available at https://github.com/lizhang1998/SVAC.",
        "url": "http://arxiv.org/abs/2509.24109v1",
        "published_date": "2025-09-28T23:02:09+00:00",
        "updated_date": "2025-09-28T23:02:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Li Zhang",
            "Haoxiang Gao",
            "Zhihao Zhang",
            "Luoxiao Huang",
            "Tao Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "SVAC proposes a unified model for Referring Video Object Segmentation that scales up input frames and segmentation tokens, achieving state-of-the-art performance on RVOS benchmarks.",
        "tldr_zh": "SVAC提出了一种统一模型，用于指代视频对象分割，通过增加输入帧和分割标记，实现了在RVOS基准上的最先进性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unified Multi-Modal Interactive & Reactive 3D Motion Generation via Rectified Flow",
        "summary": "Generating realistic, context-aware two-person motion conditioned on diverse\nmodalities remains a central challenge in computer graphics, animation, and\nhuman-computer interaction. We introduce DualFlow, a unified and efficient\nframework for multi-modal two-person motion generation. DualFlow conditions 3D\nmotion synthesis on diverse inputs, including text, music, and prior motion\nsequences. Leveraging rectified flow, it achieves deterministic straight-line\nsampling paths between noise and data, reducing inference time and mitigating\nerror accumulation common in diffusion-based models. To enhance semantic\ngrounding, DualFlow employs a Retrieval-Augmented Generation (RAG) module that\nretrieves motion exemplars using music features and LLM-based text\ndecompositions of spatial relations, body movements, and rhythmic patterns. We\nuse contrastive objective that further strengthens alignment with conditioning\nsignals and introduce synchronization loss that improves inter-person\ncoordination. Extensive evaluations across text-to-motion, music-to-motion, and\nmulti-modal interactive benchmarks show consistent gains in motion quality,\nresponsiveness, and efficiency. DualFlow produces temporally coherent and\nrhythmically synchronized motions, setting state-of-the-art in multi-modal\nhuman motion generation.",
        "url": "http://arxiv.org/abs/2509.24099v1",
        "published_date": "2025-09-28T22:36:18+00:00",
        "updated_date": "2025-09-28T22:36:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Prerit Gupta",
            "Shourya Verma",
            "Ananth Grama",
            "Aniket Bera"
        ],
        "ai_categories": [
            "Multimodality",
            "GAN",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a framework called DualFlow for generating realistic and context-aware two-person motion using diverse inputs. It achieves state-of-the-art results in multi-modal human motion generation.",
        "tldr_zh": "该论文引入了一个名为DualFlow的框架，用于使用多样化的输入生成逼真且具有情境意识的双人运动。它在多模态人类运动生成方面取得了最新的成果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Autoregressive Video Generation beyond Next Frames Prediction",
        "summary": "Autoregressive models for video generation typically operate frame-by-frame,\nextending next-token prediction from language to video's temporal dimension. We\nquestion that unlike word as token is universally agreed in language if frame\nis a appropriate prediction unit? To address this, we present VideoAR, a\nunified framework that supports a spectrum of prediction units including full\nframes, key-detail frames, multiscale refinements, and spatiotemporal cubes.\nAmong these designs, we find model video generation using\n\\textit{spatiotemporal} cubes as prediction units, which allows autoregressive\nmodels to operate across both spatial and temporal dimensions simultaneously.\nThis approach eliminates the assumption that frames are the natural atomic\nunits for video autoregression. We evaluate VideoAR across diverse prediction\nstrategies, finding that cube-based prediction consistently delivers superior\nquality, speed, and temporal coherence. By removing the frame-by-frame\nconstraint, our video generator surpasses state-of-the-art baselines on VBench\nwhile achieving faster inference and enabling seamless scaling to minute-long\nsequences. We hope this work will motivate rethinking sequence decomposition in\nvideo and other spatiotemporal domains.",
        "url": "http://arxiv.org/abs/2509.24081v1",
        "published_date": "2025-09-28T21:37:53+00:00",
        "updated_date": "2025-09-28T21:37:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sucheng Ren",
            "Chen Chen",
            "Zhenbang Wang",
            "Liangchen Song",
            "Xiangxin Zhu",
            "Alan Yuille",
            "Yinfei Yang",
            "Jiasen Lu"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "This paper presents a framework called VideoAR that allows video generation using spatiotemporal cubes as prediction units, surpassing state-of-the-art baselines in quality, speed, and coherence.",
        "tldr_zh": "本文提出了一个名为VideoAR的框架，允许使用时空立方体作为预测单位生成视频，优于现有基线在质量、速度和连贯性方面。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Uncovering Grounding IDs: How External Cues Shape Multi-Modal Binding",
        "summary": "Large vision-language models (LVLMs) show strong performance across\nmultimodal benchmarks but remain limited in structured reasoning and precise\ngrounding. Recent work has demonstrated that adding simple visual structures,\nsuch as partitions and annotations, improves accuracy, yet the internal\nmechanisms underlying these gains remain unclear. We investigate this\nphenomenon and propose the concept of Grounding IDs, latent identifiers induced\nby external cues that bind objects to their designated partitions across\nmodalities. Through representation analysis, we find that these identifiers\nemerge as robust within-partition alignment in embedding space and reduce the\nmodality gap between image and text. Causal interventions further confirm that\nthese identifiers mediate binding between objects and symbolic cues. We show\nthat Grounding IDs strengthen attention between related components, which in\nturn improves cross-modal grounding and reduces hallucinations. Taken together,\nour results identify Grounding IDs as a key symbolic mechanism explaining how\nexternal cues enhance multimodal binding, offering both interpretability and\npractical improvements in robustness.",
        "url": "http://arxiv.org/abs/2509.24072v1",
        "published_date": "2025-09-28T21:15:07+00:00",
        "updated_date": "2025-09-28T21:15:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hosein Hasani",
            "Amirmohammad Izadi",
            "Fatemeh Askari",
            "Mobin Bagherian",
            "Sadegh Mohammadian",
            "Mohammad Izadi",
            "Mahdieh Soleymani Baghshah"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer"
        ],
        "tldr": "The paper introduces the concept of Grounding IDs, which are latent identifiers induced by external cues that bind objects to their designated partitions in multimodal tasks, strengthening attention between related components and improving cross-modal grounding.",
        "tldr_zh": "本文介绍了地面ID的概念，这些由外部线索诱导的潜在标识符将对象与其在多模式任务中指定的分区绑定在一起，加强了相关组件之间的注意力，改善了跨模式基础。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Vision-and-Language Navigation with Analogical Textual Descriptions in LLMs",
        "summary": "Integrating large language models (LLMs) into embodied AI models is becoming\nincreasingly prevalent. However, existing zero-shot LLM-based\nVision-and-Language Navigation (VLN) agents either encode images as textual\nscene descriptions, potentially oversimplifying visual details, or process raw\nimage inputs, which can fail to capture abstract semantics required for\nhigh-level reasoning. In this paper, we improve the navigation agent's\ncontextual understanding by incorporating textual descriptions from multiple\nperspectives that facilitate analogical reasoning across images. By leveraging\ntext-based analogical reasoning, the agent enhances its global scene\nunderstanding and spatial reasoning, leading to more accurate action decisions.\nWe evaluate our approach on the R2R dataset, where our experiments demonstrate\nsignificant improvements in navigation performance.",
        "url": "http://arxiv.org/abs/2509.25139v1",
        "published_date": "2025-09-29T17:51:01+00:00",
        "updated_date": "2025-09-29T17:51:01+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Yue Zhang",
            "Tianyi Ma",
            "Zun Wang",
            "Yanyuan Qiao",
            "Parisa Kordjamshidi"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a method to improve navigation agents' contextual understanding by incorporating textual descriptions for analogical reasoning, leading to better navigation performance.",
        "tldr_zh": "本文提出了一种通过整合文本描述进行类比推理以改善导航智能体的情境理解的方法，从而提高导航性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "AIRoA MoMa Dataset: A Large-Scale Hierarchical Dataset for Mobile Manipulation",
        "summary": "As robots transition from controlled settings to unstructured human\nenvironments, building generalist agents that can reliably follow natural\nlanguage instructions remains a central challenge. Progress in robust mobile\nmanipulation requires large-scale multimodal datasets that capture contact-rich\nand long-horizon tasks, yet existing resources lack synchronized force-torque\nsensing, hierarchical annotations, and explicit failure cases. We address this\ngap with the AIRoA MoMa Dataset, a large-scale real-world multimodal dataset\nfor mobile manipulation. It includes synchronized RGB images, joint states,\nsix-axis wrist force-torque signals, and internal robot states, together with a\nnovel two-layer annotation schema of sub-goals and primitive actions for\nhierarchical learning and error analysis. The initial dataset comprises 25,469\nepisodes (approx. 94 hours) collected with the Human Support Robot (HSR) and is\nfully standardized in the LeRobot v2.1 format. By uniquely integrating mobile\nmanipulation, contact-rich interaction, and long-horizon structure, AIRoA MoMa\nprovides a critical benchmark for advancing the next generation of\nVision-Language-Action models. The first version of our dataset is now\navailable at https://huggingface.co/datasets/airoa-org/airoa-moma .",
        "url": "http://arxiv.org/abs/2509.25032v1",
        "published_date": "2025-09-29T16:51:47+00:00",
        "updated_date": "2025-09-29T16:51:47+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Ryosuke Takanami",
            "Petr Khrapchenkov",
            "Shu Morikuni",
            "Jumpei Arima",
            "Yuta Takaba",
            "Shunsuke Maeda",
            "Takuya Okubo",
            "Genki Sano",
            "Satoshi Sekioka",
            "Aoi Kadoya",
            "Motonari Kambara",
            "Naoya Nishiura",
            "Haruto Suzuki",
            "Takanori Yoshimoto",
            "Koya Sakamoto",
            "Shinnosuke Ono",
            "Hu Yang",
            "Daichi Yashima",
            "Aoi Horo",
            "Tomohiro Motoda",
            "Kensuke Chiyoma",
            "Hiroshi Ito",
            "Koki Fukuda",
            "Akihito Goto",
            "Kazumi Morinaga",
            "Yuya Ikeda",
            "Riko Kawada",
            "Masaki Yoshikawa",
            "Norio Kosuge",
            "Yuki Noguchi",
            "Kei Ota",
            "Tatsuya Matsushima",
            "Yusuke Iwasawa",
            "Yutaka Matsuo",
            "Tetsuya Ogata"
        ],
        "ai_categories": [
            "Dataset",
            "Multimodality"
        ],
        "tldr": "The paper introduces the AIRoA MoMa Dataset, a large-scale dataset for mobile manipulation with synchronized RGB images, force-torque signals, and hierarchical annotations for hierarchical learning and error analysis.",
        "tldr_zh": "该论文介绍了AIRoA MoMa数据集，这是一个用于移动操作的大规模数据集，包含同步RGB图像、力矩信号和用于层次学习和错误分析的层次标注。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Evaluating Temperature Scaling Calibration Effectiveness for CNNs under Varying Noise Levels in Brain Tumour Detection",
        "summary": "Precise confidence estimation in deep learning is vital for high-stakes\nfields like medical imaging, where overconfident misclassifications can have\nserious consequences. This work evaluates the effectiveness of Temperature\nScaling (TS), a post-hoc calibration technique, in improving the reliability of\nconvolutional neural networks (CNNs) for brain tumor classification. We develop\na custom CNN and train it on a merged brain MRI dataset. To simulate real-world\nuncertainty, five types of image noise are introduced: Gaussian, Poisson, Salt\n& Pepper, Speckle, and Uniform. Model performance is evaluated using precision,\nrecall, F1-score, accuracy, negative log-likelihood (NLL), and expected\ncalibration error (ECE), both before and after calibration. Results demonstrate\nthat TS significantly reduces ECE and NLL under all noise conditions without\ndegrading classification accuracy. This underscores TS as an effective and\ncomputationally efficient approach to enhance decision confidence of medical AI\nsystems, hence making model outputs more reliable in noisy or uncertain\nsettings.",
        "url": "http://arxiv.org/abs/2509.24951v1",
        "published_date": "2025-09-29T15:46:23+00:00",
        "updated_date": "2025-09-29T15:46:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ankur Chanda",
            "Kushan Choudhury",
            "Shubhrodeep Roy",
            "Shubhajit Biswas",
            "Somenath Kuiry"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "The paper evaluates the effectiveness of Temperature Scaling calibration to improve reliability of CNNs for brain tumor detection under varying noise levels.",
        "tldr_zh": "本文评估了温度缩放校准技术在改善CNN对不同噪声级别下的脑瘤检测可靠性方面的有效性。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7.5
    },
    {
        "title": "PHASE-Net: Physics-Grounded Harmonic Attention System for Efficient Remote Photoplethysmography Measurement",
        "summary": "Remote photoplethysmography (rPPG) measurement enables non-contact\nphysiological monitoring but suffers from accuracy degradation under head\nmotion and illumination changes. Existing deep learning methods are mostly\nheuristic and lack theoretical grounding, which limits robustness and\ninterpretability. In this work, we propose a physics-informed rPPG paradigm\nderived from the Navier-Stokes equations of hemodynamics, showing that the\npulse signal follows a second-order dynamical system whose discrete solution\nnaturally leads to a causal convolution. This provides a theoretical\njustification for using a Temporal Convolutional Network (TCN). Based on this\nprinciple, we design PHASE-Net, a lightweight model with three key components:\n(1) Zero-FLOPs Axial Swapper module, which swaps or transposes a few spatial\nchannels to mix distant facial regions and enhance cross-region feature\ninteraction without breaking temporal order; (2) Adaptive Spatial Filter, which\nlearns a soft spatial mask per frame to highlight signal-rich areas and\nsuppress noise; and (3) Gated TCN, a causal dilated TCN with gating that models\nlong-range temporal dynamics for accurate pulse recovery. Extensive experiments\ndemonstrate that PHASE-Net achieves state-of-the-art performance with strong\nefficiency, offering a theoretically grounded and deployment-ready rPPG\nsolution.",
        "url": "http://arxiv.org/abs/2509.24850v1",
        "published_date": "2025-09-29T14:36:45+00:00",
        "updated_date": "2025-09-29T14:36:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bo Zhao",
            "Dan Guo",
            "Junzhe Cao",
            "Yong Xu",
            "Tao Tan",
            "Yue Sun",
            "Bochao Zou",
            "Jie Zhang",
            "Zitong Yu"
        ],
        "ai_categories": [
            "AIGC"
        ],
        "tldr": "The paper proposes a physics-informed method for remote photoplethysmography monitoring, achieving state-of-the-art performance with strong efficiency.",
        "tldr_zh": "该论文提出了一种基于物理学的方法，用于远程光电容积描记术监测，实现了具有强大效率的最先进性能。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Can you SPLICE it together? A Human Curated Benchmark for Probing Visual Reasoning in VLMs",
        "summary": "In this work, we introduce SPLICE, a human-curated benchmark derived from the\nCOIN instructional video dataset, designed to probe event-based reasoning\nacross multiple dimensions: temporal, causal, spatial, contextual, and general\nknowledge. SPLICE includes 3,381 human-filtered videos spanning 12 categories\nand 180 sub-categories, such as sports, engineering, and housework. These\nvideos are segmented into a total of 11,423 event clips. We evaluate both human\nparticipants and state-of-the-art vision-language models (VLMs) on the task of\nrearranging these clips into coherent event sequences to assess visual\nreasoning capabilities. Results reveal a significant gap: VLMs struggle to\nmatch human performance. While human-annotated textual descriptions improve\nmodel accuracy, they do not affect human performance, suggesting that models\nrely more on language priors than on visual understanding. Even with\nannotations, VLMs fall short of human-level reasoning, underscoring persistent\nchallenges in visual reasoning. A deeper analysis across sub-categories shows\nthat VLMs perform relatively better on videos where temporal and causal\nreasoning are dominant, compared to those where contextual and spatial\nreasoning are dominant. They also perform better on everyday tasks than on\nspecialized ones.",
        "url": "http://arxiv.org/abs/2509.24640v1",
        "published_date": "2025-09-29T11:50:18+00:00",
        "updated_date": "2025-09-29T11:50:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mohamad Ballout",
            "Okajevo Wilfred",
            "Seyedalireza Yaghoubi",
            "Nohayr Muhammad Abdelmoneim",
            "Julius Mayer",
            "Elia Bruni"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces SPLICE, a benchmark for assessing visual reasoning in VLMs using human-filtered videos. VLMs struggle to match human performance, especially in cases requiring spatial and contextual reasoning.",
        "tldr_zh": "本文介绍了SPLICE，一个用于评估VLMs中视觉推理的基准。结果表明，VLMs在需要空间和上下文推理的情况下难以匹配人类表现。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7.5
    },
    {
        "title": "Beyond Isolated Facts: Synthesizing Narrative and Grounded Supervision for VideoQA",
        "summary": "The performance of Video Question Answering (VideoQA) models is fundamentally\nconstrained by the nature of their supervision, which typically consists of\nisolated, factual question-answer pairs. This \"bag-of-facts\" approach fails to\ncapture the underlying narrative and causal structure of events, limiting\nmodels to a shallow understanding of video content. To move beyond this\nparadigm, we introduce a framework to synthesize richer supervisory signals. We\npropose two complementary strategies: Question-Based Paraphrasing (QBP), which\nsynthesizes the diverse inquiries (what, how, why) from a video's existing set\nof question-answer pairs into a holistic narrative paragraph that reconstructs\nthe video's event structure; and Question-Based Captioning (QBC), which\ngenerates fine-grained visual rationales, grounding the answer to each question\nin specific, relevant evidence. Leveraging powerful generative models, we use\nthis synthetic data to train VideoQA models under a unified next-token\nprediction objective. Extensive experiments on STAR and NExT-QA validate our\napproach, demonstrating significant accuracy gains and establishing new\nstate-of-the-art results, such as improving a 3B model to 72.5\\% on STAR\n(+4.9\\%) and a 7B model to 80.8\\% on NExT-QA. Beyond accuracy, our analysis\nreveals that both QBP and QBC substantially enhance cross-dataset\ngeneralization, with QBP additionally accelerating model convergence by over\n2.5x. These results demonstrate that shifting data synthesis from isolated\nfacts to narrative coherence and grounded rationales yields a more accurate,\nefficient, and generalizable training paradigm.",
        "url": "http://arxiv.org/abs/2509.24445v1",
        "published_date": "2025-09-29T08:28:44+00:00",
        "updated_date": "2025-09-29T08:28:44+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Jianxin Liang",
            "Tan Yue",
            "Yuxuan Wang",
            "Yueqian Wang",
            "Zhihan Yin",
            "Huishuai Zhang",
            "Dongyan Zhao"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a framework to improve Video Question Answering models by synthesizing richer supervisory signals through Question-Based Paraphrasing and Question-Based Captioning. Experimental results show significant accuracy gains and enhanced cross-dataset generalization.",
        "tldr_zh": "该论文引入了一个框架，通过问题重述和基于问题的字幕生成来改进视频问答模型。实验证明，该方法显著提高了准确性，并增强了跨数据集的泛化能力。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "Wavelet-Assisted Mamba for Satellite-Derived Sea Surface Temperature Super-Resolution",
        "summary": "Sea surface temperature (SST) is an essential indicator of global climate\nchange and one of the most intuitive factors reflecting ocean conditions.\nObtaining high-resolution SST data remains challenging due to limitations in\nphysical imaging, and super-resolution via deep neural networks is a promising\nsolution. Recently, Mamba-based approaches leveraging State Space Models (SSM)\nhave demonstrated significant potential for long-range dependency modeling with\nlinear complexity. However, their application to SST data super-resolution\nremains largely unexplored. To this end, we propose the Wavelet-assisted Mamba\nSuper-Resolution (WMSR) framework for satellite-derived SST data. The WMSR\nincludes two key components: the Low-Frequency State Space Module (LFSSM) and\nHigh-Frequency Enhancement Module (HFEM). The LFSSM uses 2D-SSM to capture\nglobal information of the input data, and the robust global modeling\ncapabilities of SSM are exploited to preserve the critical temperature\ninformation in the low-frequency component. The HFEM employs the pixel\ndifference convolution to match and correct the high-frequency feature,\nachieving accurate and clear textures. Through comprehensive experiments on\nthree SST datasets, our WMSR demonstrated superior performance over\nstate-of-the-art methods. Our codes and datasets will be made publicly\navailable at https://github.com/oucailab/WMSR.",
        "url": "http://arxiv.org/abs/2509.24334v1",
        "published_date": "2025-09-29T06:33:07+00:00",
        "updated_date": "2025-09-29T06:33:07+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Wankun Chen",
            "Feng Gao",
            "Yanhai Gan",
            "Jingchao Cao",
            "Junyu Dong",
            "Qian Du"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a new framework, WMSR, for super-resolution of satellite-derived sea surface temperature data using Wavelet-assisted Mamba approach, showing superior performance over existing methods.",
        "tldr_zh": "本文介绍了一种新的框架，WMSR，用于利用小波辅助马巴方法对卫星获取的海表温度数据进行超分辨率处理，显示出优于现有方法的性能。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Towards Foundation Models for Cryo-ET Subtomogram Analysis",
        "summary": "Cryo-electron tomography (cryo-ET) enables in situ visualization of\nmacromolecular structures, where subtomogram analysis tasks such as\nclassification, alignment, and averaging are critical for structural\ndetermination. However, effective analysis is hindered by scarce annotations,\nsevere noise, and poor generalization. To address these challenges, we take the\nfirst step towards foundation models for cryo-ET subtomograms. First, we\nintroduce CryoEngine, a large-scale synthetic data generator that produces over\n904k subtomograms from 452 particle classes for pretraining. Second, we design\nan Adaptive Phase Tokenization-enhanced Vision Transformer (APT-ViT), which\nincorporates adaptive phase tokenization as an equivariance-enhancing module\nthat improves robustness to both geometric and semantic variations. Third, we\nintroduce a Noise-Resilient Contrastive Learning (NRCL) strategy to stabilize\nrepresentation learning under severe noise conditions. Evaluations across 24\nsynthetic and real datasets demonstrate state-of-the-art (SOTA) performance on\nall three major subtomogram tasks and strong generalization to unseen datasets,\nadvancing scalable and robust subtomogram analysis in cryo-ET.",
        "url": "http://arxiv.org/abs/2509.24311v1",
        "published_date": "2025-09-29T05:47:45+00:00",
        "updated_date": "2025-09-29T05:47:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Runmin Jiang",
            "Wanyue Feng",
            "Yuntian Yang",
            "Shriya Pingulkar",
            "Hong Wang",
            "Xi Xiao",
            "Xiaoyu Cao",
            "Genpei Zhang",
            "Xiao Wang",
            "Xiaolong Wu",
            "Tianyang Wang",
            "Yang Liu",
            "Xingjian Li",
            "Min Xu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces foundation models for cryo-ET subtomogram analysis using synthetic data generation, adaptive phase tokenization, and noise-resilient contrastive learning, achieving state-of-the-art performance and strong generalization.",
        "tldr_zh": "本文引入基于合成数据生成、自适应相位记号和噪声鲁棒对比学习的基础模型，用于冷冻电子断层成像亚体积分析，实现了最新技术性能和强大泛化能力。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Robust Partial 3D Point Cloud Registration via Confidence Estimation under Global Context",
        "summary": "Partial point cloud registration is essential for autonomous perception and\n3D scene understanding, yet it remains challenging owing to structural\nambiguity, partial visibility, and noise. We address these issues by proposing\nConfidence Estimation under Global Context (CEGC), a unified, confidence-driven\nframework for robust partial 3D registration. CEGC enables accurate alignment\nin complex scenes by jointly modeling overlap confidence and correspondence\nreliability within a shared global context. Specifically, the hybrid overlap\nconfidence estimation module integrates semantic descriptors and geometric\nsimilarity to detect overlapping regions and suppress outliers early. The\ncontext-aware matching strategy smitigates ambiguity by employing global\nattention to assign soft confidence scores to correspondences, improving\nrobustness. These scores guide a differentiable weighted singular value\ndecomposition solver to compute precise transformations. This tightly coupled\npipeline adaptively down-weights uncertain regions and emphasizes contextually\nreliable matches. Experiments on ModelNet40, ScanObjectNN, and 7Scenes 3D\nvision datasets demonstrate that CEGC outperforms state-of-the-art methods in\naccuracy, robustness, and generalization. Overall, CEGC offers an interpretable\nand scalable solution to partial point cloud registration under challenging\nconditions.",
        "url": "http://arxiv.org/abs/2509.24275v1",
        "published_date": "2025-09-29T04:36:55+00:00",
        "updated_date": "2025-09-29T04:36:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yongqiang Wang",
            "Weigang Li",
            "Wenping Liu",
            "Zhe Xu",
            "Zhiqiang Tian"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a unified framework called CEGC for robust partial 3D point cloud registration, outperforming existing methods in accuracy, robustness, and generalization.",
        "tldr_zh": "本文提出了一种名为CEGC的统一框架，用于稳健的部分3D点云配准，在准确性、稳健性和泛化能力方面优于现有方法。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "S$^2$NN: Sub-bit Spiking Neural Networks",
        "summary": "Spiking Neural Networks (SNNs) offer an energy-efficient paradigm for machine\nintelligence, but their continued scaling poses challenges for resource-limited\ndeployment. Despite recent advances in binary SNNs, the storage and\ncomputational demands remain substantial for large-scale networks. To further\nexplore the compression and acceleration potential of SNNs, we propose Sub-bit\nSpiking Neural Networks (S$^2$NNs) that represent weights with less than one\nbit. Specifically, we first establish an S$^2$NN baseline by leveraging the\nclustering patterns of kernels in well-trained binary SNNs. This baseline is\nhighly efficient but suffers from \\textit{outlier-induced codeword selection\nbias} during training. To mitigate this issue, we propose an\n\\textit{outlier-aware sub-bit weight quantization} (OS-Quant) method, which\noptimizes codeword selection by identifying and adaptively scaling outliers.\nFurthermore, we propose a \\textit{membrane potential-based feature\ndistillation} (MPFD) method, improving the performance of highly compressed\nS$^2$NN via more precise guidance from a teacher model. Extensive results on\nvision and non-vision tasks reveal that S$^2$NN outperforms existing quantized\nSNNs in both performance and efficiency, making it promising for edge computing\napplications.",
        "url": "http://arxiv.org/abs/2509.24266v1",
        "published_date": "2025-09-29T04:17:44+00:00",
        "updated_date": "2025-09-29T04:17:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenjie Wei",
            "Malu Zhang",
            "Jieyuan Zhang",
            "Ammar Belatreche",
            "Shuai Wang",
            "Yimeng Shan",
            "Hanwen Liu",
            "Honglin Cao",
            "Guoqing Wang",
            "Yang Yang",
            "Haizhou Li"
        ],
        "ai_categories": [
            "Other"
        ],
        "tldr": "The paper introduces S$^2$NN, a method for compressing and accelerating Spiking Neural Networks, achieving better performance and efficiency for edge computing applications.",
        "tldr_zh": "本文介绍了S$^2$NN方法，用于压缩和加速脉冲神经网络，在边缘计算应用中取得更好的性能和效率。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Non-Invasive Detection of PROState Cancer with Novel Time-Dependent Diffusion MRI and AI-Enhanced Quantitative Radiological Interpretation: PROS-TD-AI",
        "summary": "Prostate cancer (PCa) is the most frequently diagnosed malignancy in men and\nthe eighth leading cause of cancer death worldwide. Multiparametric MRI (mpMRI)\nhas become central to the diagnostic pathway for men at intermediate risk,\nimproving de-tection of clinically significant PCa (csPCa) while reducing\nunnecessary biopsies and over-diagnosis. However, mpMRI remains limited by\nfalse positives, false negatives, and moderate to substantial interobserver\nagreement. Time-dependent diffusion (TDD) MRI, a novel sequence that enables\ntissue microstructure characterization, has shown encouraging preclinical\nperformance in distinguishing clinically significant from insignificant PCa.\nCombining TDD-derived metrics with machine learning may provide robust,\nzone-specific risk prediction with less dependence on reader training and\nimproved accuracy compared to current standard-of-care. This study protocol\nout-lines the rationale and describes the prospective evaluation of a\nhome-developed AI-enhanced TDD-MRI software (PROSTDAI) in routine diagnostic\ncare, assessing its added value against PI-RADS v2.1 and validating results\nagainst MRI-guided prostate biopsy.",
        "url": "http://arxiv.org/abs/2509.24227v1",
        "published_date": "2025-09-29T03:12:02+00:00",
        "updated_date": "2025-09-29T03:12:02+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Baltasar Ramos",
            "Cristian Garrido",
            "Paulette Narv'aez",
            "Santiago Gelerstein Claro",
            "Haotian Li",
            "Rafael Salvador",
            "Constanza V'asquez-Venegas",
            "Iv'an Gallegos",
            "Yi Zhang",
            "V'ictor Casta~neda",
            "Cristian Acevedo",
            "Dan Wu",
            "Gonzalo C'ardenas",
            "Camilo G. Sotomayor"
        ],
        "ai_categories": [
            "Diffusion",
            "AIGC"
        ],
        "tldr": "The paper introduces a novel approach using time-dependent diffusion MRI and AI for non-invasive detection of prostate cancer, aiming to improve accuracy and reduce unnecessary biopsies.",
        "tldr_zh": "本文介绍了一种新颖的方法，利用时间相关扩散MRI和人工智能技术进行无创性前列腺癌检测，旨在提高准确性并减少不必要的活检。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "High-Order Progressive Trajectory Matching for Medical Image Dataset Distillation",
        "summary": "Medical image analysis faces significant challenges in data sharing due to\nprivacy regulations and complex institutional protocols. Dataset distillation\noffers a solution to address these challenges by synthesizing compact datasets\nthat capture essential information from real, large medical datasets.\nTrajectory matching has emerged as a promising methodology for dataset\ndistillation; however, existing methods primarily focus on terminal states,\noverlooking crucial information in intermediate optimization states. We address\nthis limitation by proposing a shape-wise potential that captures the geometric\nstructure of parameter trajectories, and an easy-to-complex matching strategy\nthat progressively addresses parameters based on their complexity. Experiments\non medical image classification tasks demonstrate that our method improves\ndistillation performance while preserving privacy and maintaining model\naccuracy comparable to training on the original datasets. Our code is available\nat https://github.com/Bian-jh/HoP-TM.",
        "url": "http://arxiv.org/abs/2509.24177v1",
        "published_date": "2025-09-29T01:45:11+00:00",
        "updated_date": "2025-09-29T01:45:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Le Dong",
            "Jinghao Bian",
            "Jingyang Hou",
            "Jingliang Hu",
            "Yilei Shi",
            "Weisheng Dong",
            "Xiao Xiang Zhu",
            "Lichao Mou"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper introduces a high-order progressive trajectory matching method for distilling medical image datasets, improving distillation performance while maintaining privacy and model accuracy.",
        "tldr_zh": "本文介绍了一种用于提炼医学图像数据集的高阶渐进轨迹匹配方法，提高了提炼性能同时保持隐私和模型准确性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Clebsch-Gordan Transformer: Fast and Global Equivariant Attention",
        "summary": "The global attention mechanism is one of the keys to the success of\ntransformer architecture, but it incurs quadratic computational costs in\nrelation to the number of tokens. On the other hand, equivariant models, which\nleverage the underlying geometric structures of problem instance, often achieve\nsuperior accuracy in physical, biochemical, computer vision, and robotic tasks,\nat the cost of additional compute requirements. As a result, existing\nequivariant transformers only support low-order equivariant features and local\ncontext windows, limiting their expressiveness and performance. This work\nproposes Clebsch-Gordan Transformer, achieving efficient global attention by a\nnovel Clebsch-Gordon Convolution on $\\SO(3)$ irreducible representations. Our\nmethod enables equivariant modeling of features at all orders while achieving\n${O}(N \\log N)$ input token complexity. Additionally, the proposed method\nscales well with high-order irreducible features, by exploiting the sparsity of\nthe Clebsch-Gordon matrix. Lastly, we also incorporate optional token\npermutation equivariance through either weight sharing or data augmentation. We\nbenchmark our method on a diverse set of benchmarks including n-body\nsimulation, QM9, ModelNet point cloud classification and a robotic grasping\ndataset, showing clear gains over existing equivariant transformers in GPU\nmemory size, speed, and accuracy.",
        "url": "http://arxiv.org/abs/2509.24093v1",
        "published_date": "2025-09-28T22:09:36+00:00",
        "updated_date": "2025-09-28T22:09:36+00:00",
        "categories": [
            "cs.LG",
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Owen Lewis Howell",
            "Linfeng Zhao",
            "Xupeng Zhu",
            "Yaoyao Qian",
            "Haojie Huang",
            "Lingfeng Sun",
            "Wil Thomason",
            "Robert Platt",
            "Robin Walters"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces Clebsch-Gordan Transformer, a model that efficiently incorporates global attention and equivariant features at all orders, outperforming existing equivariant transformers in various tasks.",
        "tldr_zh": "本文介绍了Clebsch-Gordan Transformer，这是一个能够高效地结合全局注意力和各阶等变特征的模型，优于现有的等变Transformer在各种任务中的表现。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "A Second-Order Perspective on Pruning at Initialization and Knowledge Transfer",
        "summary": "The widespread availability of pre-trained vision models has enabled numerous\ndeep learning applications through their transferable representations. However,\ntheir computational and storage costs often limit practical deployment.\nPruning-at-Initialization has emerged as a promising approach to compress\nmodels before training, enabling efficient task-specific adaptation. While\nconventional wisdom suggests that effective pruning requires task-specific\ndata, this creates a challenge when downstream tasks are unknown in advance. In\nthis paper, we investigate how data influences the pruning of pre-trained\nvision models. Surprisingly, pruning on one task retains the model's zero-shot\nperformance also on unseen tasks. Furthermore, fine-tuning these pruned models\nnot only improves performance on original seen tasks but can recover held-out\ntasks' performance. We attribute this phenomenon to the favorable loss\nlandscapes induced by extensive pre-training on large-scale datasets.",
        "url": "http://arxiv.org/abs/2509.24066v1",
        "published_date": "2025-09-28T20:55:11+00:00",
        "updated_date": "2025-09-28T20:55:11+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Leonardo Iurada",
            "Beatrice Occhiena",
            "Tatiana Tommasi"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper explores how pruning pre-trained vision models at initialization can improve performance on unseen tasks, attributing this to favorable loss landscapes from extensive pre-training on large-scale datasets.",
        "tldr_zh": "本文探讨了如何在初始化时修剪预训练视觉模型，以提高在未知任务上的性能，将其归因于对大规模数据集进行广泛预训练所产生的有利损失景观。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Generalized Category Discovery in Hyperspectral Images via Prototype Subspace Modeling",
        "summary": "Generalized category discovery~(GCD) seeks to jointly identify both known and\nnovel categories in unlabeled data. While prior works have mainly focused on\nRGB images, their assumptions and modeling strategies do not generalize well to\nhyperspectral images~(HSI), which are inherently high-dimensional and exhibit\ncomplex spectral structures. In this paper, we propose the first GCD framework\ntailored for HSI, introducing a prototype subspace modeling model to better\ncapture class structure. Instead of learning a single prototype vector for each\ncategory as in existing methods such as SimGCD, we model each category using a\nset of basis vectors, forming a subspace representation that enables greater\nexpressiveness and discrimination in a high-dimensional feature space. To guide\nthe learning of such bases, we enforce two key constraints: (1) a basis\northogonality constraint that promotes inter-class separability, and (2) a\nreconstruction constraint that ensures each prototype basis can effectively\nreconstruct its corresponding class samples. Experimental results on real-world\nHSI demonstrate that our method significantly outperforms state-of-the-art GCD\nmethods, establishing a strong foundation for generalized category discovery in\nhyperspectral settings.",
        "url": "http://arxiv.org/abs/2509.24017v1",
        "published_date": "2025-09-28T18:16:59+00:00",
        "updated_date": "2025-09-28T18:16:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xianlu Li",
            "Nicolas Nadisic",
            "Shaoguang Huang",
            "Aleksandra Pizurica"
        ],
        "ai_categories": [
            "AIGC"
        ],
        "tldr": "The paper introduces a Generalized Category Discovery framework for Hyperspectral Images using prototype subspace modeling to identify both known and novel categories more effectively.",
        "tldr_zh": "该论文引入了一个针对高光谱图像的广义类别发现框架，使用原型子空间建模更有效地识别已知和新类别。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "DRCP: Diffusion on Reinforced Cooperative Perception for Perceiving Beyond Limits",
        "summary": "Cooperative perception enabled by Vehicle-to-Everything communication has\nshown great promise in enhancing situational awareness for autonomous vehicles\nand other mobile robotic platforms. Despite recent advances in perception\nbackbones and multi-agent fusion, real-world deployments remain challenged by\nhard detection cases, exemplified by partial detections and noise accumulation\nwhich limit downstream detection accuracy. This work presents Diffusion on\nReinforced Cooperative Perception (DRCP), a real-time deployable framework\ndesigned to address aforementioned issues in dynamic driving environments. DRCP\nintegrates two key components: (1) Precise-Pyramid-Cross-Modality-Cross-Agent,\na cross-modal cooperative perception module that leverages\ncamera-intrinsic-aware angular partitioning for attention-based fusion and\nadaptive convolution to better exploit external features; and (2)\nMask-Diffusion-Mask-Aggregation, a novel lightweight diffusion-based refinement\nmodule that encourages robustness against feature perturbations and aligns\nbird's-eye-view features closer to the task-optimal manifold. The proposed\nsystem achieves real-time performance on mobile platforms while significantly\nimproving robustness under challenging conditions. Code will be released in\nlate 2025.",
        "url": "http://arxiv.org/abs/2509.24903v1",
        "published_date": "2025-09-29T15:13:03+00:00",
        "updated_date": "2025-09-29T15:13:03+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Lantao Li",
            "Kang Yang",
            "Rui Song",
            "Chen Sun"
        ],
        "ai_categories": [
            "Diffusion",
            "Multimodality"
        ],
        "tldr": "DRCP is a framework that enhances cooperative perception for autonomous vehicles by addressing detection challenges in dynamic driving environments.",
        "tldr_zh": "DRCP是一个框架，通过解决动态驾驶环境中的检测挑战，提高自动驾驶车辆的协作感知能力。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "NeMo: Needle in a Montage for Video-Language Understanding",
        "summary": "Recent advances in video large language models (VideoLLMs) call for new\nevaluation protocols and benchmarks for complex temporal reasoning in\nvideo-language understanding. Inspired by the needle in a haystack test widely\nused by LLMs, we introduce a novel task of Needle in a Montage (NeMo), designed\nto assess VideoLLMs' critical reasoning capabilities, including long-context\nrecall and temporal grounding. To generate video question answering data for\nour task, we develop a scalable automated data generation pipeline that\nfacilitates high-quality data synthesis. Built upon the proposed pipeline, we\npresent NeMoBench, a video-language benchmark centered on our task.\nSpecifically, our full set of NeMoBench features 31,378 automatically generated\nquestion-answer (QA) pairs from 13,486 videos with various durations ranging\nfrom seconds to hours. Experiments demonstrate that our pipeline can reliably\nand automatically generate high-quality evaluation data, enabling NeMoBench to\nbe continuously updated with the latest videos. We evaluate 20 state-of-the-art\nmodels on our benchmark, providing extensive results and key insights into\ntheir capabilities and limitations. Our project page is available at:\nhttps://lavi-lab.github.io/NeMoBench.",
        "url": "http://arxiv.org/abs/2509.24563v1",
        "published_date": "2025-09-29T10:16:05+00:00",
        "updated_date": "2025-09-29T10:16:05+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Zi-Yuan Hu",
            "Shuo Liang",
            "Duo Zheng",
            "Yanyang Li",
            "Yeyao Tao",
            "Shijia Huang",
            "Wei Feng",
            "Jia Qin",
            "Jianguang Yu",
            "Jing Huang",
            "Meng Fang",
            "Yin Li",
            "Liwei Wang"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper introduces a novel task called NeMo to evaluate VideoLLMs' reasoning capabilities, with a focus on long-context recall and temporal grounding.",
        "tldr_zh": "本文介绍了一项名为NeMo的新任务，旨在评估VideoLLMs的推理能力，重点放在长期上下文回想和时间基础上。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "A Novel Preprocessing Unit for Effective Deep Learning based Classification and Grading of Diabetic Retinopathy",
        "summary": "Early detection of diabetic retinopathy (DR) is crucial as it allows for\ntimely intervention, preventing vision loss and enabling effective management\nof diabetic complications. This research performs detection of DR and DME at an\nearly stage through the proposed framework which includes three stages:\npreprocessing, segmentation, feature extraction, and classification. In the\npreprocessing stage, noise filtering is performed by fuzzy filtering, artefact\nremoval is performed by non-linear diffusion filtering, and the contrast\nimprovement is performed by a novel filter called Adaptive Variable Distance\nSpeckle (AVDS) filter. The AVDS filter employs four distance calculation\nmethods such as Euclidean, Bhattacharya, Manhattan, and Hamming. The filter\nadaptively chooses a distance method which produces the highest contrast value\namongst all 3 methods. From the analysis, hamming distance method was found to\nachieve better results for contrast and Euclidean distance showing less error\nvalue with high PSNR. The segmentation stage is performed using Improved\nMask-Regional Convolutional Neural Networks (Mask RCNN). In the final stage,\nfeature extraction and classification using novel Self-Spatial Attention\ninfused VGG-16 (SSA-VGG-16), which effectively captures both global contextual\nrelationships and critical spatial regions within retinal images, thereby\nimproving the accuracy and robustness of DR and DME detection and grading. The\neffectiveness of the proposed method is assessed using two distinct datasets:\nIDRiD and MESSIDOR.",
        "url": "http://arxiv.org/abs/2509.24497v1",
        "published_date": "2025-09-29T09:11:04+00:00",
        "updated_date": "2025-09-29T09:11:04+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Pranoti Nage",
            "Sanjay Shitole"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper proposes a novel preprocessing unit for the effective classification and grading of diabetic retinopathy using deep learning techniques.",
        "tldr_zh": "本文提出了一种新颖的预处理单元，利用深度学习技术有效分类和评分糖尿病视网膜病变。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "REALIGN: Regularized Procedure Alignment with Matching Video Embeddings via Partial Gromov-Wasserstein Optimal Transport",
        "summary": "Learning from procedural videos remains a core challenge in self-supervised\nrepresentation learning, as real-world instructional data often contains\nbackground segments, repeated actions, and steps presented out of order. Such\nvariability violates the strong monotonicity assumptions underlying many\nalignment methods. Prior state-of-the-art approaches, such as OPEL, leverage\nKantorovich Optimal Transport (KOT) to build frame-to-frame correspondences,\nbut rely solely on feature similarity and fail to capture the higher-order\ntemporal structure of a task. In this paper, we introduce REALIGN, a\nself-supervised framework for procedure learning based on Regularized Fused\nPartial Gromov-Wasserstein Optimal Transport (R-FPGWOT). In contrast to KOT,\nour formulation jointly models visual correspondences and temporal relations\nunder a partial alignment scheme, enabling robust handling of irrelevant\nframes, repeated actions, and non-monotonic step orders common in instructional\nvideos. To stabilize training, we integrate FPGWOT distances with\ninter-sequence contrastive learning, avoiding the need for multiple\nregularizers and preventing collapse to degenerate solutions. Across egocentric\n(EgoProceL) and third-person (ProceL, CrossTask) benchmarks, REALIGN achieves\nup to 18.9% average F1-score improvements and over 30% temporal IoU gains,\nwhile producing more interpretable transport maps that preserve key-step\norderings and filter out noise.",
        "url": "http://arxiv.org/abs/2509.24382v1",
        "published_date": "2025-09-29T07:32:14+00:00",
        "updated_date": "2025-09-29T07:32:14+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Soumyadeep Chandra",
            "Kaushik Roy"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a self-supervised framework called REALIGN for learning procedures from videos. It utilizes Partial Gromov-Wasserstein Optimal Transport to align visual correspondences and temporal relations, improving handling of complex instructional video data.",
        "tldr_zh": "本文介绍了一种名为REALIGN的自监督框架，用于从视频中学习程序。它利用部分Gromov-Wasserstein优化传输来对齐视觉对应和时间关系，改善处理复杂的教学视频数据。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 7.25
    },
    {
        "title": "OMeGa: Joint Optimization of Explicit Meshes and Gaussian Splats for Robust Scene-Level Surface Reconstruction",
        "summary": "Neural rendering with Gaussian splatting has advanced novel view synthesis,\nand most methods reconstruct surfaces via post-hoc mesh extraction. However,\nexisting methods suffer from two limitations: (i) inaccurate geometry in\ntexture-less indoor regions, and (ii) the decoupling of mesh extraction from\noptimization, thereby missing the opportunity to leverage mesh geometry to\nguide splat optimization. In this paper, we present OMeGa, an end-to-end\nframework that jointly optimizes an explicit triangle mesh and 2D Gaussian\nsplats via a flexible binding strategy, where spatial attributes of Gaussian\nSplats are expressed in the mesh frame and texture attributes are retained on\nsplats. To further improve reconstruction accuracy, we integrate mesh\nconstraints and monocular normal supervision into the optimization, thereby\nregularizing geometry learning. In addition, we propose a heuristic, iterative\nmesh-refinement strategy that splits high-error faces and prunes unreliable\nones to further improve the detail and accuracy of the reconstructed mesh.\nOMeGa achieves state-of-the-art performance on challenging indoor\nreconstruction benchmarks, reducing Chamfer-$L_1$ by 47.3\\% over the 2DGS\nbaseline while maintaining competitive novel-view rendering quality. The\nexperimental results demonstrate that OMeGa effectively addresses prior\nlimitations in indoor texture-less reconstruction.",
        "url": "http://arxiv.org/abs/2509.24308v1",
        "published_date": "2025-09-29T05:43:40+00:00",
        "updated_date": "2025-09-29T05:43:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuhang Cao",
            "Haojun Yan",
            "Danya Yao"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "OMeGa proposes a novel framework that optimizes explicit meshes and Gaussian splats jointly for robust scene-level surface reconstruction, achieving state-of-the-art performance on indoor reconstruction benchmarks.",
        "tldr_zh": "OMeGa提出了一个新颖的框架，通过联合优化显式网格和高斯斑点来实现稳健的场景级表面重建，在室内重建基准上取得了最先进的性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Skeleton-based Robust Registration Framework for Corrupted 3D Point Clouds",
        "summary": "Point cloud registration is fundamental in 3D vision applications, including\nautonomous driving, robotics, and medical imaging, where precise alignment of\nmultiple point clouds is essential for accurate environment reconstruction.\nHowever, real-world point clouds are often affected by sensor limitations,\nenvironmental noise, and preprocessing errors, making registration challenging\ndue to density distortions, noise contamination, and geometric deformations.\nExisting registration methods rely on direct point matching or surface feature\nextraction, which are highly susceptible to these corruptions and lead to\nreduced alignment accuracy. To address these challenges, a skeleton-based\nrobust registration framework is presented, which introduces a\ncorruption-resilient skeletal representation to improve registration robustness\nand accuracy. The framework integrates skeletal structures into the\nregistration process and combines the transformations obtained from both the\ncorrupted point cloud alignment and its skeleton alignment to achieve optimal\nregistration. In addition, a distribution distance loss function is designed to\nenforce the consistency between the source and target skeletons, which\nsignificantly improves the registration performance. This framework ensures\nthat the alignment considers both the original local geometric features and the\nglobal stability of the skeleton structure, resulting in robust and accurate\nregistration results. Experimental evaluations on diverse corrupted datasets\ndemonstrate that SRRF consistently outperforms state-of-the-art registration\nmethods across various corruption scenarios, including density distortions,\nnoise contamination, and geometric deformations. The results confirm the\nrobustness of SRRF in handling corrupted point clouds, making it a potential\napproach for 3D perception tasks in real-world scenarios.",
        "url": "http://arxiv.org/abs/2509.24273v1",
        "published_date": "2025-09-29T04:31:47+00:00",
        "updated_date": "2025-09-29T04:31:47+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yongqiang Wang",
            "Weigang Li",
            "Wenping Liu",
            "Zhiqiang Tian",
            "Jinling Li"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper presents a robust registration framework for handling corrupted 3D point clouds by incorporating skeleton-based representations for improved accuracy.",
        "tldr_zh": "该论文提出了一个鲁棒的注册框架，通过引入基于骨架的表示，以处理受损3D点云，从而提高了准确性。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "EYE-DEX: Eye Disease Detection and EXplanation System",
        "summary": "Retinal disease diagnosis is critical in preventing vision loss and reducing\nsocioeconomic burdens. Globally, over 2.2 billion people are affected by some\nform of vision impairment, resulting in annual productivity losses estimated at\n$411 billion. Traditional manual grading of retinal fundus images by\nophthalmologists is time-consuming and subjective. In contrast, deep learning\nhas revolutionized medical diagnostics by automating retinal image analysis and\nachieving expert-level performance. In this study, we present EYE-DEX, an\nautomated framework for classifying 10 retinal conditions using the large-scale\nRetinal Disease Dataset comprising 21,577 eye fundus images. We benchmark three\npre-trained Convolutional Neural Network (CNN) models--VGG16, VGG19, and\nResNet50--with our finetuned VGG16 achieving a state-of-the-art global\nbenchmark test accuracy of 92.36%. To enhance transparency and explainability,\nwe integrate the Gradient-weighted Class Activation Mapping (Grad-CAM)\ntechnique to generate visual explanations highlighting disease-specific\nregions, thereby fostering clinician trust and reliability in AI-assisted\ndiagnostics.",
        "url": "http://arxiv.org/abs/2509.24136v1",
        "published_date": "2025-09-29T00:10:02+00:00",
        "updated_date": "2025-09-29T00:10:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "60G35, 62M10, 62P35, 65C20, 68T45, 68U10, 92C35, 92C40, 92C42, 93E10",
            "I.4; I.4.8; I.4.9; I.4.10; I.2; I.2.6; I.2.10; J.3; C.2.4; C.3;\n  H.2.8; H.3.4; H.3.5; I.2.4; I.5; I.5.1; I.5.4; K.6.1"
        ],
        "authors": [
            "Youssef Sabiri",
            "Walid Houmaidi",
            "Amine Abouaomar"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "EYE-DEX is an automated framework for classifying 10 retinal conditions using deep learning, achieving high accuracy. It also provides visual explanations for disease-specific regions to enhance transparency and trust in AI-assisted diagnostics.",
        "tldr_zh": "EYE-DEX是一个利用深度学习对10种视网膜疾病进行分类的自动化框架，达到了很高的准确性。它还提供了针对疾病特定区域的可视化解释，以增强在AI辅助诊断中的透明度和信任。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Fast Real-Time Pipeline for Robust Arm Gesture Recognition",
        "summary": "This paper presents a real-time pipeline for dynamic arm gesture recognition\nbased on OpenPose keypoint estimation, keypoint normalization, and a recurrent\nneural network classifier. The 1 x 1 normalization scheme and two feature\nrepresentations (coordinate- and angle-based) are presented for the pipeline.\nIn addition, an efficient method to improve robustness against camera angle\nvariations is also introduced by using artificially rotated training data.\nExperiments on a custom traffic-control gesture dataset demonstrate high\naccuracy across varying viewing angles and speeds. Finally, an approach to\ncalculate the speed of the arm signal (if necessary) is also presented.",
        "url": "http://arxiv.org/abs/2509.25042v1",
        "published_date": "2025-09-29T16:57:56+00:00",
        "updated_date": "2025-09-29T16:57:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Milán Zsolt Bagladi",
            "László Gulyás",
            "Gergő Szalay"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper presents a real-time pipeline for arm gesture recognition using OpenPose keypoint estimation and recurrent neural network, achieving high accuracy even with varying camera angles.",
        "tldr_zh": "本文提出了一种实时流水线，用于基于OpenPose关键点估计和循环神经网络进行手势识别，在不同摄像头角度下也能实现高准确度。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    },
    {
        "title": "CLASP: Adaptive Spectral Clustering for Unsupervised Per-Image Segmentation",
        "summary": "We introduce CLASP (Clustering via Adaptive Spectral Processing), a\nlightweight framework for unsupervised image segmentation that operates without\nany labeled data or finetuning. CLASP first extracts per patch features using a\nself supervised ViT encoder (DINO); then, it builds an affinity matrix and\napplies spectral clustering. To avoid manual tuning, we select the segment\ncount automatically with a eigengap silhouette search, and we sharpen the\nboundaries with a fully connected DenseCRF. Despite its simplicity and training\nfree nature, CLASP attains competitive mIoU and pixel accuracy on COCO Stuff\nand ADE20K, matching recent unsupervised baselines. The zero training design\nmakes CLASP a strong, easily reproducible baseline for large unannotated\ncorpora especially common in digital advertising and marketing workflows such\nas brand safety screening, creative asset curation, and social media content\nmoderation",
        "url": "http://arxiv.org/abs/2509.25016v1",
        "published_date": "2025-09-29T16:41:30+00:00",
        "updated_date": "2025-09-29T16:41:30+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Max Curie",
            "Paulo da Costa"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "CLASP is a lightweight framework for unsupervised image segmentation using self-supervised features, spectral clustering, and automatic segment count selection. It achieves competitive results without any training on datasets like COCO Stuff and ADE20K.",
        "tldr_zh": "CLASP是一个轻量级的框架，用于无监督图像分割，利用自监督特征，谱聚类和自动分割计数选择。它在像COCO Stuff和ADE20K这样的数据集上取得竞争性结果，而无需任何训练。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Score-based Membership Inference on Diffusion Models",
        "summary": "Membership inference attacks (MIAs) against diffusion models have emerged as\na pressing privacy concern, as these models may inadvertently reveal whether a\ngiven sample was part of their training set. We present a theoretical and\nempirical study of score-based MIAs, focusing on the predicted noise vectors\nthat diffusion models learn to approximate. We show that the expected denoiser\noutput points toward a kernel-weighted local mean of nearby training samples,\nsuch that its norm encodes proximity to the training set and thereby reveals\nmembership. Building on this observation, we propose SimA, a single-query\nattack that provides a principled, efficient alternative to existing\nmulti-query methods. SimA achieves consistently strong performance across\nvariants of DDPM, Latent Diffusion Model (LDM). Notably, we find that Latent\nDiffusion Models are surprisingly less vulnerable than pixel-space models, due\nto the strong information bottleneck imposed by their latent auto-encoder. We\nfurther investigate this by differing the regularization hyperparameters\n($\\beta$ in $\\beta$-VAE) in latent channel and suggest a strategy to make LDM\ntraining more robust to MIA. Our results solidify the theory of score-based\nMIAs, while highlighting that Latent Diffusion class of methods requires better\nunderstanding of inversion for VAE, and not simply inversion of the Diffusion\nprocess",
        "url": "http://arxiv.org/abs/2509.25003v1",
        "published_date": "2025-09-29T16:28:55+00:00",
        "updated_date": "2025-09-29T16:28:55+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Mingxing Rao",
            "Bowen Qu",
            "Daniel Moyer"
        ],
        "ai_categories": [
            "Diffusion"
        ],
        "tldr": "The paper discusses membership inference attacks on diffusion models, introducing a new attack method called SimA that performs well on different models. It also highlights the vulnerability of pixel-space models compared to Latent Diffusion Models.",
        "tldr_zh": "本文讨论了对扩散模型的成员推断攻击，引入了一种名为SimA的新攻击方法，在不同模型上表现出色。此外，它强调了像素空间模型相对于潜在扩散模型的脆弱性。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Perceive, Reflect and Understand Long Video: Progressive Multi-Granular Clue Exploration with Interactive Agents",
        "summary": "Long videos, characterized by temporal complexity and sparse task-relevant\ninformation, pose significant reasoning challenges for AI systems. Although\nvarious Large Language Model (LLM)-based approaches have advanced long video\nunderstanding, they still struggle to achieve both completeness and efficiency\nin capturing task-critical information. Inspired by human progressive visual\ncognition, we propose CogniGPT, a framework that leverages an interactive loop\nbetween Multi-Granular Perception Agent (MGPA) and Verification-Enhanced\nReflection Agent (VERA) for efficient and reliable long video understanding.\nSpecifically, MGPA mimics human visual divergent and focused attention to\ncapture task-related information, while VERA verifies perceived key clues to\nmitigate hallucination and optimize subsequent perception strategies. Through\nthis interactive process, CogniGPT explores a minimal set of informative and\nreliable task-related clues. Extensive experiments on EgoSchema, Video-MME,\nNExT-QA, and MovieChat datasets demonstrate CogniGPT's superiority in both\naccuracy and efficiency. Notably, on EgoSchema, it surpasses existing\ntraining-free methods using only 11.2 frames and achieves performance\ncomparable to Gemini 1.5-Pro.",
        "url": "http://arxiv.org/abs/2509.24943v1",
        "published_date": "2025-09-29T15:42:55+00:00",
        "updated_date": "2025-09-29T15:42:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahua Li",
            "Kun Wei",
            "Zhe Xu",
            "Zibo Su",
            "Xu Yang",
            "Cheng Deng"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces CogniGPT, a framework using interactive agents to efficiently understand long videos, outperforming existing methods in accuracy and efficiency.",
        "tldr_zh": "该论文介绍了CogniGPT，一种利用互动代理来高效理解长视频的框架，优于现有方法的准确性和效率。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Learning Goal-Oriented Language-Guided Navigation with Self-Improving Demonstrations at Scale",
        "summary": "Goal-oriented language-guided navigation requires robust exploration\ncapabilities for agents to navigate to specified goals in unknown environments\nwithout step-by-step instructions. Existing methods tend to exclusively utilize\nshortest-path trajectories, lacking effective exploration priors for training\nnavigation agents. To address the above challenges, we present SID, a\ngoal-oriented language-guided navigation learning approach with Self-Improving\nDemonstrations. Specifically, SID learns an initial agent on the shortest-path\ndata sampled from environments and then leverages this agent to generate novel\nexploration trajectories. The novel rollouts provide demonstrations with\nstronger exploration strategies to train a better agent, which in turn produces\nhigher-quality agent demonstrations for the next round of training. We show\nthat this iterative self-improving pipeline readily scales to new environments,\nand the resulting demonstrations can be transferred across a variety of\nlanguage-guided navigation tasks, elevating the performance ceiling in diverse\ngoal-oriented navigation tasks. Extensive experiments demonstrate that SID\nsignificantly boosts the exploration capabilities and generalization of\nnavigation agents. The resulting agent achieves new state-of-the-art\nperformance on goal-oriented language-guided navigation tasks, including\nREVERIE, SOON, notably achieving a 50.9% success rate on the unseen validation\nsplits of SOON, surpassing the prior leading approaches by a margin of 13.9%.",
        "url": "http://arxiv.org/abs/2509.24910v1",
        "published_date": "2025-09-29T15:15:54+00:00",
        "updated_date": "2025-09-29T15:15:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Songze Li",
            "Zun Wang",
            "Gengze Zhou",
            "Jialu Li",
            "Xiangyu Zeng",
            "Limin Wang",
            "Yu Qiao",
            "Qi Wu",
            "Mohit Bansal",
            "Yi Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper presents a learning approach for goal-oriented language-guided navigation with self-improving demonstrations, achieving state-of-the-art performance on various tasks.",
        "tldr_zh": "本文提出了一种具有自我改进示范的目标导向语言引导导航的学习方法，在各种任务中取得了最新的表现。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Accurate Cobb Angle Estimation via SVD-Based Curve Detection and Vertebral Wedging Quantification",
        "summary": "Adolescent idiopathic scoliosis (AIS) is a common spinal deformity affecting\napproximately 2.2% of boys and 4.8% of girls worldwide. The Cobb angle serves\nas the gold standard for AIS severity assessment, yet traditional manual\nmeasurements suffer from significant observer variability, compromising\ndiagnostic accuracy. Despite prior automation attempts, existing methods use\nsimplified spinal models and predetermined curve patterns that fail to address\nclinical complexity. We present a novel deep learning framework for AIS\nassessment that simultaneously predicts both superior and inferior endplate\nangles with corresponding midpoint coordinates for each vertebra, preserving\nthe anatomical reality of vertebral wedging in progressive AIS. Our approach\ncombines an HRNet backbone with Swin-Transformer modules and biomechanically\ninformed constraints for enhanced feature extraction. We employ Singular Value\nDecomposition (SVD) to analyze angle predictions directly from vertebral\nmorphology, enabling flexible detection of diverse scoliosis patterns without\npredefined curve assumptions. Using 630 full-spine anteroposterior radiographs\nfrom patients aged 10-18 years with rigorous dual-rater annotation, our method\nachieved 83.45% diagnostic accuracy and 2.55{\\deg} mean absolute error. The\nframework demonstrates exceptional generalization capability on\nout-of-distribution cases. Additionally, we introduce the Vertebral Wedging\nIndex (VWI), a novel metric quantifying vertebral deformation. Longitudinal\nanalysis revealed VWI's significant prognostic correlation with curve\nprogression while traditional Cobb angles showed no correlation, providing\nrobust support for early AIS detection, personalized treatment planning, and\nprogression monitoring.",
        "url": "http://arxiv.org/abs/2509.24898v1",
        "published_date": "2025-09-29T15:07:55+00:00",
        "updated_date": "2025-09-29T15:07:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chang Shi",
            "Nan Meng",
            "Yipeng Zhuang",
            "Moxin Zhao",
            "Jason Pui Yin Cheung",
            "Hua Huang",
            "Xiuyuan Chen",
            "Cong Nie",
            "Wenting Zhong",
            "Guiqiang Jiang",
            "Yuxin Wei",
            "Jacob Hong Man Yu",
            "Si Chen",
            "Xiaowen Ou",
            "Teng Zhang"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes a deep learning framework for accurate measurement of scoliosis severity in adolescents, introducing a novel metric for vertebral deformation prediction and prognosis.",
        "tldr_zh": "本文提出了一个深度学习框架，用于准确测量青少年脊柱侧弯症的严重程度，引入了一种预测和预后椎体变形的新指标。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Vision At Night: Exploring Biologically Inspired Preprocessing For Improved Robustness Via Color And Contrast Transformations",
        "summary": "Inspired by the human visual system's mechanisms for contrast enhancement and\ncolor-opponency, we explore biologically motivated input preprocessing for\nrobust semantic segmentation. By applying Difference-of-Gaussians (DoG)\nfiltering to RGB, grayscale, and opponent-color channels, we enhance local\ncontrast without modifying model architecture or training. Evaluations on\nCityscapes, ACDC, and Dark Zurich show that such preprocessing maintains\nin-distribution performance while improving robustness to adverse conditions\nlike night, fog, and snow. As this processing is model-agnostic and\nlightweight, it holds potential for integration into imaging pipelines,\nenabling imaging systems to deliver task-ready, robust inputs for downstream\nvision models in safety-critical environments.",
        "url": "http://arxiv.org/abs/2509.24863v1",
        "published_date": "2025-09-29T14:48:32+00:00",
        "updated_date": "2025-09-29T14:48:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lorena Stracke",
            "Lia Nimmermann",
            "Shashank Agnihotri",
            "Margret Keuper",
            "Volker Blanz"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper explores biologically inspired preprocessing techniques for improved robustness in semantic segmentation under adverse conditions like night, fog, and snow.",
        "tldr_zh": "本文探讨了受生物启发的预处理技术，用于在夜晚、雾和雪等恶劣条件下改善语义分割的稳健性。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "ELPG-DTFS: Prior-Guided Adaptive Time-Frequency Graph Neural Network for EEG Depression Diagnosis",
        "summary": "Timely and objective screening of major depressive disorder (MDD) is vital,\nyet diagnosis still relies on subjective scales. Electroencephalography (EEG)\nprovides a low-cost biomarker, but existing deep models treat spectra as static\nimages, fix inter-channel graphs, and ignore prior knowledge, limiting accuracy\nand interpretability. We propose ELPG-DTFS, a prior-guided adaptive\ntime-frequency graph neural network that introduces: (1) channel-band attention\nwith cross-band mutual information, (2) a learnable adjacency matrix for\ndynamic functional links, and (3) a residual knowledge-graph pathway injecting\nneuroscience priors. On the 128-channel MODMA dataset (53 subjects), ELPG-DTFS\nachieves 97.63% accuracy and 97.33% F1, surpassing the 2025 state-of-the-art\nACM-GNN. Ablation shows that removing any module lowers F1 by up to 4.35,\nconfirming their complementary value. ELPG-DTFS thus offers a robust and\ninterpretable framework for next-generation EEG-based MDD diagnostics.",
        "url": "http://arxiv.org/abs/2509.24860v1",
        "published_date": "2025-09-29T14:44:37+00:00",
        "updated_date": "2025-09-29T14:44:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingru Qiu",
            "Jiale Liang",
            "Xuanhan Fan",
            "Mingda Zhang",
            "Zhenli He"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "ELPG-DTFS is a new method using EEG for depression diagnosis, achieving high accuracy and F1 scores, surpassing the state-of-the-art ACM-GNN model.",
        "tldr_zh": "ELPG-DTFS是一种使用EEG进行抑郁症诊断的新方法，取得了高准确度和F1分数，超越了现有的ACM-GNN模型。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Traumatic Brain Injury Segmentation using an Ensemble of Encoder-decoder Models",
        "summary": "The identification and segmentation of moderate-severe traumatic brain injury\n(TBI) lesions pose a significant challenge in neuroimaging. This difficulty\narises from the extreme heterogeneity of these lesions, which vary in size,\nnumber, and laterality, thereby complicating downstream image processing tasks\nsuch as image registration and brain parcellation, reducing the analytical\naccuracy. Thus, developing methods for highly accurate segmentation of TBI\nlesions is essential for reliable neuroimaging analysis. This study aims to\ndevelop an effective automated segmentation pipeline to automatically detect\nand segment TBI lesions in T1-weighted MRI scans. We evaluate multiple\napproaches to achieve accurate segmentation of the TBI lesions. The core of our\npipeline leverages various architectures within the nnUNet framework for\ninitial segmentation, complemented by post-processing strategies to enhance\nevaluation metrics. Our final submission to the challenge achieved an accuracy\nof 0.8451, Dice score values of 0.4711 and 0.8514 for images with and without\nvisible lesions, respectively, with an overall Dice score of 0.5973, ranking\namong the top-6 methods in the AIMS-TBI 2025 challenge. The Python\nimplementation of our pipeline is publicly available.",
        "url": "http://arxiv.org/abs/2509.24684v1",
        "published_date": "2025-09-29T12:21:32+00:00",
        "updated_date": "2025-09-29T12:21:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ghanshyam Dhamat",
            "Vaanathi Sundaresan"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper presents an automated segmentation pipeline for detecting and segmenting traumatic brain injury lesions in MRI scans, achieving high accuracy.",
        "tldr_zh": "该论文提出了一种自动分割管道，用于在MRI扫描中检测和分割创伤性脑损伤病灶，实现高准确度。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Classifier-Centric Adaptive Framework for Open-Vocabulary Camouflaged Object Segmentation",
        "summary": "Open-vocabulary camouflaged object segmentation requires models to segment\ncamouflaged objects of arbitrary categories unseen during training, placing\nextremely high demands on generalization capabilities. Through analysis of\nexisting methods, it is observed that the classification component\nsignificantly affects overall segmentation performance. Accordingly, a\nclassifier-centric adaptive framework is proposed to enhance segmentation\nperformance by improving the classification component via a lightweight text\nadapter with a novel layered asymmetric initialization. Through the\nclassification enhancement, the proposed method achieves substantial\nimprovements in segmentation metrics compared to the OVCoser baseline on the\nOVCamo benchmark: cIoU increases from 0.443 to 0.493, cSm from 0.579 to 0.658,\nand cMAE reduces from 0.336 to 0.239. These results demonstrate that targeted\nclassification enhancement provides an effective approach for advancing\ncamouflaged object segmentation performance.",
        "url": "http://arxiv.org/abs/2509.24681v1",
        "published_date": "2025-09-29T12:16:28+00:00",
        "updated_date": "2025-09-29T12:16:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hanyu Zhang",
            "Yiming Zhou",
            "Jinxia Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a classifier-centric adaptive framework to improve camouflaged object segmentation performance by enhancing the classification component, leading to significant improvements in segmentation metrics.",
        "tldr_zh": "本文提出了一个分类器中心的自适应框架，通过改进分类组件来提高伪装物体分割性能，从而显着提高分割指标。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CEDex: Cross-Embodiment Dexterous Grasp Generation at Scale from Human-like Contact Representations",
        "summary": "Cross-embodiment dexterous grasp synthesis refers to adaptively generating\nand optimizing grasps for various robotic hands with different morphologies.\nThis capability is crucial for achieving versatile robotic manipulation in\ndiverse environments and requires substantial amounts of reliable and diverse\ngrasp data for effective model training and robust generalization. However,\nexisting approaches either rely on physics-based optimization that lacks\nhuman-like kinematic understanding or require extensive manual data collection\nprocesses that are limited to anthropomorphic structures. In this paper, we\npropose CEDex, a novel cross-embodiment dexterous grasp synthesis method at\nscale that bridges human grasping kinematics and robot kinematics by aligning\nrobot kinematic models with generated human-like contact representations. Given\nan object's point cloud and an arbitrary robotic hand model, CEDex first\ngenerates human-like contact representations using a Conditional Variational\nAuto-encoder pretrained on human contact data. It then performs kinematic human\ncontact alignment through topological merging to consolidate multiple human\nhand parts into unified robot components, followed by a signed distance\nfield-based grasp optimization with physics-aware constraints. Using CEDex, we\nconstruct the largest cross-embodiment grasp dataset to date, comprising 500K\nobjects across four gripper types with 20M total grasps. Extensive experiments\nshow that CEDex outperforms state-of-the-art approaches and our dataset\nbenefits cross-embodiment grasp learning with high-quality diverse grasps.",
        "url": "http://arxiv.org/abs/2509.24661v1",
        "published_date": "2025-09-29T12:08:04+00:00",
        "updated_date": "2025-09-29T12:08:04+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Zhiyuan Wu",
            "Rolandos Alexandros Potamias",
            "Xuyang Zhang",
            "Zhongqun Zhang",
            "Jiankang Deng",
            "Shan Luo"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces CEDex, a method for generating dexterous grasps for robotic hands by aligning human-like contact representations with robot kinematic models, outperforming existing approaches.",
        "tldr_zh": "该论文引入了CEDex方法，通过将人类的接触表示与机器人运动模型对齐，生成机器人手的灵巧抓取，优于现有方法。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Foggy Crowd Counting: Combining Physical Priors and KAN-Graph",
        "summary": "Aiming at the key challenges of crowd counting in foggy environments, such as\nlong-range target blurring, local feature degradation, and image contrast\nattenuation, this paper proposes a crowd-counting method with a physical a\npriori of atmospheric scattering, which improves crowd counting accuracy under\ncomplex meteorological conditions through the synergistic optimization of the\nphysical mechanism and data-driven.Specifically, first, the method introduces a\ndifferentiable atmospheric scattering model and employs transmittance dynamic\nestimation and scattering parameter adaptive calibration techniques to\naccurately quantify the nonlinear attenuation laws of haze on targets with\ndifferent depths of field.Secondly, the MSA-KAN was designed based on the\nKolmogorov-Arnold Representation Theorem to construct a learnable edge\nactivation function. By integrating a multi-layer progressive architecture with\nadaptive skip connections, it significantly enhances the model's nonlinear\nrepresentation capability in feature-degraded regions, effectively suppressing\nfeature confusion under fog interference.Finally, we further propose a\nweather-aware GCN that dynamically constructs spatial adjacency matrices using\ndeep features extracted by MSA-KAN. Experiments on four public datasets\ndemonstrate that our method achieves a 12.2\\%-27.5\\% reduction in MAE metrics\ncompared to mainstream algorithms in dense fog scenarios.",
        "url": "http://arxiv.org/abs/2509.24545v1",
        "published_date": "2025-09-29T09:59:36+00:00",
        "updated_date": "2025-09-29T09:59:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuhao Wang",
            "Zhuoran Zheng",
            "Han Hu",
            "Dianjie Lu",
            "Guijuan Zhang",
            "Chen Lyu"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper presents a crowd counting method for foggy environments by incorporating atmospheric scattering knowledge and a learnable edge activation function, achieving significant improvement in accuracy under complex meteorological conditions.",
        "tldr_zh": "本文提出了一种在雾天环境中进行人群计数的方法，该方法结合大气散射知识和可学习的边缘激活函数，在复杂气象条件下显着提高了准确性。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "CORE-3D: Context-aware Open-vocabulary Retrieval by Embeddings in 3D",
        "summary": "3D scene understanding is fundamental for embodied AI and robotics,\nsupporting reliable perception for interaction and navigation. Recent\napproaches achieve zero-shot, open-vocabulary 3D semantic mapping by assigning\nembedding vectors to 2D class-agnostic masks generated via vision-language\nmodels (VLMs) and projecting these into 3D. However, these methods often\nproduce fragmented masks and inaccurate semantic assignments due to the direct\nuse of raw masks, limiting their effectiveness in complex environments. To\naddress this, we leverage SemanticSAM with progressive granularity refinement\nto generate more accurate and numerous object-level masks, mitigating the\nover-segmentation commonly observed in mask generation models such as vanilla\nSAM, and improving downstream 3D semantic segmentation. To further enhance\nsemantic context, we employ a context-aware CLIP encoding strategy that\nintegrates multiple contextual views of each mask using empirically determined\nweighting, providing much richer visual context. We evaluate our approach on\nmultiple 3D scene understanding tasks, including 3D semantic segmentation and\nobject retrieval from language queries, across several benchmark datasets.\nExperimental results demonstrate significant improvements over existing\nmethods, highlighting the effectiveness of our approach.",
        "url": "http://arxiv.org/abs/2509.24528v1",
        "published_date": "2025-09-29T09:43:00+00:00",
        "updated_date": "2025-09-29T09:43:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mohamad Amin Mirzaei",
            "Pantea Amoie",
            "Ali Ekhterachian",
            "Matin Mirzababaei"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a method for generating more accurate 3D semantic mappings by refining object-level masks and integrating contextual views of each mask.",
        "tldr_zh": "本文提出了一种方法，通过细化物体级别的遮罩并整合每个遮罩的上下文视图，生成更准确的3D语义映射。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Mask Clustering-based Annotation Engine for Large-Scale Submeter Land Cover Mapping",
        "summary": "Recent advances in remote sensing technology have made submeter resolution\nimagery increasingly accessible, offering remarkable detail for fine-grained\nland cover analysis. However, its full potential remains underutilized -\nparticularly for large-scale land cover mapping - due to the lack of\nsufficient, high-quality annotated datasets. Existing labels are typically\nderived from pre-existing products or manual annotation, which are often\nunreliable or prohibitively expensive, particularly given the rich visual\ndetail and massive data volumes of submeter imagery. Inspired by the spatial\nautocorrelation principle, which suggests that objects of the same class tend\nto co-occur with similar visual features in local neighborhoods, we propose the\nMask Clustering-based Annotation Engine (MCAE), which treats semantically\nconsistent mask groups as the minimal annotating units to enable efficient,\nsimultaneous annotation of multiple instances. It significantly improves\nannotation efficiency by one to two orders of magnitude, while preserving label\nquality, semantic diversity, and spatial representativeness. With MCAE, we\nbuild a high-quality annotated dataset of about 14 billion labeled pixels,\nreferred to as HiCity-LC, which supports the generation of city-scale land\ncover maps across five major Chinese cities with classification accuracies\nabove 85%. It is the first publicly available submeter resolution city-level\nland cover benchmark, highlighting the scalability and practical utility of\nMCAE for large-scale, submeter resolution mapping. The dataset is available at\nhttps://github.com/chenhaocs/MCAE",
        "url": "http://arxiv.org/abs/2509.24374v1",
        "published_date": "2025-09-29T07:21:10+00:00",
        "updated_date": "2025-09-29T07:21:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hao Chen",
            "Fang Xu",
            "Tamer Saleh",
            "Weifeng Hao",
            "Gui-Song Xia"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces the Mask Clustering-based Annotation Engine (MCAE) for efficient annotation in large-scale land cover mapping using high-resolution imagery.",
        "tldr_zh": "本文介绍了基于掩膜聚类的注释引擎(MCAE)，用于利用高分辨率图像进行大规模土地覆盖映射的有效注释。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "DINOReg: Strong Point Cloud Registration with Vision Foundation Model",
        "summary": "Point cloud registration is a fundamental task in 3D computer vision. Most\nexisting methods rely solely on geometric information for feature extraction\nand matching. Recently, several studies have incorporated color information\nfrom RGB-D data into feature extraction. Although these methods achieve\nremarkable improvements, they have not fully exploited the abundant texture and\nsemantic information in images, and the feature fusion is performed in an\nimage-lossy manner, which limit their performance. In this paper, we propose\nDINOReg, a registration network that sufficiently utilizes both visual and\ngeometric information to solve the point cloud registration problem. Inspired\nby advances in vision foundation models, we employ DINOv2 to extract\ninformative visual features from images, and fuse visual and geometric features\nat the patch level. This design effectively combines the rich texture and\nglobal semantic information extracted by DINOv2 with the detailed geometric\nstructure information captured by the geometric backbone. Additionally, a mixed\npositional embedding is proposed to encode positional information from both\nimage space and point cloud space, which enhances the model's ability to\nperceive spatial relationships between patches. Extensive experiments on the\nRGBD-3DMatch and RGBD-3DLoMatch datasets demonstrate that our method achieves\nsignificant improvements over state-of-the-art geometry-only and multi-modal\nregistration methods, with a 14.2% increase in patch inlier ratio and a 15.7%\nincrease in registration recall. The code is publicly available at\nhttps://github.com/ccjccjccj/DINOReg.",
        "url": "http://arxiv.org/abs/2509.24370v1",
        "published_date": "2025-09-29T07:15:47+00:00",
        "updated_date": "2025-09-29T07:15:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Congjia Chen",
            "Yufu Qu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "DINOReg is a new method for point cloud registration that combines visual and geometric information effectively, achieving significant improvements over existing methods.",
        "tldr_zh": "DINOReg是一种新的点云配准方法，有效地结合了视觉和几何信息，实现了明显的改进。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "PROFusion: Robust and Accurate Dense Reconstruction via Camera Pose Regression and Optimization",
        "summary": "Real-time dense scene reconstruction during unstable camera motions is\ncrucial for robotics, yet current RGB-D SLAM systems fail when cameras\nexperience large viewpoint changes, fast motions, or sudden shaking. Classical\noptimization-based methods deliver high accuracy but fail with poor\ninitialization during large motions, while learning-based approaches provide\nrobustness but lack sufficient accuracy for dense reconstruction. We address\nthis challenge through a combination of learning-based initialization with\noptimization-based refinement. Our method employs a camera pose regression\nnetwork to predict metric-aware relative poses from consecutive RGB-D frames,\nwhich serve as reliable starting points for a randomized optimization algorithm\nthat further aligns depth images with the scene geometry. Extensive experiments\ndemonstrate promising results: our approach outperforms the best competitor on\nchallenging benchmarks, while maintaining comparable accuracy on stable motion\nsequences. The system operates in real-time, showcasing that combining simple\nand principled techniques can achieve both robustness for unstable motions and\naccuracy for dense reconstruction. Project page:\nhttps://github.com/siyandong/PROFusion.",
        "url": "http://arxiv.org/abs/2509.24236v1",
        "published_date": "2025-09-29T03:20:49+00:00",
        "updated_date": "2025-09-29T03:20:49+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Siyan Dong",
            "Zijun Wang",
            "Lulu Cai",
            "Yi Ma",
            "Yanchao Yang"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper presents a method for dense scene reconstruction in real-time using a combination of learning-based initialization and optimization-based refinement.",
        "tldr_zh": "本文提出了一种使用基于学习的初始化和基于优化的细化相结合的方法，实时进行密集场景重建。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Scalable Audio-Visual Masked Autoencoders for Efficient Affective Video Facial Analysis",
        "summary": "Affective video facial analysis (AVFA) has emerged as a key research field\nfor building emotion-aware intelligent systems, yet this field continues to\nsuffer from limited data availability. In recent years, the self-supervised\nlearning (SSL) technique of Masked Autoencoders (MAE) has gained momentum, with\ngrowing adaptations in its audio-visual contexts. While scaling has proven\nessential for breakthroughs in general multi-modal learning domains, its\nspecific impact on AVFA remains largely unexplored. Another core challenge in\nthis field is capturing both intra- and inter-modal correlations through\nscalable audio-visual representations. To tackle these issues, we propose\nAVF-MAE++, a family of audio-visual MAE models designed to efficiently\ninvestigate the scaling properties in AVFA while enhancing cross-modal\ncorrelation modeling. Our framework introduces a novel dual masking strategy\nacross audio and visual modalities and strengthens modality encoders with a\nmore holistic design to better support scalable pre-training. Additionally, we\npresent the Iterative Audio-Visual Correlation Learning Module, which improves\ncorrelation learning within the SSL paradigm, bridging the limitations of\nprevious methods. To support smooth adaptation and reduce overfitting risks, we\nfurther introduce a progressive semantic injection strategy, organizing the\nmodel training into three structured stages. Extensive experiments conducted on\n17 datasets, covering three major AVFA tasks, demonstrate that AVF-MAE++\nachieves consistent state-of-the-art performance across multiple benchmarks.\nComprehensive ablation studies further highlight the importance of each\nproposed component and provide deeper insights into the design choices driving\nthese improvements. Our code and models have been publicly released at Github.",
        "url": "http://arxiv.org/abs/2509.24214v1",
        "published_date": "2025-09-29T02:53:49+00:00",
        "updated_date": "2025-09-29T02:53:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xuecheng Wu",
            "Junxiao Xue",
            "Xinyi Yin",
            "Yunyun Shi",
            "Liangyu Fu",
            "Danlei Huang",
            "Yifan Wang",
            "Jia Zhang",
            "Jiayu Nie",
            "Jun Wang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes AVF-MAE++, a family of models for affective video facial analysis that focuses on scaling properties and cross-modal correlation modeling.",
        "tldr_zh": "本文提出了AVF-MAE++，这是一组用于情感视频面部分析的模型，重点关注了缩放属性和跨模态相关建模。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "BALR-SAM: Boundary-Aware Low-Rank Adaptation of SAM for Resource-Efficient Medical Image Segmentation",
        "summary": "Vision foundation models like the Segment Anything Model (SAM), pretrained on\nlarge-scale natural image datasets, often struggle in medical image\nsegmentation due to a lack of domain-specific adaptation. In clinical practice,\nfine-tuning such models efficiently for medical downstream tasks with minimal\nresource demands, while maintaining strong performance, is challenging. To\naddress these issues, we propose BALR-SAM, a boundary-aware low-rank adaptation\nframework that enhances SAM for medical imaging. It combines three tailored\ncomponents: (1) a Complementary Detail Enhancement Network (CDEN) using\ndepthwise separable convolutions and multi-scale fusion to capture\nboundary-sensitive features essential for accurate segmentation; (2) low-rank\nadapters integrated into SAM's Vision Transformer blocks to optimize feature\nrepresentation and attention for medical contexts, while simultaneously\nsignificantly reducing the parameter space; and (3) a low-rank tensor attention\nmechanism in the mask decoder, cutting memory usage by 75% and boosting\ninference speed. Experiments on standard medical segmentation datasets show\nthat BALR-SAM, without requiring prompts, outperforms several state-of-the-art\n(SOTA) methods, including fully fine-tuned MedSAM, while updating just 1.8%\n(11.7M) of its parameters.",
        "url": "http://arxiv.org/abs/2509.24204v1",
        "published_date": "2025-09-29T02:36:09+00:00",
        "updated_date": "2025-09-29T02:36:09+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zelin Liu",
            "Sicheng Dong",
            "Bocheng Li",
            "Yixuan Yang",
            "Jiacheng Ruan",
            "Chenxu Zhou",
            "Suncheng Xiang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "BALR-SAM proposes a boundary-aware low-rank adaptation framework to enhance SAM for medical image segmentation, outperforming state-of-the-art methods without requiring prompts and updating just 1.8% of its parameters.",
        "tldr_zh": "BALR-SAM提出了一种边界感知的低秩适应框架，用于增强SAM在医学图像分割中的性能，优于多种最新方法，而且无需提示，只需更新其1.8%的参数。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Talk in Pieces, See in Whole: Disentangling and Hierarchical Aggregating Representations for Language-based Object Detection",
        "summary": "While vision-language models (VLMs) have made significant progress in\nmultimodal perception (e.g., open-vocabulary object detection) with simple\nlanguage queries, state-of-the-art VLMs still show limited ability to perceive\ncomplex queries involving descriptive attributes and relational clauses. Our\nin-depth analysis shows that these limitations mainly stem from text encoders\nin VLMs. Such text encoders behave like bags-of-words and fail to separate\ntarget objects from their descriptive attributes and relations in complex\nqueries, resulting in frequent false positives. To address this, we propose\nrestructuring linguistic representations according to the hierarchical\nrelations within sentences for language-based object detection. A key insight\nis the necessity of disentangling textual tokens into core components-objects,\nattributes, and relations (\"talk in pieces\")-and subsequently aggregating them\ninto hierarchically structured sentence-level representations (\"see in whole\").\nBuilding on this principle, we introduce the TaSe framework with three main\ncontributions: (1) a hierarchical synthetic captioning dataset spanning three\ntiers from category names to descriptive sentences; (2) Talk in Pieces, the\nthree-component disentanglement module guided by a novel disentanglement loss\nfunction, transforms text embeddings into subspace compositions; and (3) See in\nWhole, which learns to aggregate disentangled components into hierarchically\nstructured embeddings with the guide of proposed hierarchical objectives. The\nproposed TaSe framework strengthens the inductive bias of hierarchical\nlinguistic structures, resulting in fine-grained multimodal representations for\nlanguage-based object detection. Experimental results under the OmniLabel\nbenchmark show a 24% performance improvement, demonstrating the importance of\nlinguistic compositionality.",
        "url": "http://arxiv.org/abs/2509.24192v1",
        "published_date": "2025-09-29T02:14:26+00:00",
        "updated_date": "2025-09-29T02:14:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sojung An",
            "Kwanyong Park",
            "Yong Jae Lee",
            "Donghyun Kim"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a new framework, TaSe, which disentangles and hierarchically aggregates linguistic representations for language-based object detection, showing a significant performance improvement under the OmniLabel benchmark.",
        "tldr_zh": "该论文介绍了一个新的框架，TaSe，该框架对语言为基础的物体检测进行语言表示的分离和分层聚合，显示出在OmniLabel基准下显著的性能改进。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Neural Visibility of Point Sets",
        "summary": "Point clouds are widely used representations of 3D data, but determining the\nvisibility of points from a given viewpoint remains a challenging problem due\nto their sparse nature and lack of explicit connectivity. Traditional methods,\nsuch as Hidden Point Removal (HPR), face limitations in computational\nefficiency, robustness to noise, and handling concave regions or low-density\npoint clouds. In this paper, we propose a novel approach to visibility\ndetermination in point clouds by formulating it as a binary classification\ntask. The core of our network consists of a 3D U-Net that extracts\nview-independent point-wise features and a shared multi-layer perceptron (MLP)\nthat predicts point visibility using the extracted features and view direction\nas inputs. The network is trained end-to-end with ground-truth visibility\nlabels generated from rendered 3D models. Our method significantly outperforms\nHPR in both accuracy and computational efficiency, achieving up to 126 times\nspeedup on large point clouds. Additionally, our network demonstrates\nrobustness to noise and varying point cloud densities and generalizes well to\nunseen shapes. We validate the effectiveness of our approach through extensive\nexperiments on the ShapeNet, ABC Dataset and real-world datasets, showing\nsubstantial improvements in visibility accuracy. We also demonstrate the\nversatility of our method in various applications, including point cloud\nvisualization, surface reconstruction, normal estimation, shadow rendering, and\nviewpoint optimization. Our code and models are available at\nhttps://github.com/octree-nn/neural-visibility.",
        "url": "http://arxiv.org/abs/2509.24150v1",
        "published_date": "2025-09-29T00:54:00+00:00",
        "updated_date": "2025-09-29T00:54:00+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Jun-Hao Wang",
            "Yi-Yang Tian",
            "Baoquan Chen",
            "Peng-Shuai Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes a novel approach for determining visibility in point clouds using a 3D U-Net and MLP network, outperforming traditional methods in accuracy and efficiency.",
        "tldr_zh": "该论文提出了一种新颖的方法，利用3D U-Net和MLP网络来确定点云中的可见性，优于传统方法在准确性和效率方面。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Generalist Scanner Meets Specialist Locator: A Synergistic Coarse-to-Fine Framework for Robust GUI Grounding",
        "summary": "Grounding natural language queries in graphical user interfaces (GUIs)\npresents a challenging task that requires models to comprehend diverse UI\nelements across various applications and systems, while also accurately\npredicting the spatial coordinates for the intended operation. To tackle this\nproblem, we propose GMS: Generalist Scanner Meets Specialist Locator, a\nsynergistic coarse-to-fine framework that effectively improves GUI grounding\nperformance. GMS leverages the complementary strengths of general\nvision-language models (VLMs) and small, task-specific GUI grounding models by\nassigning them distinct roles within the framework. Specifically, the general\nVLM acts as a 'Scanner' to identify potential regions of interest, while the\nfine-tuned grounding model serves as a 'Locator' that outputs precise\ncoordinates within these regions. This design is inspired by how humans perform\nGUI grounding, where the eyes scan the interface and the brain focuses on\ninterpretation and localization. Our whole framework consists of five stages\nand incorporates hierarchical search with cross-modal communication to achieve\npromising prediction results. Experimental results on the ScreenSpot-Pro\ndataset show that while the 'Scanner' and 'Locator' models achieve only $2.0\\%$\nand $3.7\\%$ accuracy respectively when used independently, their integration\nwithin GMS framework yields an overall accuracy of $35.7\\%$, representing a $10\n\\times$ improvement. Additionally, GMS significantly outperforms other strong\nbaselines under various settings, demonstrating its robustness and potential\nfor general-purpose GUI grounding.",
        "url": "http://arxiv.org/abs/2509.24133v1",
        "published_date": "2025-09-29T00:06:31+00:00",
        "updated_date": "2025-09-29T00:06:31+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Zhecheng Li",
            "Guoxian Song",
            "Yiwei Wang",
            "Zhen Xiong",
            "Junsong Yuan",
            "Yujun Cai"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a framework, GMS, that combines a general vision-language model and a specific GUI grounding model for improved GUI grounding accuracy.",
        "tldr_zh": "本文提出了一种名为GMS的框架，将通用的视觉语言模型和特定的GUI定位模型结合起来，以提高GUI定位的准确性。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Mash, Spread, Slice! Learning to Manipulate Object States via Visual Spatial Progress",
        "summary": "Most robot manipulation focuses on changing the kinematic state of objects:\npicking, placing, opening, or rotating them. However, a wide range of\nreal-world manipulation tasks involve a different class of object state\nchange--such as mashing, spreading, or slicing--where the object's physical and\nvisual state evolve progressively without necessarily changing its position. We\npresent SPARTA, the first unified framework for the family of object state\nchange manipulation tasks. Our key insight is that these tasks share a common\nstructural pattern: they involve spatially-progressing, object-centric changes\nthat can be represented as regions transitioning from an actionable to a\ntransformed state. Building on this insight, SPARTA integrates spatially\nprogressing object change segmentation maps, a visual skill to perceive\nactionable vs. transformed regions for specific object state change tasks, to\ngenerate a) structured policy observations that strip away appearance\nvariability, and b) dense rewards that capture incremental progress over time.\nThese are leveraged in two SPARTA policy variants: reinforcement learning for\nfine-grained control without demonstrations or simulation; and greedy control\nfor fast, lightweight deployment. We validate SPARTA on a real robot for three\nchallenging tasks across 10 diverse real-world objects, achieving significant\nimprovements in training time and accuracy over sparse rewards and visual\ngoal-conditioned baselines. Our results highlight progress-aware visual\nrepresentations as a versatile foundation for the broader family of object\nstate manipulation tasks. Project website:\nhttps://vision.cs.utexas.edu/projects/sparta-robot",
        "url": "http://arxiv.org/abs/2509.24129v1",
        "published_date": "2025-09-28T23:56:07+00:00",
        "updated_date": "2025-09-28T23:56:07+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Priyanka Mandikal",
            "Jiaheng Hu",
            "Shivin Dass",
            "Sagnik Majumder",
            "Roberto Martín-Martín",
            "Kristen Grauman"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces SPARTA, a framework for manipulating objects in ways like mashing, spreading, or slicing that involve progressive changes without moving the object. It uses visual representations and reinforcement learning for control.",
        "tldr_zh": "该论文介绍了SPARTA，这是一个用于以不移动对象的方式进行渐进性变化的框架，如捣碎、扩散或切片。它利用视觉表示和强化学习进行控制。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "End-to-end Topographic Auditory Models Replicate Signatures of Human Auditory Cortex",
        "summary": "The human auditory cortex is topographically organized. Neurons with similar\nresponse properties are spatially clustered, forming smooth maps for acoustic\nfeatures such as frequency in early auditory areas, and modular regions\nselective for music and speech in higher-order cortex. Yet, evaluations for\ncurrent computational models of auditory perception do not measure whether such\ntopographic structure is present in a candidate model. Here, we show that\ncortical topography is not present in the previous best-performing models at\npredicting human auditory fMRI responses. To encourage the emergence of\ntopographic organization, we adapt a cortical wiring-constraint loss originally\ndesigned for visual perception. The new class of topographic auditory models,\nTopoAudio, are trained to classify speech, and environmental sounds from\ncochleagram inputs, with an added constraint that nearby units on a 2D cortical\nsheet develop similar tuning. Despite these additional constraints, TopoAudio\nachieves high accuracy on benchmark tasks comparable to the unconstrained\nnon-topographic baseline models. Further, TopoAudio predicts the fMRI responses\nin the brain as well as standard models, but unlike standard models, TopoAudio\ndevelops smooth, topographic maps for tonotopy and amplitude modulation (common\nproperties of early auditory representation, as well as clustered response\nmodules for music and speech (higher-order selectivity observed in the human\nauditory cortex). TopoAudio is the first end-to-end biologically grounded\nauditory model to exhibit emergent topography, and our results emphasize that a\nwiring-length constraint can serve as a general-purpose regularization tool to\nachieve biologically aligned representations.",
        "url": "http://arxiv.org/abs/2509.24039v1",
        "published_date": "2025-09-28T19:20:30+00:00",
        "updated_date": "2025-09-28T19:20:30+00:00",
        "categories": [
            "q-bio.NC",
            "cs.AI",
            "cs.CV",
            "cs.SD"
        ],
        "authors": [
            "Haider Al-Tahan",
            "Mayukh Deb",
            "Jenelle Feather",
            "N. Apurva Ratan Murty"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a new class of topographic auditory models called TopoAudio, which exhibit topographic organization similar to the human auditory cortex. These models achieve high accuracy on classification tasks and provide biologically aligned representations.",
        "tldr_zh": "本文介绍了一种新的拓扑听觉模型——TopoAudio，这些模型展示了与人类听觉皮层相似的拓扑结构。这些模型在分类任务上取得了很高的准确度，并提供了生物学对应的表示。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Hazy Pedestrian Trajectory Prediction via Physical Priors and Graph-Mamba",
        "summary": "To address the issues of physical information degradation and ineffective\npedestrian interaction modeling in pedestrian trajectory prediction under hazy\nweather conditions, we propose a deep learning model that combines physical\npriors of atmospheric scattering with topological modeling of pedestrian\nrelationships. Specifically, we first construct a differentiable atmospheric\nscattering model that decouples haze concentration from light degradation\nthrough a network with physical parameter estimation, enabling the learning of\nhaze-mitigated feature representations. Second, we design an adaptive scanning\nstate space model for feature extraction. Our adaptive Mamba variant achieves a\n78% inference speed increase over native Mamba while preserving long-range\ndependency modeling.\n  Finally, to efficiently model pedestrian relationships, we develop a\nheterogeneous graph attention network, using graph matrices to model\nmulti-granularity interactions between pedestrians and groups, combined with a\nspatio-temporal fusion module to capture the collaborative evolution patterns\nof pedestrian movements. Furthermore, we constructed a new pedestrian\ntrajectory prediction dataset based on ETH/UCY to evaluate the effectiveness of\nthe proposed method. Experiments show that our method reduces the minADE /\nminFDE metrics by 37.2% and 41.5%, respectively, compared to the SOTA models in\ndense haze scenarios (visibility < 30m), providing a new modeling paradigm for\nreliable perception in intelligent transportation systems in adverse\nenvironments.",
        "url": "http://arxiv.org/abs/2509.24020v1",
        "published_date": "2025-09-28T18:29:43+00:00",
        "updated_date": "2025-09-28T18:29:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jian Chen",
            "Zhuoran Zheng",
            "Han Hu",
            "Guijuan Zhang",
            "Dianjie Lu",
            "Liang Li",
            "Chen Lyu"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a deep learning model that combines physical priors of atmospheric scattering with topological modeling of pedestrian relationships for improved pedestrian trajectory prediction in hazy weather conditions.",
        "tldr_zh": "该论文提出了一种深度学习模型，将大气散射的物理先验信息与对行人关系的拓扑建模相结合，以改善雾天条件下的行人轨迹预测。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.0
    },
    {
        "title": "Combining Discrepancy-Confusion Uncertainty and Calibration Diversity for Active Fine-Grained Image Classification",
        "summary": "Active learning (AL) aims to build high-quality labeled datasets by\niteratively selecting the most informative samples from an unlabeled pool under\nlimited annotation budgets. However, in fine-grained image classification,\nassessing this informativeness is especially challenging due to subtle\ninter-class differences. In this paper, we introduce a novel method, combining\ndiscrepancy-confusion uncertainty and calibration diversity for active\nfine-grained image classification (DECERN), to effectively perceive the\ndistinctiveness between fine-grained images and evaluate the sample value.\nDECERN introduces a multifaceted informativeness measure that combines\ndiscrepancy-confusion uncertainty and calibration diversity. The\ndiscrepancy-confusion uncertainty quantifies the category directionality and\nstructural stability of fine-grained unlabeled data during local feature\nfusion. Subsequently, uncertainty-weighted clustering is performed to diversify\nthe uncertainty samples. Then we calibrate the diversity to maximize the global\ndiversity of the selected sample while maintaining its local\nrepresentativeness. Extensive experiments conducted on 7 fine-grained image\ndatasets across 26 distinct experimental settings demonstrate that our method\nachieves superior performance compared to state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2509.24181v1",
        "published_date": "2025-09-29T02:03:44+00:00",
        "updated_date": "2025-09-29T02:03:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yinghao Jin",
            "Xi Yang"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces a method for active fine-grained image classification, combining uncertainty and diversity measures to select informative samples, outperforming existing methods in experiments.",
        "tldr_zh": "该论文引入了一种用于主动细粒度图像分类的方法，结合了不确定度和多样性度量来选择信息丰富的样本，在实验证明优于现有方法。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6.75
    },
    {
        "title": "Unsupervised Representation Learning for 3D Mesh Parameterization with Semantic and Visibility Objectives",
        "summary": "Recent 3D generative models produce high-quality textures for 3D mesh\nobjects. However, they commonly rely on the heavy assumption that input 3D\nmeshes are accompanied by manual mesh parameterization (UV mapping), a manual\ntask that requires both technical precision and artistic judgment. Industry\nsurveys show that this process often accounts for a significant share of asset\ncreation, creating a major bottleneck for 3D content creators. Moreover,\nexisting automatic methods often ignore two perceptually important criteria:\n(1) semantic awareness (UV charts should align semantically similar 3D parts\nacross shapes) and (2) visibility awareness (cutting seams should lie in\nregions unlikely to be seen). To overcome these shortcomings and to automate\nthe mesh parameterization process, we present an unsupervised differentiable\nframework that augments standard geometry-preserving UV learning with semantic-\nand visibility-aware objectives. For semantic-awareness, our pipeline (i)\nsegments the mesh into semantic 3D parts, (ii) applies an unsupervised learned\nper-part UV-parameterization backbone, and (iii) aggregates per-part charts\ninto a unified UV atlas. For visibility-awareness, we use ambient occlusion\n(AO) as an exposure proxy and back-propagate a soft differentiable AO-weighted\nseam objective to steer cutting seams toward occluded regions. By conducting\nqualitative and quantitative evaluations against state-of-the-art methods, we\nshow that the proposed method produces UV atlases that better support texture\ngeneration and reduce perceptible seam artifacts compared to recent baselines.\nOur implementation code is publicly available at:\nhttps://github.com/AHHHZ975/Semantic-Visibility-UV-Param.",
        "url": "http://arxiv.org/abs/2509.25094v1",
        "published_date": "2025-09-29T17:28:58+00:00",
        "updated_date": "2025-09-29T17:28:58+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "AmirHossein Zamani",
            "Bruno Roy",
            "Arianna Rampini"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces an unsupervised framework for 3D mesh parameterization with semantic and visibility objectives, aiming to automate the manual process and improve texture generation quality.",
        "tldr_zh": "该论文介绍了一种无监督的框架，用于3D网格参数化，具有语义和可见性目标，旨在自动化手动过程并提高纹理生成质量。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 6.5
    },
    {
        "title": "Vehicle Classification under Extreme Imbalance: A Comparative Study of Ensemble Learning and CNNs",
        "summary": "Accurate vehicle type recognition underpins intelligent transportation and\nlogistics, but severe class imbalance in public datasets suppresses performance\non rare categories. We curate a 16-class corpus (~47k images) by merging\nKaggle, ImageNet, and web-crawled data, and create six balanced variants via\nSMOTE oversampling and targeted undersampling. Lightweight ensembles, such as\nRandom Forest, AdaBoost, and a soft-voting combiner built on MobileNet-V2\nfeatures are benchmarked against a configurable ResNet-style CNN trained with\nstrong augmentation and label smoothing. The best ensemble (SMOTE-combined)\nattains 74.8% test accuracy, while the CNN achieves 79.19% on the full test set\nand 81.25% on an unseen inference batch, confirming the advantage of deep\nmodels. Nonetheless, the most under-represented class (Barge) remains a failure\nmode, highlighting the limits of rebalancing alone. Results suggest\nprioritizing additional minority-class collection and cost-sensitive objectives\n(e.g., focal loss) and exploring hybrid ensemble or CNN pipelines to combine\ninterpretability with representational power.",
        "url": "http://arxiv.org/abs/2509.24880v1",
        "published_date": "2025-09-29T14:56:56+00:00",
        "updated_date": "2025-09-29T14:56:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Abu Hanif Muhammad Syarubany"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper compares ensemble learning and CNNs for vehicle classification in highly imbalanced datasets, highlighting the advantages and limitations of each approach.",
        "tldr_zh": "本文比较了集成学习和CNN在高度不平衡数据集中的车辆分类能力，突出了每种方法的优势和局限性。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Biomechanical-phase based Temporal Segmentation in Sports Videos: a Demonstration on Javelin-Throw",
        "summary": "Precise analysis of athletic motion is central to sports analytics,\nparticularly in disciplines where nuanced biomechanical phases directly impact\nperformance outcomes. Traditional analytics techniques rely on manual\nannotation or laboratory-based instrumentation, which are time-consuming,\ncostly, and lack scalability. Automatic extraction of relevant kinetic\nvariables requires a robust and contextually appropriate temporal segmentation.\nConsidering the specific case of elite javelin-throw, we present a novel\nunsupervised framework for such a contextually aware segmentation, which\napplies the structured optimal transport (SOT) concept to augment the\nwell-known Attention-based Spatio-Temporal Graph Convolutional Network\n(ASTGCN). This enables the identification of motion phase transitions without\nrequiring expensive manual labeling. Extensive experiments demonstrate that our\napproach outperforms state-of-the-art unsupervised methods, achieving 71.02%\nmean average precision (mAP) and 74.61% F1-score on test data, substantially\nhigher than competing baselines. We also release a new dataset of 211 manually\nannotated professional javelin-throw videos with frame-level annotations,\ncovering key biomechanical phases: approach steps, drive, throw, and recovery.",
        "url": "http://arxiv.org/abs/2509.24606v1",
        "published_date": "2025-09-29T11:11:46+00:00",
        "updated_date": "2025-09-29T11:11:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bikash Kumar Badatya",
            "Vipul Baghel",
            "Jyotirmoy Amin",
            "Ravi Hegde"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a new framework for automatically segmenting biomechanical phases in sports videos, focusing on javelin-throw, outperforming previous methods and releasing a new dataset.",
        "tldr_zh": "本文介绍了一种新的框架，用于自动分割体育视频中的生物力学阶段，重点是标枪投掷，在超过以前的方法并释放了一个新数据集。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Comprehensive Benchmarking of YOLOv11 Architectures for Scalable and Granular Peripheral Blood Cell Detection",
        "summary": "Manual peripheral blood smear (PBS) analysis is labor intensive and\nsubjective. While deep learning offers a promising alternative, a systematic\nevaluation of state of the art models such as YOLOv11 for fine grained PBS\ndetection is still lacking. In this work, we make two key contributions. First,\nwe curate a large scale annotated dataset for blood cell detection and\nclassification, comprising 16,891 images across 12 peripheral blood cell (PBC)\nclasses, along with the red blood cell class, all carefully re annotated for\nobject detection tasks. In total, the dataset contains 298,850 annotated cells.\nSecond, we leverage this dataset to conduct a comprehensive evaluation of five\nYOLOv11 variants (ranging from Nano to XLarge). These models are rigorously\nbenchmarked under two data splitting strategies (70:20:10 and 80:10:10) and\nsystematically assessed using multiple performance criteria, including mean\nAverage Precision (mAP), precision, recall, F1 score, and computational\nefficiency. Our experiments show that the YOLOv11 Medium variant achieves the\nbest trade off, reaching a mAP@0.5 of 0.934 under the 8:1:1 split. Larger\nmodels (Large and XLarge) provide only marginal accuracy gains at substantially\nhigher computational cost. Moreover, the 8:1:1 split consistently outperforms\nthe 7:2:1 split across all models. These findings highlight YOLOv11,\nparticularly the Medium variant, as a highly effective framework for automated,\nfine grained PBS detection. Beyond benchmarking, our publicly released dataset\n(github.com/Mohamad-AbouAli/OI-PBC-Dataset) offers a valuable resource to\nadvance research on blood cell detection and classification in hematology.",
        "url": "http://arxiv.org/abs/2509.24595v1",
        "published_date": "2025-09-29T11:00:19+00:00",
        "updated_date": "2025-09-29T11:00:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mohamad Abou Ali",
            "Mariam Abdulfattah",
            "Baraah Al Hussein",
            "Fadi Dornaika",
            "Ali Cherry",
            "Mohamad Hajj-Hassan",
            "Lara Hamawy"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper presents a comprehensive benchmarking study of YOLOv11 architectures for automated blood cell detection in peripheral blood smears, showing that the Medium variant performs the best.",
        "tldr_zh": "这篇论文对YOLOv11架构在周围血液涂片自动检测方面进行了全面基准评估，结果显示中等变体表现最佳。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Performance-Efficiency Trade-off for Fashion Image Retrieval",
        "summary": "The fashion industry has been identified as a major contributor to waste and\nemissions, leading to an increased interest in promoting the second-hand\nmarket. Machine learning methods play an important role in facilitating the\ncreation and expansion of second-hand marketplaces by enabling the large-scale\nvaluation of used garments. We contribute to this line of work by addressing\nthe scalability of second-hand image retrieval from databases. By introducing a\nselective representation framework, we can shrink databases to 10% of their\noriginal size without sacrificing retrieval accuracy. We first explore\nclustering and coreset selection methods to identify representative samples\nthat capture the key features of each garment and its internal variability.\nThen, we introduce an efficient outlier removal method, based on a\nneighbour-homogeneity consistency score measure, that filters out\nuncharacteristic samples prior to selection. We evaluate our approach on three\npublic datasets: DeepFashion Attribute, DeepFashion Con2Shop, and DeepFashion2.\nThe results demonstrate a clear performance-efficiency trade-off by\nstrategically pruning and selecting representative vectors of images. The\nretrieval system maintains near-optimal accuracy, while greatly reducing\ncomputational costs by reducing the images added to the vector database.\nFurthermore, applying our outlier removal method to clustering techniques\nyields even higher retrieval performance by removing non-discriminative samples\nbefore the selection.",
        "url": "http://arxiv.org/abs/2509.24477v1",
        "published_date": "2025-09-29T08:51:04+00:00",
        "updated_date": "2025-09-29T08:51:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Julio Hurtado",
            "Haoran Ni",
            "Duygu Sap",
            "Connor Mattinson",
            "Martin Lotz"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a selective representation framework for fashion image retrieval to shrink databases while maintaining accuracy and reducing computational costs.",
        "tldr_zh": "本文引入了一种选择性表示框架，用于时尚图像检索，可以缩小数据库的同时保持准确性和降低计算成本。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Dynamic Orchestration of Multi-Agent System for Real-World Multi-Image Agricultural VQA",
        "summary": "Agricultural visual question answering is essential for providing farmers and\nresearchers with accurate and timely knowledge. However, many existing\napproaches are predominantly developed for evidence-constrained settings such\nas text-only queries or single-image cases. This design prevents them from\ncoping with real-world agricultural scenarios that often require multi-image\ninputs with complementary views across spatial scales, and growth stages.\nMoreover, limited access to up-to-date external agricultural context makes\nthese systems struggle to adapt when evidence is incomplete. In addition, rigid\npipelines often lack systematic quality control. To address this gap, we\npropose a self-reflective and self-improving multi-agent framework that\nintegrates four roles, the Retriever, the Reflector, the Answerer, and the\nImprover. They collaborate to enable context enrichment, reflective reasoning,\nanswer drafting, and iterative improvement.\n  A Retriever formulates queries and gathers external information, while a\nReflector assesses adequacy and triggers sequential reformulation and renewed\nretrieval. Two Answerers draft candidate responses in parallel to reduce bias.\nThe Improver refines them through iterative checks while ensuring that\ninformation from multiple images is effectively aligned and utilized.\nExperiments on the AgMMU benchmark show that our framework achieves competitive\nperformance on multi-image agricultural QA.",
        "url": "http://arxiv.org/abs/2509.24350v1",
        "published_date": "2025-09-29T06:52:10+00:00",
        "updated_date": "2025-09-29T06:52:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yan Ke",
            "Xin Yu",
            "Heming Du",
            "Scott Chapman",
            "Helen Huang"
        ],
        "ai_categories": [
            "AIGC",
            "MultiModal",
            "Dataset"
        ],
        "tldr": "The paper presents a multi-agent framework for agricultural visual question answering using multiple images, showing competitive performance on this task.",
        "tldr_zh": "本文提出了一种多智能体框架，用于农业视觉问题回答，使用多个图像，在这一任务上表现出竞争力。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Joint Superpixel and Self-Representation Learning for Scalable Hyperspectral Image Clustering",
        "summary": "Subspace clustering is a powerful unsupervised approach for hyperspectral\nimage (HSI) analysis, but its high computational and memory costs limit\nscalability. Superpixel segmentation can improve efficiency by reducing the\nnumber of data points to process. However, existing superpixel-based methods\nusually perform segmentation independently of the clustering task, often\nproducing partitions that do not align with the subsequent clustering\nobjective. To address this, we propose a unified end-to-end framework that\njointly optimizes superpixel segmentation and subspace clustering. Its core is\na feedback mechanism: a self-representation network based on unfolded\nAlternating Direction Method of Multipliers (ADMM) provides a model-driven\nsignal to guide a differentiable superpixel module. This joint optimization\nyields clustering-aware partitions that preserve both spectral and spatial\nstructure. Furthermore, our superpixel network learns a unique compactness\nparameter for each superpixel, enabling more flexible and adaptive\nsegmentation. Extensive experiments on benchmark HSI datasets demonstrate that\nour method consistently achieves superior accuracy compared with\nstate-of-the-art clustering approaches.",
        "url": "http://arxiv.org/abs/2509.24027v1",
        "published_date": "2025-09-28T18:42:48+00:00",
        "updated_date": "2025-09-28T18:42:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xianlu Li",
            "Nicolas Nadisic",
            "Shaoguang Huang",
            "Aleksandra Pizurica"
        ],
        "ai_categories": [
            "LoRA",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes a unified framework that integrates superpixel segmentation and subspace clustering for hyperspectral image analysis, achieving superior accuracy compared to existing methods.",
        "tldr_zh": "该论文提出了一个统一框架，将超像素分割和子空间聚类相结合，用于高光谱图像分析，相较于现有方法取得了更好的准确性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "SkyLink: Unifying Street-Satellite Geo-Localization via UAV-Mediated 3D Scene Alignment",
        "summary": "Cross-view geo-localization aims at establishing location correspondences\nbetween different viewpoints. Existing approaches typically learn cross-view\ncorrelations through direct feature similarity matching, often overlooking\nsemantic degradation caused by extreme viewpoint disparities. To address this\nunique problem, we focus on robust feature retrieval under viewpoint variation\nand propose the novel SkyLink method. We firstly utilize the Google Retrieval\nEnhancement Module to perform data enhancement on street images, which\nmitigates the occlusion of the key target due to restricted street viewpoints.\nThe Patch-Aware Feature Aggregation module is further adopted to emphasize\nmultiple local feature aggregations to ensure the consistent feature extraction\nacross viewpoints. Meanwhile, we integrate the 3D scene information constructed\nfrom multi-scale UAV images as a bridge between street and satellite\nviewpoints, and perform feature alignment through self-supervised and\ncross-view contrastive learning. Experimental results demonstrate robustness\nand generalization across diverse urban scenarios, which achieve 25.75$\\%$\nRecall@1 accuracy on University-1652 in the UAVM2025 Challenge. Code will be\nreleased at https://github.com/HRT00/CVGL-3D.",
        "url": "http://arxiv.org/abs/2509.24783v1",
        "published_date": "2025-09-29T13:43:18+00:00",
        "updated_date": "2025-09-29T13:43:18+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Hongyang Zhang",
            "Yinhao Liu",
            "Zhenyu Kuang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces SkyLink, a method for cross-view geo-localization using UAV-mediated 3D scene alignment, achieving high accuracy in urban scenarios.",
        "tldr_zh": "本文介绍了SkyLink，一种利用UAV介导的3D场景对齐进行跨视图地理定位的方法，在城市场景中实现了高准确度。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.0
    },
    {
        "title": "AQUAIR: A High-Resolution Indoor Environmental Quality Dataset for Smart Aquaculture Monitoring",
        "summary": "Smart aquaculture systems depend on rich environmental data streams to\nprotect fish welfare, optimize feeding, and reduce energy use. Yet public\ndatasets that describe the air surrounding indoor tanks remain scarce, limiting\nthe development of forecasting and anomaly-detection tools that couple\nhead-space conditions with water-quality dynamics. We therefore introduce\nAQUAIR, an open-access public dataset that logs six Indoor Environmental\nQuality (IEQ) variables--air temperature, relative humidity, carbon dioxide,\ntotal volatile organic compounds, PM2.5 and PM10--inside a fish aquaculture\nfacility in Amghass, Azrou, Morocco. A single Awair HOME monitor sampled every\nfive minutes from 14 October 2024 to 9 January 2025, producing more than 23,000\ntime-stamped observations that are fully quality-controlled and publicly\narchived on Figshare. We describe the sensor placement, ISO-compliant mounting\nheight, calibration checks against reference instruments, and an open-source\nprocessing pipeline that normalizes timestamps, interpolates short gaps, and\nexports analysis-ready tables. Exploratory statistics show stable conditions\n(median CO2 = 758 ppm; PM2.5 = 12 micrograms/m3) with pronounced feeding-time\npeaks, offering rich structure for short-horizon forecasting, event detection,\nand sensor drift studies. AQUAIR thus fills a critical gap in smart aquaculture\ninformatics and provides a reproducible benchmark for data-centric machine\nlearning curricula and environmental sensing research focused on head-space\ndynamics in recirculating aquaculture systems.",
        "url": "http://arxiv.org/abs/2509.24069v1",
        "published_date": "2025-09-28T21:07:10+00:00",
        "updated_date": "2025-09-28T21:07:10+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "stat.AP",
            "62M10, 68T45, 62P35, 92C40, 65C20, 60G35, 92C42, 92C35, 93E10",
            "I.2.6; C.2.4; H.3.4; I.2.4; H.3.5; C.2.4; C.3; I.4.8; I.5.1; J.3;\n  K.6.1; H.2.8"
        ],
        "authors": [
            "Youssef Sabiri",
            "Walid Houmaidi",
            "Ouail El Maadi",
            "Yousra Chtouki"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC",
            "Other"
        ],
        "tldr": "AQUAIR introduces a dataset on indoor environmental quality in a fish aquaculture facility, enabling forecasting and anomaly detection tools.",
        "tldr_zh": "AQUAIR介绍了一个室内环境质量数据集，可用于鱼类水产养殖设施，促进预测和异常检测工具的发展。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "GPS-MTM: Capturing Pattern of Normalcy in GPS-Trajectories with self-supervised learning",
        "summary": "Foundation models have driven remarkable progress in text, vision, and video\nunderstanding, and are now poised to unlock similar breakthroughs in trajectory\nmodeling. We introduce the GPSMasked Trajectory Transformer (GPS-MTM), a\nfoundation model for large-scale mobility data that captures patterns of\nnormalcy in human movement. Unlike prior approaches that flatten trajectories\ninto coordinate streams, GPS-MTM decomposes mobility into two complementary\nmodalities: states (point-of-interest categories) and actions (agent\ntransitions). Leveraging a bi-directional Transformer with a self-supervised\nmasked modeling objective, the model reconstructs missing segments across\nmodalities, enabling it to learn rich semantic correlations without manual\nlabels. Across benchmark datasets, including Numosim-LA, Urban Anomalies, and\nGeolife, GPS-MTM consistently outperforms on downstream tasks such as\ntrajectory infilling and next-stop prediction. Its advantages are most\npronounced in dynamic tasks (inverse and forward dynamics), where contextual\nreasoning is critical. These results establish GPS-MTM as a robust foundation\nmodel for trajectory analytics, positioning mobility data as a first-class\nmodality for large-scale representation learning. Code is released for further\nreference.",
        "url": "http://arxiv.org/abs/2509.24031v1",
        "published_date": "2025-09-28T19:00:50+00:00",
        "updated_date": "2025-09-28T19:00:50+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.MA"
        ],
        "authors": [
            "Umang Garg",
            "Bowen Zhang",
            "Anantanjit Subrahmanya",
            "Chandrakanth Gudavalli",
            "BS Manjunath"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "GPS-MTM is a model for analyzing large-scale mobility data that captures patterns of normalcy in human movement through self-supervised learning.",
        "tldr_zh": "GPS-MTM是一个用于分析大规模移动数据的模型，通过自监督学习捕获人类移动中的正常模式。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.0
    },
    {
        "title": "$\\mathbf{R}^3$: Reconstruction, Raw, and Rain: Deraining Directly in the Bayer Domain",
        "summary": "Image reconstruction from corrupted images is crucial across many domains.\nMost reconstruction networks are trained on post-ISP sRGB images, even though\nthe image-signal-processing pipeline irreversibly mixes colors, clips dynamic\nrange, and blurs fine detail. This paper uses the rain degradation problem as a\nuse case to show that these losses are avoidable, and demonstrates that\nlearning directly on raw Bayer mosaics yields superior reconstructions. To\nsubstantiate the claim, we (i) evaluate post-ISP and Bayer reconstruction\npipelines, (ii) curate Raw-Rain, the first public benchmark of real rainy\nscenes captured in both 12-bit Bayer and bit-depth-matched sRGB, and (iii)\nintroduce Information Conservation Score (ICS), a color-invariant metric that\naligns more closely with human opinion than PSNR or SSIM. On the test split,\nour raw-domain model improves sRGB results by up to +0.99 dB PSNR and +1.2%\nICS, while running faster with half of the GFLOPs. The results advocate an\nISP-last paradigm for low-level vision and open the door to end-to-end\nlearnable camera pipelines.",
        "url": "http://arxiv.org/abs/2509.24022v1",
        "published_date": "2025-09-28T18:31:24+00:00",
        "updated_date": "2025-09-28T18:31:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nate Rothschild",
            "Moshe Kimhi",
            "Avi Mendelson",
            "Chaim Baskin"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "This paper proposes a method for deraining directly in the Bayer domain, showing superior results compared to post-ISP sRGB reconstruction. It introduces a new benchmark and metric to evaluate the performance of the proposed method.",
        "tldr_zh": "本文提出了一种直接在Bayer域中去雨的方法，展示了与后ISP sRGB重建相比更好的结果。它引入了一个新的基准和度量标准来评估所提方法的性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]