[
    {
        "title": "Controlling Multimodal LLMs via Reward-guided Decoding",
        "summary": "As Multimodal Large Language Models (MLLMs) gain widespread applicability, it\nis becoming increasingly desirable to adapt them for diverse user needs. In\nthis paper, we study the adaptation of MLLMs through controlled decoding. To\nachieve this, we introduce the first method for reward-guided decoding of MLLMs\nand demonstrate its application in improving their visual grounding. Our method\ninvolves building reward models for visual grounding and using them to guide\nthe MLLM's decoding process. Concretely, we build two separate reward models to\nindependently control the degree of object precision and recall in the model's\noutput. Our approach enables on-the-fly controllability of an MLLM's inference\nprocess in two ways: first, by giving control over the relative importance of\neach reward function during decoding, allowing a user to dynamically trade off\nobject precision for recall in image captioning tasks; second, by giving\ncontrol over the breadth of the search during decoding, allowing the user to\ncontrol the trade-off between the amount of test-time compute and the degree of\nvisual grounding. We evaluate our method on standard object hallucination\nbenchmarks, showing that it provides significant controllability over MLLM\ninference, while consistently outperforming existing hallucination mitigation\nmethods.",
        "url": "http://arxiv.org/abs/2508.11616v1",
        "published_date": "2025-08-15T17:29:06+00:00",
        "updated_date": "2025-08-15T17:29:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Oscar Mañas",
            "Pierluca D'Oro",
            "Koustuv Sinha",
            "Adriana Romero-Soriano",
            "Michal Drozdzal",
            "Aishwarya Agrawal"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer"
        ],
        "tldr": "This paper introduces a method for reward-guided decoding of Multimodal Large Language Models (MLLMs) to improve visual grounding, enabling dynamic control over object precision and recall during image captioning tasks.",
        "tldr_zh": "本文介绍了一种通过奖励引导解码多模态大语言模型（MLLMs）以改善视觉基础的方法，使用户可以在图像字幕任务中动态控制物体精度和召回。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models",
        "summary": "Despite significant advances in video synthesis, research into multi-shot\nvideo generation remains in its infancy. Even with scaled-up models and massive\ndatasets, the shot transition capabilities remain rudimentary and unstable,\nlargely confining generated videos to single-shot sequences. In this work, we\nintroduce CineTrans, a novel framework for generating coherent multi-shot\nvideos with cinematic, film-style transitions. To facilitate insights into the\nfilm editing style, we construct a multi-shot video-text dataset Cine250K with\ndetailed shot annotations. Furthermore, our analysis of existing video\ndiffusion models uncovers a correspondence between attention maps in the\ndiffusion model and shot boundaries, which we leverage to design a mask-based\ncontrol mechanism that enables transitions at arbitrary positions and transfers\neffectively in a training-free setting. After fine-tuning on our dataset with\nthe mask mechanism, CineTrans produces cinematic multi-shot sequences while\nadhering to the film editing style, avoiding unstable transitions or naive\nconcatenations. Finally, we propose specialized evaluation metrics for\ntransition control, temporal consistency and overall quality, and demonstrate\nthrough extensive experiments that CineTrans significantly outperforms existing\nbaselines across all criteria.",
        "url": "http://arxiv.org/abs/2508.11484v1",
        "published_date": "2025-08-15T13:58:22+00:00",
        "updated_date": "2025-08-15T13:58:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoxue Wu",
            "Bingjie Gao",
            "Yu Qiao",
            "Yaohui Wang",
            "Xinyuan Chen"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces CineTrans, a framework for generating multi-shot videos with cinematic transitions, outperforming existing baselines across all criteria.",
        "tldr_zh": "该论文介绍了CineTrans，这是一个用于生成带有电影过渡的多镜头视频的框架，它在所有标准上都优于现有基线。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation",
        "summary": "Multimodal Large Language Models (MLLMs) with unified architectures excel\nacross a wide range of vision-language tasks, yet aligning them with\npersonalized image generation remains a significant challenge. Existing methods\nfor MLLMs are frequently subject-specific, demanding a data-intensive\nfine-tuning process for every new subject, which limits their scalability. In\nthis paper, we introduce MM-R1, a framework that integrates a cross-modal\nChain-of-Thought (X-CoT) reasoning strategy to unlock the inherent potential of\nunified MLLMs for personalized image generation. Specifically, we structure\npersonalization as an integrated visual reasoning and generation process: (1)\ngrounding subject concepts by interpreting and understanding user-provided\nimages and contextual cues, and (2) generating personalized images conditioned\non both the extracted subject representations and user prompts. To further\nenhance the reasoning capability, we adopt Grouped Reward Proximal Policy\nOptimization (GRPO) to explicitly align the generation. Experiments demonstrate\nthat MM-R1 unleashes the personalization capability of unified MLLMs to\ngenerate images with high subject fidelity and strong text alignment in a\nzero-shot manner.",
        "url": "http://arxiv.org/abs/2508.11433v1",
        "published_date": "2025-08-15T12:20:27+00:00",
        "updated_date": "2025-08-15T12:20:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qian Liang",
            "Yujia Wu",
            "Kuncheng Li",
            "Jiwei Wei",
            "Shiyuan He",
            "Jinyu Guo",
            "Ning Xie"
        ],
        "ai_categories": [
            "Multimodality",
            "GAN"
        ],
        "tldr": "MM-R1 introduces a framework that leverages unified multimodal large language models for personalized image generation, achieving high subject fidelity and text alignment in a zero-shot manner.",
        "tldr_zh": "MM-R1引入了一个框架，利用统一的多模态大型语言模型进行个性化图像生成，在零 shot 方式下实现高主题保真度和文本对齐。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation",
        "summary": "Recent advances in audio-driven portrait animation have demonstrated\nimpressive capabilities. However, existing methods struggle to align with\nfine-grained human preferences across multiple dimensions, such as motion\nnaturalness, lip-sync accuracy, and visual quality. This is due to the\ndifficulty of optimizing among competing preference objectives, which often\nconflict with one another, and the scarcity of large-scale, high-quality\ndatasets with multidimensional preference annotations. To address these, we\nfirst introduce Talking-Critic, a multimodal reward model that learns\nhuman-aligned reward functions to quantify how well generated videos satisfy\nmultidimensional expectations. Leveraging this model, we curate Talking-NSQ, a\nlarge-scale multidimensional human preference dataset containing 410K\npreference pairs. Finally, we propose Timestep-Layer adaptive multi-expert\nPreference Optimization (TLPO), a novel framework for aligning diffusion-based\nportrait animation models with fine-grained, multidimensional preferences. TLPO\ndecouples preferences into specialized expert modules, which are then fused\nacross timesteps and network layers, enabling comprehensive, fine-grained\nenhancement across all dimensions without mutual interference. Experiments\ndemonstrate that Talking-Critic significantly outperforms existing methods in\naligning with human preference ratings. Meanwhile, TLPO achieves substantial\nimprovements over baseline models in lip-sync accuracy, motion naturalness, and\nvisual quality, exhibiting superior performance in both qualitative and\nquantitative evaluations. Ours project page:\nhttps://fantasy-amap.github.io/fantasy-talking2/",
        "url": "http://arxiv.org/abs/2508.11255v1",
        "published_date": "2025-08-15T06:43:46+00:00",
        "updated_date": "2025-08-15T06:43:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "MengChao Wang",
            "Qiang Wang",
            "Fan Jiang",
            "Mu Xu"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper presents a novel framework for aligning audio-driven portrait animation with human preferences, achieving significant improvements in quality.",
        "tldr_zh": "本文提出了一种新的框架，用于使音频驱动的肖像动画与人类偏好相匹配，在质量方面取得了显著改进。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "LEARN: A Story-Driven Layout-to-Image Generation Framework for STEM Instruction",
        "summary": "LEARN is a layout-aware diffusion framework designed to generate\npedagogically aligned illustrations for STEM education. It leverages a curated\nBookCover dataset that provides narrative layouts and structured visual cues,\nenabling the model to depict abstract and sequential scientific concepts with\nstrong semantic alignment. Through layout-conditioned generation, contrastive\nvisual-semantic training, and prompt modulation, LEARN produces coherent visual\nsequences that support mid-to-high-level reasoning in line with Bloom's\ntaxonomy while reducing extraneous cognitive load as emphasized by Cognitive\nLoad Theory. By fostering spatially organized and story-driven narratives, the\nframework counters fragmented attention often induced by short-form media and\npromotes sustained conceptual focus. Beyond static diagrams, LEARN demonstrates\npotential for integration with multimodal systems and curriculum-linked\nknowledge graphs to create adaptive, exploratory educational content. As the\nfirst generative approach to unify layout-based storytelling, semantic\nstructure learning, and cognitive scaffolding, LEARN represents a novel\ndirection for generative AI in education. The code and dataset will be released\nto facilitate future research and practical deployment.",
        "url": "http://arxiv.org/abs/2508.11153v1",
        "published_date": "2025-08-15T01:49:58+00:00",
        "updated_date": "2025-08-15T01:49:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Maoquan Zhang",
            "Bisser Raytchev",
            "Xiujuan Sun"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Dataset",
            "Other"
        ],
        "tldr": "LEARN is a generative framework for creating educational illustrations aligned with STEM concepts through layout-driven storytelling and cognitive scaffolding.",
        "tldr_zh": "LEARN是一个生成框架，通过布局驱动的叙述和认知支架，创作与STEM概念对齐的教育插图。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "LD-LAudio-V1: Video-to-Long-Form-Audio Generation Extension with Dual Lightweight Adapters",
        "summary": "Generating high-quality and temporally synchronized audio from video content\nis essential for video editing and post-production tasks, enabling the creation\nof semantically aligned audio for silent videos. However, most existing\napproaches focus on short-form audio generation for video segments under 10\nseconds or rely on noisy datasets for long-form video-to-audio zsynthesis. To\naddress these limitations, we introduce LD-LAudio-V1, an extension of\nstate-of-the-art video-to-audio models and it incorporates dual lightweight\nadapters to enable long-form audio generation. In addition, we release a clean\nand human-annotated video-to-audio dataset that contains pure sound effects\nwithout noise or artifacts. Our method significantly reduces splicing artifacts\nand temporal inconsistencies while maintaining computational efficiency.\nCompared to direct fine-tuning with short training videos, LD-LAudio-V1\nachieves significant improvements across multiple metrics: $FD_{\\text{passt}}$\n450.00 $\\rightarrow$ 327.29 (+27.27%), $FD_{\\text{panns}}$ 34.88 $\\rightarrow$\n22.68 (+34.98%), $FD_{\\text{vgg}}$ 3.75 $\\rightarrow$ 1.28 (+65.87%),\n$KL_{\\text{panns}}$ 2.49 $\\rightarrow$ 2.07 (+16.87%), $KL_{\\text{passt}}$ 1.78\n$\\rightarrow$ 1.53 (+14.04%), $IS_{\\text{panns}}$ 4.17 $\\rightarrow$ 4.30\n(+3.12%), $IB_{\\text{score}}$ 0.25 $\\rightarrow$ 0.28 (+12.00%),\n$Energy\\Delta10\\text{ms}$ 0.3013 $\\rightarrow$ 0.1349 (+55.23%),\n$Energy\\Delta10\\text{ms(vs.GT)}$ 0.0531 $\\rightarrow$ 0.0288 (+45.76%), and\n$Sem.\\,Rel.$ 2.73 $\\rightarrow$ 3.28 (+20.15%). Our dataset aims to facilitate\nfurther research in long-form video-to-audio generation and is available at\nhttps://github.com/deepreasonings/long-form-video2audio.",
        "url": "http://arxiv.org/abs/2508.11074v1",
        "published_date": "2025-08-14T21:11:57+00:00",
        "updated_date": "2025-08-14T21:11:57+00:00",
        "categories": [
            "cs.SD",
            "cs.AI",
            "cs.CV",
            "eess.AS"
        ],
        "authors": [
            "Haomin Zhang",
            "Kristin Qi",
            "Shuxin Yang",
            "Zihao Chen",
            "Chaofan Ding",
            "Xinhan Di"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces LD-LAudio-V1, a method for generating high-quality and temporally synchronized audio from video content for long-form videos. It also includes a clean and human-annotated dataset to enable further research in this area.",
        "tldr_zh": "本文介绍了LD-LAudio-V1，一种用于从视频内容中生成高质量且时间同步的音频的方法，适用于长格式视频。同时还提供了一个干净且人工标注的数据集，以促进进一步的研究。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Reinforcing Video Reasoning Segmentation to Think Before It Segments",
        "summary": "Video reasoning segmentation (VRS) endeavors to delineate referred objects in\nvideos guided by implicit instructions that encapsulate human intent and\ntemporal logic. Previous approaches leverage large vision language models\n(LVLMs) to encode object semantics into <SEG> tokens for mask prediction.\nHowever, this paradigm suffers from limited interpretability during inference\nand suboptimal performance due to inadequate spatiotemporal reasoning. Drawing\ninspiration from seminal breakthroughs in reinforcement learning, we introduce\nVeason-R1, a specialized LVLM for VRS that emphasizes structured reasoning in\nsegmentation. Veason-R1 is trained through Group Relative Policy Optimization\n(GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, we\ncurate high-quality CoT training data to instill structured reasoning\ntrajectories, bridging video-level semantics and frame-level spatial grounding,\nyielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPO\nfine-tuning encourages efficient exploration of the reasoning space by\noptimizing reasoning chains. To this end, we incorporate a holistic reward\nmechanism that synergistically enhances spatial alignment and temporal\nconsistency, bolstering keyframe localization and fine-grained grounding.\nComprehensive empirical evaluations demonstrate that Veason-R1 achieves\nstate-of-the-art performance on multiple benchmarks, surpassing prior art by\nsignificant margins (e.g., +1.3 J &F in ReVOS and +10.0 J &F in ReasonVOS),\nwhile exhibiting robustness to hallucinations (+8.8 R). Our code and model\nweights will be available at Veason-R1.",
        "url": "http://arxiv.org/abs/2508.11538v1",
        "published_date": "2025-08-15T15:34:56+00:00",
        "updated_date": "2025-08-15T15:34:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sitong Gong",
            "Lu Zhang",
            "Yunzhi Zhuge",
            "Xu Jia",
            "Pingping Zhang",
            "Huchuan Lu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces Veason-R1, a specialized vision language model for video reasoning segmentation that achieves state-of-the-art performance and robustness to hallucinations.",
        "tldr_zh": "该论文介绍了Veason-R1，一种专门用于视频推理分割的视觉语言模型，实现了最先进的性能和对幻觉的稳健性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "GANDiff FR: Hybrid GAN Diffusion Synthesis for Causal Bias Attribution in Face Recognition",
        "summary": "We introduce GANDiff FR, the first synthetic framework that precisely\ncontrols demographic and environmental factors to measure, explain, and reduce\nbias with reproducible rigor. GANDiff FR unifies StyleGAN3-based\nidentity-preserving generation with diffusion-based attribute control, enabling\nfine-grained manipulation of pose around 30 degrees, illumination (four\ndirections), and expression (five levels) under ceteris paribus conditions. We\nsynthesize 10,000 demographically balanced faces across five cohorts validated\nfor realism via automated detection (98.2%) and human review (89%) to isolate\nand quantify bias drivers. Benchmarking ArcFace, CosFace, and AdaFace under\nmatched operating points shows AdaFace reduces inter-group TPR disparity by 60%\n(2.5% vs. 6.3%), with illumination accounting for 42% of residual bias.\nCross-dataset evaluation on RFW, BUPT, and CASIA WebFace confirms strong\nsynthetic-to-real transfer (r 0.85). Despite around 20% computational overhead\nrelative to pure GANs, GANDiff FR yields three times more attribute-conditioned\nvariants, establishing a reproducible, regulation-aligned (EU AI Act) standard\nfor fairness auditing. Code and data are released to support transparent,\nscalable bias evaluation.",
        "url": "http://arxiv.org/abs/2508.11334v1",
        "published_date": "2025-08-15T09:05:57+00:00",
        "updated_date": "2025-08-15T09:05:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Md Asgor Hossain Reaj",
            "Rajan Das Gupta",
            "Md Yeasin Rahat",
            "Nafiz Fahad",
            "Md Jawadul Hasan",
            "Tze Hui Liew"
        ],
        "ai_categories": [
            "GAN",
            "Diffusion",
            "Dataset"
        ],
        "tldr": "GANDiff FR is a synthetic framework that controls demographic and environmental factors to measure, explain, and reduce bias in face recognition. It combines StyleGAN3-based identity-preserving generation with diffusion-based attribute control.",
        "tldr_zh": "GANDiff FR 是一种合成框架，可以控制人口统计学和环境因素，以衡量、解释和减少人脸识别中的偏见。 它结合了基于StyleGAN3的保留身份生成和基于扩散的属性控制。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models",
        "summary": "Vision-Language Models (VLMs), exemplified by CLIP, have emerged as\nfoundational for multimodal intelligence. However, their capacity for logical\nunderstanding remains significantly underexplored, resulting in critical\n''logical blindspots'' that limit their reliability in practical applications.\nTo systematically diagnose this, we introduce LogicBench, a comprehensive\nbenchmark with over 50,000 vision-language pairs across 9 logical categories\nand 4 diverse scenarios: images, videos, anomaly detection, and medical\ndiagnostics. Our evaluation reveals that existing VLMs, even the\nstate-of-the-art ones, fall at over 40 accuracy points below human performance,\nparticularly in challenging tasks like Causality and Conditionality,\nhighlighting their reliance on surface semantics over critical logical\nstructures. To bridge this gap, we propose LogicCLIP, a novel training\nframework designed to boost VLMs' logical sensitivity through advancements in\nboth data generation and optimization objectives. LogicCLIP utilizes\nlogic-aware data generation and a contrastive learning strategy that combines\ncoarse-grained alignment, a fine-grained multiple-choice objective, and a novel\nlogical structure-aware objective. Extensive experiments demonstrate\nLogicCLIP's substantial improvements in logical comprehension across all\nLogicBench domains, significantly outperforming baselines. Moreover, LogicCLIP\nretains, and often surpasses, competitive performance on general\nvision-language benchmarks, demonstrating that the enhanced logical\nunderstanding does not come at the expense of general alignment. We believe\nthat LogicBench and LogicCLIP will be important resources for advancing VLM\nlogical capabilities.",
        "url": "http://arxiv.org/abs/2508.11317v1",
        "published_date": "2025-08-15T08:40:13+00:00",
        "updated_date": "2025-08-15T08:40:13+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Yuchen Zhou",
            "Jiayu Tang",
            "Shuo Yang",
            "Xiaoyan Xiao",
            "Yuqin Dai",
            "Wenhao Yang",
            "Chao Gou",
            "Xiaobo Xia",
            "Tat-Seng Chua"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces LogicBench, a benchmark to evaluate the logical understanding of Vision-Language Models (VLMs), and proposes LogicCLIP, a training framework to improve VLMs' logical sensitivity.",
        "tldr_zh": "本文引入了LogicBench，一个评估视觉-语言模型逻辑理解能力的基准，并提出了LogicCLIP，以提高VLMs的逻辑敏感度。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Advancing 3D Scene Understanding with MV-ScanQA Multi-View Reasoning Evaluation and TripAlign Pre-training Dataset",
        "summary": "The advancement of 3D vision-language (3D VL) learning is hindered by several\nlimitations in existing 3D VL datasets: they rarely necessitate reasoning\nbeyond a close range of objects in single viewpoint, and annotations often link\ninstructions to single objects, missing richer contextual alignments between\nmultiple objects. This significantly curtails the development of models capable\nof deep, multi-view 3D scene understanding over distant objects. To address\nthese challenges, we introduce MV-ScanQA, a novel 3D question answering dataset\nwhere 68% of questions explicitly require integrating information from multiple\nviews (compared to less than 7% in existing datasets), thereby rigorously\ntesting multi-view compositional reasoning. To facilitate the training of\nmodels for such demanding scenarios, we present TripAlign dataset, a\nlarge-scale and low-cost 2D-3D-language pre-training corpus containing 1M <2D\nview, set of 3D objects, text> triplets that explicitly aligns groups of\ncontextually related objects with text, providing richer, view-grounded\nmulti-object multimodal alignment signals than previous single-object\nannotations. We further develop LEGO, a baseline method for the multi-view\nreasoning challenge in MV-ScanQA, transferring knowledge from pre-trained 2D\nLVLMs to 3D domain with TripAlign. Empirically, LEGO pre-trained on TripAlign\nachieves state-of-the-art performance not only on the proposed MV-ScanQA, but\nalso on existing benchmarks for 3D dense captioning and question answering.\nDatasets and code are available at\nhttps://matthewdm0816.github.io/tripalign-mvscanqa.",
        "url": "http://arxiv.org/abs/2508.11058v1",
        "published_date": "2025-08-14T20:35:59+00:00",
        "updated_date": "2025-08-14T20:35:59+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Wentao Mo",
            "Qingchao Chen",
            "Yuxin Peng",
            "Siyuan Huang",
            "Yang Liu"
        ],
        "ai_categories": [
            "Dataset",
            "Multimodality"
        ],
        "tldr": "The paper introduces MV-ScanQA, a dataset for 3D question answering that requires reasoning across multiple views, and TripAlign, a pre-training dataset for 2D-3D-language alignment. A baseline method called LEGO achieves state-of-the-art performance on various tasks.",
        "tldr_zh": "本文引入了MV-ScanQA数据集，用于3D问题回答，需要跨多个视角进行推理，以及TripAlign，一个用于2D-3D语言对齐的预训练数据集。一个名为LEGO的基线方法在各种任务上取得了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Thyme: Think Beyond Images",
        "summary": "Following OpenAI's introduction of the ``thinking with images'' concept,\nrecent efforts have explored stimulating the use of visual information in the\nreasoning process to enhance model performance in perception and reasoning\ntasks. However, to the best of our knowledge, no open-source work currently\noffers a feature set as rich as proprietary models (O3), which can perform\ndiverse image manipulations and simultaneously enhance logical reasoning\ncapabilities through code. In this paper, we make a preliminary attempt in this\ndirection by introducing Thyme (Think Beyond Images), a novel paradigm for\nenabling MLLMs to transcend existing ``think with images'' approaches by\nautonomously generating and executing diverse image processing and\ncomputational operations via executable code. This approach not only\nfacilitates a rich, on-the-fly set of image manipulations (e.g., cropping,\nrotation, contrast enhancement) but also allows for mathematical computations,\nall while maintaining high autonomy in deciding when and how to apply these\noperations. We activate this capability through a two-stage training strategy:\nan initial SFT on a curated dataset of 500K samples to teach code generation,\nfollowed by a RL phase to refine decision-making. For the RL stage, we manually\ncollect and design high-resolution question-answer pairs to increase the\nlearning difficulty, and we propose GRPO-ATS (Group Relative Policy\nOptimization with Adaptive Temperature Sampling), an algorithm that applies\ndistinct temperatures to text and code generation to balance reasoning\nexploration with code execution precision. We conduct extensive experimental\nanalysis and ablation studies. Comprehensive evaluations on nearly 20\nbenchmarks show that Thyme yields significant and consistent performance gains,\nparticularly in challenging high-resolution perception and complex reasoning\ntasks.",
        "url": "http://arxiv.org/abs/2508.11630v1",
        "published_date": "2025-08-15T17:59:49+00:00",
        "updated_date": "2025-08-15T17:59:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yi-Fan Zhang",
            "Xingyu Lu",
            "Shukang Yin",
            "Chaoyou Fu",
            "Wei Chen",
            "Xiao Hu",
            "Bin Wen",
            "Kaiyu Jiang",
            "Changyi Liu",
            "Tianke Zhang",
            "Haonan Fan",
            "Kaibing Chen",
            "Jiankang Chen",
            "Haojie Ding",
            "Kaiyu Tang",
            "Zhang Zhang",
            "Liang Wang",
            "Fan Yang",
            "Tingting Gao",
            "Guorui Zhou"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "GAN",
            "Other"
        ],
        "tldr": "The paper introduces Thyme, a novel approach to enable MLLMs to generate and execute diverse image processing and computational operations through executable code, showing significant performance gains in perception and reasoning tasks.",
        "tldr_zh": "本文介绍了一种新颖的方法Thyme，通过可执行代码使MLLMs生成和执行多样的图像处理和计算操作，在感知和推理任务中表现出显著的性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "LoRAtorio: An intrinsic approach to LoRA Skill Composition",
        "summary": "Low-Rank Adaptation (LoRA) has become a widely adopted technique in\ntext-to-image diffusion models, enabling the personalisation of visual concepts\nsuch as characters, styles, and objects. However, existing approaches struggle\nto effectively compose multiple LoRA adapters, particularly in open-ended\nsettings where the number and nature of required skills are not known in\nadvance. In this work, we present LoRAtorio, a novel train-free framework for\nmulti-LoRA composition that leverages intrinsic model behaviour. Our method is\nmotivated by two key observations: (1) LoRA adapters trained on narrow domains\nproduce denoised outputs that diverge from the base model, and (2) when\noperating out-of-distribution, LoRA outputs show behaviour closer to the base\nmodel than when conditioned in distribution. The balance between these two\nobservations allows for exceptional performance in the single LoRA scenario,\nwhich nevertheless deteriorates when multiple LoRAs are loaded. Our method\noperates in the latent space by dividing it into spatial patches and computing\ncosine similarity between each patch's predicted noise and that of the base\nmodel. These similarities are used to construct a spatially-aware weight\nmatrix, which guides a weighted aggregation of LoRA outputs. To address domain\ndrift, we further propose a modification to classifier-free guidance that\nincorporates the base model's unconditional score into the composition. We\nextend this formulation to a dynamic module selection setting, enabling\ninference-time selection of relevant LoRA adapters from a large pool. LoRAtorio\nachieves state-of-the-art performance, showing up to a 1.3% improvement in\nClipScore and a 72.43% win rate in GPT-4V pairwise evaluations, and generalises\neffectively to multiple latent diffusion models.",
        "url": "http://arxiv.org/abs/2508.11624v1",
        "published_date": "2025-08-15T17:52:56+00:00",
        "updated_date": "2025-08-15T17:52:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Niki Foteinopoulou",
            "Ignas Budvytis",
            "Stephan Liwicki"
        ],
        "ai_categories": [
            "LoRA",
            "Transformer",
            "Diffusion"
        ],
        "tldr": "The paper introduces LoRAtorio, a framework for composing multiple LoRA adapters for text-to-image diffusion models, achieving state-of-the-art performance and generalizing effectively to multiple latent diffusion models.",
        "tldr_zh": "本文介绍了LoRAtorio，一个用于合成多个LoRA适配器的框架，实现了最先进的性能，并有效地推广到多个潜在扩散模型。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DashCam Video: A complementary low-cost data stream for on-demand forest-infrastructure system monitoring",
        "summary": "Our study introduces a novel, low-cost, and reproducible framework for\nreal-time, object-level structural assessment and geolocation of roadside\nvegetation and infrastructure with commonly available but underutilized\ndashboard camera (dashcam) video data. We developed an end-to-end pipeline that\ncombines monocular depth estimation, depth error correction, and geometric\ntriangulation to generate accurate spatial and structural data from\nstreet-level video streams from vehicle-mounted dashcams. Depth maps were first\nestimated using a state-of-the-art monocular depth model, then refined via a\ngradient-boosted regression framework to correct underestimations, particularly\nfor distant objects. The depth correction model achieved strong predictive\nperformance (R2 = 0.92, MAE = 0.31 on transformed scale), significantly\nreducing bias beyond 15 m. Further, object locations were estimated using\nGPS-based triangulation, while object heights were calculated using pin hole\ncamera geometry. Our method was evaluated under varying conditions of camera\nplacement and vehicle speed. Low-speed vehicle with inside camera gave the\nhighest accuracy, with mean geolocation error of 2.83 m, and mean absolute\nerror (MAE) in height estimation of 2.09 m for trees and 0.88 m for poles. To\nthe best of our knowledge, it is the first framework to combine monocular depth\nmodeling, triangulated GPS-based geolocation, and real-time structural\nassessment for urban vegetation and infrastructure using consumer-grade video\ndata. Our approach complements conventional RS methods, such as LiDAR and image\nby offering a fast, real-time, and cost-effective solution for object-level\nmonitoring of vegetation risks and infrastructure exposure, making it\nespecially valuable for utility companies, and urban planners aiming for\nscalable and frequent assessments in dynamic urban environments.",
        "url": "http://arxiv.org/abs/2508.11591v1",
        "published_date": "2025-08-15T16:55:12+00:00",
        "updated_date": "2025-08-15T16:55:12+00:00",
        "categories": [
            "cs.CV",
            "cs.ET"
        ],
        "authors": [
            "Durga Joshi",
            "Chandi Witharana",
            "Robert Fahey",
            "Thomas Worthley",
            "Zhe Zhu",
            "Diego Cerrai"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a framework for real-time monitoring of roadside vegetation and infrastructure using dashcam videos, offering a low-cost and effective solution.",
        "tldr_zh": "本文介绍了一种利用车载摄像头视频进行道路边的植被和基础设施实时监测的框架，提供了一种低成本高效的解决方案。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Causality Matters: How Temporal Information Emerges in Video Language Models",
        "summary": "Video language models (VideoLMs) have made significant progress in multimodal\nunderstanding. However, temporal understanding, which involves identifying\nevent order, duration, and relationships across time, still remains a core\nchallenge. Prior works emphasize positional encodings (PEs) as a key mechanism\nfor encoding temporal structure. Surprisingly, we find that removing or\nmodifying PEs in video inputs yields minimal degradation in the performance of\ntemporal understanding. In contrast, reversing the frame sequence while\npreserving the original PEs causes a substantial drop. To explain this\nbehavior, we conduct substantial analysis experiments to trace how temporal\ninformation is integrated within the model. We uncover a causal information\npathway: temporal cues are progressively synthesized through inter-frame\nattention, aggregated in the final frame, and subsequently integrated into the\nquery tokens. This emergent mechanism shows that temporal reasoning emerges\nfrom inter-visual token interactions under the constraints of causal attention,\nwhich implicitly encodes temporal structure. Based on these insights, we\npropose two efficiency-oriented strategies: staged cross-modal attention and a\ntemporal exit mechanism for early token truncation. Experiments on two\nbenchmarks validate the effectiveness of both approaches. To the best of our\nknowledge, this is the first work to systematically investigate video temporal\nunderstanding in VideoLMs, offering insights for future model improvement.",
        "url": "http://arxiv.org/abs/2508.11576v1",
        "published_date": "2025-08-15T16:33:14+00:00",
        "updated_date": "2025-08-15T16:33:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yumeng Shi",
            "Quanyu Long",
            "Yin Wu",
            "Wenya Wang"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper explores how temporal information is integrated into video language models, proposing new efficiency-oriented strategies that improve performance.",
        "tldr_zh": "本文探讨了时间信息如何融入视频语言模型，并提出了提高性能的新的效率导向策略。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TrajSV: A Trajectory-based Model for Sports Video Representations and Applications",
        "summary": "Sports analytics has received significant attention from both academia and\nindustry in recent years. Despite the growing interest and efforts in this\nfield, several issues remain unresolved, including (1) data unavailability, (2)\nlack of an effective trajectory-based framework, and (3) requirement for\nsufficient supervision labels. In this paper, we present TrajSV, a\ntrajectory-based framework that addresses various issues in existing studies.\nTrajSV comprises three components: data preprocessing, Clip Representation\nNetwork (CRNet), and Video Representation Network (VRNet). The data\npreprocessing module extracts player and ball trajectories from sports\nbroadcast videos. CRNet utilizes a trajectory-enhanced Transformer module to\nlearn clip representations based on these trajectories. Additionally, VRNet\nlearns video representations by aggregating clip representations and visual\nfeatures with an encoder-decoder architecture. Finally, a triple contrastive\nloss is introduced to optimize both video and clip representations in an\nunsupervised manner. The experiments are conducted on three broadcast video\ndatasets to verify the effectiveness of TrajSV for three types of sports (i.e.,\nsoccer, basketball, and volleyball) with three downstream applications (i.e.,\nsports video retrieval, action spotting, and video captioning). The results\ndemonstrate that TrajSV achieves state-of-the-art performance in sports video\nretrieval, showcasing a nearly 70% improvement. It outperforms baselines in\naction spotting, achieving state-of-the-art results in 9 out of 17 action\ncategories, and demonstrates a nearly 20% improvement in video captioning.\nAdditionally, we introduce a deployed system along with the three applications\nbased on TrajSV.",
        "url": "http://arxiv.org/abs/2508.11569v1",
        "published_date": "2025-08-15T16:23:36+00:00",
        "updated_date": "2025-08-15T16:23:36+00:00",
        "categories": [
            "cs.CV",
            "cs.IR"
        ],
        "authors": [
            "Zheng Wang",
            "Shihao Xu",
            "Wei Shi"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces TrajSV, a trajectory-based framework for sports video analysis. It achieves state-of-the-art results in sports video retrieval, action spotting, and video captioning.",
        "tldr_zh": "本文介绍了TrajSV，一个基于轨迹的体育视频分析框架。它在体育视频检索、动作定位和视频字幕生成方面取得了最先进的结果。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Training-Free Anomaly Generation via Dual-Attention Enhancement in Diffusion Model",
        "summary": "Industrial anomaly detection (AD) plays a significant role in manufacturing\nwhere a long-standing challenge is data scarcity. A growing body of works have\nemerged to address insufficient anomaly data via anomaly generation. However,\nthese anomaly generation methods suffer from lack of fidelity or need to be\ntrained with extra data. To this end, we propose a training-free anomaly\ngeneration framework dubbed AAG, which is based on Stable Diffusion (SD)'s\nstrong generation ability for effective anomaly image generation. Given a\nnormal image, mask and a simple text prompt, AAG can generate realistic and\nnatural anomalies in the specific regions and simultaneously keep contents in\nother regions unchanged. In particular, we propose Cross-Attention Enhancement\n(CAE) to re-engineer the cross-attention mechanism within Stable Diffusion\nbased on the given mask. CAE increases the similarity between visual tokens in\nspecific regions and text embeddings, which guides these generated visual\ntokens in accordance with the text description. Besides, generated anomalies\nneed to be more natural and plausible with object in given image. We propose\nSelf-Attention Enhancement (SAE) which improves similarity between each normal\nvisual token and anomaly visual tokens. SAE ensures that generated anomalies\nare coherent with original pattern. Extensive experiments on MVTec AD and VisA\ndatasets demonstrate effectiveness of AAG in anomaly generation and its\nutility. Furthermore, anomaly images generated by AAG can bolster performance\nof various downstream anomaly inspection tasks.",
        "url": "http://arxiv.org/abs/2508.11550v1",
        "published_date": "2025-08-15T15:52:02+00:00",
        "updated_date": "2025-08-15T15:52:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zuo Zuo",
            "Jiahao Dong",
            "Yanyun Qu",
            "Zongze Wu"
        ],
        "ai_categories": [
            "AIGC",
            "Diffusion",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a training-free anomaly generation framework called AAG for creating realistic anomalies in images with specific regions highlighted and other regions unchanged, enhancing anomaly detection in industrial settings.",
        "tldr_zh": "该论文提出了一个名为AAG的无需训练的异常生成框架，可以在图像中生成逼真的异常，突出特定区域并保持其他区域不变，增强工业环境中的异常检测。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multi-State Tracker: Enhancing Efficient Object Tracking via Multi-State Specialization and Interaction",
        "summary": "Efficient trackers achieve faster runtime by reducing computational\ncomplexity and model parameters. However, this efficiency often compromises the\nexpense of weakened feature representation capacity, thus limiting their\nability to accurately capture target states using single-layer features. To\novercome this limitation, we propose Multi-State Tracker (MST), which utilizes\nhighly lightweight state-specific enhancement (SSE) to perform specialized\nenhancement on multi-state features produced by multi-state generation (MSG)\nand aggregates them in an interactive and adaptive manner using cross-state\ninteraction (CSI). This design greatly enhances feature representation while\nincurring minimal computational overhead, leading to improved tracking\nrobustness in complex environments. Specifically, the MSG generates multiple\nstate representations at multiple stages during feature extraction, while SSE\nrefines them to highlight target-specific features. The CSI module facilitates\ninformation exchange between these states and ensures the integration of\ncomplementary features. Notably, the introduced SSE and CSI modules adopt a\nhighly lightweight hidden state adaptation-based state space duality (HSA-SSD)\ndesign, incurring only 0.1 GFLOPs in computation and 0.66 M in parameters.\nExperimental results demonstrate that MST outperforms all previous efficient\ntrackers across multiple datasets, significantly improving tracking accuracy\nand robustness. In particular, it shows excellent runtime performance, with an\nAO score improvement of 4.5\\% over the previous SOTA efficient tracker HCAT on\nthe GOT-10K dataset. The code is available at https://github.com/wsumel/MST.",
        "url": "http://arxiv.org/abs/2508.11531v1",
        "published_date": "2025-08-15T15:19:39+00:00",
        "updated_date": "2025-08-15T15:19:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shilei Wang",
            "Gong Cheng",
            "Pujian Lai",
            "Dong Gao",
            "Junwei Han"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces the Multi-State Tracker (MST) for efficient object tracking using state-specific enhancement and cross-state interaction, outperforming previous trackers in accuracy and runtime performance.",
        "tldr_zh": "本文介绍了 Multi-State Tracker（MST），通过状态特定增强和跨状态交互进行高效物体跟踪，在准确性和运行时性能方面优于先前的跟踪器。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Semi-Supervised Learning with Online Knowledge Distillation for Skin Lesion Classification",
        "summary": "Deep Learning has emerged as a promising approach for skin lesion analysis.\nHowever, existing methods mostly rely on fully supervised learning, requiring\nextensive labeled data, which is challenging and costly to obtain. To alleviate\nthis annotation burden, this study introduces a novel semi-supervised deep\nlearning approach that integrates ensemble learning with online knowledge\ndistillation for enhanced skin lesion classification. Our methodology involves\ntraining an ensemble of convolutional neural network models, using online\nknowledge distillation to transfer insights from the ensemble to its members.\nThis process aims to enhance the performance of each model within the ensemble,\nthereby elevating the overall performance of the ensemble itself.\nPost-training, any individual model within the ensemble can be deployed at test\ntime, as each member is trained to deliver comparable performance to the\nensemble. This is particularly beneficial in resource-constrained environments.\nExperimental results demonstrate that the knowledge-distilled individual model\nperforms better than independently trained models. Our approach demonstrates\nsuperior performance on both the \\emph{International Skin Imaging\nCollaboration} 2018 and 2019 public benchmark datasets, surpassing current\nstate-of-the-art results. By leveraging ensemble learning and online knowledge\ndistillation, our method reduces the need for extensive labeled data while\nproviding a more resource-efficient solution for skin lesion classification in\nreal-world scenarios.",
        "url": "http://arxiv.org/abs/2508.11511v1",
        "published_date": "2025-08-15T14:40:48+00:00",
        "updated_date": "2025-08-15T14:40:48+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Siyamalan Manivannan"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces a semi-supervised deep learning approach using ensemble learning and online knowledge distillation for skin lesion classification, achieving superior performance on public benchmark datasets.",
        "tldr_zh": "本文介绍了一种半监督深度学习方法，利用集成学习和在线知识蒸馏进行皮肤病变分类，在公共基准数据集上取得了卓越的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AIM: Amending Inherent Interpretability via Self-Supervised Masking",
        "summary": "It has been observed that deep neural networks (DNNs) often use both genuine\nas well as spurious features. In this work, we propose \"Amending Inherent\nInterpretability via Self-Supervised Masking\" (AIM), a simple yet interestingly\neffective method that promotes the network's utilization of genuine features\nover spurious alternatives without requiring additional annotations. In\nparticular, AIM uses features at multiple encoding stages to guide a\nself-supervised, sample-specific feature-masking process. As a result, AIM\nenables the training of well-performing and inherently interpretable models\nthat faithfully summarize the decision process. We validate AIM across a\ndiverse range of challenging datasets that test both out-of-distribution\ngeneralization and fine-grained visual understanding. These include\ngeneral-purpose classification benchmarks such as ImageNet100, HardImageNet,\nand ImageWoof, as well as fine-grained classification datasets such as\nWaterbirds, TravelingBirds, and CUB-200. AIM demonstrates significant dual\nbenefits: interpretability improvements, as measured by the Energy Pointing\nGame (EPG) score, and accuracy gains over strong baselines. These consistent\ngains across domains and architectures provide compelling evidence that AIM\npromotes the use of genuine and meaningful features that directly contribute to\nimproved generalization and human-aligned interpretability.",
        "url": "http://arxiv.org/abs/2508.11502v1",
        "published_date": "2025-08-15T14:29:59+00:00",
        "updated_date": "2025-08-15T14:29:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Eyad Alshami",
            "Shashank Agnihotri",
            "Bernt Schiele",
            "Margret Keuper"
        ],
        "ai_categories": [
            "Other"
        ],
        "tldr": "The paper introduces AIM, a method that enhances interpretability in deep neural networks by promoting the use of genuine features over spurious ones. It demonstrates improved interpretability and accuracy on various datasets.",
        "tldr_zh": "该论文提出了AIM方法，通过促进深度神经网络对真实特征的使用而非虚假特征，提高了解释性能。它在各种数据集上展示了提高的解释性能和准确性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Hierarchical Graph Feature Enhancement with Adaptive Frequency Modulation for Visual Recognition",
        "summary": "Convolutional neural networks (CNNs) have\n  demonstrated strong performance in visual recognition tasks,\n  but their inherent reliance on regular grid structures limits\n  their capacity to model complex topological relationships and\n  non-local semantics within images. To address this limita tion, we propose\nthe hierarchical graph feature enhancement\n  (HGFE), a novel framework that integrates graph-based rea soning into CNNs to\nenhance both structural awareness and\n  feature representation. HGFE builds two complementary levels\n  of graph structures: intra-window graph convolution to cap ture local spatial\ndependencies and inter-window supernode\n  interactions to model global semantic relationships. Moreover,\n  we introduce an adaptive frequency modulation module that\n  dynamically balances low-frequency and high-frequency signal\n  propagation, preserving critical edge and texture information\n  while mitigating over-smoothing. The proposed HGFE module\n  is lightweight, end-to-end trainable, and can be seamlessly\n  integrated into standard CNN backbone networks. Extensive\n  experiments on CIFAR-100 (classification), PASCAL VOC,\n  and VisDrone (detection), as well as CrackSeg and CarParts\n  (segmentation), validated the effectiveness of the HGFE in\n  improving structural representation and enhancing overall\n  recognition performance.",
        "url": "http://arxiv.org/abs/2508.11497v1",
        "published_date": "2025-08-15T14:19:50+00:00",
        "updated_date": "2025-08-15T14:19:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Feiyue Zhao",
            "Zhichao Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a framework called Hierarchical Graph Feature Enhancement (HGFE) that integrates graph-based reasoning into CNNs to improve structural awareness and feature representation in visual recognition tasks.",
        "tldr_zh": "本文提出了一种名为分层图特征增强（HGFE）的框架，将基于图的推理集成到CNN中，以提高视觉识别任务中的结构意识和特征表示。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TACR-YOLO: A Real-time Detection Framework for Abnormal Human Behaviors Enhanced with Coordinate and Task-Aware Representations",
        "summary": "Abnormal Human Behavior Detection (AHBD) under special scenarios is becoming\nincreasingly crucial. While YOLO-based detection methods excel in real-time\ntasks, they remain hindered by challenges including small objects, task\nconflicts, and multi-scale fusion in AHBD. To tackle them, we propose\nTACR-YOLO, a new real-time framework for AHBD. We introduce a Coordinate\nAttention Module to enhance small object detection, a Task-Aware Attention\nModule to deal with classification-regression conflicts, and a Strengthen Neck\nNetwork for refined multi-scale fusion, respectively. In addition, we optimize\nAnchor Box sizes using K-means clustering and deploy DIoU-Loss to improve\nbounding box regression. The Personnel Anomalous Behavior Detection (PABD)\ndataset, which includes 8,529 samples across four behavior categories, is also\npresented. Extensive experimental results indicate that TACR-YOLO achieves\n91.92% mAP on PABD, with competitive speed and robustness. Ablation studies\nhighlight the contribution of each improvement. This work provides new insights\nfor abnormal behavior detection under special scenarios, advancing its\nprogress.",
        "url": "http://arxiv.org/abs/2508.11478v1",
        "published_date": "2025-08-15T13:45:21+00:00",
        "updated_date": "2025-08-15T13:45:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinyi Yin",
            "Wenbo Yuan",
            "Xuecheng Wu",
            "Liangyu Fu",
            "Danlei Huang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "TACR-YOLO is a real-time detection framework designed to detect abnormal human behaviors, overcoming challenges like small objects, task conflicts, and multi-scale fusion through novel modules and optimizations. It achieves high accuracy on a specialized dataset.",
        "tldr_zh": "TACR-YOLO是一个实时检测框架，旨在检测异常人类行为，在小物体、任务冲突和多尺度融合等挑战上有所突破。它在专用数据集上取得了较高的准确率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SPG: Style-Prompting Guidance for Style-Specific Content Creation",
        "summary": "Although recent text-to-image (T2I) diffusion models excel at aligning\ngenerated images with textual prompts, controlling the visual style of the\noutput remains a challenging task. In this work, we propose Style-Prompting\nGuidance (SPG), a novel sampling strategy for style-specific image generation.\nSPG constructs a style noise vector and leverages its directional deviation\nfrom unconditional noise to guide the diffusion process toward the target style\ndistribution. By integrating SPG with Classifier-Free Guidance (CFG), our\nmethod achieves both semantic fidelity and style consistency. SPG is simple,\nrobust, and compatible with controllable frameworks like ControlNet and\nIPAdapter, making it practical and widely applicable. Extensive experiments\ndemonstrate the effectiveness and generality of our approach compared to\nstate-of-the-art methods. Code is available at\nhttps://github.com/Rumbling281441/SPG.",
        "url": "http://arxiv.org/abs/2508.11476v1",
        "published_date": "2025-08-15T13:44:56+00:00",
        "updated_date": "2025-08-15T13:44:56+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Qian Liang",
            "Zichong Chen",
            "Yang Zhou",
            "Hui Huang"
        ],
        "ai_categories": [
            "Diffusion"
        ],
        "tldr": "The paper introduces a new method called SPG for generating style-specific images by guiding the diffusion process with a style noise vector. It achieves semantic fidelity and style consistency while being simple, robust, and compatible with various frameworks.",
        "tldr_zh": "本文介绍了一种名为SPG的新方法，通过引导扩散过程使用样式噪声向量生成风格特定的图像。它在保持语义准确性和风格一致性的同时简单、稳健，并与各种框架兼容。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving",
        "summary": "Autonomous driving requires rich contextual comprehension and precise\npredictive reasoning to navigate dynamic and complex environments safely.\nVision-Language Models (VLMs) and Driving World Models (DWMs) have\nindependently emerged as powerful recipes addressing different aspects of this\nchallenge. VLMs provide interpretability and robust action prediction through\ntheir ability to understand multi-modal context, while DWMs excel in generating\ndetailed and plausible future driving scenarios essential for proactive\nplanning. Integrating VLMs with DWMs is an intuitive, promising, yet\nunderstudied strategy to exploit the complementary strengths of accurate\nbehavioral prediction and realistic scene generation. Nevertheless, this\nintegration presents notable challenges, particularly in effectively connecting\naction-level decisions with high-fidelity pixel-level predictions and\nmaintaining computational efficiency. In this paper, we propose ImagiDrive, a\nnovel end-to-end autonomous driving framework that integrates a VLM-based\ndriving agent with a DWM-based scene imaginer to form a unified\nimagination-and-planning loop. The driving agent predicts initial driving\ntrajectories based on multi-modal inputs, guiding the scene imaginer to\ngenerate corresponding future scenarios. These imagined scenarios are\nsubsequently utilized to iteratively refine the driving agent's planning\ndecisions. To address efficiency and predictive accuracy challenges inherent in\nthis integration, we introduce an early stopping mechanism and a trajectory\nselection strategy. Extensive experimental validation on the nuScenes and\nNAVSIM datasets demonstrates the robustness and superiority of ImagiDrive over\nprevious alternatives under both open-loop and closed-loop conditions.",
        "url": "http://arxiv.org/abs/2508.11428v1",
        "published_date": "2025-08-15T12:06:55+00:00",
        "updated_date": "2025-08-15T12:06:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingyu Li",
            "Bozhou Zhang",
            "Xin Jin",
            "Jiankang Deng",
            "Xiatian Zhu",
            "Li Zhang"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "ImagiDrive proposes an integrated framework combining Vision-Language Models and Driving World Models for autonomous driving, demonstrating improved performance over existing methods.",
        "tldr_zh": "ImagiDrive提出了一种集成框架，结合视觉-语言模型和驾驶世界模型，用于自动驾驶，在现有方法上表现出更好的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SelfAdapt: Unsupervised Domain Adaptation of Cell Segmentation Models",
        "summary": "Deep neural networks have become the go-to method for biomedical instance\nsegmentation. Generalist models like Cellpose demonstrate state-of-the-art\nperformance across diverse cellular data, though their effectiveness often\ndegrades on domains that differ from their training data. While supervised\nfine-tuning can address this limitation, it requires annotated data that may\nnot be readily available. We propose SelfAdapt, a method that enables the\nadaptation of pre-trained cell segmentation models without the need for labels.\nOur approach builds upon student-teacher augmentation consistency training,\nintroducing L2-SP regularization and label-free stopping criteria. We evaluate\nour method on the LiveCell and TissueNet datasets, demonstrating relative\nimprovements in AP0.5 of up to 29.64% over baseline Cellpose. Additionally, we\nshow that our unsupervised adaptation can further improve models that were\npreviously fine-tuned with supervision. We release SelfAdapt as an easy-to-use\nextension of the Cellpose framework. The code for our method is publicly\navailable at https: //github.com/Kainmueller-Lab/self_adapt.",
        "url": "http://arxiv.org/abs/2508.11411v1",
        "published_date": "2025-08-15T11:31:48+00:00",
        "updated_date": "2025-08-15T11:31:48+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Fabian H. Reith",
            "Jannik Franzen",
            "Dinesh R. Palli",
            "J. Lorenz Rumberger",
            "Dagmar Kainmueller"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "SelfAdapt proposes a method for unsupervised domain adaptation of cell segmentation models, improving performance without the need for labeled data.",
        "tldr_zh": "SelfAdapt提出了一种用于细胞分割模型的无监督域自适应方法，提高性能而无需标签数据。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator",
        "summary": "Atmospheric turbulence severely degrades video quality by introducing\ndistortions such as geometric warping, blur, and temporal flickering, posing\nsignificant challenges to both visual clarity and temporal consistency. Current\nstate-of-the-art methods are based on transformer and 3D architectures and\nrequire multi-frame input, but their large computational cost and memory usage\nlimit real-time deployment, especially in resource-constrained scenarios. In\nthis work, we propose RMFAT: Recurrent Multi-scale Feature Atmospheric\nTurbulence Mitigator, designed for efficient and temporally consistent video\nrestoration under AT conditions. RMFAT adopts a lightweight recurrent framework\nthat restores each frame using only two inputs at a time, significantly\nreducing temporal window size and computational burden. It further integrates\nmulti-scale feature encoding and decoding with temporal warping modules at both\nencoder and decoder stages to enhance spatial detail and temporal coherence.\nExtensive experiments on synthetic and real-world atmospheric turbulence\ndatasets demonstrate that RMFAT not only outperforms existing methods in terms\nof clarity restoration (with nearly a 9\\% improvement in SSIM) but also\nachieves significantly improved inference speed (more than a fourfold reduction\nin runtime), making it particularly suitable for real-time atmospheric\nturbulence suppression tasks.",
        "url": "http://arxiv.org/abs/2508.11409v1",
        "published_date": "2025-08-15T11:20:18+00:00",
        "updated_date": "2025-08-15T11:20:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhiming Liu",
            "Nantheera Anantrasirichai"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "RMFAT is a lightweight recurrent framework that efficiently mitigates atmospheric turbulence in videos, outperforming existing methods in clarity restoration and runtime speed.",
        "tldr_zh": "RMFAT是一种轻量级循环框架，有效地减轻视频中的大气湍流，优于现有方法在清晰度恢复和运行时速度方面。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LKFMixer: Exploring Large Kernel Feature For Efficient Image Super-Resolution",
        "summary": "The success of self-attention (SA) in Transformer demonstrates the importance\nof non-local information to image super-resolution (SR), but the huge computing\npower required makes it difficult to implement lightweight models. To solve\nthis problem, we propose a pure convolutional neural network (CNN) model,\nLKFMixer, which utilizes large convolutional kernel to simulate the ability of\nself-attention to capture non-local features. Specifically, we increase the\nkernel size to 31 to obtain the larger receptive field as possible, and reduce\nthe parameters and computations by coordinate decomposition. Meanwhile, a\nspatial feature modulation block (SFMB) is designed to enhance the focus of\nfeature information on both spatial and channel dimension. In addition, by\nintroducing feature selection block (FSB), the model can adaptively adjust the\nweights between local features and non-local features. Extensive experiments\nshow that the proposed LKFMixer family outperform other state-of-the-art (SOTA)\nmethods in terms of SR performance and reconstruction quality. In particular,\ncompared with SwinIR-light on Manga109 dataset, LKFMixer-L achieves 0.6dB PSNR\nimprovement at $\\times$4 scale, while the inference speed is $\\times$5 times\nfaster. The code is available at https://github.com/Supereeeee/LKFMixer.",
        "url": "http://arxiv.org/abs/2508.11391v1",
        "published_date": "2025-08-15T10:50:38+00:00",
        "updated_date": "2025-08-15T10:50:38+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Yinggan Tang",
            "Quanwei Hu"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes LKFMixer, a pure CNN model for image super-resolution that utilizes large convolutional kernels to capture non-local features. It outperforms state-of-the-art methods in terms of performance and reconstruction quality.",
        "tldr_zh": "本文提出了LKFMixer，一个纯CNN模型，用于图像超分辨率，利用大卷积核捕获非局部特征。在性能和重建质量方面优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration",
        "summary": "We introduce G-CUT3R, a novel feed-forward approach for guided 3D scene\nreconstruction that enhances the CUT3R model by integrating prior information.\nUnlike existing feed-forward methods that rely solely on input images, our\nmethod leverages auxiliary data, such as depth, camera calibrations, or camera\npositions, commonly available in real-world scenarios. We propose a lightweight\nmodification to CUT3R, incorporating a dedicated encoder for each modality to\nextract features, which are fused with RGB image tokens via zero convolution.\nThis flexible design enables seamless integration of any combination of prior\ninformation during inference. Evaluated across multiple benchmarks, including\n3D reconstruction and other multi-view tasks, our approach demonstrates\nsignificant performance improvements, showing its ability to effectively\nutilize available priors while maintaining compatibility with varying input\nmodalities.",
        "url": "http://arxiv.org/abs/2508.11379v1",
        "published_date": "2025-08-15T10:25:58+00:00",
        "updated_date": "2025-08-15T10:25:58+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ramil Khafizov",
            "Artem Komarichev",
            "Ruslan Rakhimov",
            "Peter Wonka",
            "Evgeny Burnaev"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "G-CUT3R is a novel approach for 3D scene reconstruction that incorporates prior information like depth and camera calibrations, leading to improved performance across various benchmarks.",
        "tldr_zh": "G-CUT3R是一种新颖的3D场景重建方法，整合了深度和相机校准等先验信息，从而在各种基准测试中实现了性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Unified Knowledge Distillation Framework: Fine-Grained Alignment and Geometric Relationship Preservation for Deep Face Recognition",
        "summary": "Knowledge Distillation is crucial for optimizing face recognition models for\ndeployment in computationally limited settings, such as edge devices.\nTraditional KD methods, such as Raw L2 Feature Distillation or Feature\nConsistency loss, often fail to capture both fine-grained instance-level\ndetails and complex relational structures, leading to suboptimal performance.\nWe propose a unified approach that integrates two novel loss functions,\nInstance-Level Embedding Distillation and Relation-Based Pairwise Similarity\nDistillation. Instance-Level Embedding Distillation focuses on aligning\nindividual feature embeddings by leveraging a dynamic hard mining strategy,\nthereby enhancing learning from challenging examples. Relation-Based Pairwise\nSimilarity Distillation captures relational information through pairwise\nsimilarity relationships, employing a memory bank mechanism and a sample mining\nstrategy. This unified framework ensures both effective instance-level\nalignment and preservation of geometric relationships between samples, leading\nto a more comprehensive distillation process. Our unified framework outperforms\nstate-of-the-art distillation methods across multiple benchmark face\nrecognition datasets, as demonstrated by extensive experimental evaluations.\nInterestingly, when using strong teacher networks compared to the student, our\nunified KD enables the student to even surpass the teacher's accuracy.",
        "url": "http://arxiv.org/abs/2508.11376v1",
        "published_date": "2025-08-15T10:20:29+00:00",
        "updated_date": "2025-08-15T10:20:29+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Durgesh Mishra",
            "Rishabh Uikey"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a unified knowledge distillation framework for deep face recognition that captures fine-grained details and complex relational structures, outperforming state-of-the-art methods.",
        "tldr_zh": "本文提出了一个统一的知识蒸馏框架，用于深度人脸识别，可以捕捉细粒度的细节和复杂的关系结构，表现优于最先进的方法。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "AnatoMaskGAN: GNN-Driven Slice Feature Fusion and Noise Augmentation for Medical Semantic Image Synthesis",
        "summary": "Medical semantic-mask synthesis boosts data augmentation and analysis, yet\nmost GAN-based approaches still produce one-to-one images and lack spatial\nconsistency in complex scans. To address this, we propose AnatoMaskGAN, a novel\nsynthesis framework that embeds slice-related spatial features to precisely\naggregate inter-slice contextual dependencies, introduces diverse\nimage-augmentation strategies, and optimizes deep feature learning to improve\nperformance on complex medical images. Specifically, we design a GNN-based\nstrongly correlated slice-feature fusion module to model spatial relationships\nbetween slices and integrate contextual information from neighboring slices,\nthereby capturing anatomical details more comprehensively; we introduce a\nthree-dimensional spatial noise-injection strategy that weights and fuses\nspatial features with noise to enhance modeling of structural diversity; and we\nincorporate a grayscale-texture classifier to optimize grayscale distribution\nand texture representation during generation. Extensive experiments on the\npublic L2R-OASIS and L2R-Abdomen CT datasets show that AnatoMaskGAN raises PSNR\non L2R-OASIS to 26.50 dB (0.43 dB higher than the current state of the art) and\nachieves an SSIM of 0.8602 on L2R-Abdomen CT--a 0.48 percentage-point gain over\nthe best model, demonstrating its superiority in reconstruction accuracy and\nperceptual quality. Ablation studies that successively remove the slice-feature\nfusion module, spatial 3D noise-injection strategy, and grayscale-texture\nclassifier reveal that each component contributes significantly to PSNR, SSIM,\nand LPIPS, further confirming the independent value of each core design in\nenhancing reconstruction accuracy and perceptual quality.",
        "url": "http://arxiv.org/abs/2508.11375v1",
        "published_date": "2025-08-15T10:19:38+00:00",
        "updated_date": "2025-08-15T10:19:38+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "I.4.9"
        ],
        "authors": [
            "Zonglin Wu",
            "Yule Xue",
            "Qianxiang Hu",
            "Yaoyao Feng",
            "Yuqi Ma",
            "Shanxiong Chen"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "AnatoMaskGAN is a novel synthesis framework for medical semantic image synthesis that integrates slice-related spatial features, noise augmentation, and deep feature learning to improve performance on complex medical images.",
        "tldr_zh": "AnatoMaskGAN是一种新颖的合成框架，用于医学语义图像合成，整合了与切片相关的空间特征、噪声增强和深层特征学习，以提高复杂医学图像的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model",
        "summary": "Understanding and recognizing human-object interaction (HOI) is a pivotal\napplication in AR/VR and robotics. Recent open-vocabulary HOI detection\napproaches depend exclusively on large language models for richer textual\nprompts, neglecting their inherent 3D spatial understanding capabilities. To\naddress this shortcoming, we introduce HOID-R1, the first HOI detection\nframework that integrates chain-of-thought (CoT) guided supervised fine-tuning\n(SFT) with group relative policy optimization (GRPO) within a reinforcement\nlearning (RL) paradigm. Specifically, we initially apply SFT to imbue the model\nwith essential reasoning capabilities, forcing the model to articulate its\nthought process in the output. Subsequently, we integrate GRPO to leverage\nmulti-reward signals for policy optimization, thereby enhancing alignment\nacross diverse modalities. To mitigate hallucinations in the CoT reasoning, we\nintroduce an \"MLLM-as-a-judge\" mechanism that supervises the CoT outputs,\nfurther improving generalization. Extensive experiments show that HOID-R1\nachieves state-of-the-art performance on HOI detection benchmarks and\noutperforms existing methods in open-world generalization to novel scenarios.",
        "url": "http://arxiv.org/abs/2508.11350v1",
        "published_date": "2025-08-15T09:28:57+00:00",
        "updated_date": "2025-08-15T09:28:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhenhao Zhang",
            "Hanqing Wang",
            "Xiangyu Zeng",
            "Ziyu Cheng",
            "Jiaxin Liu",
            "Haoyu Yan",
            "Zhirui Liu",
            "Kaiyang Ji",
            "Tianxiang Gui",
            "Ke Hu",
            "Kangyi Chen",
            "Yahao Fan",
            "Mokai Pan"
        ],
        "ai_categories": [
            "Multimodality",
            "AIGC"
        ],
        "tldr": "HOID-R1 is a reinforcement learning framework that integrates supervised fine-tuning and policy optimization to improve human-object interaction detection performance.",
        "tldr_zh": "HOID-R1是一个强化学习框架，整合了监督微调和策略优化，以提高人物-物体交互检测性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Semantically Guided Adversarial Testing of Vision Models Using Language Models",
        "summary": "In targeted adversarial attacks on vision models, the selection of the target\nlabel is a critical yet often overlooked determinant of attack success. This\ntarget label corresponds to the class that the attacker aims to force the model\nto predict. Now, existing strategies typically rely on randomness, model\npredictions, or static semantic resources, limiting interpretability,\nreproducibility, or flexibility. This paper then proposes a semantics-guided\nframework for adversarial target selection using the cross-modal knowledge\ntransfer from pretrained language and vision-language models. We evaluate\nseveral state-of-the-art models (BERT, TinyLLAMA, and CLIP) as similarity\nsources to select the most and least semantically related labels with respect\nto the ground truth, forming best- and worst-case adversarial scenarios. Our\nexperiments on three vision models and five attack methods reveal that these\nmodels consistently render practical adversarial targets and surpass static\nlexical databases, such as WordNet, particularly for distant class\nrelationships. We also observe that static testing of target labels offers a\npreliminary assessment of the effectiveness of similarity sources, \\textit{a\npriori} testing. Our results corroborate the suitability of pretrained models\nfor constructing interpretable, standardized, and scalable adversarial\nbenchmarks across architectures and datasets.",
        "url": "http://arxiv.org/abs/2508.11341v1",
        "published_date": "2025-08-15T09:11:22+00:00",
        "updated_date": "2025-08-15T09:11:22+00:00",
        "categories": [
            "cs.CV",
            "cs.CR",
            "cs.LG",
            "68T45, 68T01, 68T07, 68T10, 68M25",
            "I.2.10; I.5.4; I.2.6; I.2.7; K.6.5"
        ],
        "authors": [
            "Katarzyna Filus",
            "Jorge M. Cruz-Duarte"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a semantics-guided framework for selecting targets in adversarial attacks on vision models using pretrained language and vision-language models.",
        "tldr_zh": "本文介绍了一种使用预训练语言和视觉-语言模型选择目标的语义引导框架，用于对视觉模型进行对抗攻击。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Guiding WaveMamba with Frequency Maps for Image Debanding",
        "summary": "Compression at low bitrates in modern codecs often introduces banding\nartifacts, especially in smooth regions such as skies. These artifacts degrade\nvisual quality and are common in user-generated content due to repeated\ntranscoding. We propose a banding restoration method that employs the Wavelet\nState Space Model and a frequency masking map to preserve high-frequency\ndetails. Furthermore, we provide a benchmark of open-source banding restoration\nmethods and evaluate their performance on two public banding image datasets.\nExperimentation on the available datasets suggests that the proposed\npost-processing approach effectively suppresses banding compared to the\nstate-of-the-art method (a DBI value of 0.082 on BAND-2k) while preserving\nimage textures. Visual inspections of the results confirm this. Code and\nsupplementary material are available at:\nhttps://github.com/xinyiW915/Debanding-PCS2025.",
        "url": "http://arxiv.org/abs/2508.11331v1",
        "published_date": "2025-08-15T09:03:40+00:00",
        "updated_date": "2025-08-15T09:03:40+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Xinyi Wang",
            "Smaranda Tasmoc",
            "Nantheera Anantrasirichai",
            "Angeliki Katsenou"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "The paper presents a banding restoration method using Wavelet State Space Model and frequency maps to reduce compression artifacts in images. It outperforms state-of-the-art methods in suppressing banding while preserving image textures.",
        "tldr_zh": "本文提出了一种使用小波状态空间模型和频率映射的去条带复原方法，以减少图像的压缩伪影。相比于现有方法，它在抑制条带效果方面表现更佳，并保留了图像的纹理。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Denoise-then-Retrieve: Text-Conditioned Video Denoising for Video Moment Retrieval",
        "summary": "Current text-driven Video Moment Retrieval (VMR) methods encode all video\nclips, including irrelevant ones, disrupting multimodal alignment and hindering\noptimization. To this end, we propose a denoise-then-retrieve paradigm that\nexplicitly filters text-irrelevant clips from videos and then retrieves the\ntarget moment using purified multimodal representations. Following this\nparadigm, we introduce the Denoise-then-Retrieve Network (DRNet), comprising\nText-Conditioned Denoising (TCD) and Text-Reconstruction Feedback (TRF)\nmodules. TCD integrates cross-attention and structured state space blocks to\ndynamically identify noisy clips and produce a noise mask to purify multimodal\nvideo representations. TRF further distills a single query embedding from\npurified video representations and aligns it with the text embedding, serving\nas auxiliary supervision for denoising during training. Finally, we perform\nconditional retrieval using text embeddings on purified video representations\nfor accurate VMR. Experiments on Charades-STA and QVHighlights demonstrate that\nour approach surpasses state-of-the-art methods on all metrics. Furthermore,\nour denoise-then-retrieve paradigm is adaptable and can be seamlessly\nintegrated into advanced VMR models to boost performance.",
        "url": "http://arxiv.org/abs/2508.11313v1",
        "published_date": "2025-08-15T08:34:05+00:00",
        "updated_date": "2025-08-15T08:34:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weijia Liu",
            "Jiuxin Cao",
            "Bo Miao",
            "Zhiheng Fu",
            "Xuelin Zhu",
            "Jiawei Ge",
            "Bo Liu",
            "Mehwish Nasim",
            "Ajmal Mian"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a denoise-then-retrieve paradigm for text-driven Video Moment Retrieval, achieving state-of-the-art results by filtering irrelevant video clips and enhancing multimodal alignment.",
        "tldr_zh": "本文介绍了一种去噪再检索的范式，用于基于文本的视频时刻检索，通过过滤不相关的视频片段和增强多模态对齐，实现了领先水平的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Allen: Rethinking MAS Design through Step-Level Policy Autonomy",
        "summary": "We introduce a new Multi-Agent System (MAS) - Allen, designed to address two\ncore challenges in current MAS design: (1) improve system's policy autonomy,\nempowering agents to dynamically adapt their behavioral strategies, and (2)\nachieving the trade-off between collaborative efficiency, task supervision, and\nhuman oversight in complex network topologies.\n  Our core insight is to redefine the basic execution unit in the MAS, allowing\nagents to autonomously form different patterns by combining these units. We\nhave constructed a four-tier state architecture (Task, Stage, Agent, Step) to\nconstrain system behavior from both task-oriented and execution-oriented\nperspectives. This achieves a unification of topological optimization and\ncontrollable progress.\n  Allen grants unprecedented Policy Autonomy, while making a trade-off for the\ncontrollability of the collaborative structure. The project code has been open\nsource at: https://github.com/motern88/Allen",
        "url": "http://arxiv.org/abs/2508.11294v1",
        "published_date": "2025-08-15T08:02:34+00:00",
        "updated_date": "2025-08-15T08:02:34+00:00",
        "categories": [
            "cs.MA",
            "cs.CV"
        ],
        "authors": [
            "Qiangong Zhou",
            "Zhiting Wang",
            "Mingyou Yao",
            "Zongyang Liu"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality"
        ],
        "tldr": "The paper introduces a new Multi-Agent System (MAS) called Allen, which aims to improve policy autonomy of agents in complex network topologies.",
        "tldr_zh": "该论文介绍了一种名为Allen的新型多Agent系统(MAS)，旨在改善复杂网络拓扑中代理的策略自主性。",
        "relevance_score": 1,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "TimeMachine: Fine-Grained Facial Age Editing with Identity Preservation",
        "summary": "With the advancement of generative models, facial image editing has made\nsignificant progress. However, achieving fine-grained age editing while\npreserving personal identity remains a challenging task.In this paper, we\npropose TimeMachine, a novel diffusion-based framework that achieves accurate\nage editing while keeping identity features unchanged. To enable fine-grained\nage editing, we inject high-precision age information into the multi-cross\nattention module, which explicitly separates age-related and identity-related\nfeatures. This design facilitates more accurate disentanglement of age\nattributes, thereby allowing precise and controllable manipulation of facial\naging.Furthermore, we propose an Age Classifier Guidance (ACG) module that\npredicts age directly in the latent space, instead of performing denoising\nimage reconstruction during training. By employing a lightweight module to\nincorporate age constraints, this design enhances age editing accuracy by\nmodest increasing training cost. Additionally, to address the lack of\nlarge-scale, high-quality facial age datasets, we construct a HFFA dataset\n(High-quality Fine-grained Facial-Age dataset) which contains one million\nhigh-resolution images labeled with identity and facial attributes.\nExperimental results demonstrate that TimeMachine achieves state-of-the-art\nperformance in fine-grained age editing while preserving identity consistency.",
        "url": "http://arxiv.org/abs/2508.11284v1",
        "published_date": "2025-08-15T07:46:37+00:00",
        "updated_date": "2025-08-15T07:46:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yilin Mi",
            "Qixin Yan",
            "Zheng-Peng Duan",
            "Chunle Guo",
            "Hubery Yin",
            "Hao Liu",
            "Chen Li",
            "Chongyi Li"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces TimeMachine, a framework for fine-grained facial age editing while preserving personal identity, achieving state-of-the-art performance.",
        "tldr_zh": "该论文介绍了TimeMachine框架，用于精细的面部年龄编辑，同时保持个人身份，实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Probing the Representational Power of Sparse Autoencoders in Vision Models",
        "summary": "Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting\nthe hidden states of large language models (LLMs). By learning to reconstruct\nactivations from a sparse bottleneck layer, SAEs discover interpretable\nfeatures from the high-dimensional internal representations of LLMs. Despite\ntheir popularity with language models, SAEs remain understudied in the visual\ndomain. In this work, we provide an extensive evaluation the representational\npower of SAEs for vision models using a broad range of image-based tasks. Our\nexperimental results demonstrate that SAE features are semantically meaningful,\nimprove out-of-distribution generalization, and enable controllable generation\nacross three vision model architectures: vision embedding models, multi-modal\nLMMs and diffusion models. In vision embedding models, we find that learned SAE\nfeatures can be used for OOD detection and provide evidence that they recover\nthe ontological structure of the underlying model. For diffusion models, we\ndemonstrate that SAEs enable semantic steering through text encoder\nmanipulation and develop an automated pipeline for discovering\nhuman-interpretable attributes. Finally, we conduct exploratory experiments on\nmulti-modal LLMs, finding evidence that SAE features reveal shared\nrepresentations across vision and language modalities. Our study provides a\nfoundation for SAE evaluation in vision models, highlighting their strong\npotential improving interpretability, generalization, and steerability in the\nvisual domain.",
        "url": "http://arxiv.org/abs/2508.11277v1",
        "published_date": "2025-08-15T07:29:42+00:00",
        "updated_date": "2025-08-15T07:29:42+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Matthew Lyle Olson",
            "Musashi Hinck",
            "Neale Ratzlaff",
            "Changbai Li",
            "Phillip Howard",
            "Vasudev Lal",
            "Shao-Yen Tseng"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper investigates the use of Sparse Autoencoders in vision models, showing that they improve interpretability, generalization, and steerability in the visual domain.",
        "tldr_zh": "本文研究了稀疏自编码器在视觉模型中的应用，表明它们提高了在视觉领域的可解释性、泛化性和可操控性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Temporally-Similar Structure-Aware Spatiotemporal Fusion of Satellite Images",
        "summary": "This paper proposes a novel spatiotemporal (ST) fusion framework for\nsatellite images, named Temporally-Similar Structure-Aware ST fusion (TSSTF).\nST fusion is a promising approach to address the trade-off between the spatial\nand temporal resolution of satellite images. In real-world scenarios, observed\nsatellite images are severely degraded by noise due to measurement equipment\nand environmental conditions. Consequently, some recent studies have focused on\nenhancing the robustness of ST fusion methods against noise. However, existing\nnoise-robust ST fusion approaches often fail to capture fine spatial structure,\nleading to oversmoothing and artifacts. To address this issue, TSSTF introduces\ntwo key mechanisms: Temporally-Guided Total Variation (TGTV) and\nTemporally-Guided Edge Constraint (TGEC). TGTV is a novel regularization\nfunction that promotes spatial piecewise smoothness while preserving structural\ndetails, guided by a reference high spatial resolution image acquired on a\nnearby date. TGEC enforces consistency in edge locations between two temporally\nadjacent images, while allowing for spectral variations. We formulate the ST\nfusion task as a constrained optimization problem incorporating TGTV and TGEC,\nand develop an efficient algorithm based on a preconditioned primal-dual\nsplitting method. Experimental results demonstrate that TSSTF performs\ncomparably to state-of-the-art methods under noise-free conditions and\noutperforms them under noisy conditions. Additionally, we provide a\ncomprehensive set of recommended parameter values that consistently yield high\nperformance across diverse target regions and noise conditions, aiming to\nenhance reproducibility and practical utility.",
        "url": "http://arxiv.org/abs/2508.11259v1",
        "published_date": "2025-08-15T06:50:34+00:00",
        "updated_date": "2025-08-15T06:50:34+00:00",
        "categories": [
            "eess.SP",
            "cs.CV"
        ],
        "authors": [
            "Ryosuke Isono",
            "Shunsuke Ono"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "TSSTF proposes a novel spatiotemporal fusion framework for satellite images to address noise and enhance spatial details, outperforming existing methods under noisy conditions.",
        "tldr_zh": "TSSTF提出了一种新颖的卫星图像时空融合框架，旨在处理噪音并增强空间细节，在嘈杂条件下优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception",
        "summary": "Dense visual perception tasks have been constrained by their reliance on\npredefined categories, limiting their applicability in real-world scenarios\nwhere visual concepts are unbounded. While Vision-Language Models (VLMs) like\nCLIP have shown promise in open-vocabulary tasks, their direct application to\ndense perception often leads to suboptimal performance due to limitations in\nlocal feature representation. In this work, we present our observation that\nCLIP's image tokens struggle to effectively aggregate information from\nspatially or semantically related regions, resulting in features that lack\nlocal discriminability and spatial consistency. To address this issue, we\npropose DeCLIP, a novel framework that enhances CLIP by decoupling the\nself-attention module to obtain ``content'' and ``context'' features\nrespectively. \\revise{The context features are enhanced by jointly distilling\nsemantic correlations from Vision Foundation Models (VFMs) and object integrity\ncues from diffusion models, thereby enhancing spatial consistency. In parallel,\nthe content features are aligned with image crop representations and\nconstrained by region correlations from VFMs to improve local discriminability.\nExtensive experiments demonstrate that DeCLIP establishes a solid foundation\nfor open-vocabulary dense perception, consistently achieving state-of-the-art\nperformance across a broad spectrum of tasks, including 2D detection and\nsegmentation, 3D instance segmentation, video instance segmentation, and 6D\nobject pose estimation.} Code is available at\nhttps://github.com/xiaomoguhz/DeCLIP",
        "url": "http://arxiv.org/abs/2508.11256v1",
        "published_date": "2025-08-15T06:43:51+00:00",
        "updated_date": "2025-08-15T06:43:51+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Junjie Wang",
            "Keyu Chen",
            "Yulin Li",
            "Bin Chen",
            "Hengshuang Zhao",
            "Xiaojuan Qi",
            "Zhuotao Tian"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces DeCLIP, a framework that enhances CLIP for dense visual perception tasks, achieving state-of-the-art performance in various tasks.",
        "tldr_zh": "该论文介绍了DeCLIP，一种增强CLIP用于密集视觉感知任务的框架，在各种任务中取得了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian Re-Identification in Autonomous Driving",
        "summary": "Re-Identification (ReID) is a critical technology in intelligent perception\nsystems, especially within autonomous driving, where onboard cameras must\nidentify pedestrians across views and time in real-time to support safe\nnavigation and trajectory prediction. However, the presence of uncertain or\nmissing input modalities--such as RGB, infrared, sketches, or textual\ndescriptions--poses significant challenges to conventional ReID approaches.\nWhile large-scale pre-trained models offer strong multimodal semantic modeling\ncapabilities, their computational overhead limits practical deployment in\nresource-constrained environments. To address these challenges, we propose a\nlightweight Uncertainty Modal Modeling (UMM) framework, which integrates a\nmultimodal token mapper, synthetic modality augmentation strategy, and\ncross-modal cue interactive learner. Together, these components enable unified\nfeature representation, mitigate the impact of missing modalities, and extract\ncomplementary information across different data types. Additionally, UMM\nleverages CLIP's vision-language alignment ability to fuse multimodal inputs\nefficiently without extensive finetuning. Experimental results demonstrate that\nUMM achieves strong robustness, generalization, and computational efficiency\nunder uncertain modality conditions, offering a scalable and practical solution\nfor pedestrian re-identification in autonomous driving scenarios.",
        "url": "http://arxiv.org/abs/2508.11218v1",
        "published_date": "2025-08-15T04:50:27+00:00",
        "updated_date": "2025-08-15T04:50:27+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Jialin Li",
            "Shuqi Wu",
            "Ning Wang"
        ],
        "ai_categories": [
            "Multimodality",
            "Other"
        ],
        "tldr": "The paper proposes a lightweight Uncertainty Modal Modeling (UMM) framework for pedestrian re-identification in autonomous driving, integrating multimodal token mapper, synthetic modality augmentation strategy, and cross-modal cue interactive learner.",
        "tldr_zh": "本文提出了一种轻量级不确定模态建模（UMM）框架，用于自动驾驶中的行人重新识别，集成了多模态令牌映射器、合成模态增强策略和跨模态提示交互学习器。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Efficient Image-to-Image Schrödinger Bridge for CT Field of View Extension",
        "summary": "Computed tomography (CT) is a cornerstone imaging modality for non-invasive,\nhigh-resolution visualization of internal anatomical structures. However, when\nthe scanned object exceeds the scanner's field of view (FOV), projection data\nare truncated, resulting in incomplete reconstructions and pronounced artifacts\nnear FOV boundaries. Conventional reconstruction algorithms struggle to recover\naccurate anatomy from such data, limiting clinical reliability. Deep learning\napproaches have been explored for FOV extension, with diffusion generative\nmodels representing the latest advances in image synthesis. Yet, conventional\ndiffusion models are computationally demanding and slow at inference due to\ntheir iterative sampling process. To address these limitations, we propose an\nefficient CT FOV extension framework based on the image-to-image Schr\\\"odinger\nBridge (I$^2$SB) diffusion model. Unlike traditional diffusion models that\nsynthesize images from pure Gaussian noise, I$^2$SB learns a direct stochastic\nmapping between paired limited-FOV and extended-FOV images. This direct\ncorrespondence yields a more interpretable and traceable generative process,\nenhancing anatomical consistency and structural fidelity in reconstructions.\nI$^2$SB achieves superior quantitative performance, with root-mean-square error\n(RMSE) values of 49.8\\,HU on simulated noisy data and 152.0HU on real data,\noutperforming state-of-the-art diffusion models such as conditional denoising\ndiffusion probabilistic models (cDDPM) and patch-based diffusion methods.\nMoreover, its one-step inference enables reconstruction in just 0.19s per 2D\nslice, representing over a 700-fold speedup compared to cDDPM (135s) and\nsurpassing diffusionGAN (0.58s), the second fastest. This combination of\naccuracy and efficiency makes I$^2$SB highly suitable for real-time or clinical\ndeployment.",
        "url": "http://arxiv.org/abs/2508.11211v1",
        "published_date": "2025-08-15T04:41:05+00:00",
        "updated_date": "2025-08-15T04:41:05+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Zhenhao Li",
            "Long Yang",
            "Xiaojie Yin",
            "Haijun Yu",
            "Jiazhou Wang",
            "Hongbin Han",
            "Weigang Hu",
            "Yixing Huang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes an efficient framework for extending the field of view in CT images using an image-to-image Schrödinger Bridge diffusion model, achieving superior performance and speed compared to existing methods.",
        "tldr_zh": "本文提出了一种使用图像到图像Schrödinger桥扩展CT图像视野的高效框架，与现有方法相比，性能和速度均表现优异。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation",
        "summary": "We introduce StyleMM, a novel framework that can construct a stylized 3D\nMorphable Model (3DMM) based on user-defined text descriptions specifying a\ntarget style. Building upon a pre-trained mesh deformation network and a\ntexture generator for original 3DMM-based realistic human faces, our approach\nfine-tunes these models using stylized facial images generated via text-guided\nimage-to-image (i2i) translation with a diffusion model, which serve as\nstylization targets for the rendered mesh. To prevent undesired changes in\nidentity, facial alignment, or expressions during i2i translation, we introduce\na stylization method that explicitly preserves the facial attributes of the\nsource image. By maintaining these critical attributes during image\nstylization, the proposed approach ensures consistent 3D style transfer across\nthe 3DMM parameter space through image-based training. Once trained, StyleMM\nenables feed-forward generation of stylized face meshes with explicit control\nover shape, expression, and texture parameters, producing meshes with\nconsistent vertex connectivity and animatability. Quantitative and qualitative\nevaluations demonstrate that our approach outperforms state-of-the-art methods\nin terms of identity-level facial diversity and stylization capability. The\ncode and videos are available at\n[kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).",
        "url": "http://arxiv.org/abs/2508.11203v1",
        "published_date": "2025-08-15T04:29:46+00:00",
        "updated_date": "2025-08-15T04:29:46+00:00",
        "categories": [
            "cs.GR",
            "cs.AI",
            "cs.CV",
            "cs.MM",
            "51-04",
            "I.3.8; I.4.9"
        ],
        "authors": [
            "Seungmi Lee",
            "Kwan Yun",
            "Junyong Noh"
        ],
        "ai_categories": [
            "Multimodality",
            "GAN",
            "Dataset"
        ],
        "tldr": "StyleMM is a framework for constructing a stylized 3D Morphable Model based on user-defined text descriptions, outperforming state-of-the-art methods in facial diversity and stylization capability.",
        "tldr_zh": "StyleMM是一个框架，可以根据用户定义的文本描述构建艺术化的3D可变模型，在面部多样性和艺术化能力方面优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Versatile Video Tokenization with Generative 2D Gaussian Splatting",
        "summary": "Video tokenization procedure is critical for a wide range of video processing\ntasks. Most existing approaches directly transform video into fixed-grid and\npatch-wise tokens, which exhibit limited versatility. Spatially, uniformly\nallocating a fixed number of tokens often leads to over-encoding in\nlow-information regions. Temporally, reducing redundancy remains challenging\nwithout explicitly distinguishing between static and dynamic content. In this\nwork, we propose the Gaussian Video Transformer (GVT), a versatile video\ntokenizer built upon a generative 2D Gaussian Splatting (2DGS) strategy. We\nfirst extract latent rigid features from a video clip and represent them with a\nset of 2D Gaussians generated by our proposed Spatio-Temporal Gaussian\nEmbedding (STGE) mechanism in a feed-forward manner. Such generative 2D\nGaussians not only enhance spatial adaptability by assigning higher (resp.,\nlower) rendering weights to regions with higher (resp., lower) information\ncontent during rasterization, but also improve generalization by avoiding\nper-video optimization.To enhance the temporal versatility, we introduce a\nGaussian Set Partitioning (GSP) strategy that separates the 2D Gaussians into\nstatic and dynamic sets, which explicitly model static content shared across\ndifferent time-steps and dynamic content specific to each time-step, enabling a\ncompact representation.We primarily evaluate GVT on the video reconstruction,\nwhile also assessing its performance on action recognition and compression\nusing the UCF101, Kinetics, and DAVIS datasets. Extensive experiments\ndemonstrate that GVT achieves a state-of-the-art video reconstruction quality,\noutperforms the baseline MAGVIT-v2 in action recognition, and delivers\ncomparable compression performance.",
        "url": "http://arxiv.org/abs/2508.11183v1",
        "published_date": "2025-08-15T03:16:45+00:00",
        "updated_date": "2025-08-15T03:16:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhenghao Chen",
            "Zicong Chen",
            "Lei Liu",
            "Yiming Wu",
            "Dong Xu"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces the Gaussian Video Transformer (GVT) for versatile video tokenization using generative 2D Gaussian splatting, achieving state-of-the-art video reconstruction quality, improved action recognition, and comparable compression performance.",
        "tldr_zh": "本文引入了高斯视频转换器（GVT），利用生成式2D高斯短语化实现了全面的视频编码，达到了最先进的视频重建质量，改进了动作识别，并获得了可比较的压缩性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HistoViT: Vision Transformer for Accurate and Scalable Histopathological Cancer Diagnosis",
        "summary": "Accurate and scalable cancer diagnosis remains a critical challenge in modern\npathology, particularly for malignancies such as breast, prostate, bone, and\ncervical, which exhibit complex histological variability. In this study, we\npropose a transformer-based deep learning framework for multi-class tumor\nclassification in histopathological images. Leveraging a fine-tuned Vision\nTransformer (ViT) architecture, our method addresses key limitations of\nconventional convolutional neural networks, offering improved performance,\nreduced preprocessing requirements, and enhanced scalability across tissue\ntypes. To adapt the model for histopathological cancer images, we implement a\nstreamlined preprocessing pipeline that converts tiled whole-slide images into\nPyTorch tensors and standardizes them through data normalization. This ensures\ncompatibility with the ViT architecture and enhances both convergence stability\nand overall classification performance. We evaluate our model on four benchmark\ndatasets: ICIAR2018 (breast), SICAPv2 (prostate), UT-Osteosarcoma (bone), and\nSipakMed (cervical) dataset -- demonstrating consistent outperformance over\nexisting deep learning methods. Our approach achieves classification accuracies\nof 99.32%, 96.92%, 95.28%, and 96.94% for breast, prostate, bone, and cervical\ncancers respectively, with area under the ROC curve (AUC) scores exceeding 99%\nacross all datasets. These results confirm the robustness, generalizability,\nand clinical potential of transformer-based architectures in digital pathology.\nOur work represents a significant advancement toward reliable, automated, and\ninterpretable cancer diagnosis systems that can alleviate diagnostic burdens\nand improve healthcare outcomes.",
        "url": "http://arxiv.org/abs/2508.11181v1",
        "published_date": "2025-08-15T03:10:52+00:00",
        "updated_date": "2025-08-15T03:10:52+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Faisal Ahmed"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a Vision Transformer-based deep learning framework for accurate cancer diagnosis in histopathological images, achieving high classification accuracies and demonstrating potential in digital pathology.",
        "tldr_zh": "本文提出了一种基于Transformer的深度学习框架，用于组织病理图像中的癌症诊断，实现了高分类准确性，并展示了在数字病理学中的潜力。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 10,
        "overall_priority_score": 8
    },
    {
        "title": "Fine-Grained VLM Fine-tuning via Latent Hierarchical Adapter Learning",
        "summary": "Adapter-based approaches have garnered attention for fine-tuning pre-trained\nVision-Language Models (VLMs) on few-shot classification tasks. These methods\nstrive to develop a lightweight module that better aligns visual and (category)\ntextual representations, thereby enhancing performance on downstream few-shot\nlearning tasks. However, existing adapters generally learn/align (category)\ntextual-visual modalities via explicit spatial proximity in the underlying\nembedding space, which i) fails to capture the inherent one-to-many\nassociations between categories and image samples and ii) struggles to\nestablish accurate associations between the unknown categories and images. To\naddress these issues, inspired by recent works on hyperbolic learning, we\ndevelop a novel Latent Hierarchical Adapter (LatHAdapter) for fine-tuning VLMs\non downstream few-shot classification tasks. The core of LatHAdapter is to\nexploit the latent semantic hierarchy of downstream training data and employ it\nto provide richer, fine-grained guidance for the adapter learning process.\nSpecifically, LatHAdapter first introduces some learnable `attribute' prompts\nas the bridge to align categories and images. Then, it projects the categories,\nattribute prompts, and images within each batch in a hyperbolic space, and\nemploys hierarchical regularization to learn the latent semantic hierarchy of\nthem, thereby fully modeling the inherent one-to-many associations among\ncategories, learnable attributes, and image samples. Extensive experiments on\nfour challenging few-shot tasks show that the proposed LatHAdapter consistently\noutperforms many other fine-tuning approaches, particularly in adapting known\nclasses and generalizing to unknown classes.",
        "url": "http://arxiv.org/abs/2508.11176v1",
        "published_date": "2025-08-15T03:02:36+00:00",
        "updated_date": "2025-08-15T03:02:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yumiao Zhao",
            "Bo Jiang",
            "Yuhe Ding",
            "Xiao Wang",
            "Jin Tang",
            "Bin Luo"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality"
        ],
        "tldr": "The paper introduces a novel Latent Hierarchical Adapter for fine-tuning Vision-Language Models on few-shot classification tasks, outperforming existing methods.",
        "tldr_zh": "本文介绍了一种用于微调视觉-语言模型的新型潜在分层适配器，优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Better Supervised Fine-tuning for VQA: Integer-Only Loss",
        "summary": "With the rapid advancement of vision language models(VLM), their ability to\nassess visual content based on specific criteria and dimensions has become\nincreasingly critical for applications such as video-theme consistency\nassessment and visual quality scoring. However, existing methods often suffer\nfrom imprecise results and inefficient loss calculation, which limit the focus\nof the model on key evaluation indicators. To address this, we propose\nIOVQA(Integer-only VQA), a novel fine-tuning approach tailored for VLMs to\nenhance their performance in video quality assessment tasks. The key innovation\nof IOVQA lies in its label construction and its targeted loss calculation\nmechanism. Specifically, during dataset curation, we constrain the model's\noutput to integers within the range of [10,50], ensuring numerical stability,\nand convert decimal Overall_MOS to integer before using them as labels. We also\nintroduce a target-mask strategy: when computing the loss, only the first\ntwo-digit-integer of the label is unmasked, forcing the model to learn the\ncritical components of the numerical evaluation. After fine-tuning the\nQwen2.5-VL model using the constructed dataset, experimental results\ndemonstrate that the proposed method significantly improves the model's\naccuracy and consistency in the VQA task, ranking 3rd in VQualA 2025\nGenAI-Bench AIGC Video Quality Assessment Challenge -- Track I. Our work\nhighlights the effectiveness of merely leaving integer labels during\nfine-tuning, providing an effective idea for optimizing VLMs in quantitative\nevaluation scenarios.",
        "url": "http://arxiv.org/abs/2508.11170v1",
        "published_date": "2025-08-15T02:40:43+00:00",
        "updated_date": "2025-08-15T02:40:43+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Baihong Qian",
            "Haotian Fan",
            "Wenjie Liao",
            "Yunqiu Wang",
            "Tao Li",
            "Junhui Cui"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces IOVQA, a fine-tuning approach for VLMs tailored for video quality assessment, improving model performance by using integer-only labels and targeted loss calculation.",
        "tldr_zh": "本文介绍了IOVQA，一种为视频质量评估量身定制的VLM微调方法，通过使用仅限整数标签和有针对性的损失计算来提高模型性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Semi-supervised Image Dehazing via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models",
        "summary": "Existing dehazing methods deal with real-world haze images with difficulty,\nespecially scenes with thick haze. One of the main reasons is the lack of\nreal-world paired data and robust priors. To avoid the costly collection of\npaired hazy and clear images, we propose an efficient semi-supervised image\ndehazing method via Expectation-Maximization and Bidirectional Brownian Bridge\nDiffusion Models (EM-B3DM) with a two-stage learning scheme. In the first\nstage, we employ the EM algorithm to decouple the joint distribution of paired\nhazy and clear images into two conditional distributions, which are then\nmodeled using a unified Brownian Bridge diffusion model to directly capture the\nstructural and content-related correlations between hazy and clear images. In\nthe second stage, we leverage the pre-trained model and large-scale unpaired\nhazy and clear images to further improve the performance of image dehazing.\nAdditionally, we introduce a detail-enhanced Residual Difference Convolution\nblock (RDC) to capture gradient-level information, significantly enhancing the\nmodel's representation capability. Extensive experiments demonstrate that our\nEM-B3DM achieves superior or at least comparable performance to\nstate-of-the-art methods on both synthetic and real-world datasets.",
        "url": "http://arxiv.org/abs/2508.11165v1",
        "published_date": "2025-08-15T02:33:44+00:00",
        "updated_date": "2025-08-15T02:33:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bing Liu",
            "Le Wang",
            "Mingming Liu",
            "Hao Liu",
            "Rui Yao",
            "Yong Zhou",
            "Peng Liu",
            "Tongqiang Xia"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a semi-supervised image dehazing method using Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models, achieving superior performance on both synthetic and real-world datasets.",
        "tldr_zh": "本文提出了一种利用期望最大化和双向布朗桥扩散模型的半监督图像去雾方法，在合成和真实数据集上取得了优秀表现。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Residual-based Efficient Bidirectional Diffusion Model for Image Dehazing and Haze Generation",
        "summary": "Current deep dehazing methods only focus on removing haze from hazy images,\nlacking the capability to translate between hazy and haze-free images. To\naddress this issue, we propose a residual-based efficient bidirectional\ndiffusion model (RBDM) that can model the conditional distributions for both\ndehazing and haze generation. Firstly, we devise dual Markov chains that can\neffectively shift the residuals and facilitate bidirectional smooth transitions\nbetween them. Secondly, the RBDM perturbs the hazy and haze-free images at\nindividual timesteps and predicts the noise in the perturbed data to\nsimultaneously learn the conditional distributions. Finally, to enhance\nperformance on relatively small datasets and reduce computational costs, our\nmethod introduces a unified score function learned on image patches instead of\nentire images. Our RBDM successfully implements size-agnostic bidirectional\ntransitions between haze-free and hazy images with only 15 sampling steps.\nExtensive experiments demonstrate that the proposed method achieves superior or\nat least comparable performance to state-of-the-art methods on both synthetic\nand real-world datasets.",
        "url": "http://arxiv.org/abs/2508.11134v1",
        "published_date": "2025-08-15T01:00:15+00:00",
        "updated_date": "2025-08-15T01:00:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bing Liu",
            "Le Wang",
            "Hao Liu",
            "Mingming Liu"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a residual-based efficient bidirectional diffusion model for image dehazing and haze generation, enabling smooth transitions between hazy and haze-free images with only 15 sampling steps.",
        "tldr_zh": "本文介绍了一种基于残差的高效双向扩散模型，用于图像去雾和雾生成，在只有15个采样步骤的情况下实现了在有雾和无雾图像之间的平滑过渡。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HierOctFusion: Multi-scale Octree-based 3D Shape Generation via Part-Whole-Hierarchy Message Passing",
        "summary": "3D content generation remains a fundamental yet challenging task due to the\ninherent structural complexity of 3D data. While recent octree-based diffusion\nmodels offer a promising balance between efficiency and quality through\nhierarchical generation, they often overlook two key insights: 1) existing\nmethods typically model 3D objects as holistic entities, ignoring their\nsemantic part hierarchies and limiting generalization; and 2) holistic\nhigh-resolution modeling is computationally expensive, whereas real-world\nobjects are inherently sparse and hierarchical, making them well-suited for\nlayered generation. Motivated by these observations, we propose HierOctFusion,\na part-aware multi-scale octree diffusion model that enhances hierarchical\nfeature interaction for generating fine-grained and sparse object structures.\nFurthermore, we introduce a cross-attention conditioning mechanism that injects\npart-level information into the generation process, enabling semantic features\nto propagate effectively across hierarchical levels from parts to the whole.\nAdditionally, we construct a 3D dataset with part category annotations using a\npre-trained segmentation model to facilitate training and evaluation.\nExperiments demonstrate that HierOctFusion achieves superior shape quality and\nefficiency compared to prior methods.",
        "url": "http://arxiv.org/abs/2508.11106v1",
        "published_date": "2025-08-14T23:12:18+00:00",
        "updated_date": "2025-08-14T23:12:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinjie Gao",
            "Bi'an Du",
            "Wei Hu"
        ],
        "ai_categories": [
            "GAN",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "HierOctFusion is a part-aware multi-scale octree diffusion model for generating fine-grained and sparse 3D object structures, achieving superior shape quality and efficiency compared to prior methods.",
        "tldr_zh": "HierOctFusion是一个具有部件感知的多尺度八叉树扩散模型，用于生成细粒度稀疏的3D对象结构，与先前方法相比，在形状质量和效率方面表现更优秀。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GenFlowRL: Shaping Rewards with Generative Object-Centric Flow in Visual Reinforcement Learning",
        "summary": "Recent advances have shown that video generation models can enhance robot\nlearning by deriving effective robot actions through inverse dynamics. However,\nthese methods heavily depend on the quality of generated data and struggle with\nfine-grained manipulation due to the lack of environment feedback. While\nvideo-based reinforcement learning improves policy robustness, it remains\nconstrained by the uncertainty of video generation and the challenges of\ncollecting large-scale robot datasets for training diffusion models. To address\nthese limitations, we propose GenFlowRL, which derives shaped rewards from\ngenerated flow trained from diverse cross-embodiment datasets. This enables\nlearning generalizable and robust policies from diverse demonstrations using\nlow-dimensional, object-centric features. Experiments on 10 manipulation tasks,\nboth in simulation and real-world cross-embodiment evaluations, demonstrate\nthat GenFlowRL effectively leverages manipulation features extracted from\ngenerated object-centric flow, consistently achieving superior performance\nacross diverse and challenging scenarios. Our Project Page:\nhttps://colinyu1.github.io/genflowrl",
        "url": "http://arxiv.org/abs/2508.11049v1",
        "published_date": "2025-08-14T20:19:20+00:00",
        "updated_date": "2025-08-14T20:19:20+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Kelin Yu",
            "Sheng Zhang",
            "Harshit Soora",
            "Furong Huang",
            "Heng Huang",
            "Pratap Tokekar",
            "Ruohan Gao"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Transformer"
        ],
        "tldr": "GenFlowRL proposes a method to derive shaped rewards from generated flow for visual reinforcement learning, achieving superior performance in diverse manipulation tasks.",
        "tldr_zh": "GenFlowRL提出了一种方法，从生成的流中导出形成奖励，用于视觉强化学习，在各种操纵任务中表现出优越性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MedSAMix: A Training-Free Model Merging Approach for Medical Image Segmentation",
        "summary": "Universal medical image segmentation models have emerged as a promising\nparadigm due to their strong generalizability across diverse tasks, showing\ngreat potential for a wide range of clinical applications. This potential has\nbeen partly driven by the success of general-purpose vision models such as the\nSegment Anything Model (SAM), which has inspired the development of various\nfine-tuned variants for medical segmentation tasks. However, fine-tuned\nvariants like MedSAM are trained on comparatively limited medical imaging data\nthat often suffers from heterogeneity, scarce annotations, and distributional\nshifts. These challenges limit their ability to generalize across a wide range\nof medical segmentation tasks. In this regard, we propose MedSAMix, a\ntraining-free model merging method that integrates the strengths of both\ngeneralist models (e.g., SAM) and specialist models (e.g., MedSAM) for medical\nimage segmentation. In contrast to traditional model merging approaches that\nrely on manual configuration and often result in suboptimal outcomes, we\npropose a zero-order optimization method to automatically discover optimal\nlayer-wise merging solutions. Furthermore, for clinical applications, we\ndevelop two regimes to meet the demand of domain-specificity and\ngeneralizability in different scenarios by single-task optimization and\nmulti-objective optimization respectively. Extensive evaluations on 25 medical\nsegmentation tasks demonstrate that MedSAMix effectively mitigates model bias\nand consistently improves performance in both domain-specific accuracy and\ngeneralization, achieving improvements of 6.67% on specialized tasks and 4.37%\non multi-task evaluations.",
        "url": "http://arxiv.org/abs/2508.11032v1",
        "published_date": "2025-08-14T19:35:57+00:00",
        "updated_date": "2025-08-14T19:35:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanwu Yang",
            "Guinan Su",
            "Jiesi Hu",
            "Francesco Sammarco",
            "Jonas Geiping",
            "Thomas Wolfers"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "MedSAMix is a training-free model merging approach for medical image segmentation, combining the strengths of generalist and specialist models to improve performance across various medical segmentation tasks.",
        "tldr_zh": "MedSAMix是一种无需训练的模型融合方法，用于医学图像分割，结合了通用模型和专业模型的优势，以改善各种医学分割任务的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Are Large Pre-trained Vision Language Models Effective Construction Safety Inspectors?",
        "summary": "Construction safety inspections typically involve a human inspector\nidentifying safety concerns on-site. With the rise of powerful Vision Language\nModels (VLMs), researchers are exploring their use for tasks such as detecting\nsafety rule violations from on-site images. However, there is a lack of open\ndatasets to comprehensively evaluate and further fine-tune VLMs in construction\nsafety inspection. Current applications of VLMs use small, supervised datasets,\nlimiting their applicability in tasks they are not directly trained for. In\nthis paper, we propose the ConstructionSite 10k, featuring 10,000 construction\nsite images with annotations for three inter-connected tasks, including image\ncaptioning, safety rule violation visual question answering (VQA), and\nconstruction element visual grounding. Our subsequent evaluation of current\nstate-of-the-art large pre-trained VLMs shows notable generalization abilities\nin zero-shot and few-shot settings, while additional training is needed to make\nthem applicable to actual construction sites. This dataset allows researchers\nto train and evaluate their own VLMs with new architectures and techniques,\nproviding a valuable benchmark for construction safety inspection.",
        "url": "http://arxiv.org/abs/2508.11011v1",
        "published_date": "2025-08-14T18:23:09+00:00",
        "updated_date": "2025-08-14T18:23:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xuezheng Chen",
            "Zhengbo Zou"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper introduces ConstructionSite 10k, a dataset for training and evaluating Vision Language Models for construction safety inspection tasks.",
        "tldr_zh": "本文介绍了ConstructionSite 10k，这是一个用于训练和评估Vision Language Models在建筑安全检查任务中的数据集。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Match & Choose: Model Selection Framework for Fine-tuning Text-to-Image Diffusion Models",
        "summary": "Text-to-image (T2I) models based on diffusion and transformer architectures\nadvance rapidly. They are often pretrained on large corpora, and openly shared\non a model platform, such as HuggingFace. Users can then build up AI\napplications, e.g., generating media contents, by adopting pretrained T2I\nmodels and fine-tuning them on the target dataset. While public pretrained T2I\nmodels facilitate the democratization of the models, users face a new\nchallenge: which model can be best fine-tuned based on the target data domain?\nModel selection is well addressed in classification tasks, but little is known\nin (pretrained) T2I models and their performance indication on the target\ndomain. In this paper, we propose the first model selection framework, M&C,\nwhich enables users to efficiently choose a pretrained T2I model from a model\nplatform without exhaustively fine-tuning them all on the target dataset. The\ncore of M&C is a matching graph, which consists of: (i) nodes of available\nmodels and profiled datasets, and (ii) edges of model-data and data-data pairs\ncapturing the fine-tuning performance and data similarity, respectively. We\nthen build a model that, based on the inputs of model/data feature, and,\ncritically, the graph embedding feature, extracted from the matching graph,\npredicts the model achieving the best quality after fine-tuning for the target\ndomain. We evaluate M&C on choosing across ten T2I models for 32 datasets\nagainst three baselines. Our results show that M&C successfully predicts the\nbest model for fine-tuning in 61.3% of the cases and a closely performing model\nfor the rest.",
        "url": "http://arxiv.org/abs/2508.10993v1",
        "published_date": "2025-08-14T18:00:50+00:00",
        "updated_date": "2025-08-14T18:00:50+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Basile Lewandowski",
            "Robert Birke",
            "Lydia Y. Chen"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer",
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper presents a model selection framework, M&C, for choosing the best pretrained Text-to-Image model for fine-tuning on a target dataset efficiently.",
        "tldr_zh": "本文提出了一个模型选择框架M＆C，用于高效地选择最佳的预训练文本到图像模型，以在目标数据集上进行微调。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Automated Building Heritage Assessment Using Street-Level Imagery",
        "summary": "Detailed data is required to quantify energy conservation measures in\nbuildings, such as envelop retrofits, without compromising cultural heritage.\nNovel artificial intelligence tools may improve efficiency in identifying\nheritage values in buildings compared to costly and time-consuming traditional\ninventories. In this study, the large language model GPT was used to detect\nvarious aspects of cultural heritage value in fa\\c{c}ade images. Using this\ndata and building register data as features, machine learning models were\ntrained to classify multi-family and non-residential buildings in Stockholm,\nSweden. Validation against an expert-created inventory shows a macro F1-score\nof 0.71 using a combination of register data and features retrieved from GPT,\nand a score of 0.60 using only GPT-derived data. The presented methodology can\ncontribute to a higher-quality database and thus support careful energy\nefficiency measures and integrated consideration of heritage value in\nlarge-scale energetic refurbishment scenarios.",
        "url": "http://arxiv.org/abs/2508.11486v1",
        "published_date": "2025-08-15T13:59:24+00:00",
        "updated_date": "2025-08-15T13:59:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kristina Dabrock",
            "Tim Johansson",
            "Anna Donarelli",
            "Mikael Mangold",
            "Noah Pflugradt",
            "Jann Michael Weinand",
            "Jochen Linßen"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper discusses using artificial intelligence to assess heritage values in buildings to support energy efficiency measures.",
        "tldr_zh": "本文讨论了利用人工智能评估建筑物的文化遗产价值，以支持能源效率措施。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "Training-free Dimensionality Reduction via Feature Truncation: Enhancing Efficiency in Privacy-preserving Multi-Biometric Systems",
        "summary": "Biometric recognition is widely used, making the privacy and security of\nextracted templates a critical concern. Biometric Template Protection schemes,\nespecially those utilizing Homomorphic Encryption, introduce significant\ncomputational challenges due to increased workload. Recent advances in deep\nneural networks have enabled state-of-the-art feature extraction for face,\nfingerprint, and iris modalities. The ubiquity and affordability of biometric\nsensors further facilitate multi-modal fusion, which can enhance security by\ncombining features from different modalities. This work investigates the\nbiometric performance of reduced multi-biometric template sizes. Experiments\nare conducted on an in-house virtual multi-biometric database, derived from\nDNN-extracted features for face, fingerprint, and iris, using the FRGC, MCYT,\nand CASIA databases. The evaluated approaches are (i) explainable and\nstraightforward to implement under encryption, (ii) training-free, and (iii)\ncapable of generalization. Dimensionality reduction of feature vectors leads to\nfewer operations in the Homomorphic Encryption (HE) domain, enabling more\nefficient encrypted processing while maintaining biometric accuracy and\nsecurity at a level equivalent to or exceeding single-biometric recognition.\nOur results demonstrate that, by fusing feature vectors from multiple\nmodalities, template size can be reduced by 67 % with no loss in Equal Error\nRate (EER) compared to the best-performing single modality.",
        "url": "http://arxiv.org/abs/2508.11419v1",
        "published_date": "2025-08-15T11:49:19+00:00",
        "updated_date": "2025-08-15T11:49:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Florian Bayer",
            "Maximilian Russo",
            "Christian Rathgeb"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper explores reducing the size of biometric templates in privacy-preserving systems without compromising accuracy by fusing feature vectors from multiple modalities.",
        "tldr_zh": "本文探讨了在隐私保护系统中通过融合多种模态的特征向量来减小生物特征模板的尺寸，同时保持准确性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "A Coarse-to-Fine Human Pose Estimation Method based on Two-stage Distillation and Progressive Graph Neural Network",
        "summary": "Human pose estimation has been widely applied in the human-centric\nunderstanding and generation, but most existing state-of-the-art human pose\nestimation methods require heavy computational resources for accurate\npredictions. In order to obtain an accurate, robust yet lightweight human pose\nestimator, one feasible way is to transfer pose knowledge from a powerful\nteacher model to a less-parameterized student model by knowledge distillation.\nHowever, the traditional knowledge distillation framework does not fully\nexplore the contextual information among human joints. Thus, in this paper, we\npropose a novel coarse-to-fine two-stage knowledge distillation framework for\nhuman pose estimation. In the first-stage distillation, we introduce the human\njoints structure loss to mine the structural information among human joints so\nas to transfer high-level semantic knowledge from the teacher model to the\nstudent model. In the second-stage distillation, we utilize an Image-Guided\nProgressive Graph Convolutional Network (IGP-GCN) to refine the initial human\npose obtained from the first-stage distillation and supervise the training of\nthe IGP-GCN in the progressive way by the final output pose of teacher model.\nThe extensive experiments on the benchmark dataset: COCO keypoint and CrowdPose\ndatasets, show that our proposed method performs favorably against lots of the\nexisting state-of-the-art human pose estimation methods, especially for the\nmore complex CrowdPose dataset, the performance improvement of our model is\nmore significant.",
        "url": "http://arxiv.org/abs/2508.11212v1",
        "published_date": "2025-08-15T04:41:49+00:00",
        "updated_date": "2025-08-15T04:41:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhangjian Ji",
            "Wenjin Zhang",
            "Shaotong Qiao",
            "Kai Feng",
            "Yuhua Qian"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel two-stage knowledge distillation framework for human pose estimation, achieving superior performance compared to existing methods.",
        "tldr_zh": "本文介绍了一种新的两阶段知识蒸馏框架，用于人体姿势估计，在性能上优于现有方法。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning",
        "summary": "Recent advances in vision-language models (VLMs) have demonstrated strong\ngeneralization in natural image tasks. However, their performance often\ndegrades on unmanned aerial vehicle (UAV)-based aerial imagery, which features\nhigh resolution, complex spatial semantics, and strict real-time constraints.\nThese challenges limit the applicability of general-purpose VLMs to structured\naerial reasoning tasks. To address these challenges, we propose UAV-VL-R1, a\nlightweight VLM explicitly designed for aerial visual reasoning. It is trained\nusing a hybrid method that combines supervised fine-tuning (SFT) and\nmulti-stage reinforcement learning (RL). We leverage the group relative policy\noptimization (GRPO) algorithm to promote structured and interpretable reasoning\nthrough rule-guided rewards and intra-group policy alignment. To support model\ntraining and evaluation, we introduce a high-resolution visual question\nanswering dataset named HRVQA-VL, which consists of 50,019 annotated samples\ncovering eight UAV-relevant reasoning tasks, including object counting,\ntransportation recognition, and spatial scene inference. Experimental results\nshow that UAV-VL-R1 achieves a 48.17% higher zero-shot accuracy than the\nQwen2-VL-2B-Instruct baseline and even outperforms its 72B-scale variant, which\nis 36x larger, on multiple tasks. Ablation studies reveal that while SFT\nimproves semantic alignment, it may reduce reasoning diversity in mathematical\ntasks. GRPO-based RL compensates for this limitation by enhancing logical\nflexibility and the robustness of inference. Additionally, UAV-VL-R1 requires\nonly 3.9GB of memory under FP16 inference and can be quantized to 2.5GB with\nINT8, supporting real-time deployment on resource-constrained UAV platforms.",
        "url": "http://arxiv.org/abs/2508.11196v1",
        "published_date": "2025-08-15T04:06:40+00:00",
        "updated_date": "2025-08-15T04:06:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiajin Guan",
            "Haibo Mei",
            "Bonan Zhang",
            "Dan Liu",
            "Yuanshuang Fu",
            "Yue Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces UAV-VL-R1, a vision-language model designed for aerial visual reasoning using a hybrid method of supervised fine-tuning and multi-stage reinforcement learning. It outperforms existing models on UAV tasks and can support real-time deployment on resource-constrained UAV platforms.",
        "tldr_zh": "该论文介绍了UAV-VL-R1，这是一个专为航空视觉推理而设计的视觉语言模型，使用了监督微调和多阶段强化学习的混合方法。它在无人机任务上表现优异，并且可以支持资源受限的无人机平台的实时部署。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text and Image internal Correlations",
        "summary": "Existing rumor detection methods often neglect the content within images as\nwell as the inherent relationships between contexts and images across different\nvisual scales, thereby resulting in the loss of critical information pertinent\nto rumor identification. To address these issues, this paper presents a novel\ncross-modal rumor detection scheme based on contrastive learning, namely the\nMulti-scale Image and Context Correlation exploration algorithm (MICC).\nSpecifically, we design an SCLIP encoder to generate unified semantic\nembeddings for text and multi-scale image patches through contrastive\npretraining, enabling their relevance to be measured via dot-product\nsimilarity. Building upon this, a Cross-Modal Multi-Scale Alignment module is\nintroduced to identify image regions most relevant to the textual semantics,\nguided by mutual information maximization and the information bottleneck\nprinciple, through a Top-K selection strategy based on a cross-modal relevance\nmatrix constructed between the text and multi-scale image patches. Moreover, a\nscale-aware fusion network is designed to integrate the highly correlated\nmulti-scale image features with global text features by assigning adaptive\nweights to image regions based on their semantic importance and cross-modal\nrelevance. The proposed methodology has been extensively evaluated on two\nreal-world datasets. The experimental results demonstrate that it achieves a\nsubstantial performance improvement over existing state-of-the-art approaches\nin rumor detection, highlighting its effectiveness and potential for practical\napplications.",
        "url": "http://arxiv.org/abs/2508.11141v1",
        "published_date": "2025-08-15T01:13:50+00:00",
        "updated_date": "2025-08-15T01:13:50+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Bin Ma",
            "Yifei Zhang",
            "Yongjin Xian",
            "Qi Li",
            "Linna Zhou",
            "Gongxun Miao"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper presents a cross-modal rumor detection scheme using contrastive learning to explore text and image correlations. It outperforms existing methods in rumor detection.",
        "tldr_zh": "本文通过对比学习来探索文本和图像的相关性，在跨模态谣言检测方案中取得了优异表现，胜过现有的方法。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "UWB-PostureGuard: A Privacy-Preserving RF Sensing System for Continuous Ergonomic Sitting Posture Monitoring",
        "summary": "Improper sitting posture during prolonged computer use has become a\nsignificant public health concern. Traditional posture monitoring solutions\nface substantial barriers, including privacy concerns with camera-based systems\nand user discomfort with wearable sensors. This paper presents\nUWB-PostureGuard, a privacy-preserving ultra-wideband (UWB) sensing system that\nadvances mobile technologies for preventive health management through\ncontinuous, contactless monitoring of ergonomic sitting posture. Our system\nleverages commercial UWB devices, utilizing comprehensive feature engineering\nto extract multiple ergonomic sitting posture features. We develop PoseGBDT to\neffectively capture temporal dependencies in posture patterns, addressing\nlimitations of traditional frame-wise classification approaches. Extensive\nreal-world evaluation across 10 participants and 19 distinct postures\ndemonstrates exceptional performance, achieving 99.11% accuracy while\nmaintaining robustness against environmental variables such as clothing\nthickness, additional devices, and furniture configurations. Our system\nprovides a scalable, privacy-preserving mobile health solution on existing\nplatforms for proactive ergonomic management, improving quality of life at low\ncosts.",
        "url": "http://arxiv.org/abs/2508.11115v1",
        "published_date": "2025-08-14T23:40:37+00:00",
        "updated_date": "2025-08-14T23:40:37+00:00",
        "categories": [
            "cs.CV",
            "cs.HC",
            "eess.SP"
        ],
        "authors": [
            "Haotang Li",
            "Zhenyu Qi",
            "Sen He",
            "Kebin Peng",
            "Sheng Tan",
            "Yili Ren",
            "Tomas Cerny",
            "Jiyue Zhao",
            "Zi Wang"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a system called UWB-PostureGuard that uses ultra-wideband sensing to monitor ergonomic sitting posture without compromising privacy.",
        "tldr_zh": "本文介绍了一种名为UWB-PostureGuard的系统，利用超宽带传感监测人体工程学座姿，而不会侵犯隐私。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7.5
    },
    {
        "title": "An Efficient Medical Image Classification Method Based on a Lightweight Improved ConvNeXt-Tiny Architecture",
        "summary": "Intelligent analysis of medical imaging plays a crucial role in assisting\nclinical diagnosis. However, achieving efficient and high-accuracy image\nclassification in resource-constrained computational environments remains\nchallenging. This study proposes a medical image classification method based on\nan improved ConvNeXt-Tiny architecture. Through structural optimization and\nloss function design, the proposed method enhances feature extraction\ncapability and classification performance while reducing computational\ncomplexity. Specifically, the method introduces a dual global pooling (Global\nAverage Pooling and Global Max Pooling) feature fusion strategy into the\nConvNeXt-Tiny backbone to simultaneously preserve global statistical features\nand salient response information. A lightweight channel attention module,\ntermed Squeeze-and-Excitation Vector (SEVector), is designed to improve the\nadaptive allocation of channel weights while minimizing parameter overhead.\nAdditionally, a Feature Smoothing Loss is incorporated into the loss function\nto enhance intra-class feature consistency and suppress intra-class variance.\nUnder CPU-only conditions (8 threads), the method achieves a maximum\nclassification accuracy of 89.10% on the test set within 10 training epochs,\nexhibiting a stable convergence trend in loss values. Experimental results\ndemonstrate that the proposed method effectively improves medical image\nclassification performance in resource-limited settings, providing a feasible\nand efficient solution for the deployment and promotion of medical imaging\nanalysis models.",
        "url": "http://arxiv.org/abs/2508.11532v1",
        "published_date": "2025-08-15T15:20:25+00:00",
        "updated_date": "2025-08-15T15:20:25+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Jingsong Xia",
            "Yue Yin",
            "Xiuhan Li"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer"
        ],
        "tldr": "The paper introduces a method for efficient medical image classification using an improved ConvNeXt-Tiny architecture, achieving high accuracy in resource-constrained environments.",
        "tldr_zh": "本文介绍了一种利用改进的ConvNeXt-Tiny架构进行高效医学图像分类的方法，在资源受限环境中取得高准确性。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7.25
    },
    {
        "title": "A Real-time Concrete Crack Detection and Segmentation Model Based on YOLOv11",
        "summary": "Accelerated aging of transportation infrastructure in the rapidly developing\nYangtze River Delta region necessitates efficient concrete crack detection, as\ncrack deterioration critically compromises structural integrity and regional\neconomic growth. To overcome the limitations of inefficient manual inspection\nand the suboptimal performance of existing deep learning models, particularly\nfor small-target crack detection within complex backgrounds, this paper\nproposes YOLOv11-KW-TA-FP, a multi-task concrete crack detection and\nsegmentation model based on the YOLOv11n architecture. The proposed model\nintegrates a three-stage optimization framework: (1) Embedding dynamic\nKernelWarehouse convolution (KWConv) within the backbone network to enhance\nfeature representation through a dynamic kernel sharing mechanism; (2)\nIncorporating a triple attention mechanism (TA) into the feature pyramid to\nstrengthen channel-spatial interaction modeling; and (3) Designing an FP-IoU\nloss function to facilitate adaptive bounding box regression penalization.\nExperimental validation demonstrates that the enhanced model achieves\nsignificant performance improvements over the baseline, attaining 91.3%\nprecision, 76.6% recall, and 86.4% mAP@50. Ablation studies confirm the\nsynergistic efficacy of the proposed modules. Furthermore, robustness tests\nindicate stable performance under conditions of data scarcity and noise\ninterference. This research delivers an efficient computer vision solution for\nautomated infrastructure inspection, exhibiting substantial practical\nengineering value.",
        "url": "http://arxiv.org/abs/2508.11517v1",
        "published_date": "2025-08-15T14:57:00+00:00",
        "updated_date": "2025-08-15T14:57:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shaoze Huang",
            "Qi Liu",
            "Chao Chen",
            "Yuhang Chen"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a real-time concrete crack detection and segmentation model based on YOLOv11, achieving significant performance improvements over existing models.",
        "tldr_zh": "本文提出了一个基于YOLOv11的实时混凝土裂缝检测和分割模型，相比现有模型取得了显著的性能提升。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7.25
    },
    {
        "title": "Perception in Plan: Coupled Perception and Planning for End-to-End Autonomous Driving",
        "summary": "End-to-end autonomous driving has achieved remarkable advancements in recent\nyears. Existing methods primarily follow a perception-planning paradigm, where\nperception and planning are executed sequentially within a fully differentiable\nframework for planning-oriented optimization. We further advance this paradigm\nthrough a perception-in-plan framework design, which integrates perception into\nthe planning process. This design facilitates targeted perception guided by\nevolving planning objectives over time, ultimately enhancing planning\nperformance. Building on this insight, we introduce VeteranAD, a coupled\nperception and planning framework for end-to-end autonomous driving. By\nincorporating multi-mode anchored trajectories as planning priors, the\nperception module is specifically designed to gather traffic elements along\nthese trajectories, enabling comprehensive and targeted perception. Planning\ntrajectories are then generated based on both the perception results and the\nplanning priors. To make perception fully serve planning, we adopt an\nautoregressive strategy that progressively predicts future trajectories while\nfocusing on relevant regions for targeted perception at each step. With this\nsimple yet effective design, VeteranAD fully unleashes the potential of\nplanning-oriented end-to-end methods, leading to more accurate and reliable\ndriving behavior. Extensive experiments on the NAVSIM and Bench2Drive datasets\ndemonstrate that our VeteranAD achieves state-of-the-art performance.",
        "url": "http://arxiv.org/abs/2508.11488v1",
        "published_date": "2025-08-15T14:05:57+00:00",
        "updated_date": "2025-08-15T14:05:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bozhou Zhang",
            "Jingyu Li",
            "Nan Song",
            "Li Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a coupled perception and planning framework for end-to-end autonomous driving called VeteranAD, which improves planning performance by integrating perception into the planning process.",
        "tldr_zh": "本文介绍了一种称为VeteranAD的端到端自动驾驶的耦合感知和规划框架，通过将感知整合到规划过程中，提高了规划性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "CoFi: A Fast Coarse-to-Fine Few-Shot Pipeline for Glomerular Basement Membrane Segmentation",
        "summary": "Accurate segmentation of the glomerular basement membrane (GBM) in electron\nmicroscopy (EM) images is fundamental for quantifying membrane thickness and\nsupporting the diagnosis of various kidney diseases. While supervised deep\nlearning approaches achieve high segmentation accuracy, their reliance on\nextensive pixel-level annotation renders them impractical for clinical\nworkflows. Few-shot learning can reduce this annotation burden but often\nstruggles to capture the fine structural details necessary for GBM analysis. In\nthis study, we introduce CoFi, a fast and efficient coarse-to-fine few-shot\nsegmentation pipeline designed for GBM delineation in EM images. CoFi first\ntrains a lightweight neural network using only three annotated images to\nproduce an initial coarse segmentation mask. This mask is then automatically\nprocessed to generate high-quality point prompts with morphology-aware pruning,\nwhich are subsequently used to guide SAM in refining the segmentation. The\nproposed method achieved exceptional GBM segmentation performance, with a Dice\ncoefficient of 74.54% and an inference speed of 1.9 FPS. We demonstrate that\nCoFi not only alleviates the annotation and computational burdens associated\nwith conventional methods, but also achieves accurate and reliable segmentation\nresults. The pipeline's speed and annotation efficiency make it well-suited for\nresearch and hold strong potential for clinical applications in renal\npathology. The pipeline is publicly available at:\nhttps://github.com/ddrrnn123/CoFi.",
        "url": "http://arxiv.org/abs/2508.11469v1",
        "published_date": "2025-08-15T13:34:24+00:00",
        "updated_date": "2025-08-15T13:34:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hongjin Fang",
            "Daniel Reisenbüchler",
            "Kenji Ikemura",
            "Mert R. Sabuncu",
            "Yihe Yang",
            "Ruining Deng"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "CoFi introduces a fast few-shot segmentation pipeline for glomerular basement membrane in EM images, achieving high accuracy and speed.",
        "tldr_zh": "CoFi 提出了一种快速的少样本分割管道，用于电子显微镜图像中的肾小球基底膜，实现了高精度和速度。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Inside Knowledge: Graph-based Path Generation with Explainable Data Augmentation and Curriculum Learning for Visual Indoor Navigation",
        "summary": "Indoor navigation is a difficult task, as it generally comes with poor GPS\naccess, forcing solutions to rely on other sources of information. While\nsignificant progress continues to be made in this area, deployment to\nproduction applications is still lacking, given the complexity and additional\nrequirements of current solutions. Here, we introduce an efficient, real-time\nand easily deployable deep learning approach, based on visual input only, that\ncan predict the direction towards a target from images captured by a mobile\ndevice. Our technical approach, based on a novel graph-based path generation\nmethod, combined with explainable data augmentation and curriculum learning,\nincludes contributions that make the process of data collection, annotation and\ntraining, as automatic as possible, efficient and robust. On the practical\nside, we introduce a novel largescale dataset, with video footage inside a\nrelatively large shopping mall, in which each frame is annotated with the\ncorrect next direction towards different specific target destinations.\nDifferent from current methods, ours relies solely on vision, avoiding the need\nof special sensors, additional markers placed along the path, knowledge of the\nscene map or internet access. We also created an easy to use application for\nAndroid, which we plan to make publicly available. We make all our data and\ncode available along with visual demos on our project site",
        "url": "http://arxiv.org/abs/2508.11446v1",
        "published_date": "2025-08-15T12:54:13+00:00",
        "updated_date": "2025-08-15T12:54:13+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Daniel Airinei",
            "Elena Burceanu",
            "Marius Leordeanu"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces an efficient deep learning approach for indoor navigation based on visual input only, with a novel graph-based path generation method and explainable data augmentation.",
        "tldr_zh": "本文介绍了一个基于视觉输入的室内导航的高效深度学习方法，采用了一种新颖的基于图的路径生成方法和可解释的数据增强。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Robust Convolution Neural ODEs via Contractivity-promoting regularization",
        "summary": "Neural networks can be fragile to input noise and adversarial attacks.\n  In this work, we consider Convolutional Neural Ordinary Differential\nEquations (NODEs), a family of continuous-depth neural networks represented by\ndynamical systems, and propose to use contraction theory to improve their\nrobustness.\n  For a contractive dynamical system two trajectories starting from different\ninitial conditions converge to each other exponentially fast.\n  Contractive Convolutional NODEs can enjoy increased robustness as slight\nperturbations of the features do not cause a significant change in the output.\n  Contractivity can be induced during training by using a regularization term\ninvolving the Jacobian of the system dynamics.\n  To reduce the computational burden, we show that it can also be promoted\nusing carefully selected weight regularization terms for a class of NODEs with\nslope-restricted activation functions.\n  The performance of the proposed regularizers is illustrated through benchmark\nimage classification tasks on MNIST and FashionMNIST datasets, where images are\ncorrupted by different kinds of noise and attacks.",
        "url": "http://arxiv.org/abs/2508.11432v1",
        "published_date": "2025-08-15T12:18:44+00:00",
        "updated_date": "2025-08-15T12:18:44+00:00",
        "categories": [
            "cs.LG",
            "cs.CV",
            "cs.SY",
            "eess.SY"
        ],
        "authors": [
            "Muhammad Zakwan",
            "Liang Xu",
            "Giancarlo Ferrari-Trecate"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a method to improve the robustness of Convolutional Neural Ordinary Differential Equations by using contraction theory during training, showing increased resistance to noise and adversarial attacks.",
        "tldr_zh": "本文提出了一种方法，通过在训练过程中使用收缩理论来改善卷积神经常微分方程的稳健性，从而显示出对噪声和对抗性攻击的抵抗力增强。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Leveraging the RETFound foundation model for optic disc segmentation in retinal images",
        "summary": "RETFound is a well-known foundation model (FM) developed for fundus camera\nand optical coherence tomography images. It has shown promising performance\nacross multiple datasets in diagnosing diseases, both eye-specific and\nsystemic, from retinal images. However, to our best knowledge, it has not been\nused for other tasks. We present the first adaptation of RETFound for optic\ndisc segmentation, a ubiquitous and foundational task in retinal image\nanalysis. The resulting segmentation system outperforms state-of-the-art,\nsegmentation-specific baseline networks after training a head with only a very\nmodest number of task-specific examples. We report and discuss results with\nfour public datasets, IDRID, Drishti-GS, RIM-ONE-r3, and REFUGE, and a private\ndataset, GoDARTS, achieving about 96% Dice consistently across all datasets.\nOverall, our method obtains excellent performance in internal verification,\ndomain generalization and domain adaptation, and exceeds most of the\nstate-of-the-art baseline results. We discuss the results in the framework of\nthe debate about FMs as alternatives to task-specific architectures. The code\nis available at: [link to be added after the paper is accepted]",
        "url": "http://arxiv.org/abs/2508.11354v1",
        "published_date": "2025-08-15T09:43:49+00:00",
        "updated_date": "2025-08-15T09:43:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Zhenyi Zhao",
            "Muthu Rama Krishnan Mookiah",
            "Emanuele Trucco"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces the adaptation of the RETFound foundation model for optic disc segmentation in retinal images, achieving excellent performance across multiple datasets.",
        "tldr_zh": "本文介绍了 RETFound 基于视网膜图像分析的领域模型，针对视网膜图像中的视盘分割进行了适配，在多个数据集上表现出色。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Index-Aligned Query Distillation for Transformer-based Incremental Object Detection",
        "summary": "Incremental object detection (IOD) aims to continuously expand the capability\nof a model to detect novel categories while preserving its performance on\npreviously learned ones. When adopting a transformer-based detection model to\nperform IOD, catastrophic knowledge forgetting may inevitably occur, meaning\nthe detection performance on previously learned categories may severely\ndegenerate. Previous typical methods mainly rely on knowledge distillation (KD)\nto mitigate the catastrophic knowledge forgetting of transformer-based\ndetection models. Specifically, they utilize Hungarian Matching to build a\ncorrespondence between the queries of the last-phase and current-phase\ndetection models and align the classifier and regressor outputs between matched\nqueries to avoid knowledge forgetting. However, we observe that in IOD task,\nHungarian Matching is not a good choice. With Hungarian Matching, the query of\nthe current-phase model may match different queries of the last-phase model at\ndifferent iterations during KD. As a result, the knowledge encoded in each\nquery may be reshaped towards new categories, leading to the forgetting of\npreviously encoded knowledge of old categories. Based on our observations, we\npropose a new distillation approach named Index-Aligned Query Distillation\n(IAQD) for transformer-based IOD. Beyond using Hungarian Matching, IAQD\nestablishes a correspondence between queries of the previous and current phase\nmodels that have the same index. Moreover, we perform index-aligned\ndistillation only on partial queries which are critical for the detection of\nprevious categories. In this way, IAQD largely preserves the previous semantic\nand spatial encoding capabilities without interfering with the learning of new\ncategories. Extensive experiments on representative benchmarks demonstrate that\nIAQD effectively mitigates knowledge forgetting, achieving new state-of-the-art\nperformance.",
        "url": "http://arxiv.org/abs/2508.11339v1",
        "published_date": "2025-08-15T09:10:05+00:00",
        "updated_date": "2025-08-15T09:10:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mingxiao Ma",
            "Shunyao Zhu",
            "Guoliang Kang"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper proposes a new distillation approach called Index-Aligned Query Distillation (IAQD) for transformer-based Incremental Object Detection, which effectively mitigates knowledge forgetting and achieves state-of-the-art performance.",
        "tldr_zh": "本文提出了一种新的蒸馏方法，称为索引对齐查询蒸馏（IAQD），用于基于Transformer的增量目标检测，有效减轻了知识遗忘问题并取得了最新的成果。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking",
        "summary": "3D multi-object tracking is a critical and challenging task in the field of\nautonomous driving. A common paradigm relies on modeling individual object\nmotion, e.g., Kalman filters, to predict trajectories. While effective in\nsimple scenarios, this approach often struggles in crowded environments or with\ninaccurate detections, as it overlooks the rich geometric relationships between\nobjects. This highlights the need to leverage spatial cues. However, existing\ngeometry-aware methods can be susceptible to interference from irrelevant\nobjects, leading to ambiguous features and incorrect associations. To address\nthis, we propose focusing on cue-consistency: identifying and matching stable\nspatial patterns over time. We introduce the Dynamic Scene Cue-Consistency\nTracker (DSC-Track) to implement this principle. Firstly, we design a unified\nspatiotemporal encoder using Point Pair Features (PPF) to learn discriminative\ntrajectory embeddings while suppressing interference. Secondly, our\ncue-consistency transformer module explicitly aligns consistent feature\nrepresentations between historical tracks and current detections. Finally, a\ndynamic update mechanism preserves salient spatiotemporal information for\nstable online tracking. Extensive experiments on the nuScenes and Waymo Open\nDatasets validate the effectiveness and robustness of our approach. On the\nnuScenes benchmark, for instance, our method achieves state-of-the-art\nperformance, reaching 73.2% and 70.3% AMOTA on the validation and test sets,\nrespectively.",
        "url": "http://arxiv.org/abs/2508.11323v1",
        "published_date": "2025-08-15T08:48:13+00:00",
        "updated_date": "2025-08-15T08:48:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haonan Zhang",
            "Xinyao Wang",
            "Boxi Wu",
            "Tu Zheng",
            "Wang Yunhua",
            "Zheng Yang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a Dynamic Scene Cue-Consistency Tracker for robust 3D multi-object tracking in autonomous driving, achieving state-of-the-art performance on benchmark datasets.",
        "tldr_zh": "本文提出了一种适用于自动驾驶中的稳健的3D多目标追踪的动态场景线索一致性跟踪器，在基准数据集上取得了最先进的性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Scene Graph-Guided Proactive Replanning for Failure-Resilient Embodied Agent",
        "summary": "When humans perform everyday tasks, we naturally adjust our actions based on\nthe current state of the environment. For instance, if we intend to put\nsomething into a drawer but notice it is closed, we open it first. However,\nmany autonomous robots lack this adaptive awareness. They often follow\npre-planned actions that may overlook subtle yet critical changes in the scene,\nwhich can result in actions being executed under outdated assumptions and\neventual failure. While replanning is critical for robust autonomy, most\nexisting methods respond only after failures occur, when recovery may be\ninefficient or infeasible. While proactive replanning holds promise for\npreventing failures in advance, current solutions often rely on manually\ndesigned rules and extensive supervision. In this work, we present a proactive\nreplanning framework that detects and corrects failures at subtask boundaries\nby comparing scene graphs constructed from current RGB-D observations against\nreference graphs extracted from successful demonstrations. When the current\nscene fails to align with reference trajectories, a lightweight reasoning\nmodule is activated to diagnose the mismatch and adjust the plan. Experiments\nin the AI2-THOR simulator demonstrate that our approach detects semantic and\nspatial mismatches before execution failures occur, significantly improving\ntask success and robustness.",
        "url": "http://arxiv.org/abs/2508.11286v1",
        "published_date": "2025-08-15T07:48:51+00:00",
        "updated_date": "2025-08-15T07:48:51+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Che Rin Yu",
            "Daewon Chae",
            "Dabin Seo",
            "Sangwon Lee",
            "Hyeongwoo Im",
            "Jinkyu Kim"
        ],
        "ai_categories": [
            "AIGC"
        ],
        "tldr": "The paper introduces a proactive replanning framework for autonomous robots that detects and corrects failures before they occur by comparing scene graphs.",
        "tldr_zh": "该论文介绍了一种主动重规划框架，用于检测和纠正机器人在失败发生之前的错误。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Enhancing Supervised Composed Image Retrieval via Reasoning-Augmented Representation Engineering",
        "summary": "Composed Image Retrieval (CIR) presents a significant challenge as it\nrequires jointly understanding a reference image and a modified textual\ninstruction to find relevant target images. Some existing methods attempt to\nuse a two-stage approach to further refine retrieval results. However, this\noften requires additional training of a ranking model. Despite the success of\nChain-of-Thought (CoT) techniques in reducing training costs for language\nmodels, their application in CIR tasks remains limited -- compressing visual\ninformation into text or relying on elaborate prompt designs. Besides, existing\nworks only utilize it for zero-shot CIR, as it is challenging to achieve\nsatisfactory results in supervised CIR with a well-trained model. In this work,\nwe proposed a framework that includes the Pyramid Matching Model with\nTraining-Free Refinement (PMTFR) to address these challenges. Through a simple\nbut effective module called Pyramid Patcher, we enhanced the Pyramid Matching\nModel's understanding of visual information at different granularities.\nInspired by representation engineering, we extracted representations from COT\ndata and injected them into the LVLMs. This approach allowed us to obtain\nrefined retrieval scores in the Training-Free Refinement paradigm without\nrelying on explicit textual reasoning, further enhancing performance. Extensive\nexperiments on CIR benchmarks demonstrate that PMTFR surpasses state-of-the-art\nmethods in supervised CIR tasks. The code will be made public.",
        "url": "http://arxiv.org/abs/2508.11272v1",
        "published_date": "2025-08-15T07:10:10+00:00",
        "updated_date": "2025-08-15T07:10:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jun Li",
            "Kai Li",
            "Shaoguo Liu",
            "Tingting Gao"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces a framework called PMTFR to enhance Composed Image Retrieval tasks through representation engineering and Training-Free Refinement.",
        "tldr_zh": "本文介绍了一个名为PMTFR的框架，通过表示工程和无需训练的优化来提高复合图像检索任务。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Domain-aware Category-level Geometry Learning Segmentation for 3D Point Clouds",
        "summary": "Domain generalization in 3D segmentation is a critical challenge in deploying\nmodels to unseen environments. Current methods mitigate the domain shift by\naugmenting the data distribution of point clouds. However, the model learns\nglobal geometric patterns in point clouds while ignoring the category-level\ndistribution and alignment. In this paper, a category-level geometry learning\nframework is proposed to explore the domain-invariant geometric features for\ndomain generalized 3D semantic segmentation. Specifically, Category-level\nGeometry Embedding (CGE) is proposed to perceive the fine-grained geometric\nproperties of point cloud features, which constructs the geometric properties\nof each class and couples geometric embedding to semantic learning. Secondly,\nGeometric Consistent Learning (GCL) is proposed to simulate the latent 3D\ndistribution and align the category-level geometric embeddings, allowing the\nmodel to focus on the geometric invariant information to improve\ngeneralization. Experimental results verify the effectiveness of the proposed\nmethod, which has very competitive segmentation accuracy compared with the\nstate-of-the-art domain generalized point cloud methods.",
        "url": "http://arxiv.org/abs/2508.11265v1",
        "published_date": "2025-08-15T07:02:08+00:00",
        "updated_date": "2025-08-15T07:02:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pei He",
            "Lingling Li",
            "Licheng Jiao",
            "Ronghua Shang",
            "Fang Liu",
            "Shuang Wang",
            "Xu Liu",
            "Wenping Ma"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a category-level geometry learning framework for domain-generalized 3D semantic segmentation in point clouds, achieving competitive results compared to existing methods.",
        "tldr_zh": "本文提出了一种针对点云中的领域广义3D语义分割的类别级几何学习框架，与现有方法相比取得了竞争性的结果。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 7.25
    },
    {
        "title": "Can Multi-modal (reasoning) LLMs detect document manipulation?",
        "summary": "Document fraud poses a significant threat to industries reliant on secure and\nverifiable documentation, necessitating robust detection mechanisms. This study\ninvestigates the efficacy of state-of-the-art multi-modal large language models\n(LLMs)-including OpenAI O1, OpenAI 4o, Gemini Flash (thinking), Deepseek Janus,\nGrok, Llama 3.2 and 4, Qwen 2 and 2.5 VL, Mistral Pixtral, and Claude 3.5 and\n3.7 Sonnet-in detecting fraudulent documents. We benchmark these models against\neach other and prior work on document fraud detection techniques using a\nstandard dataset with real transactional documents. Through prompt optimization\nand detailed analysis of the models' reasoning processes, we evaluate their\nability to identify subtle indicators of fraud, such as tampered text,\nmisaligned formatting, and inconsistent transactional sums. Our results reveal\nthat top-performing multi-modal LLMs demonstrate superior zero-shot\ngeneralization, outperforming conventional methods on out-of-distribution\ndatasets, while several vision LLMs exhibit inconsistent or subpar performance.\nNotably, model size and advanced reasoning capabilities show limited\ncorrelation with detection accuracy, suggesting task-specific fine-tuning is\ncritical. This study underscores the potential of multi-modal LLMs in enhancing\ndocument fraud detection systems and provides a foundation for future research\ninto interpretable and scalable fraud mitigation strategies.",
        "url": "http://arxiv.org/abs/2508.11021v1",
        "published_date": "2025-08-14T18:57:07+00:00",
        "updated_date": "2025-08-14T18:57:07+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Zisheng Liang",
            "Kidus Zewde",
            "Rudra Pratap Singh",
            "Disha Patil",
            "Zexi Chen",
            "Jiayu Xue",
            "Yao Yao",
            "Yifei Chen",
            "Qinzhe Liu",
            "Simiao Ren"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper explores the use of multi-modal large language models for detecting document fraud, showing superior performance compared to traditional methods.",
        "tldr_zh": "本文探讨了利用多模态大型语言模型来检测文档欺诈，在性能上优于传统方法。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "CoreEditor: Consistent 3D Editing via Correspondence-constrained Diffusion",
        "summary": "Text-driven 3D editing seeks to modify 3D scenes according to textual\ndescriptions, and most existing approaches tackle this by adapting pre-trained\n2D image editors to multi-view inputs. However, without explicit control over\nmulti-view information exchange, they often fail to maintain cross-view\nconsistency, leading to insufficient edits and blurry details. We introduce\nCoreEditor, a novel framework for consistent text-to-3D editing. The key\ninnovation is a correspondence-constrained attention mechanism that enforces\nprecise interactions between pixels expected to remain consistent throughout\nthe diffusion denoising process. Beyond relying solely on geometric alignment,\nwe further incorporate semantic similarity estimated during denoising, enabling\nmore reliable correspondence modeling and robust multi-view editing. In\naddition, we design a selective editing pipeline that allows users to choose\npreferred results from multiple candidates, offering greater flexibility and\nuser control. Extensive experiments show that CoreEditor produces high-quality,\n3D-consistent edits with sharper details, significantly outperforming prior\nmethods.",
        "url": "http://arxiv.org/abs/2508.11603v1",
        "published_date": "2025-08-15T17:13:11+00:00",
        "updated_date": "2025-08-15T17:13:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhe Zhu",
            "Honghua Chen",
            "Peng Li",
            "Mingqiang Wei"
        ],
        "ai_categories": [
            "Diffusion"
        ],
        "tldr": "CoreEditor introduces a framework for consistent text-to-3D editing with a correspondence-constrained attention mechanism, producing high-quality 3D-consistent edits with sharper details.",
        "tldr_zh": "CoreEditor引入了一个框架，通过一种对应约束的注意机制实现了一致的文本到3D编辑，可以产生质量高、细节更清晰的3D编辑结果。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.0
    },
    {
        "title": "Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks",
        "summary": "Deploying multiple machine learning models on resource-constrained robotic\nplatforms for different perception tasks often results in redundant\ncomputations, large memory footprints, and complex integration challenges. In\nresponse, this work presents Visual Perception Engine (VPEngine), a modular\nframework designed to enable efficient GPU usage for visual multitasking while\nmaintaining extensibility and developer accessibility. Our framework\narchitecture leverages a shared foundation model backbone that extracts image\nrepresentations, which are efficiently shared, without any unnecessary GPU-CPU\nmemory transfers, across multiple specialized task-specific model heads running\nin parallel. This design eliminates the computational redundancy inherent in\nfeature extraction component when deploying traditional sequential models while\nenabling dynamic task prioritization based on application demands. We\ndemonstrate our framework's capabilities through an example implementation\nusing DINOv2 as the foundation model with multiple task (depth, object\ndetection and semantic segmentation) heads, achieving up to 3x speedup compared\nto sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine\noffers efficient GPU utilization and maintains a constant memory footprint\nwhile allowing per-task inference frequencies to be adjusted dynamically during\nruntime. The framework is written in Python and is open source with ROS2 C++\n(Humble) bindings for ease of use by the robotics community across diverse\nrobotic platforms. Our example implementation demonstrates end-to-end real-time\nperformance at $\\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized\nmodels.",
        "url": "http://arxiv.org/abs/2508.11584v1",
        "published_date": "2025-08-15T16:42:23+00:00",
        "updated_date": "2025-08-15T16:42:23+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Jakub Łucki",
            "Jonathan Becktor",
            "Georgios Georgakis",
            "Robert Royce",
            "Shehryar Khattak"
        ],
        "ai_categories": [
            "Transformer",
            "AIGC",
            "Multimodality"
        ],
        "tldr": "The paper introduces Visual Perception Engine (VPEngine), a framework for efficient GPU usage in robotic vision tasks by sharing image representations across multiple specialized models, resulting in up to 3x speedup compared to sequential execution.",
        "tldr_zh": "该论文介绍了Visual Perception Engine (VPEngine)，一个用于在机器人视觉任务中高效利用GPU的框架，通过在多个专门模型之间共享图像表示，实现了相比顺序执行高达3倍的加速。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Handwritten Text Recognition of Historical Manuscripts Using Transformer-Based Models",
        "summary": "Historical handwritten text recognition (HTR) is essential for unlocking the\ncultural and scholarly value of archival documents, yet digitization is often\nhindered by scarce transcriptions, linguistic variation, and highly diverse\nhandwriting styles. In this study, we apply TrOCR, a state-of-the-art\ntransformer-based HTR model, to 16th-century Latin manuscripts authored by\nRudolf Gwalther. We investigate targeted image preprocessing and a broad suite\nof data augmentation techniques, introducing four novel augmentation methods\ndesigned specifically for historical handwriting characteristics. We also\nevaluate ensemble learning approaches to leverage the complementary strengths\nof augmentation-trained models. On the Gwalther dataset, our best single-model\naugmentation (Elastic) achieves a Character Error Rate (CER) of 1.86, while a\ntop-5 voting ensemble achieves a CER of 1.60 - representing a 50% relative\nimprovement over the best reported TrOCR_BASE result and a 42% improvement over\nthe previous state of the art. These results highlight the impact of\ndomain-specific augmentations and ensemble strategies in advancing HTR\nperformance for historical manuscripts.",
        "url": "http://arxiv.org/abs/2508.11499v1",
        "published_date": "2025-08-15T14:20:58+00:00",
        "updated_date": "2025-08-15T14:20:58+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.DL",
            "cs.LG"
        ],
        "authors": [
            "Erez Meoded"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces TrOCR, a transformer-based HTR model, for recognizing historical handwritten text with domain-specific augmentations and ensemble learning strategies.",
        "tldr_zh": "该论文介绍了TrOCR，一个基于transformer的HTR模型，用于识别具有领域特定增强和集成学习策略的历史手写文本。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Relative Position Matters: Trajectory Prediction and Planning with Polar Representation",
        "summary": "Trajectory prediction and planning in autonomous driving are highly\nchallenging due to the complexity of predicting surrounding agents' movements\nand planning the ego agent's actions in dynamic environments. Existing methods\nencode map and agent positions and decode future trajectories in Cartesian\ncoordinates. However, modeling the relationships between the ego vehicle and\nsurrounding traffic elements in Cartesian space can be suboptimal, as it does\nnot naturally capture the varying influence of different elements based on\ntheir relative distances and directions. To address this limitation, we adopt\nthe Polar coordinate system, where positions are represented by radius and\nangle. This representation provides a more intuitive and effective way to model\nspatial changes and relative relationships, especially in terms of distance and\ndirectional influence. Based on this insight, we propose Polaris, a novel\nmethod that operates entirely in Polar coordinates, distinguishing itself from\nconventional Cartesian-based approaches. By leveraging the Polar\nrepresentation, this method explicitly models distance and direction variations\nand captures relative relationships through dedicated encoding and refinement\nmodules, enabling more structured and spatially aware trajectory prediction and\nplanning. Extensive experiments on the challenging prediction (Argoverse 2) and\nplanning benchmarks (nuPlan) demonstrate that Polaris achieves state-of-the-art\nperformance.",
        "url": "http://arxiv.org/abs/2508.11492v1",
        "published_date": "2025-08-15T14:15:11+00:00",
        "updated_date": "2025-08-15T14:15:11+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Bozhou Zhang",
            "Nan Song",
            "Bingzhao Gao",
            "Li Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel method, Polaris, for trajectory prediction and planning in autonomous driving, using Polar coordinates to better model spatial relationships.",
        "tldr_zh": "本文提出了一种新颖的方法，Polaris，用于自动驾驶中的轨迹预测和规划，利用极坐标更好地建模空间关系。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Data-Driven Deepfake Image Detection Method -- The 2024 Global Deepfake Image Detection Challenge",
        "summary": "With the rapid development of technology in the field of AI, deepfake\ntechnology has emerged as a double-edged sword. It has not only created a large\namount of AI-generated content but also posed unprecedented challenges to\ndigital security. The task of the competition is to determine whether a face\nimage is a Deepfake image and output its probability score of being a Deepfake\nimage. In the image track competition, our approach is based on the Swin\nTransformer V2-B classification network. And online data augmentation and\noffline sample generation methods are employed to enrich the diversity of\ntraining samples and increase the generalization ability of the model. Finally,\nwe got the award of excellence in Deepfake image detection.",
        "url": "http://arxiv.org/abs/2508.11464v1",
        "published_date": "2025-08-15T13:24:47+00:00",
        "updated_date": "2025-08-15T13:24:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoya Zhu",
            "Yibing Nan",
            "Shiguo Lian"
        ],
        "ai_categories": [
            "GAN",
            "Transformer"
        ],
        "tldr": "The paper introduces a data-driven deepfake image detection method that won an award, utilizing the Swin Transformer V2-B classification network and various sample generation methods for enhanced performance.",
        "tldr_zh": "本文介绍了一个基于数据驱动的深度伪造图像检测方法，利用Swin Transformer V2-B分类网络和各种样本生成方法提高性能，获得了优秀奖。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Subcortical Masks Generation in CT Images via Ensemble-Based Cross-Domain Label Transfer",
        "summary": "Subcortical segmentation in neuroimages plays an important role in\nunderstanding brain anatomy and facilitating computer-aided diagnosis of\ntraumatic brain injuries and neurodegenerative disorders. However, training\naccurate automatic models requires large amounts of labelled data. Despite the\navailability of publicly available subcortical segmentation datasets for\nMagnetic Resonance Imaging (MRI), a significant gap exists for Computed\nTomography (CT). This paper proposes an automatic ensemble framework to\ngenerate high-quality subcortical segmentation labels for CT scans by\nleveraging existing MRI-based models. We introduce a robust ensembling pipeline\nto integrate them and apply it to unannotated paired MRI-CT data, resulting in\na comprehensive CT subcortical segmentation dataset. Extensive experiments on\nmultiple public datasets demonstrate the superior performance of our proposed\nframework. Furthermore, using our generated CT dataset, we train segmentation\nmodels that achieve improved performance on related segmentation tasks. To\nfacilitate future research, we make our source code, generated dataset, and\ntrained models publicly available at\nhttps://github.com/SCSE-Biomedical-Computing-Group/CT-Subcortical-Segmentation,\nmarking the first open-source release for CT subcortical segmentation to the\nbest of our knowledge.",
        "url": "http://arxiv.org/abs/2508.11450v1",
        "published_date": "2025-08-15T12:57:35+00:00",
        "updated_date": "2025-08-15T12:57:35+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Augustine X. W. Lee",
            "Pak-Hei Yeung",
            "Jagath C. Rajapakse"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes an automatic ensemble framework to generate subcortical segmentation labels for CT scans using existing MRI-based models, resulting in a comprehensive CT subcortical segmentation dataset.",
        "tldr_zh": "本文提出了一种自动集成框架，利用现有的基于MRI的模型为CT扫描生成亚皮层分割标签，从而得到一个全面的CT亚皮层分割数据集。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Cost-Effective Active Labeling for Data-Efficient Cervical Cell Classification",
        "summary": "Information on the number and category of cervical cells is crucial for the\ndiagnosis of cervical cancer. However, existing classification methods capable\nof automatically measuring this information require the training dataset to be\nrepresentative, which consumes an expensive or even unaffordable human cost. We\nherein propose active labeling that enables us to construct a representative\ntraining dataset using a much smaller human cost for data-efficient cervical\ncell classification. This cost-effective method efficiently leverages the\nclassifier's uncertainty on the unlabeled cervical cell images to accurately\nselect images that are most beneficial to label. With a fast estimation of the\nuncertainty, this new algorithm exhibits its validity and effectiveness in\nenhancing the representative ability of the constructed training dataset. The\nextensive empirical results confirm its efficacy again in navigating the usage\nof human cost, opening the avenue for data-efficient cervical cell\nclassification.",
        "url": "http://arxiv.org/abs/2508.11340v1",
        "published_date": "2025-08-15T09:11:15+00:00",
        "updated_date": "2025-08-15T09:11:15+00:00",
        "categories": [
            "cs.CV",
            "q-bio.TO"
        ],
        "authors": [
            "Yuanlin Liu",
            "Zhihan Zhou",
            "Mingqiang Wei",
            "Youyi Song"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "The paper proposes a cost-effective method for data-efficient cervical cell classification by using active labeling to construct a representative training dataset with less human cost.",
        "tldr_zh": "本文提出了一种经济高效的方法，通过主动标记构建代表性训练数据集，以更小的人力成本实现数据高效的宫颈细胞分类。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Hyperspectral vs. RGB for Pedestrian Segmentation in Urban Driving Scenes: A Comparative Study",
        "summary": "Pedestrian segmentation in automotive perception systems faces critical\nsafety challenges due to metamerism in RGB imaging, where pedestrians and\nbackgrounds appear visually indistinguishable.. This study investigates the\npotential of hyperspectral imaging (HSI) for enhanced pedestrian segmentation\nin urban driving scenarios using the Hyperspectral City v2 (H-City) dataset. We\ncompared standard RGB against two dimensionality-reduction approaches by\nconverting 128-channel HSI data into three-channel representations: Principal\nComponent Analysis (PCA) and optimal band selection using Contrast\nSignal-to-Noise Ratio with Joint Mutual Information Maximization (CSNR-JMIM).\nThree semantic segmentation models were evaluated: U-Net, DeepLabV3+, and\nSegFormer. CSNR-JMIM consistently outperformed RGB with an average improvements\nof 1.44% in Intersection over Union (IoU) and 2.18% in F1-score for pedestrian\nsegmentation. Rider segmentation showed similar gains with 1.43% IoU and 2.25%\nF1-score improvements. These improved performance results from enhanced\nspectral discrimination of optimally selected HSI bands effectively reducing\nfalse positives. This study demonstrates robust pedestrian segmentation through\noptimal HSI band selection, showing significant potential for safety-critical\nautomotive applications.",
        "url": "http://arxiv.org/abs/2508.11301v1",
        "published_date": "2025-08-15T08:10:19+00:00",
        "updated_date": "2025-08-15T08:10:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiarong Li",
            "Imad Ali Shah",
            "Enda Ward",
            "Martin Glavin",
            "Edward Jones",
            "Brian Deegan"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer",
            "Other"
        ],
        "tldr": "The paper compares hyperspectral imaging and RGB for pedestrian segmentation in urban driving scenes, showing improved performance with hyperspectral imaging.",
        "tldr_zh": "该论文比较了高光谱成像和RGB在城市驾驶场景中的行人分割，结果显示高光谱成像表现更好。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Unifying Scale-Aware Depth Prediction and Perceptual Priors for Monocular Endoscope Pose Estimation and Tissue Reconstruction",
        "summary": "Accurate endoscope pose estimation and 3D tissue surface reconstruction\nsignificantly enhances monocular minimally invasive surgical procedures by\nenabling accurate navigation and improved spatial awareness. However, monocular\nendoscope pose estimation and tissue reconstruction face persistent challenges,\nincluding depth ambiguity, physiological tissue deformation, inconsistent\nendoscope motion, limited texture fidelity, and a restricted field of view. To\novercome these limitations, a unified framework for monocular endoscopic tissue\nreconstruction that integrates scale-aware depth prediction with\ntemporally-constrained perceptual refinement is presented. This framework\nincorporates a novel MAPIS-Depth module, which leverages Depth Pro for robust\ninitialisation and Depth Anything for efficient per-frame depth prediction, in\nconjunction with L-BFGS-B optimisation, to generate pseudo-metric depth\nestimates. These estimates are temporally refined by computing pixel\ncorrespondences using RAFT and adaptively blending flow-warped frames based on\nLPIPS perceptual similarity, thereby reducing artefacts arising from\nphysiological tissue deformation and motion. To ensure accurate registration of\nthe synthesised pseudo-RGBD frames from MAPIS-Depth, a novel WEMA-RTDL module\nis integrated, optimising both rotation and translation. Finally, truncated\nsigned distance function-based volumetric fusion and marching cubes are applied\nto extract a comprehensive 3D surface mesh. Evaluations on HEVD and SCARED,\nwith ablation and comparative analyses, demonstrate the framework's robustness\nand superiority over state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2508.11282v1",
        "published_date": "2025-08-15T07:41:17+00:00",
        "updated_date": "2025-08-15T07:41:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Muzammil Khan",
            "Enzo Kerkhof",
            "Matteo Fusaglia",
            "Koert Kuhlmann",
            "Theo Ruers",
            "Françoise J. Siepel"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a unified framework for monocular endoscopic tissue reconstruction that combines depth prediction and perceptual refinement to improve accuracy and reduce artifacts.",
        "tldr_zh": "该论文介绍了一种统一框架，结合深度预测和感知细化，用于提高单眼内窥镜组织重建的准确性并减少伪影。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Boosting the Robustness-Accuracy Trade-off of SNNs by Robust Temporal Self-Ensemble",
        "summary": "Spiking Neural Networks (SNNs) offer a promising direction for\nenergy-efficient and brain-inspired computing, yet their vulnerability to\nadversarial perturbations remains poorly understood. In this work, we revisit\nthe adversarial robustness of SNNs through the lens of temporal ensembling,\ntreating the network as a collection of evolving sub-networks across discrete\ntimesteps. This formulation uncovers two critical but underexplored\nchallenges-the fragility of individual temporal sub-networks and the tendency\nfor adversarial vulnerabilities to transfer across time. To overcome these\nlimitations, we propose Robust Temporal self-Ensemble (RTE), a training\nframework that improves the robustness of each sub-network while reducing the\ntemporal transferability of adversarial perturbations. RTE integrates both\nobjectives into a unified loss and employs a stochastic sampling strategy for\nefficient optimization. Extensive experiments across multiple benchmarks\ndemonstrate that RTE consistently outperforms existing training methods in\nrobust-accuracy trade-off. Additional analyses reveal that RTE reshapes the\ninternal robustness landscape of SNNs, leading to more resilient and temporally\ndiversified decision boundaries. Our study highlights the importance of\ntemporal structure in adversarial learning and offers a principled foundation\nfor building robust spiking models.",
        "url": "http://arxiv.org/abs/2508.11279v1",
        "published_date": "2025-08-15T07:34:06+00:00",
        "updated_date": "2025-08-15T07:34:06+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Jihang Wang",
            "Dongcheng Zhao",
            "Ruolin Chen",
            "Qian Zhang",
            "Yi Zeng"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper introduces a training framework, Robust Temporal self-Ensemble (RTE), to improve the robustness and accuracy trade-off of Spiking Neural Networks (SNNs) by addressing temporal vulnerabilities.",
        "tldr_zh": "该论文引入了一种训练框架，Robust Temporal self-Ensemble（RTE），通过解决时间漏洞来改善脉冲神经网络（SNNs）的稳健性和准确性的权衡。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Fluid Dynamics and Domain Reconstruction from Noisy Flow Images Using Physics-Informed Neural Networks and Quasi-Conformal Mapping",
        "summary": "Blood flow imaging provides important information for hemodynamic behavior\nwithin the vascular system and plays an essential role in medical diagnosis and\ntreatment planning. However, obtaining high-quality flow images remains a\nsignificant challenge. In this work, we address the problem of denoising flow\nimages that may suffer from artifacts due to short acquisition times or\ndevice-induced errors. We formulate this task as an optimization problem, where\nthe objective is to minimize the discrepancy between the modeled velocity\nfield, constrained to satisfy the Navier-Stokes equations, and the observed\nnoisy velocity data. To solve this problem, we decompose it into two\nsubproblems: a fluid subproblem and a geometry subproblem. The fluid subproblem\nleverages a Physics-Informed Neural Network to reconstruct the velocity field\nfrom noisy observations, assuming a fixed domain. The geometry subproblem aims\nto infer the underlying flow region by optimizing a quasi-conformal mapping\nthat deforms a reference domain. These two subproblems are solved in an\nalternating Gauss-Seidel fashion, iteratively refining both the velocity field\nand the domain. Upon convergence, the framework yields a high-quality\nreconstruction of the flow image. We validate the proposed method through\nexperiments on synthetic flow data in a converging channel geometry under\nvarying levels of Gaussian noise, and on real-like flow data in an aortic\ngeometry with signal-dependent noise. The results demonstrate the effectiveness\nand robustness of the approach. Additionally, ablation studies are conducted to\nassess the influence of key hyperparameters.",
        "url": "http://arxiv.org/abs/2508.11216v1",
        "published_date": "2025-08-15T04:49:07+00:00",
        "updated_date": "2025-08-15T04:49:07+00:00",
        "categories": [
            "math.NA",
            "cs.CV",
            "cs.NA"
        ],
        "authors": [
            "Han Zhang",
            "Xue-Cheng Tai",
            "Jean-Michel Morel",
            "Raymond H. Chan"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper addresses denoising flow images for medical applications using a Physics-Informed Neural Network and quasi-conformal mapping.",
        "tldr_zh": "本文利用物理信息神经网络和拟合共形映射处理医学应用中的流场图像去噪。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Generating Dialogues from Egocentric Instructional Videos for Task Assistance: Dataset, Method and Benchmark",
        "summary": "Many everyday tasks ranging from fixing appliances, cooking recipes to car\nmaintenance require expert knowledge, especially when tasks are complex and\nmulti-step. Despite growing interest in AI agents, there is a scarcity of\ndialogue-video datasets grounded for real world task assistance. In this paper,\nwe propose a simple yet effective approach that transforms single-person\ninstructional videos into task-guidance two-person dialogues, aligned with fine\ngrained steps and video-clips. Our fully automatic approach, powered by large\nlanguage models, offers an efficient alternative to the substantial cost and\neffort required for human-assisted data collection. Using this technique, we\nbuild HowToDIV, a large-scale dataset containing 507 conversations, 6636\nquestion-answer pairs and 24 hours of videoclips across diverse tasks in\ncooking, mechanics, and planting. Each session includes multi-turn conversation\nwhere an expert teaches a novice user how to perform a task step by step, while\nobserving user's surrounding through a camera and microphone equipped wearable\ndevice. We establish the baseline benchmark performance on HowToDIV dataset\nthrough Gemma-3 model for future research on this new task of dialogues for\nprocedural-task assistance.",
        "url": "http://arxiv.org/abs/2508.11192v1",
        "published_date": "2025-08-15T03:57:20+00:00",
        "updated_date": "2025-08-15T03:57:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lavisha Aggarwal",
            "Vikas Bahirwani",
            "Lin Li",
            "Andrea Colaco"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a method to generate dialogues from instructional videos to assist in tasks, creating a dataset and benchmark for future research.",
        "tldr_zh": "该论文介绍了一种从指导视频中生成对话以辅助任务的方法，并为将来的研究创建了数据集和基准。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector",
        "summary": "Monocular 3D object detectors, while effective on data from one ego camera\nheight, struggle with unseen or out-of-distribution camera heights. Existing\nmethods often rely on Plucker embeddings, image transformations or data\naugmentation. This paper takes a step towards this understudied problem by\nfirst investigating the impact of camera height variations on state-of-the-art\n(SoTA) Mono3D models. With a systematic analysis on the extended CARLA dataset\nwith multiple camera heights, we observe that depth estimation is a primary\nfactor influencing performance under height variations. We mathematically prove\nand also empirically observe consistent negative and positive trends in mean\ndepth error of regressed and ground-based depth models, respectively, under\ncamera height changes. To mitigate this, we propose Camera Height Robust\nMonocular 3D Detector (CHARM3R), which averages both depth estimates within the\nmodel. CHARM3R improves generalization to unseen camera heights by more than\n$45\\%$, achieving SoTA performance on the CARLA dataset. Codes and Models at\nhttps://github.com/abhi1kumar/CHARM3R",
        "url": "http://arxiv.org/abs/2508.11185v1",
        "published_date": "2025-08-15T03:27:17+00:00",
        "updated_date": "2025-08-15T03:27:17+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Abhinav Kumar",
            "Yuliang Guo",
            "Zhihao Zhang",
            "Xinyu Huang",
            "Liu Ren",
            "Xiaoming Liu"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces CHARM3R, a Monocular 3D Detector that improves generalization to unseen camera heights by combining depth estimates within the model, achieving state-of-the-art performance on the CARLA dataset.",
        "tldr_zh": "该论文介绍了CHARM3R，一种单目3D检测器，通过在模型内部结合深度估计来改善对未见摄像头高度的泛化，在CARLA数据集上取得了最新成果。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Exploring the Tradeoff Between Diversity and Discrimination for Continuous Category Discovery",
        "summary": "Continuous category discovery (CCD) aims to automatically discover novel\ncategories in continuously arriving unlabeled data. This is a challenging\nproblem considering that there is no number of categories and labels in the\nnewly arrived data, while also needing to mitigate catastrophic forgetting.\nMost CCD methods cannot handle the contradiction between novel class discovery\nand classification well. They are also prone to accumulate errors in the\nprocess of gradually discovering novel classes. Moreover, most of them use\nknowledge distillation and data replay to prevent forgetting, occupying more\nstorage space. To address these limitations, we propose Independence-based\nDiversity and Orthogonality-based Discrimination (IDOD). IDOD mainly includes\nindependent enrichment of diversity module, joint discovery of novelty module,\nand continuous increment by orthogonality module. In independent enrichment,\nthe backbone is trained separately using contrastive loss to avoid it focusing\nonly on features for classification. Joint discovery transforms multi-stage\nnovel class discovery into single-stage, reducing error accumulation impact.\nContinuous increment by orthogonality module generates mutually orthogonal\nprototypes for classification and prevents forgetting with lower space overhead\nvia representative representation replay. Experimental results show that on\nchallenging fine-grained datasets, our method outperforms the state-of-the-art\nmethods.",
        "url": "http://arxiv.org/abs/2508.11173v1",
        "published_date": "2025-08-15T02:51:30+00:00",
        "updated_date": "2025-08-15T02:51:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruobing Jiang",
            "Yang Liu",
            "Haobing Liu",
            "Yanwei Yu",
            "Chunyang Wang"
        ],
        "ai_categories": [
            "Other"
        ],
        "tldr": "The paper introduces a new method, IDOD, to address the challenges of continuous category discovery, outperforming state-of-the-art methods on fine-grained datasets.",
        "tldr_zh": "本文介绍了一种新方法IDOD，用于解决连续类别发现的挑战，在细粒度数据集上优于现有方法。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "VFM-Guided Semi-Supervised Detection Transformer for Source-Free Object Detection in Remote Sensing Images",
        "summary": "Unsupervised domain adaptation methods have been widely explored to bridge\ndomain gaps. However, in real-world remote-sensing scenarios, privacy and\ntransmission constraints often preclude access to source domain data, which\nlimits their practical applicability. Recently, Source-Free Object Detection\n(SFOD) has emerged as a promising alternative, aiming at cross-domain\nadaptation without relying on source data, primarily through a self-training\nparadigm. Despite its potential, SFOD frequently suffers from training collapse\ncaused by noisy pseudo-labels, especially in remote sensing imagery with dense\nobjects and complex backgrounds. Considering that limited target domain\nannotations are often feasible in practice, we propose a Vision\nfoundation-Guided DEtection TRansformer (VG-DETR), built upon a semi-supervised\nframework for SFOD in remote sensing images. VG-DETR integrates a Vision\nFoundation Model (VFM) into the training pipeline in a \"free lunch\" manner,\nleveraging a small amount of labeled target data to mitigate pseudo-label noise\nwhile improving the detector's feature-extraction capability. Specifically, we\nintroduce a VFM-guided pseudo-label mining strategy that leverages the VFM's\nsemantic priors to further assess the reliability of the generated\npseudo-labels. By recovering potentially correct predictions from\nlow-confidence outputs, our strategy improves pseudo-label quality and\nquantity. In addition, a dual-level VFM-guided alignment method is proposed,\nwhich aligns detector features with VFM embeddings at both the instance and\nimage levels. Through contrastive learning among fine-grained prototypes and\nsimilarity matching between feature maps, this dual-level alignment further\nenhances the robustness of feature representations against domain gaps.\nExtensive experiments demonstrate that VG-DETR achieves superior performance in\nsource-free remote sensing detection tasks.",
        "url": "http://arxiv.org/abs/2508.11167v1",
        "published_date": "2025-08-15T02:35:56+00:00",
        "updated_date": "2025-08-15T02:35:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jianhong Han",
            "Yupei Wang",
            "Liang Chen"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a Vision Foundation-Guided Detection Transformer for source-free object detection in remote sensing images, which shows superior performance in extensive experiments.",
        "tldr_zh": "该论文提出了一种基于视觉基础的检测变换器，用于遥感图像中的无源目标检测，在广泛的实验中表现出优秀的性能。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Remove360: Benchmarking Residuals After Object Removal in 3D Gaussian Splatting",
        "summary": "Understanding what semantic information persists after object removal is\ncritical for privacy-preserving 3D reconstruction and editable scene\nrepresentations. In this work, we introduce a novel benchmark and evaluation\nframework to measure semantic residuals, the unintended semantic traces left\nbehind, after object removal in 3D Gaussian Splatting. We conduct experiments\nacross a diverse set of indoor and outdoor scenes, showing that current methods\ncan preserve semantic information despite the absence of visual geometry. We\nalso release Remove360, a dataset of pre/post-removal RGB images and\nobject-level masks captured in real-world environments. While prior datasets\nhave focused on isolated object instances, Remove360 covers a broader and more\ncomplex range of indoor and outdoor scenes, enabling evaluation of object\nremoval in the context of full-scene representations. Given ground truth images\nof a scene before and after object removal, we assess whether we can truly\neliminate semantic presence, and if downstream models can still infer what was\nremoved. Our findings reveal critical limitations in current 3D object removal\ntechniques and underscore the need for more robust solutions capable of\nhandling real-world complexity. The evaluation framework is available at\ngithub.com/spatial-intelligence-ai/Remove360.git. Data are available at\nhuggingface.co/datasets/simkoc/Remove360.",
        "url": "http://arxiv.org/abs/2508.11431v1",
        "published_date": "2025-08-15T12:15:06+00:00",
        "updated_date": "2025-08-15T12:15:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Simona Kocour",
            "Assia Benbihi",
            "Torsten Sattler"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a benchmark to measure residual semantic information after object removal in 3D scenes using Gaussian splatting, highlighting limitations in current techniques and the need for more robust solutions.",
        "tldr_zh": "该论文引入了一个基准来衡量通过高斯喷墨在3D场景中去除对象后余留的语义信息，突出了当前技术的局限性以及对更健壮解决方案的需求。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 6.75
    },
    {
        "title": "Noise Matters: Optimizing Matching Noise for Diffusion Classifiers",
        "summary": "Although today's pretrained discriminative vision-language models (e.g.,\nCLIP) have demonstrated strong perception abilities, such as zero-shot image\nclassification, they also suffer from the bag-of-words problem and spurious\nbias. To mitigate these problems, some pioneering studies leverage powerful\ngenerative models (e.g., pretrained diffusion models) to realize generalizable\nimage classification, dubbed Diffusion Classifier (DC). Specifically, by\nrandomly sampling a Gaussian noise, DC utilizes the differences of denoising\neffects with different category conditions to classify categories.\nUnfortunately, an inherent and notorious weakness of existing DCs is noise\ninstability: different random sampled noises lead to significant performance\nchanges. To achieve stable classification performance, existing DCs always\nensemble the results of hundreds of sampled noises, which significantly reduces\nthe classification speed. To this end, we firstly explore the role of noise in\nDC, and conclude that: there are some ``good noises'' that can relieve the\ninstability. Meanwhile, we argue that these good noises should meet two\nprinciples: Frequency Matching and Spatial Matching. Regarding both principles,\nwe propose a novel Noise Optimization method to learn matching (i.e., good)\nnoise for DCs: NoOp. For frequency matching, NoOp first optimizes a\ndataset-specific noise: Given a dataset and a timestep t, optimize one randomly\ninitialized parameterized noise. For Spatial Matching, NoOp trains a\nMeta-Network that adopts an image as input and outputs image-specific noise\noffset. The sum of optimized noise and noise offset will be used in DC to\nreplace random noise. Extensive ablations on various datasets demonstrated the\neffectiveness of NoOp.",
        "url": "http://arxiv.org/abs/2508.11330v1",
        "published_date": "2025-08-15T09:01:03+00:00",
        "updated_date": "2025-08-15T09:01:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanghao Wang",
            "Long Chen"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a Noise Optimization method to improve stability in Diffusion Classifiers by learning 'good noises' for classification, demonstrating effectiveness on various datasets.",
        "tldr_zh": "本文引入了一种噪声优化方法，通过学习用于分类的'好噪声'来提高扩散分类器的稳定性，在各种数据集上展示了其有效性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Data-Driven Abdominal Phenotypes of Type 2 Diabetes in Lean, Overweight, and Obese Cohorts",
        "summary": "Purpose: Although elevated BMI is a well-known risk factor for type 2\ndiabetes, the disease's presence in some lean adults and absence in others with\nobesity suggests that detailed body composition may uncover abdominal\nphenotypes of type 2 diabetes. With AI, we can now extract detailed\nmeasurements of size, shape, and fat content from abdominal structures in 3D\nclinical imaging at scale. This creates an opportunity to empirically define\nbody composition signatures linked to type 2 diabetes risk and protection using\nlarge-scale clinical data. Approach: To uncover BMI-specific diabetic abdominal\npatterns from clinical CT, we applied our design four times: once on the full\ncohort (n = 1,728) and once on lean (n = 497), overweight (n = 611), and obese\n(n = 620) subgroups separately. Briefly, our experimental design transforms\nabdominal scans into collections of explainable measurements through\nsegmentation, classifies type 2 diabetes through a cross-validated random\nforest, measures how features contribute to model-estimated risk or protection\nthrough SHAP analysis, groups scans by shared model decision patterns\n(clustering from SHAP) and links back to anatomical differences\n(classification). Results: The random-forests achieved mean AUCs of 0.72-0.74.\nThere were shared type 2 diabetes signatures in each group; fatty skeletal\nmuscle, older age, greater visceral and subcutaneous fat, and a smaller or\nfat-laden pancreas. Univariate logistic regression confirmed the direction of\n14-18 of the top 20 predictors within each subgroup (p < 0.05). Conclusions:\nOur findings suggest that abdominal drivers of type 2 diabetes may be\nconsistent across weight classes.",
        "url": "http://arxiv.org/abs/2508.11063v1",
        "published_date": "2025-08-14T20:48:08+00:00",
        "updated_date": "2025-08-14T20:48:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lucas W. Remedios",
            "Chloe Choe",
            "Trent M. Schwartz",
            "Dingjie Su",
            "Gaurav Rudravaram",
            "Chenyu Gao",
            "Aravind R. Krishnan",
            "Adam M. Saunders",
            "Michael E. Kim",
            "Shunxing Bao",
            "Alvin C. Powers",
            "Bennett A. Landman",
            "John Virostko"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper explores abdominal phenotypes of type 2 diabetes in lean, overweight, and obese individuals using AI and 3D clinical imaging, finding common factors across weight classes.",
        "tldr_zh": "该论文利用人工智能和3D临床成像探索了瘦人、超重和肥胖人群中的2型糖尿病腹部表型，发现各体重类别之间存在共同因素。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Is ChatGPT-5 Ready for Mammogram VQA?",
        "summary": "Mammogram visual question answering (VQA) integrates image interpretation\nwith clinical reasoning and has potential to support breast cancer screening.\nWe systematically evaluated the GPT-5 family and GPT-4o model on four public\nmammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment,\nabnormality detection, and malignancy classification tasks. GPT-5 consistently\nwas the best performing model but lagged behind both human experts and\ndomain-specific fine-tuned models. On EMBED, GPT-5 achieved the highest scores\namong GPT variants in density (56.8%), distortion (52.5%), mass (64.5%),\ncalcification (63.5%), and malignancy (52.8%) classification. On InBreast, it\nattained 36.9% BI-RADS accuracy, 45.9% abnormality detection, and 35.0%\nmalignancy classification. On CMMD, GPT-5 reached 32.3% abnormality detection\nand 55.0% malignancy accuracy. On CBIS-DDSM, it achieved 69.3% BI-RADS\naccuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy. Compared\nwith human expert estimations, GPT-5 exhibited lower sensitivity (63.5%) and\nspecificity (52.3%). While GPT-5 exhibits promising capabilities for screening\ntasks, its performance remains insufficient for high-stakes clinical imaging\napplications without targeted domain adaptation and optimization. However, the\ntremendous improvements in performance from GPT-4o to GPT-5 show a promising\ntrend in the potential for general large language models (LLMs) to assist with\nmammography VQA tasks.",
        "url": "http://arxiv.org/abs/2508.11628v1",
        "published_date": "2025-08-15T17:56:24+00:00",
        "updated_date": "2025-08-15T17:56:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Qiang Li",
            "Shansong Wang",
            "Mingzhe Hu",
            "Mojtaba Safari",
            "Zachary Eidex",
            "Xiaofeng Yang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper evaluates the performance of GPT-5 for mammogram VQA tasks, showing promising results but indicating the need for further optimization for clinical applications.",
        "tldr_zh": "本文评估了GPT-5在乳腺X光检查VQA任务中的表现，结果表现有希望，但指出需要进一步优化以适用于临床应用。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "OpenConstruction: A Systematic Synthesis of Open Visual Datasets for Data-Centric Artificial Intelligence in Construction Monitoring",
        "summary": "The construction industry increasingly relies on visual data to support\nArtificial Intelligence (AI) and Machine Learning (ML) applications for site\nmonitoring. High-quality, domain-specific datasets, comprising images, videos,\nand point clouds, capture site geometry and spatiotemporal dynamics, including\nthe location and interaction of objects, workers, and materials. However,\ndespite growing interest in leveraging visual datasets, existing resources vary\nwidely in sizes, data modalities, annotation quality, and representativeness of\nreal-world construction conditions. A systematic review to categorize their\ndata characteristics and application contexts is still lacking, limiting the\ncommunity's ability to fully understand the dataset landscape, identify\ncritical gaps, and guide future directions toward more effective, reliable, and\nscalable AI applications in construction. To address this gap, this study\nconducts an extensive search of academic databases and open-data platforms,\nyielding 51 publicly available visual datasets that span the 2005-2024 period.\nThese datasets are categorized using a structured data schema covering (i) data\nfundamentals (e.g., size and license), (ii) data modalities (e.g., RGB and\npoint cloud), (iii) annotation frameworks (e.g., bounding boxes), and (iv)\ndownstream application domains (e.g., progress tracking). This study\nsynthesizes these findings into an open-source catalog, OpenConstruction,\nsupporting data-driven method development. Furthermore, the study discusses\nseveral critical limitations in the existing construction dataset landscape and\npresents a roadmap for future data infrastructure anchored in the Findability,\nAccessibility, Interoperability, and Reusability (FAIR) principles. By\nreviewing the current landscape and outlining strategic priorities, this study\nsupports the advancement of data-centric solutions in the construction sector.",
        "url": "http://arxiv.org/abs/2508.11482v1",
        "published_date": "2025-08-15T13:56:21+00:00",
        "updated_date": "2025-08-15T13:56:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruoxin Xiong",
            "Yanyu Wang",
            "Jiannan Cai",
            "Kaijian Liu",
            "Yuansheng Zhu",
            "Pingbo Tang",
            "Nora El-Gohary"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper presents OpenConstruction, an open-source catalog of visual datasets for AI in construction monitoring. It categorizes 51 datasets and outlines a roadmap for future data infrastructure.",
        "tldr_zh": "本文介绍了OpenConstruction，这是一个用于建筑监测人工智能的开源数据集目录。它对51个数据集进行了分类，并概述了未来数据基础设施的发展路线。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "Model Interpretability and Rationale Extraction by Input Mask Optimization",
        "summary": "Concurrent to the rapid progress in the development of neural-network based\nmodels in areas like natural language processing and computer vision, the need\nfor creating explanations for the predictions of these black-box models has\nrisen steadily. We propose a new method to generate extractive explanations for\npredictions made by neural networks, that is based on masking parts of the\ninput which the model does not consider to be indicative of the respective\nclass. The masking is done using gradient-based optimization combined with a\nnew regularization scheme that enforces sufficiency, comprehensiveness and\ncompactness of the generated explanation, three properties that are known to be\ndesirable from the related field of rationale extraction in natural language\nprocessing. In this way, we bridge the gap between model interpretability and\nrationale extraction, thereby proving that the latter of which can be performed\nwithout training a specialized model, only on the basis of a trained\nclassifier. We further apply the same method to image inputs and obtain high\nquality explanations for image classifications, which indicates that the\nconditions proposed for rationale extraction in natural language processing are\nmore broadly applicable to different input types.",
        "url": "http://arxiv.org/abs/2508.11388v1",
        "published_date": "2025-08-15T10:41:09+00:00",
        "updated_date": "2025-08-15T10:41:09+00:00",
        "categories": [
            "cs.CL",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Marc Brinner",
            "Sina Zarriess"
        ],
        "ai_categories": [
            "Transformer",
            "AIGC"
        ],
        "tldr": "This paper introduces a method to generate explanations for black-box neural network predictions by masking irrelevant input parts, bridging model interpretability and rationale extraction.",
        "tldr_zh": "本文介绍了一种通过屏蔽无关输入部分来生成黑盒神经网络预测解释的方法，弥合了模型可解释性和理由提取之间的差距。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.0
    },
    {
        "title": "Vision-Language Models display a strong gender bias",
        "summary": "Vision-language models (VLM) align images and text in a shared representation\nspace that is useful for retrieval and zero-shot transfer. Yet, this alignment\ncan encode and amplify social stereotypes in subtle ways that are not obvious\nfrom standard accuracy metrics. In this study, we test whether the contrastive\nvision-language encoder exhibits gender-linked associations when it places\nembeddings of face images near embeddings of short phrases that describe\noccupations and activities. We assemble a dataset of 220 face photographs split\nby perceived binary gender and a set of 150 unique statements distributed\nacross six categories covering emotional labor, cognitive labor, domestic\nlabor, technical labor, professional roles, and physical labor. We compute\nunit-norm image embeddings for every face and unit-norm text embeddings for\nevery statement, then define a statement-level association score as the\ndifference between the mean cosine similarity to the male set and the mean\ncosine similarity to the female set, where positive values indicate stronger\nassociation with the male set and negative values indicate stronger association\nwith the female set. We attach bootstrap confidence intervals by resampling\nimages within each gender group, aggregate by category with a separate\nbootstrap over statements, and run a label-swap null model that estimates the\nlevel of mean absolute association we would expect if no gender structure were\npresent. The outcome is a statement-wise and category-wise map of gender\nassociations in a contrastive vision-language space, accompanied by\nuncertainty, simple sanity checks, and a robust gender bias evaluation\nframework.",
        "url": "http://arxiv.org/abs/2508.11262v1",
        "published_date": "2025-08-15T06:57:26+00:00",
        "updated_date": "2025-08-15T06:57:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Aiswarya Konavoor",
            "Raj Abhijit Dandekar",
            "Rajat Dandekar",
            "Sreedath Panat"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper explores how vision-language models exhibit gender biases by analyzing the associations between face images and descriptions of occupations and activities.",
        "tldr_zh": "本文探讨了视觉-语言模型如何通过分析面部图像和职业活动描述之间的关联展现性别偏见。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "Deep Learning-Based Automated Segmentation of Uterine Myomas",
        "summary": "Uterine fibroids (myomas) are the most common benign tumors of the female\nreproductive system, particularly among women of childbearing age. With a\nprevalence exceeding 70%, they pose a significant burden on female reproductive\nhealth. Clinical symptoms such as abnormal uterine bleeding, infertility,\npelvic pain, and pressure-related discomfort play a crucial role in guiding\ntreatment decisions, which are largely influenced by the size, number, and\nanatomical location of the fibroids. Magnetic Resonance Imaging (MRI) is a\nnon-invasive and highly accurate imaging modality commonly used by clinicians\nfor the diagnosis of uterine fibroids. Segmenting uterine fibroids requires a\nprecise assessment of both the uterus and fibroids on MRI scans, including\nmeasurements of volume, shape, and spatial location. However, this process is\nlabor intensive and time consuming and subjected to variability due to intra-\nand inter-expert differences at both pre- and post-treatment stages. As a\nresult, there is a critical need for an accurate and automated segmentation\nmethod for uterine fibroids. In recent years, deep learning algorithms have\nshown re-markable improvements in medical image segmentation, outperforming\ntraditional methods. These approaches offer the potential for fully automated\nsegmentation. Several studies have explored the use of deep learning models to\nachieve automated segmentation of uterine fibroids. However, most of the\nprevious work has been conducted using private datasets, which poses challenges\nfor validation and comparison between studies. In this study, we leverage the\npublicly available Uterine Myoma MRI Dataset (UMD) to establish a baseline for\nautomated segmentation of uterine fibroids, enabling standardized evaluation\nand facilitating future research in this domain.",
        "url": "http://arxiv.org/abs/2508.11010v1",
        "published_date": "2025-08-14T18:22:14+00:00",
        "updated_date": "2025-08-14T18:22:14+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Tausifa Jan Saleem",
            "Mohammad Yaqub"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper proposes a deep learning-based automated segmentation method for uterine fibroids using MRI scans.",
        "tldr_zh": "本文提出了一种利用MRI扫描进行子宫肌瘤深度学习自动分割的方法。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 6
    },
    {
        "title": "Does the Skeleton-Recall Loss Really Work?",
        "summary": "Image segmentation is an important and widely performed task in computer\nvision. Accomplishing effective image segmentation in diverse settings often\nrequires custom model architectures and loss functions. A set of models that\nspecialize in segmenting thin tubular structures are topology\npreservation-based loss functions. These models often utilize a pixel\nskeletonization process claimed to generate more precise segmentation masks of\nthin tubes and better capture the structures that other models often miss. One\nsuch model, Skeleton Recall Loss (SRL) proposed by Kirchhoff et al.~\\cite\n{kirchhoff2024srl}, was stated to produce state-of-the-art results on benchmark\ntubular datasets. In this work, we performed a theoretical analysis of the\ngradients for the SRL loss. Upon comparing the performance of the proposed\nmethod on some of the tubular datasets (used in the original work, along with\nsome additional datasets), we found that the performance of SRL-based\nsegmentation models did not exceed traditional baseline models. By providing\nboth a theoretical explanation and empirical evidence, this work critically\nevaluates the limitations of topology-based loss functions, offering valuable\ninsights for researchers aiming to develop more effective segmentation models\nfor complex tubular structures.",
        "url": "http://arxiv.org/abs/2508.11374v1",
        "published_date": "2025-08-15T10:16:34+00:00",
        "updated_date": "2025-08-15T10:16:34+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Devansh Arora",
            "Nitin Kumar",
            "Sukrit Gupta"
        ],
        "ai_categories": [
            "Other"
        ],
        "tldr": "The paper evaluates the Skeleton Recall Loss (SRL) for image segmentation of tubular structures, finding it does not outperform traditional models.",
        "tldr_zh": "本文评估了骨架回忆损失（SRL）在图像分割中的应用，发现其并未超越传统模型。",
        "relevance_score": 3,
        "novelty_claim_score": 5,
        "clarity_score": 7,
        "potential_impact_score": 4,
        "overall_priority_score": 4
    }
]