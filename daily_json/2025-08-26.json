[
    {
        "title": "Instant Preference Alignment for Text-to-Image Diffusion Models",
        "summary": "Text-to-image (T2I) generation has greatly enhanced creative expression, yet\nachieving preference-aligned generation in a real-time and training-free manner\nremains challenging. Previous methods often rely on static, pre-collected\npreferences or fine-tuning, limiting adaptability to evolving and nuanced user\nintents. In this paper, we highlight the need for instant preference-aligned\nT2I generation and propose a training-free framework grounded in multimodal\nlarge language model (MLLM) priors. Our framework decouples the task into two\ncomponents: preference understanding and preference-guided generation. For\npreference understanding, we leverage MLLMs to automatically extract global\npreference signals from a reference image and enrich a given prompt using\nstructured instruction design. Our approach supports broader and more\nfine-grained coverage of user preferences than existing methods. For\npreference-guided generation, we integrate global keyword-based control and\nlocal region-aware cross-attention modulation to steer the diffusion model\nwithout additional training, enabling precise alignment across both global\nattributes and local elements. The entire framework supports multi-round\ninteractive refinement, facilitating real-time and context-aware image\ngeneration. Extensive experiments on the Viper dataset and our collected\nbenchmark demonstrate that our method outperforms prior approaches in both\nquantitative metrics and human evaluations, and opens up new possibilities for\ndialog-based generation and MLLM-diffusion integration.",
        "url": "http://arxiv.org/abs/2508.17718v1",
        "published_date": "2025-08-25T06:51:15+00:00",
        "updated_date": "2025-08-25T06:51:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yang Li",
            "Songlin Yang",
            "Xiaoxuan Han",
            "Wei Wang",
            "Jing Dong",
            "Yueming Lyu",
            "Ziyu Xue"
        ],
        "ai_categories": [
            "Multimodality",
            "Diffusion",
            "Transformer",
            "Dataset"
        ],
        "tldr": "This paper introduces a training-free framework for instant preference-aligned text-to-image generation, outperforming previous methods in both quantitative metrics and human evaluations.",
        "tldr_zh": "本文引入了一种无需训练的框架，用于文本到图像的即时偏好对齐生成，在定量指标和人类评估方面优于先前的方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "EventTracer: Fast Path Tracing-based Event Stream Rendering",
        "summary": "Simulating event streams from 3D scenes has become a common practice in\nevent-based vision research, as it meets the demand for large-scale, high\ntemporal frequency data without setting up expensive hardware devices or\nundertaking extensive data collections. Yet existing methods in this direction\ntypically work with noiseless RGB frames that are costly to render, and\ntherefore they can only achieve a temporal resolution equivalent to 100-300\nFPS, far lower than that of real-world event data. In this work, we propose\nEventTracer, a path tracing-based rendering pipeline that simulates\nhigh-fidelity event sequences from complex 3D scenes in an efficient and\nphysics-aware manner. Specifically, we speed up the rendering process via low\nsample-per-pixel (SPP) path tracing, and train a lightweight event spiking\nnetwork to denoise the resulting RGB videos into realistic event sequences. To\ncapture the physical properties of event streams, the network is equipped with\na bipolar leaky integrate-and-fired (BiLIF) spiking unit and trained with a\nbidirectional earth mover distance (EMD) loss. Our EventTracer pipeline runs at\na speed of about 4 minutes per second of 720p video, and it inherits the merit\nof accurate spatiotemporal modeling from its path tracing backbone. We show in\ntwo downstream tasks that EventTracer captures better scene details and\ndemonstrates a greater similarity to real-world event data than other event\nsimulators, which establishes it as a promising tool for creating large-scale\nevent-RGB datasets at a low cost, narrowing the sim-to-real gap in event-based\nvision, and boosting various application scenarios such as robotics, autonomous\ndriving, and VRAR.",
        "url": "http://arxiv.org/abs/2508.18071v1",
        "published_date": "2025-08-25T14:33:09+00:00",
        "updated_date": "2025-08-25T14:33:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhenyang Li",
            "Xiaoyang Bai",
            "Jinfan Lu",
            "Pengfei Shen",
            "Edmund Y. Lam",
            "Yifan Peng"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces EventTracer, a path tracing-based rendering pipeline that efficiently simulates high-fidelity event sequences from 3D scenes, showing promising results for creating large-scale event-RGB datasets at a low cost.",
        "tldr_zh": "本文介绍了一种基于路径追踪的渲染管线EventTracer，能够高效地从3D场景中模拟高保真事件序列，为以低成本创建大规模事件-RGB数据集提供有望的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "Edge-Enhanced Vision Transformer Framework for Accurate AI-Generated Image Detection",
        "summary": "The rapid advancement of generative models has led to a growing prevalence of\nhighly realistic AI-generated images, posing significant challenges for digital\nforensics and content authentication. Conventional detection methods mainly\nrely on deep learning models that extract global features, which often overlook\nsubtle structural inconsistencies and demand substantial computational\nresources. To address these limitations, we propose a hybrid detection\nframework that combines a fine-tuned Vision Transformer (ViT) with a novel\nedge-based image processing module. The edge-based module computes variance\nfrom edge-difference maps generated before and after smoothing, exploiting the\nobservation that AI-generated images typically exhibit smoother textures,\nweaker edges, and reduced noise compared to real images. When applied as a\npost-processing step on ViT predictions, this module enhances sensitivity to\nfine-grained structural cues while maintaining computational efficiency.\nExtensive experiments on the CIFAKE, Artistic, and Custom Curated datasets\ndemonstrate that the proposed framework achieves superior detection performance\nacross all benchmarks, attaining 97.75% accuracy and a 97.77% F1-score on\nCIFAKE, surpassing widely adopted state-of-the-art models. These results\nestablish the proposed method as a lightweight, interpretable, and effective\nsolution for both still images and video frames, making it highly suitable for\nreal-world applications in automated content verification and digital\nforensics.",
        "url": "http://arxiv.org/abs/2508.17877v1",
        "published_date": "2025-08-25T10:30:56+00:00",
        "updated_date": "2025-08-25T10:30:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Dabbrata Das",
            "Mahshar Yahan",
            "Md Tareq Zaman",
            "Md Rishadul Bayesh"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a hybrid detection framework combining Vision Transformer with edge-based image processing to accurately detect AI-generated images, outperforming existing models.",
        "tldr_zh": "该论文介绍了一种混合检测框架，结合视觉转换器和基于边缘的图像处理，以准确检测人工智能生成的图像，优于现有模型。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "Propose and Rectify: A Forensics-Driven MLLM Framework for Image Manipulation Localization",
        "summary": "The increasing sophistication of image manipulation techniques demands robust\nforensic solutions that can both reliably detect alterations and precisely\nlocalize tampered regions. Recent Multimodal Large Language Models (MLLMs) show\npromise by leveraging world knowledge and semantic understanding for\ncontext-aware detection, yet they struggle with perceiving subtle, low-level\nforensic artifacts crucial for accurate manipulation localization. This paper\npresents a novel Propose-Rectify framework that effectively bridges semantic\nreasoning with forensic-specific analysis. In the proposal stage, our approach\nutilizes a forensic-adapted LLaVA model to generate initial manipulation\nanalysis and preliminary localization of suspicious regions based on semantic\nunderstanding and contextual reasoning. In the rectification stage, we\nintroduce a Forensics Rectification Module that systematically validates and\nrefines these initial proposals through multi-scale forensic feature analysis,\nintegrating technical evidence from several specialized filters. Additionally,\nwe present an Enhanced Segmentation Module that incorporates critical forensic\ncues into SAM's encoded image embeddings, thereby overcoming inherent semantic\nbiases to achieve precise delineation of manipulated regions. By\nsynergistically combining advanced multimodal reasoning with established\nforensic methodologies, our framework ensures that initial semantic proposals\nare systematically validated and enhanced through concrete technical evidence,\nresulting in comprehensive detection accuracy and localization precision.\nExtensive experimental validation demonstrates state-of-the-art performance\nacross diverse datasets with exceptional robustness and generalization\ncapabilities.",
        "url": "http://arxiv.org/abs/2508.17976v1",
        "published_date": "2025-08-25T12:43:53+00:00",
        "updated_date": "2025-08-25T12:43:53+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Keyang Zhang",
            "Chenqi Kong",
            "Hui Liu",
            "Bo Ding",
            "Xinghao Jiang",
            "Haoliang Li"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper presents a framework that combines semantic reasoning with forensic analysis to detect and localize image manipulation with high accuracy and precision.",
        "tldr_zh": "本文提出了一个框架，结合语义推理与法证分析，以高准确性和精度检测和定位图像篡改。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "TuningIQA: Fine-Grained Blind Image Quality Assessment for Livestreaming Camera Tuning",
        "summary": "Livestreaming has become increasingly prevalent in modern visual\ncommunication, where automatic camera quality tuning is essential for\ndelivering superior user Quality of Experience (QoE). Such tuning requires\naccurate blind image quality assessment (BIQA) to guide parameter optimization\ndecisions. Unfortunately, the existing BIQA models typically only predict an\noverall coarse-grained quality score, which cannot provide fine-grained\nperceptual guidance for precise camera parameter tuning. To bridge this gap, we\nfirst establish FGLive-10K, a comprehensive fine-grained BIQA database\ncontaining 10,185 high-resolution images captured under varying camera\nparameter configurations across diverse livestreaming scenarios. The dataset\nfeatures 50,925 multi-attribute quality annotations and 19,234 fine-grained\npairwise preference annotations. Based on FGLive-10K, we further develop\nTuningIQA, a fine-grained BIQA metric for livestreaming camera tuning, which\nintegrates human-aware feature extraction and graph-based camera parameter\nfusion. Extensive experiments and comparisons demonstrate that TuningIQA\nsignificantly outperforms state-of-the-art BIQA methods in both score\nregression and fine-grained quality ranking, achieving superior performance\nwhen deployed for livestreaming camera tuning.",
        "url": "http://arxiv.org/abs/2508.17965v1",
        "published_date": "2025-08-25T12:26:12+00:00",
        "updated_date": "2025-08-25T12:26:12+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Xiangfei Sheng",
            "Zhichao Duan",
            "Xiaofeng Pan",
            "Yipo Huang",
            "Zhichao Yang",
            "Pengfei Chen",
            "Leida Li"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper introduces TuningIQA, a fine-grained blind image quality assessment metric for livestreaming camera tuning, outperforming existing methods.",
        "tldr_zh": "本文介绍了 TuningIQA，一种用于直播摄像头调优的细粒度盲图像质量评估度量，优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization for Efficient MLLMs Inference",
        "summary": "In this study, we introduce a novel method called group-wise \\textbf{VI}sual\ntoken \\textbf{S}election and \\textbf{A}ggregation (VISA) to address the issue\nof inefficient inference stemming from excessive visual tokens in multimoal\nlarge language models (MLLMs). Compared with previous token pruning approaches,\nour method can preserve more visual information while compressing visual\ntokens. We first propose a graph-based visual token aggregation (VTA) module.\nVTA treats each visual token as a node, forming a graph based on semantic\nsimilarity among visual tokens. It then aggregates information from removed\ntokens into kept tokens based on this graph, producing a more compact visual\ntoken representation. Additionally, we introduce a group-wise token selection\nstrategy (GTS) to divide visual tokens into kept and removed ones, guided by\ntext tokens from the final layers of each group. This strategy progressively\naggregates visual information, enhancing the stability of the visual\ninformation extraction process. We conduct comprehensive experiments on\nLLaVA-1.5, LLaVA-NeXT, and Video-LLaVA across various benchmarks to validate\nthe efficacy of VISA. Our method consistently outperforms previous methods,\nachieving a superior trade-off between model performance and inference speed.\nThe code is available at https://github.com/mobiushy/VISA.",
        "url": "http://arxiv.org/abs/2508.17857v1",
        "published_date": "2025-08-25T10:07:07+00:00",
        "updated_date": "2025-08-25T10:07:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Pengfei Jiang",
            "Hanjun Li",
            "Linglan Zhao",
            "Fei Chao",
            "Ke Yan",
            "Shouhong Ding",
            "Rongrong Ji"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a method called VISA to improve the efficiency of inference in large language models by selecting and aggregating visual tokens using graph summarization techniques.",
        "tldr_zh": "本文介绍了一种名为VISA的方法，通过使用图的总结技术选择和聚合视觉令牌，从而改善大型语言模型中推断的效率。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "TemCoCo: Temporally Consistent Multi-modal Video Fusion with Visual-Semantic Collaboration",
        "summary": "Existing multi-modal fusion methods typically apply static frame-based image\nfusion techniques directly to video fusion tasks, neglecting inherent temporal\ndependencies and leading to inconsistent results across frames. To address this\nlimitation, we propose the first video fusion framework that explicitly\nincorporates temporal modeling with visual-semantic collaboration to\nsimultaneously ensure visual fidelity, semantic accuracy, and temporal\nconsistency. First, we introduce a visual-semantic interaction module\nconsisting of a semantic branch and a visual branch, with Dinov2 and VGG19\nemployed for targeted distillation, allowing simultaneous enhancement of both\nthe visual and semantic representations. Second, we pioneer integrate the video\ndegradation enhancement task into the video fusion pipeline by constructing a\ntemporal cooperative module, which leverages temporal dependencies to\nfacilitate weak information recovery. Third, to ensure temporal consistency, we\nembed a temporal-enhanced mechanism into the network and devise a temporal loss\nto guide the optimization process. Finally, we introduce two innovative\nevaluation metrics tailored for video fusion, aimed at assessing the temporal\nconsistency of the generated fused videos. Extensive experimental results on\npublic video datasets demonstrate the superiority of our method. Our code is\nreleased at https://github.com/Meiqi-Gong/TemCoCo.",
        "url": "http://arxiv.org/abs/2508.17817v1",
        "published_date": "2025-08-25T09:12:55+00:00",
        "updated_date": "2025-08-25T09:12:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Meiqi Gong",
            "Hao Zhang",
            "Xunpeng Yi",
            "Linfeng Tang",
            "Jiayi Ma"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a video fusion framework that incorporates temporal modeling and visual-semantic collaboration to improve visual fidelity, semantic accuracy, and temporal consistency in videos.",
        "tldr_zh": "这篇论文介绍了一个视频融合框架，通过时间建模和视觉语义协作来提高视频的视觉保真度、语义准确性和时间一致性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "BRAIN: Bias-Mitigation Continual Learning Approach to Vision-Brain Understanding",
        "summary": "Memory decay makes it harder for the human brain to recognize visual objects\nand retain details. Consequently, recorded brain signals become weaker,\nuncertain, and contain poor visual context over time. This paper presents one\nof the first vision-learning approaches to address this problem. First, we\nstatistically and experimentally demonstrate the existence of inconsistency in\nbrain signals and its impact on the Vision-Brain Understanding (VBU) model. Our\nfindings show that brain signal representations shift over recording sessions,\nleading to compounding bias, which poses challenges for model learning and\ndegrades performance. Then, we propose a new Bias-Mitigation Continual Learning\n(BRAIN) approach to address these limitations. In this approach, the model is\ntrained in a continual learning setup and mitigates the growing bias from each\nlearning step. A new loss function named De-bias Contrastive Learning is also\nintroduced to address the bias problem. In addition, to prevent catastrophic\nforgetting, where the model loses knowledge from previous sessions, the new\nAngular-based Forgetting Mitigation approach is introduced to preserve learned\nknowledge in the model. Finally, the empirical experiments demonstrate that our\napproach achieves State-of-the-Art (SOTA) performance across various\nbenchmarks, surpassing prior and non-continual learning methods.",
        "url": "http://arxiv.org/abs/2508.18187v1",
        "published_date": "2025-08-25T16:44:43+00:00",
        "updated_date": "2025-08-25T16:44:43+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xuan-Bac Nguyen",
            "Thanh-Dat Truong",
            "Pawan Sinha",
            "Khoa Luu"
        ],
        "ai_categories": [
            "AIGC"
        ],
        "tldr": "The paper introduces a novel approach called BRAIN to mitigate bias in brain signals for vision understanding, achieving state-of-the-art performance.",
        "tldr_zh": "本文提出了一种名为BRAIN的新方法，用于减轻脑信号中的偏见，以实现视觉理解的最新性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "HLG: Comprehensive 3D Room Construction via Hierarchical Layout Generation",
        "summary": "Realistic 3D indoor scene generation is crucial for virtual reality, interior\ndesign, embodied intelligence, and scene understanding. While existing methods\nhave made progress in coarse-scale furniture arrangement, they struggle to\ncapture fine-grained object placements, limiting the realism and utility of\ngenerated environments. This gap hinders immersive virtual experiences and\ndetailed scene comprehension for embodied AI applications. To address these\nissues, we propose Hierarchical Layout Generation (HLG), a novel method for\nfine-grained 3D scene generation. HLG is the first to adopt a coarse-to-fine\nhierarchical approach, refining scene layouts from large-scale furniture\nplacement to intricate object arrangements. Specifically, our fine-grained\nlayout alignment module constructs a hierarchical layout through vertical and\nhorizontal decoupling, effectively decomposing complex 3D indoor scenes into\nmultiple levels of granularity. Additionally, our trainable layout optimization\nnetwork addresses placement issues, such as incorrect positioning, orientation\nerrors, and object intersections, ensuring structurally coherent and physically\nplausible scene generation. We demonstrate the effectiveness of our approach\nthrough extensive experiments, showing superior performance in generating\nrealistic indoor scenes compared to existing methods. This work advances the\nfield of scene generation and opens new possibilities for applications\nrequiring detailed 3D environments. We will release our code upon publication\nto encourage future research.",
        "url": "http://arxiv.org/abs/2508.17832v1",
        "published_date": "2025-08-25T09:32:57+00:00",
        "updated_date": "2025-08-25T09:32:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiping Wang",
            "Yuxi Wang",
            "Mengqi Zhou",
            "Junsong Fan",
            "Zhaoxiang Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper presents a novel method, HLG, for fine-grained 3D scene generation, improving realism and utility for virtual reality and embodied AI applications.",
        "tldr_zh": "本文提出了一种新颖的方法HLG，用于细粒度3D场景生成，提高了虚拟现实和体验智能应用的逼真度和实用性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "OmniMRI: A Unified Vision--Language Foundation Model for Generalist MRI Interpretation",
        "summary": "Magnetic Resonance Imaging (MRI) is indispensable in clinical practice but\nremains constrained by fragmented, multi-stage workflows encompassing\nacquisition, reconstruction, segmentation, detection, diagnosis, and reporting.\nWhile deep learning has achieved progress in individual tasks, existing\napproaches are often anatomy- or application-specific and lack generalizability\nacross diverse clinical settings. Moreover, current pipelines rarely integrate\nimaging data with complementary language information that radiologists rely on\nin routine practice. Here, we introduce OmniMRI, a unified vision-language\nfoundation model designed to generalize across the entire MRI workflow. OmniMRI\nis trained on a large-scale, heterogeneous corpus curated from 60 public\ndatasets, over 220,000 MRI volumes and 19 million MRI slices, incorporating\nimage-only data, paired vision-text data, and instruction-response data. Its\nmulti-stage training paradigm, comprising self-supervised vision pretraining,\nvision-language alignment, multimodal pretraining, and multi-task instruction\ntuning, progressively equips the model with transferable visual\nrepresentations, cross-modal reasoning, and robust instruction-following\ncapabilities. Qualitative results demonstrate OmniMRI's ability to perform\ndiverse tasks within a single architecture, including MRI reconstruction,\nanatomical and pathological segmentation, abnormality detection, diagnostic\nsuggestion, and radiology report generation. These findings highlight OmniMRI's\npotential to consolidate fragmented pipelines into a scalable, generalist\nframework, paving the way toward foundation models that unify imaging and\nclinical language for comprehensive, end-to-end MRI interpretation.",
        "url": "http://arxiv.org/abs/2508.17524v1",
        "published_date": "2025-08-24T21:11:28+00:00",
        "updated_date": "2025-08-24T21:11:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xingxin He",
            "Aurora Rofena",
            "Ruimin Feng",
            "Haozhe Liao",
            "Zhaoye Zhou",
            "Albert Jang",
            "Fang Liu"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "OmniMRI is a unified vision-language model for MRI interpretation, covering various tasks and integrating both imaging and language data.",
        "tldr_zh": "OmniMRI是一个统一的视觉-语言模型，用于MRI解释，包括各种任务，并整合了图像和语言数据。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion Models",
        "summary": "3D inpainting often relies on multi-view 2D image inpainting, where the\ninherent inconsistencies across different inpainted views can result in blurred\ntextures, spatial discontinuities, and distracting visual artifacts. These\ninconsistencies pose significant challenges when striving for accurate and\nrealistic 3D object completion, particularly in applications that demand high\nfidelity and structural coherence. To overcome these limitations, we propose\nObjFiller-3D, a novel method designed for the completion and editing of\nhigh-quality and consistent 3D objects. Instead of employing a conventional 2D\nimage inpainting model, our approach leverages a curated selection of\nstate-of-the-art video editing model to fill in the masked regions of 3D\nobjects. We analyze the representation gap between 3D and videos, and propose\nan adaptation of a video inpainting model for 3D scene inpainting. In addition,\nwe introduce a reference-based 3D inpainting method to further enhance the\nquality of reconstruction. Experiments across diverse datasets show that\ncompared to previous methods, ObjFiller-3D produces more faithful and\nfine-grained reconstructions (PSNR of 26.6 vs. NeRFiller (15.9) and LPIPS of\n0.19 vs. Instant3dit (0.25)). Moreover, it demonstrates strong potential for\npractical deployment in real-world 3D editing applications. Project page:\nhttps://objfiller3d.github.io/ Code:\nhttps://github.com/objfiller3d/ObjFiller-3D .",
        "url": "http://arxiv.org/abs/2508.18271v1",
        "published_date": "2025-08-25T17:59:40+00:00",
        "updated_date": "2025-08-25T17:59:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haitang Feng",
            "Jie Liu",
            "Jie Tang",
            "Gangshan Wu",
            "Beiqi Chen",
            "Jianhuang Lai",
            "Guangcong Wang"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset",
            "Other"
        ],
        "tldr": "ObjFiller-3D proposes a novel method for consistent 3D object completion, leveraging video diffusion models to improve quality and realism.",
        "tldr_zh": "ObjFiller-3D 提出了一种新颖的方法，利用视频扩散模型来提高一致性的3D对象完成。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency",
        "summary": "We introduce InternVL 3.5, a new family of open-source multimodal models that\nsignificantly advances versatility, reasoning capability, and inference\nefficiency along the InternVL series. A key innovation is the Cascade\nReinforcement Learning (Cascade RL) framework, which enhances reasoning through\na two-stage process: offline RL for stable convergence and online RL for\nrefined alignment. This coarse-to-fine training strategy leads to substantial\nimprovements on downstream reasoning tasks, e.g., MMMU and MathVista. To\noptimize efficiency, we propose a Visual Resolution Router (ViR) that\ndynamically adjusts the resolution of visual tokens without compromising\nperformance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD)\nstrategy separates the vision encoder and language model across different GPUs,\neffectively balancing computational load. These contributions collectively\nenable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning\nperformance and a 4.05$\\times$ inference speedup compared to its predecessor,\ni.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as\nGUI interaction and embodied agency. Notably, our largest model, i.e.,\nInternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs\nacross general multimodal, reasoning, text, and agentic tasks -- narrowing the\nperformance gap with leading commercial models like GPT-5. All models and code\nare publicly released.",
        "url": "http://arxiv.org/abs/2508.18265v1",
        "published_date": "2025-08-25T17:58:17+00:00",
        "updated_date": "2025-08-25T17:58:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weiyun Wang",
            "Zhangwei Gao",
            "Lixin Gu",
            "Hengjun Pu",
            "Long Cui",
            "Xingguang Wei",
            "Zhaoyang Liu",
            "Linglin Jing",
            "Shenglong Ye",
            "Jie Shao",
            "Zhaokai Wang",
            "Zhe Chen",
            "Hongjie Zhang",
            "Ganlin Yang",
            "Haomin Wang",
            "Qi Wei",
            "Jinhui Yin",
            "Wenhao Li",
            "Erfei Cui",
            "Guanzhou Chen",
            "Zichen Ding",
            "Changyao Tian",
            "Zhenyu Wu",
            "Jingjing Xie",
            "Zehao Li",
            "Bowen Yang",
            "Yuchen Duan",
            "Xuehui Wang",
            "Songze Li",
            "Xiangyu Zhao",
            "Haodong Duan",
            "Nianchen Deng",
            "Bin Fu",
            "Yinan He",
            "Yi Wang",
            "Conghui He",
            "Botian Shi",
            "Junjun He",
            "Yingtong Xiong",
            "Han Lv",
            "Lijun Wu",
            "Wenqi Shao",
            "Kaipeng Zhang",
            "Huipeng Deng",
            "Biqing Qi",
            "Jiaye Ge",
            "Qipeng Guo",
            "Wenwei Zhang",
            "Wanli Ouyang",
            "Limin Wang",
            "Min Dou",
            "Xizhou Zhu",
            "Tong Lu",
            "Dahua Lin",
            "Jifeng Dai",
            "Bowen Zhou",
            "Weijie Su",
            "Kai Chen",
            "Yu Qiao",
            "Wenhai Wang",
            "Gen Luo"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "InternVL 3.5 introduces new open-source multimodal models with enhanced versatility, reasoning, and efficiency using innovative techniques like Cascade RL and ViR.",
        "tldr_zh": "InternVL 3.5引入了新的开源多模态模型，利用创新技术如级联RL和ViR增强了多样性、推理能力和效率。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs",
        "summary": "Vision-Language Models (VLMs) demonstrate impressive performance in\nunderstanding visual content with language instruction by converting visual\ninput to vision tokens. However, redundancy in vision tokens results in the\ndegenerated inference efficiency of VLMs. While many algorithms have been\nproposed to reduce the number of vision tokens, most of them apply only\nunimodal information (i.e., vision/text) for pruning and ignore the inherent\nmultimodal property of vision-language tasks. Moreover, it lacks a generic\ncriterion that can be applied to different modalities. To mitigate this\nlimitation, in this work, we propose to leverage both vision and text tokens to\nselect informative vision tokens by the criterion of coverage. We first\nformulate the subset selection problem as a maximum coverage problem.\nAfterward, a subset of vision tokens is optimized to cover the text tokens and\nthe original set of vision tokens, simultaneously. Finally, a VLM agent can be\nadopted to further improve the quality of text tokens for guiding vision\npruning. The proposed method MMTok is extensively evaluated on benchmark\ndatasets with different VLMs. The comparison illustrates that vision and text\ninformation are complementary, and combining multimodal information can surpass\nthe unimodal baseline with a clear margin. Moreover, under the maximum coverage\ncriterion on the POPE dataset, our method achieves a 1.87x speedup while\nmaintaining 98.7% of the original performance on LLaVA-NeXT-13B. Furthermore,\nwith only four vision tokens, it still preserves 87.7% of the original\nperformance on LLaVA-1.5-7B. These results highlight the effectiveness of\ncoverage in token selection.",
        "url": "http://arxiv.org/abs/2508.18264v1",
        "published_date": "2025-08-25T17:57:49+00:00",
        "updated_date": "2025-08-25T17:57:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sixun Dong",
            "Juhua Hu",
            "Mian Zhang",
            "Ming Yin",
            "Yanjie Fu",
            "Qi Qian"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "MMTok proposes a method to improve the efficiency of Vision-Language Models by selecting informative vision tokens based on coverage criteria, achieving significant speedups while maintaining high performance.",
        "tldr_zh": "MMTok提出了一种通过覆盖准则选择信息视觉标记来改善视觉语言模型的效率的方法，在保持高性能的同时实现了显着的加速。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework",
        "summary": "Traversability estimation is critical for enabling robots to navigate across\ndiverse terrains and environments. While recent self-supervised learning\nmethods achieve promising results, they often fail to capture the\ncharacteristics of non-traversable regions. Moreover, most prior works\nconcentrate on a single modality, overlooking the complementary strengths\noffered by integrating heterogeneous sensory modalities for more robust\ntraversability estimation. To address these limitations, we propose a\nmultimodal self-supervised framework for traversability labeling and\nestimation. First, our annotation pipeline integrates footprint, LiDAR, and\ncamera data as prompts for a vision foundation model, generating traversability\nlabels that account for both semantic and geometric cues. Then, leveraging\nthese labels, we train a dual-stream network that jointly learns from different\nmodalities in a decoupled manner, enhancing its capacity to recognize diverse\ntraversability patterns. In addition, we incorporate sparse LiDAR-based\nsupervision to mitigate the noise introduced by pseudo labels. Finally,\nextensive experiments conducted across urban, off-road, and campus environments\ndemonstrate the effectiveness of our approach. The proposed automatic labeling\nmethod consistently achieves around 88% IoU across diverse datasets. Compared\nto existing self-supervised state-of-the-art methods, our multimodal\ntraversability estimation network yields consistently higher IoU, improving by\n1.6-3.5% on all evaluated datasets.",
        "url": "http://arxiv.org/abs/2508.18249v1",
        "published_date": "2025-08-25T17:40:16+00:00",
        "updated_date": "2025-08-25T17:40:16+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Zipeng Fang",
            "Yanbo Wang",
            "Lei Zhao",
            "Weidong Chen"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes a multimodal self-supervised framework for traversability estimation, achieving high accuracy across diverse datasets.",
        "tldr_zh": "本文提出了一种多模态自监督框架用于可通行性估计，在不同数据集上达到了高准确度。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Follow My Hold: Hand-Object Interaction Reconstruction through Geometric Guidance",
        "summary": "We propose a novel diffusion-based framework for reconstructing 3D geometry\nof hand-held objects from monocular RGB images by leveraging hand-object\ninteraction as geometric guidance. Our method conditions a latent diffusion\nmodel on an inpainted object appearance and uses inference-time guidance to\noptimize the object reconstruction, while simultaneously ensuring plausible\nhand-object interactions. Unlike prior methods that rely on extensive\npost-processing or produce low-quality reconstructions, our approach directly\ngenerates high-quality object geometry during the diffusion process by\nintroducing guidance with an optimization-in-the-loop design. Specifically, we\nguide the diffusion model by applying supervision to the velocity field while\nsimultaneously optimizing the transformations of both the hand and the object\nbeing reconstructed. This optimization is driven by multi-modal geometric cues,\nincluding normal and depth alignment, silhouette consistency, and 2D keypoint\nreprojection. We further incorporate signed distance field supervision and\nenforce contact and non-intersection constraints to ensure physical\nplausibility of hand-object interaction. Our method yields accurate, robust and\ncoherent reconstructions under occlusion while generalizing well to in-the-wild\nscenarios.",
        "url": "http://arxiv.org/abs/2508.18213v1",
        "published_date": "2025-08-25T17:11:53+00:00",
        "updated_date": "2025-08-25T17:11:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ayce Idil Aytekin",
            "Helge Rhodin",
            "Rishabh Dabral",
            "Christian Theobalt"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a diffusion-based framework for reconstructing 3D geometry of hand-held objects from monocular RGB images by leveraging hand-object interaction as geometric guidance.",
        "tldr_zh": "本文提出了一种基于扩散的框架，通过利用手对象交互作为几何引导，重建手持物体的3D几何结构。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Explain and Monitor Deep Learning Models for Computer Vision using Obz AI",
        "summary": "Deep learning has transformed computer vision (CV), achieving outstanding\nperformance in classification, segmentation, and related tasks. Such AI-based\nCV systems are becoming prevalent, with applications spanning from medical\nimaging to surveillance. State of the art models such as convolutional neural\nnetworks (CNNs) and vision transformers (ViTs) are often regarded as ``black\nboxes,'' offering limited transparency into their decision-making processes.\nDespite a recent advancement in explainable AI (XAI), explainability remains\nunderutilized in practical CV deployments. A primary obstacle is the absence of\nintegrated software solutions that connect XAI techniques with robust knowledge\nmanagement and monitoring frameworks. To close this gap, we have developed Obz\nAI, a comprehensive software ecosystem designed to facilitate state-of-the-art\nexplainability and observability for vision AI systems. Obz AI provides a\nseamless integration pipeline, from a Python client library to a full-stack\nanalytics dashboard. With Obz AI, a machine learning engineer can easily\nincorporate advanced XAI methodologies, extract and analyze features for\noutlier detection, and continuously monitor AI models in real time. By making\nthe decision-making mechanisms of deep models interpretable, Obz AI promotes\nobservability and responsible deployment of computer vision systems.",
        "url": "http://arxiv.org/abs/2508.18188v1",
        "published_date": "2025-08-25T16:46:21+00:00",
        "updated_date": "2025-08-25T16:46:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.HC",
            "cs.SE"
        ],
        "authors": [
            "Neo Christopher Chung",
            "Jakub Binda"
        ],
        "ai_categories": [
            "AIGC",
            "Other"
        ],
        "tldr": "The paper introduces Obz AI, a software ecosystem designed to explain and monitor deep learning models in computer vision systems.",
        "tldr_zh": "该论文介绍了Obz AI，这是一个软件生态系统，旨在解释和监视计算机视觉系统中的深度学习模型。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SEAM: Semantically Equivalent Across Modalities Benchmark for Vision-Language Models",
        "summary": "Evaluating whether vision-language models (VLMs) reason consistently across\nrepresentations is challenging because modality comparisons are typically\nconfounded by task differences and asymmetric information. We introduce SEAM, a\nbenchmark that pairs semantically equivalent inputs across four domains that\nhave existing standardized textual and visual notations. By employing distinct\nnotation systems across modalities, in contrast to OCR-based image-text\npairing, SEAM provides a rigorous comparative assessment of the\ntextual-symbolic and visual-spatial reasoning capabilities of VLMs. Across 21\ncontemporary models, we observe systematic modality imbalance: vision\nfrequently lags language in overall performance, despite the problems\ncontaining semantically equivalent information, and cross-modal agreement is\nrelatively low. Our error analysis reveals two main drivers: textual perception\nfailures from tokenization in domain notation and visual perception failures\nthat induce hallucinations. We also show that our results are largely robust to\nvisual transformations. SEAM establishes a controlled, semantically equivalent\nsetting for measuring and improving modality-agnostic reasoning.",
        "url": "http://arxiv.org/abs/2508.18179v1",
        "published_date": "2025-08-25T16:33:07+00:00",
        "updated_date": "2025-08-25T16:33:07+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Zhenwei Tang",
            "Difan Jiao",
            "Blair Yang",
            "Ashton Anderson"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "SEAM introduces a benchmark for evaluating vision-language models across different modalities, revealing a modality imbalance and reasons for errors, providing a controlled setting for improving reasoning.",
        "tldr_zh": "SEAM引入了一个基准测试，用于评估视觉-语言模型在不同模态下的表现，揭示了模态失衡和错误原因，并提供了一个控制环境来改进推理能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Scene-Aware Vectorized Memory Multi-Agent Framework with Cross-Modal Differentiated Quantization VLMs for Visually Impaired Assistance",
        "summary": "This study proposes the dual technological innovation framework, including a\ncross-modal differ entiated quantization framework for vision-language models\n(VLMs) and a scene-aware vectorized\n  memory multi-agent system for visually impaired assistance. The modular\nframework was developed\n  implementing differentiated processing strategies, effectively reducing\nmemory requirements from\n  38GB to 16GB while maintaining model performance. The multi-agent\narchitecture combines\n  scene classification, vectorized memory, and multimodal interaction, enabling\npersistent storage\n  and efficient retrieval of scene memories. Through\nperception-memory-reasoning workflows, the\n  system provides environmental information beyond the current view using\nhistorical memories.\n  Experiments show the quantized 19B-parameter model only experiences a 2.05%\nperformance drop\n  on MMBench and maintains 63.7 accuracy on OCR-VQA (original: 64.9),\noutperforming smaller\n  models with equivalent memory requirements like the Molmo-7B series. The\nsystem maintains\n  response latency between 2.83-3.52 seconds from scene analysis to initial\nspeech output, substantially\n  faster than non-streaming methods. This research advances computational\nefficiency and assistive\n  technology, offering visually impaired users comprehensive real-time\nassistance in scene perception,\n  text recognition, and navigation.",
        "url": "http://arxiv.org/abs/2508.18177v1",
        "published_date": "2025-08-25T16:32:32+00:00",
        "updated_date": "2025-08-25T16:32:32+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.MA"
        ],
        "authors": [
            "Xiangxiang Wang",
            "Xuanyu Wang",
            "YiJia Luo",
            "Yongbin Yu",
            "Manping Fan",
            "Jingtao Zhang",
            "Liyong Ren"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel framework combining differentiated quantization for vision-language models and a multi-agent system for visually impaired assistance. It offers real-time assistance in scene perception, text recognition, and navigation.",
        "tldr_zh": "本文介绍了一种新颖的框架，结合了不同量化视觉-语言模型和多代理系统，为视障人士提供实时场景感知、文本识别和导航帮助。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "SpotEdit: Evaluating Visually-Guided Image Editing Methods",
        "summary": "Visually-guided image editing, where edits are conditioned on both visual\ncues and textual prompts, has emerged as a powerful paradigm for fine-grained,\ncontrollable content generation. Although recent generative models have shown\nremarkable capabilities, existing evaluations remain simple and insufficiently\nrepresentative of real-world editing challenges. We present SpotEdit, a\ncomprehensive benchmark designed to systematically assess visually-guided image\nediting methods across diverse diffusion, autoregressive, and hybrid generative\nmodels, uncovering substantial performance disparities. To address a critical\nyet underexplored challenge, our benchmark includes a dedicated component on\nhallucination, highlighting how leading models, such as GPT-4o, often\nhallucinate the existence of a visual cue and erroneously perform the editing\ntask. Our code and benchmark are publicly released at\nhttps://github.com/SaraGhazanfari/SpotEdit.",
        "url": "http://arxiv.org/abs/2508.18159v1",
        "published_date": "2025-08-25T16:08:57+00:00",
        "updated_date": "2025-08-25T16:08:57+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Sara Ghazanfari",
            "Wei-An Lin",
            "Haitong Tian",
            "Ersin Yumer"
        ],
        "ai_categories": [
            "Dataset",
            "GAN"
        ],
        "tldr": "SpotEdit introduces a benchmark to evaluate visually-guided image editing methods, revealing performance gaps and addressing hallucination challenges in leading models like GPT-4o.",
        "tldr_zh": "SpotEdit引入了一个评估视觉引导图像编辑方法的基准，揭示了性能差距，并解决了诸如GPT-4o之类领先模型中的幻境挑战。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Assessing the Noise Robustness of Class Activation Maps: A Framework for Reliable Model Interpretability",
        "summary": "Class Activation Maps (CAMs) are one of the important methods for visualizing\nregions used by deep learning models. Yet their robustness to different noise\nremains underexplored. In this work, we evaluate and report the resilience of\nvarious CAM methods for different noise perturbations across multiple\narchitectures and datasets. By analyzing the influence of different noise types\non CAM explanations, we assess the susceptibility to noise and the extent to\nwhich dataset characteristics may impact explanation stability. The findings\nhighlight considerable variability in noise sensitivity for various CAMs. We\npropose a robustness metric for CAMs that captures two key properties:\nconsistency and responsiveness. Consistency reflects the ability of CAMs to\nremain stable under input perturbations that do not alter the predicted class,\nwhile responsiveness measures the sensitivity of CAMs to changes in the\nprediction caused by such perturbations. The metric is evaluated empirically\nacross models, different perturbations, and datasets along with complementary\nstatistical tests to exemplify the applicability of our proposed approach.",
        "url": "http://arxiv.org/abs/2508.18154v1",
        "published_date": "2025-08-25T15:59:06+00:00",
        "updated_date": "2025-08-25T15:59:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Syamantak Sarkar",
            "Revoti P. Bora",
            "Bhupender Kaushal",
            "Sudhish N George",
            "Kiran Raja"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper assesses the noise robustness of Class Activation Maps for deep learning models and proposes a metric to measure stability and sensitivity.",
        "tldr_zh": "本文评估了用于深度学习模型的类激活图的噪声稳健性，并提出了一种衡量稳定性和敏感性的度量标准。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Incorporating Pre-trained Diffusion Models in Solving the Schrödinger Bridge Problem",
        "summary": "This paper aims to unify Score-based Generative Models (SGMs), also known as\nDiffusion models, and the Schr\\\"odinger Bridge (SB) problem through three\nreparameterization techniques: Iterative Proportional Mean-Matching (IPMM),\nIterative Proportional Terminus-Matching (IPTM), and Iterative Proportional\nFlow-Matching (IPFM). These techniques significantly accelerate and stabilize\nthe training of SB-based models. Furthermore, the paper introduces novel\ninitialization strategies that use pre-trained SGMs to effectively train\nSB-based models. By using SGMs as initialization, we leverage the advantages of\nboth SB-based models and SGMs, ensuring efficient training of SB-based models\nand further improving the performance of SGMs. Extensive experiments\ndemonstrate the significant effectiveness and improvements of the proposed\nmethods. We believe this work contributes to and paves the way for future\nresearch on generative models.",
        "url": "http://arxiv.org/abs/2508.18095v1",
        "published_date": "2025-08-25T14:56:16+00:00",
        "updated_date": "2025-08-25T14:56:16+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Zhicong Tang",
            "Tiankai Hang",
            "Shuyang Gu",
            "Dong Chen",
            "Baining Guo"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN"
        ],
        "tldr": "This paper integrates Score-based Generative Models (Diffusion models) and the Schrödinger Bridge problem using reparameterization techniques and pre-trained models to improve training efficiency and performance.",
        "tldr_zh": "本文通过重新参数化技术和预训练模型，将评分型生成模型（扩散模型）与薛定谔桥问题相结合，以提高训练效率和性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ArgusCogito: Chain-of-Thought for Cross-Modal Synergy and Omnidirectional Reasoning in Camouflaged Object Segmentation",
        "summary": "Camouflaged Object Segmentation (COS) poses a significant challenge due to\nthe intrinsic high similarity between targets and backgrounds, demanding models\ncapable of profound holistic understanding beyond superficial cues. Prevailing\nmethods, often limited by shallow feature representation, inadequate reasoning\nmechanisms, and weak cross-modal integration, struggle to achieve this depth of\ncognition, resulting in prevalent issues like incomplete target separation and\nimprecise segmentation. Inspired by the perceptual strategy of the Hundred-eyed\nGiant-emphasizing holistic observation, omnidirectional focus, and intensive\nscrutiny-we introduce ArgusCogito, a novel zero-shot, chain-of-thought\nframework underpinned by cross-modal synergy and omnidirectional reasoning\nwithin Vision-Language Models (VLMs). ArgusCogito orchestrates three\ncognitively-inspired stages: (1) Conjecture: Constructs a strong cognitive\nprior through global reasoning with cross-modal fusion (RGB, depth, semantic\nmaps), enabling holistic scene understanding and enhanced target-background\ndisambiguation. (2) Focus: Performs omnidirectional, attention-driven scanning\nand focused reasoning, guided by semantic priors from Conjecture, enabling\nprecise target localization and region-of-interest refinement. (3) Sculpting:\nProgressively sculpts high-fidelity segmentation masks by integrating\ncross-modal information and iteratively generating dense positive/negative\npoint prompts within focused regions, emulating Argus' intensive scrutiny.\nExtensive evaluations on four challenging COS benchmarks and three Medical\nImage Segmentation (MIS) benchmarks demonstrate that ArgusCogito achieves\nstate-of-the-art (SOTA) performance, validating the framework's exceptional\nefficacy, superior generalization capability, and robustness.",
        "url": "http://arxiv.org/abs/2508.18050v1",
        "published_date": "2025-08-25T14:08:17+00:00",
        "updated_date": "2025-08-25T14:08:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jianwen Tan",
            "Huiyao Zhang",
            "Rui Xiong",
            "Han Zhou",
            "Hongfei Wang",
            "Ye Li"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality"
        ],
        "tldr": "ArgusCogito is a novel framework for camouflaged object segmentation using a chain-of-thought approach that incorporates cross-modal synergy and omnidirectional reasoning within Vision-Language Models.",
        "tldr_zh": "ArgusCogito是一个使用思维链方法进行伪装目标分割的新框架，该方法在视觉语言模型中融合了跨模态协同和全向推理。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation",
        "summary": "Despite the promising progress of recent autoregressive models in\ntext-to-image (T2I) generation, their ability to handle multi-attribute and\nambiguous prompts remains limited. To address these limitations, existing works\nhave applied chain-of-thought (CoT) to enable stage-aware visual synthesis and\nemployed reinforcement learning (RL) to improve reasoning capabilities.\nHowever, most models provide reward signals only at the end of the generation\nstage. This monolithic final-only guidance makes it difficult to identify which\nstages contribute positively to the final outcome and may lead to suboptimal\npolicies. To tackle this issue, we propose a Visual-Chain of Guidance\n(Visual-CoG) paradigm consisting of three stages: semantic reasoning, process\nrefining, and outcome evaluation, with stage-aware rewards providing immediate\nguidance throughout the image generation pipeline. We further construct a\nvisual cognition benchmark, VisCog-Bench, which comprises four subtasks to\nevaluate the effectiveness of semantic reasoning. Comprehensive evaluations on\nGenEval, T2I-CompBench, and the proposed VisCog-Bench show improvements of 15%,\n5%, and 19%, respectively, demonstrating the superior performance of the\nproposed Visual-CoG. We will release all the resources soon.",
        "url": "http://arxiv.org/abs/2508.18032v1",
        "published_date": "2025-08-25T13:53:02+00:00",
        "updated_date": "2025-08-25T13:53:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yaqi Li",
            "Peng Chen",
            "Mingyang Han",
            "Bu Pi",
            "Haoxiang Shi",
            "Runzhou Zhao",
            "Yang Yao",
            "Xuan Zhang",
            "Jun Song"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a new paradigm called Visual-CoG for text-to-image generation, with stage-aware rewards for immediate guidance throughout the image generation process.",
        "tldr_zh": "本文提出了一种名为Visual-CoG的新范式，用于文本到图像生成，通过阶段感知奖励提供图像生成过程中的即时引导。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FCR: Investigating Generative AI models for Forensic Craniofacial Reconstruction",
        "summary": "Craniofacial reconstruction in forensics is one of the processes to identify\nvictims of crime and natural disasters. Identifying an individual from their\nremains plays a crucial role when all other identification methods fail.\nTraditional methods for this task, such as clay-based craniofacial\nreconstruction, require expert domain knowledge and are a time-consuming\nprocess. At the same time, other probabilistic generative models like the\nstatistical shape model or the Basel face model fail to capture the skull and\nface cross-domain attributes. Looking at these limitations, we propose a\ngeneric framework for craniofacial reconstruction from 2D X-ray images. Here,\nwe used various generative models (i.e., CycleGANs, cGANs, etc) and fine-tune\nthe generator and discriminator parts to generate more realistic images in two\ndistinct domains, which are the skull and face of an individual. This is the\nfirst time where 2D X-rays are being used as a representation of the skull by\ngenerative models for craniofacial reconstruction. We have evaluated the\nquality of generated faces using FID, IS, and SSIM scores. Finally, we have\nproposed a retrieval framework where the query is the generated face image and\nthe gallery is the database of real faces. By experimental results, we have\nfound that this can be an effective tool for forensic science.",
        "url": "http://arxiv.org/abs/2508.18031v1",
        "published_date": "2025-08-25T13:52:59+00:00",
        "updated_date": "2025-08-25T13:52:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ravi Shankar Prasad",
            "Dinesh Singh"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a generative AI framework for craniofacial reconstruction from 2D X-ray images, which shows promise for forensic science.",
        "tldr_zh": "本文提出了一种从2D X射线图像进行颅颜面重建的生成AI框架，为法医科学带来了希望。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Continual Visual Anomaly Detection in the Medical Domain",
        "summary": "Visual Anomaly Detection (VAD) seeks to identify abnormal images and\nprecisely localize the corresponding anomalous regions, relying solely on\nnormal data during training. This approach has proven essential in domains such\nas manufacturing and, more recently, in the medical field, where accurate and\nexplainable detection is critical. Despite its importance, the impact of\nevolving input data distributions over time has received limited attention,\neven though such changes can significantly degrade model performance. In\nparticular, given the dynamic and evolving nature of medical imaging data,\nContinual Learning (CL) provides a natural and effective framework to\nincrementally adapt models while preserving previously acquired knowledge. This\nstudy explores for the first time the application of VAD models in a CL\nscenario for the medical field. In this work, we utilize a CL version of the\nwell-established PatchCore model, called PatchCoreCL, and evaluate its\nperformance using BMAD, a real-world medical imaging dataset with both\nimage-level and pixel-level annotations. Our results demonstrate that\nPatchCoreCL is an effective solution, achieving performance comparable to the\ntask-specific models, with a forgetting value less than a 1%, highlighting the\nfeasibility and potential of CL for adaptive VAD in medical imaging.",
        "url": "http://arxiv.org/abs/2508.18013v1",
        "published_date": "2025-08-25T13:28:15+00:00",
        "updated_date": "2025-08-25T13:28:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Manuel Barusco",
            "Francesco Borsatti",
            "Nicola Beda",
            "Davide Dalle Pezze",
            "Gian Antonio Susto"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper explores the use of Continual Learning to adapt Visual Anomaly Detection models in the medical field, achieving comparable performance to task-specific models with minimal forgetting.",
        "tldr_zh": "本文探讨了在医学领域利用持续学习来调整视觉异常检测模型，实现了与任务特定模型相当的性能，遗忘率少于1%。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Development of a Neural Network Model for Currency Detection to aid visually impaired people in Nigeria",
        "summary": "Neural networks in assistive technology for visually impaired leverage\nartificial intelligence's capacity to recognize patterns in complex data. They\nare used for converting visual data into auditory or tactile representations,\nhelping the visually impaired understand their surroundings. The primary aim of\nthis research is to explore the potential of artificial neural networks to\nfacilitate the differentiation of various forms of cash for individuals with\nvisual impairments. In this study, we built a custom dataset of 3,468 images,\nwhich was subsequently used to train an SSD neural network model. The proposed\nsystem can accurately identify Nigerian cash, thereby streamlining commercial\ntransactions. The performance of the system in terms of accuracy was assessed,\nand the Mean Average Precision score was over 90%. We believe that our system\nhas the potential to make a substantial contribution to the field of assistive\ntechnology while also improving the quality of life of visually challenged\npersons in Nigeria and beyond.",
        "url": "http://arxiv.org/abs/2508.18012v1",
        "published_date": "2025-08-25T13:27:27+00:00",
        "updated_date": "2025-08-25T13:27:27+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Sochukwuma Nwokoye",
            "Desmond Moru"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper presents a neural network model for detecting currency to assist visually impaired individuals in Nigeria.",
        "tldr_zh": "本文提出了用于检测货币以帮助尼日利亚视障人士的神经网络模型。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Topology Aware Neural Interpolation of Scalar Fields",
        "summary": "This paper presents a neural scheme for the topology-aware interpolation of\ntime-varying scalar fields. Given a time-varying sequence of persistence\ndiagrams, along with a sparse temporal sampling of the corresponding scalar\nfields, denoted as keyframes, our interpolation approach aims at \"inverting\"\nthe non-keyframe diagrams to produce plausible estimations of the\ncorresponding, missing data. For this, we rely on a neural architecture which\nlearns the relation from a time value to the corresponding scalar field, based\non the keyframe examples, and reliably extends this relation to the\nnon-keyframe time steps. We show how augmenting this architecture with specific\ntopological losses exploiting the input diagrams both improves the geometrical\nand topological reconstruction of the non-keyframe time steps. At query time,\ngiven an input time value for which an interpolation is desired, our approach\ninstantaneously produces an output, via a single propagation of the time input\nthrough the network. Experiments interpolating 2D and 3D time-varying datasets\nshow our approach superiority, both in terms of data and topological fitting,\nwith regard to reference interpolation schemes.",
        "url": "http://arxiv.org/abs/2508.17995v1",
        "published_date": "2025-08-25T13:04:21+00:00",
        "updated_date": "2025-08-25T13:04:21+00:00",
        "categories": [
            "cs.LG",
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Mohamed Kissi",
            "Keanu Sisouk",
            "Joshua A. Levine",
            "Julien Tierny"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a neural interpolation approach for time-varying scalar fields, utilizing topological information to improve interpolation results.",
        "tldr_zh": "本文提出了一种神经插值方法用于时间变化的标量场，利用拓扑信息改进插值结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhanced Drift-Aware Computer Vision Architecture for Autonomous Driving",
        "summary": "The use of computer vision in automotive is a trending research in which\nsafety and security are a primary concern. In particular, for autonomous\ndriving, preventing road accidents requires highly accurate object detection\nunder diverse conditions. To address this issue, recently the International\nOrganization for Standardization (ISO) released the 8800 norm, providing\nstructured frameworks for managing associated AI relevant risks. However,\nchallenging scenarios such as adverse weather or low lighting often introduce\ndata drift, leading to degraded model performance and potential safety\nviolations. In this work, we present a novel hybrid computer vision\narchitecture trained with thousands of synthetic image data from the road\nenvironment to improve robustness in unseen drifted environments. Our dual mode\nframework utilized YOLO version 8 for swift detection and incorporated a\nfive-layer CNN for verification. The system functioned in sequence and improved\nthe detection accuracy by more than 90\\% when tested with drift-augmented road\nimages. The focus was to demonstrate how such a hybrid model can provide better\nroad safety when working together in a hybrid structure.",
        "url": "http://arxiv.org/abs/2508.17975v1",
        "published_date": "2025-08-25T12:43:29+00:00",
        "updated_date": "2025-08-25T12:43:29+00:00",
        "categories": [
            "cs.CV",
            "math.LO"
        ],
        "authors": [
            "Md Shahi Amran Hossain",
            "Abu Shad Ahammed",
            "Sayeri Mukherjee",
            "Roman Obermaisser"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces a hybrid computer vision architecture to enhance object detection for autonomous driving by addressing data drift issues.",
        "tldr_zh": "该论文介绍了一种混合计算机视觉架构，通过解决数据漂移问题来增强自动驾驶中的物体检测。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SAIL-Recon: Large SfM by Augmenting Scene Regression with Localization",
        "summary": "Scene regression methods, such as VGGT, solve the Structure-from-Motion (SfM)\nproblem by directly regressing camera poses and 3D scene structures from input\nimages. They demonstrate impressive performance in handling images under\nextreme viewpoint changes. However, these methods struggle to handle a large\nnumber of input images. To address this problem, we introduce SAIL-Recon, a\nfeed-forward Transformer for large scale SfM, by augmenting the scene\nregression network with visual localization capabilities. Specifically, our\nmethod first computes a neural scene representation from a subset of anchor\nimages. The regression network is then fine-tuned to reconstruct all input\nimages conditioned on this neural scene representation. Comprehensive\nexperiments show that our method not only scales efficiently to large-scale\nscenes, but also achieves state-of-the-art results on both camera pose\nestimation and novel view synthesis benchmarks, including TUM-RGBD, CO3Dv2, and\nTanks & Temples. We will publish our model and code. Code and models are\npublicly available at: https://hkust-sail.github.io/ sail-recon/.",
        "url": "http://arxiv.org/abs/2508.17972v1",
        "published_date": "2025-08-25T12:38:26+00:00",
        "updated_date": "2025-08-25T12:38:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junyuan Deng",
            "Heng Li",
            "Tao Xie",
            "Weiqiang Ren",
            "Qian Zhang",
            "Ping Tan",
            "Xiaoyang Guo"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "SAIL-Recon introduces a feed-forward Transformer for large-scale Structure-from-Motion (SfM) tasks by combining scene regression with visual localization, achieving state-of-the-art results on camera pose estimation and novel view synthesis benchmarks.",
        "tldr_zh": "SAIL-Recon通过将场景回归与视觉定位相结合，引入了一个用于大规模SfM任务的前馈Transformer，实现了在相机姿势估计和新视图合成基准方面的最新结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Generative Feature Imputing - A Technique for Error-resilient Semantic Communication",
        "summary": "Semantic communication (SemCom) has emerged as a promising paradigm for\nachieving unprecedented communication efficiency in sixth-generation (6G)\nnetworks by leveraging artificial intelligence (AI) to extract and transmit the\nunderlying meanings of source data. However, deploying SemCom over digital\nsystems presents new challenges, particularly in ensuring robustness against\ntransmission errors that may distort semantically critical content. To address\nthis issue, this paper proposes a novel framework, termed generative feature\nimputing, which comprises three key techniques. First, we introduce a spatial\nerror concentration packetization strategy that spatially concentrates feature\ndistortions by encoding feature elements based on their channel mappings, a\nproperty crucial for both the effectiveness and reduced complexity of the\nsubsequent techniques. Second, building on this strategy, we propose a\ngenerative feature imputing method that utilizes a diffusion model to\nefficiently reconstruct missing features caused by packet losses. Finally, we\ndevelop a semantic-aware power allocation scheme that enables unequal error\nprotection by allocating transmission power according to the semantic\nimportance of each packet. Experimental results demonstrate that the proposed\nframework outperforms conventional approaches, such as Deep Joint\nSource-Channel Coding (DJSCC) and JPEG2000, under block fading conditions,\nachieving higher semantic accuracy and lower Learned Perceptual Image Patch\nSimilarity (LPIPS) scores.",
        "url": "http://arxiv.org/abs/2508.17957v1",
        "published_date": "2025-08-25T12:19:48+00:00",
        "updated_date": "2025-08-25T12:19:48+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Jianhao Huang",
            "Qunsong Zeng",
            "Hongyang Du",
            "Kaibin Huang"
        ],
        "ai_categories": [
            "AIGC",
            "Diffusion",
            "Transformer"
        ],
        "tldr": "The paper proposes a framework called generative feature imputing to enhance semantic communication in 6G networks by addressing robustness against transmission errors.",
        "tldr_zh": "本文提出了一个名为生成特征填补的框架，通过解决对传输错误强劲的鲁棒性，以增强6G网络中的语义通信。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health Biomarkers Estimation",
        "summary": "Progress in remote PhotoPlethysmoGraphy (rPPG) is limited by the critical\nissues of existing publicly available datasets: small size, privacy concerns\nwith facial videos, and lack of diversity in conditions. The paper introduces a\nnovel comprehensive large-scale multi-view video dataset for rPPG and health\nbiomarkers estimation. Our dataset comprises 3600 synchronized video recordings\nfrom 600 subjects, captured under varied conditions (resting and post-exercise)\nusing multiple consumer-grade cameras at different angles. To enable multimodal\nanalysis of physiological states, each recording is paired with a 100 Hz PPG\nsignal and extended health metrics, such as electrocardiogram, arterial blood\npressure, biomarkers, temperature, oxygen saturation, respiratory rate, and\nstress level. Using this data, we train an efficient rPPG model and compare its\nquality with existing approaches in cross-dataset scenarios. The public release\nof our dataset and model should significantly speed up the progress in the\ndevelopment of AI medical assistants.",
        "url": "http://arxiv.org/abs/2508.17924v1",
        "published_date": "2025-08-25T11:46:40+00:00",
        "updated_date": "2025-08-25T11:46:40+00:00",
        "categories": [
            "cs.CV",
            "68T45",
            "I.4.9"
        ],
        "authors": [
            "Konstantin Egorov",
            "Stepan Botman",
            "Pavel Blinov",
            "Galina Zubkova",
            "Anton Ivaschenko",
            "Alexander Kolsanov",
            "Andrey Savchenko"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a large-scale multi-view video dataset for rPPG and health biomarkers estimation, aiming to improve remote Photoplethysmography (rPPG) research.",
        "tldr_zh": "该论文介绍了一个大规模的多视角视频数据集，用于远程光电容量脉搏图(rPPG)和健康生物标志物估计，旨在改善远程光电容量脉搏图(rPPG)研究。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Egocentric Instruction-oriented Affordance Prediction via Large Multimodal Model",
        "summary": "Affordance is crucial for intelligent robots in the context of object\nmanipulation. In this paper, we argue that affordance should be\ntask-/instruction-dependent, which is overlooked by many previous works. That\nis, different instructions can lead to different manipulation regions and\ndirections even for the same object. According to this observation, we present\na new dataset comprising fifteen thousand object-instruction-affordance\ntriplets. All scenes in the dataset are from an egocentric viewpoint, designed\nto approximate the perspective of a human-like robot. Furthermore, we\ninvestigate how to enable large multimodal models (LMMs) to serve as affordance\npredictors by implementing a ``search against verifiers'' pipeline. An LMM is\nasked to progressively predict affordances, with the output at each step being\nverified by itself during the iterative process, imitating a reasoning process.\nExperiments show that our method not only unlocks new instruction-oriented\naffordance prediction capabilities, but also achieves outstanding performance\nbroadly.",
        "url": "http://arxiv.org/abs/2508.17922v1",
        "published_date": "2025-08-25T11:40:31+00:00",
        "updated_date": "2025-08-25T11:40:31+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Bokai Ji",
            "Jie Gu",
            "Xiaokang Ma",
            "Chu Tang",
            "Jingmin Chen",
            "Guangxia Li"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces a new dataset and a method using large multimodal models for instruction-oriented affordance prediction in object manipulation from an egocentric viewpoint, showing significant performance improvement.",
        "tldr_zh": "本文介绍了一种新的数据集和方法，使用大型多模态模型进行从自我中心视角的操作指导能力预测，显示出显著的性能改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Designing Practical Models for Isolated Word Visual Speech Recognition",
        "summary": "Visual speech recognition (VSR) systems decode spoken words from an input\nsequence using only the video data. Practical applications of such systems\ninclude medical assistance as well as human-machine interactions. A VSR system\nis typically employed in a complementary role in cases where the audio is\ncorrupt or not available. In order to accurately predict the spoken words,\nthese architectures often rely on deep neural networks in order to extract\nmeaningful representations from the input sequence. While deep architectures\nachieve impressive recognition performance, relying on such models incurs\nsignificant computation costs which translates into increased resource demands\nin terms of hardware requirements and results in limited applicability in\nreal-world scenarios where resources might be constrained. This factor prevents\nwider adoption and deployment of speech recognition systems in more practical\napplications. In this work, we aim to alleviate this issue by developing\narchitectures for VSR that have low hardware costs. Following the standard\ntwo-network design paradigm, where one network handles visual feature\nextraction and another one utilizes the extracted features to classify the\nentire sequence, we develop lightweight end-to-end architectures by first\nbenchmarking efficient models from the image classification literature, and\nthen adopting lightweight block designs in a temporal convolution network\nbackbone. We create several unified models with low resource requirements but\nstrong recognition performance. Experiments on the largest public database for\nEnglish words demonstrate the effectiveness and practicality of our developed\nmodels. Code and trained models will be made publicly available.",
        "url": "http://arxiv.org/abs/2508.17894v1",
        "published_date": "2025-08-25T11:04:36+00:00",
        "updated_date": "2025-08-25T11:04:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Iason Ioannis Panagos",
            "Giorgos Sfikas",
            "Christophoros Nikou"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper proposes lightweight architectures for visual speech recognition to reduce resource costs while maintaining strong recognition performance.",
        "tldr_zh": "该论文提出了用于视觉语音识别的轻量级架构，以降低资源成本，同时保持较强的识别性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "UniAPO: Unified Multimodal Automated Prompt Optimization",
        "summary": "Prompting is fundamental to unlocking the full potential of large language\nmodels. To automate and enhance this process, automatic prompt optimization\n(APO) has been developed, demonstrating effectiveness primarily in text-only\ninput scenarios. However, extending existing APO methods to multimodal tasks,\nsuch as video-language generation introduces two core challenges: (i) visual\ntoken inflation, where long visual token sequences restrict context capacity\nand result in insufficient feedback signals; (ii) a lack of process-level\nsupervision, as existing methods focus on outcome-level supervision and\noverlook intermediate supervision, limiting prompt optimization. We present\nUniAPO: Unified Multimodal Automated Prompt Optimization, the first framework\ntailored for multimodal APO. UniAPO adopts an EM-inspired optimization process\nthat decouples feedback modeling and prompt refinement, making the optimization\nmore stable and goal-driven. To further address the aforementioned challenges,\nwe introduce a short-long term memory mechanism: historical feedback mitigates\ncontext limitations, while historical prompts provide directional guidance for\neffective prompt optimization. UniAPO achieves consistent gains across text,\nimage, and video benchmarks, establishing a unified framework for efficient and\ntransferable prompt optimization.",
        "url": "http://arxiv.org/abs/2508.17890v1",
        "published_date": "2025-08-25T10:56:39+00:00",
        "updated_date": "2025-08-25T10:56:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qipeng Zhu",
            "Yanzhe Chen",
            "Huasong Zhong",
            "Yan Li",
            "Jie Chen",
            "Zhixin Zhang",
            "Junping Zhang",
            "Zhenheng Yang"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Other"
        ],
        "tldr": "UniAPO is a framework for multimodal prompt optimization, improving on existing methods by addressing challenges in visual token inflation and process-level supervision.",
        "tldr_zh": "UniAPO 是一个用于多模态提示优化的框架，通过解决视觉记号膨胀和过程级监督等挑战，改进了现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Camera Pose Refinement via 3D Gaussian Splatting",
        "summary": "Camera pose refinement aims at improving the accuracy of initial pose\nestimation for applications in 3D computer vision. Most refinement approaches\nrely on 2D-3D correspondences with specific descriptors or dedicated networks,\nrequiring reconstructing the scene again for a different descriptor or fully\nretraining the network for each scene. Some recent methods instead infer pose\nfrom feature similarity, but their lack of geometry constraints results in less\naccuracy. To overcome these limitations, we propose a novel camera pose\nrefinement framework leveraging 3D Gaussian Splatting (3DGS), referred to as\nGS-SMC. Given the widespread usage of 3DGS, our method can employ an existing\n3DGS model to render novel views, providing a lightweight solution that can be\ndirectly applied to diverse scenes without additional training or fine-tuning.\nSpecifically, we introduce an iterative optimization approach, which refines\nthe camera pose using epipolar geometric constraints among the query and\nmultiple rendered images. Our method allows flexibly choosing feature\nextractors and matchers to establish these constraints. Extensive empirical\nevaluations on the 7-Scenes and the Cambridge Landmarks datasets demonstrate\nthat our method outperforms state-of-the-art camera pose refinement approaches,\nachieving 53.3% and 56.9% reductions in median translation and rotation errors\non 7-Scenes, and 40.7% and 53.2% on Cambridge.",
        "url": "http://arxiv.org/abs/2508.17876v1",
        "published_date": "2025-08-25T10:29:59+00:00",
        "updated_date": "2025-08-25T10:29:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lulu Hao",
            "Lipu Zhou",
            "Zhenzhong Wei",
            "Xu Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a camera pose refinement framework using 3D Gaussian Splatting, achieving significant improvements in accuracy compared to existing methods.",
        "tldr_zh": "本文提出了一种使用3D高斯分裂的相机姿态精调框架，与现有方法相比，在准确性方面取得了显著的改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AVAM: Universal Training-free Adaptive Visual Anchoring Embedded into Multimodal Large Language Model for Multi-image Question Answering",
        "summary": "The advancement of Multimodal Large Language Models (MLLMs) has driven\nsignificant progress in Visual Question Answering (VQA), evolving from Single\nto Multi Image VQA (MVQA). However, the increased number of images in MVQA\ninevitably introduces substantial visual redundancy that is irrelevant to\nquestion answering, negatively impacting both accuracy and efficiency. To\naddress this issue, existing methods lack flexibility in controlling the number\nof compressed visual tokens and tend to produce discrete visual fragments,\nwhich hinder MLLMs' ability to comprehend images holistically. In this paper,\nwe propose a straightforward yet universal Adaptive Visual Anchoring strategy,\nwhich can be seamlessly integrated into existing MLLMs, offering significant\naccuracy improvements through adaptive compression. Meanwhile, to balance the\nresults derived from both global and compressed visual input, we further\nintroduce a novel collaborative decoding mechanism, enabling optimal\nperformance. Extensive experiments validate the effectiveness of our method,\ndemonstrating consistent performance improvements across various MLLMs. The\ncode will be publicly available.",
        "url": "http://arxiv.org/abs/2508.17860v1",
        "published_date": "2025-08-25T10:10:46+00:00",
        "updated_date": "2025-08-25T10:10:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kang Zeng",
            "Guojin Zhong",
            "Jintao Cheng",
            "Jin Yuan",
            "Zhiyong Li"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a method to improve Multi-image Visual Question Answering by reducing visual redundancy while enhancing accuracy through adaptive compression and collaborative decoding.",
        "tldr_zh": "本文介绍了一种通过自适应压缩和协作解码来减少视觉冗余并提高准确性的方法，以改进多图像视觉问答。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Diffusion-Based Data Augmentation for Medical Image Segmentation",
        "summary": "Medical image segmentation models struggle with rare abnormalities due to\nscarce annotated pathological data. We propose DiffAug a novel framework that\ncombines textguided diffusion-based generation with automatic segmentation\nvalidation to address this challenge. Our proposed approach uses latent\ndiffusion models conditioned on medical text descriptions and spatial masks to\nsynthesize abnormalities via inpainting on normal images. Generated samples\nundergo dynamic quality validation through a latentspace segmentation network\nthat ensures accurate localization while enabling single-step inference. The\ntext prompts, derived from medical literature, guide the generation of diverse\nabnormality types without requiring manual annotation. Our validation mechanism\nfilters synthetic samples based on spatial accuracy, maintaining quality while\noperating efficiently through direct latent estimation. Evaluated on three\nmedical imaging benchmarks (CVC-ClinicDB, Kvasir-SEG, REFUGE2), our framework\nachieves state-of-the-art performance with 8-10% Dice improvements over\nbaselines and reduces false negative rates by up to 28% for challenging cases\nlike small polyps and flat lesions critical for early detection in screening\napplications.",
        "url": "http://arxiv.org/abs/2508.17844v1",
        "published_date": "2025-08-25T09:49:27+00:00",
        "updated_date": "2025-08-25T09:49:27+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Maham Nazir",
            "Muhammad Aqeel",
            "Francesco Setti"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper introduces DiffAug, a framework that combines text-guided diffusion-based generation with automatic segmentation validation for medical image segmentation to address the challenge of rare abnormalities. It achieves state-of-the-art performance with significant improvements over baselines.",
        "tldr_zh": "本文介绍了DiffAug，这是一个将文本引导的扩散生成与自动分割验证相结合的框架，用于医学图像分割，以解决罕见异常的挑战。它在基线上取得了显著的性能提升，达到了最先进水平。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "SCOUT: Semi-supervised Camouflaged Object Detection by Utilizing Text and Adaptive Data Selection",
        "summary": "The difficulty of pixel-level annotation has significantly hindered the\ndevelopment of the Camouflaged Object Detection (COD) field. To save on\nannotation costs, previous works leverage the semi-supervised COD framework\nthat relies on a small number of labeled data and a large volume of unlabeled\ndata. We argue that there is still significant room for improvement in the\neffective utilization of unlabeled data. To this end, we introduce a\nSemi-supervised Camouflaged Object Detection by Utilizing Text and Adaptive\nData Selection (SCOUT). It includes an Adaptive Data Augment and Selection\n(ADAS) module and a Text Fusion Module (TFM). The ADSA module selects valuable\ndata for annotation through an adversarial augment and sampling strategy. The\nTFM module further leverages the selected valuable data by combining\ncamouflage-related knowledge and text-visual interaction. To adapt to this\nwork, we build a new dataset, namely RefTextCOD. Extensive experiments show\nthat the proposed method surpasses previous semi-supervised methods in the COD\nfield and achieves state-of-the-art performance. Our code will be released at\nhttps://github.com/Heartfirey/SCOUT.",
        "url": "http://arxiv.org/abs/2508.17843v1",
        "published_date": "2025-08-25T09:47:12+00:00",
        "updated_date": "2025-08-25T09:47:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weiqi Yan",
            "Lvhai Chen",
            "Shengchuan Zhang",
            "Yan Zhang",
            "Liujuan Cao"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces SCOUT, a semi-supervised method for Camouflaged Object Detection that leverages text and adaptive data selection to outperform existing methods.",
        "tldr_zh": "本文介绍了SCOUT，一种利用文本和自适应数据选择的半监督方法，用于伪装物体检测，表现优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniSino: Physics-Driven Foundational Model for Universal CT Sinogram Standardization",
        "summary": "During raw-data acquisition in CT imaging, diverse factors can degrade the\ncollected sinograms, with undersampling and noise leading to severe artifacts\nand noise in reconstructed images and compromising diagnostic accuracy.\nConventional correction methods rely on manually designed algorithms or fixed\nempirical parameters, but these approaches often lack generalizability across\nheterogeneous artifact types. To address these limitations, we propose UniSino,\na foundation model for universal CT sinogram standardization. Unlike existing\nfoundational models that operate in image domain, UniSino directly standardizes\ndata in the projection domain, which enables stronger generalization across\ndiverse undersampling scenarios. Its training framework incorporates the\nphysical characteristics of sinograms, enhancing generalization and enabling\nrobust performance across multiple subtasks spanning four benchmark datasets.\nExperimental results demonstrate thatUniSino achieves superior reconstruction\nquality both single and mixed undersampling case, demonstrating exceptional\nrobustness and generalization in sinogram enhancement for CT imaging. The code\nis available at: https://github.com/yqx7150/UniSino.",
        "url": "http://arxiv.org/abs/2508.17816v1",
        "published_date": "2025-08-25T09:12:14+00:00",
        "updated_date": "2025-08-25T09:12:14+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xingyu Ai",
            "Shaoyu Wang",
            "Zhiyuan Jia",
            "Ao Xu",
            "Hongming Shan",
            "Jianhua Ma",
            "Qiegen Liu"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "UniSino proposes a physics-driven foundational model for universal CT sinogram standardization, achieving superior reconstruction quality and generalization across diverse undersampling scenarios.",
        "tldr_zh": "UniSino提出了一种物理驱动的基础模型，用于通用CT正弦图标准化，在各种欠采样场景下实现了优越的重建质量和泛化能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian Splatting",
        "summary": "Surface reconstruction has been widely studied in computer vision and\ngraphics. However, existing surface reconstruction works struggle to recover\naccurate scene geometry when the input views are extremely sparse. To address\nthis issue, we propose MeshSplat, a generalizable sparse-view surface\nreconstruction framework via Gaussian Splatting. Our key idea is to leverage\n2DGS as a bridge, which connects novel view synthesis to learned geometric\npriors and then transfers these priors to achieve surface reconstruction.\nSpecifically, we incorporate a feed-forward network to predict per-view\npixel-aligned 2DGS, which enables the network to synthesize novel view images\nand thus eliminates the need for direct 3D ground-truth supervision. To improve\nthe accuracy of 2DGS position and orientation prediction, we propose a Weighted\nChamfer Distance Loss to regularize the depth maps, especially in overlapping\nareas of input views, and also a normal prediction network to align the\norientation of 2DGS with normal vectors predicted by a monocular normal\nestimator. Extensive experiments validate the effectiveness of our proposed\nimprovement, demonstrating that our method achieves state-of-the-art\nperformance in generalizable sparse-view mesh reconstruction tasks. Project\nPage: https://hanzhichang.github.io/meshsplat_web",
        "url": "http://arxiv.org/abs/2508.17811v1",
        "published_date": "2025-08-25T09:04:20+00:00",
        "updated_date": "2025-08-25T09:04:20+00:00",
        "categories": [
            "cs.GR",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Hanzhi Chang",
            "Ruijie Zhu",
            "Wenjie Chang",
            "Mulin Yu",
            "Yanzhe Liang",
            "Jiahao Lu",
            "Zhuoyuan Li",
            "Tianzhu Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces MeshSplat, a framework for reconstructing surfaces from sparse views using Gaussian Splatting, showing state-of-the-art performance in mesh reconstruction tasks.",
        "tldr_zh": "该论文介绍了MeshSplat，一种使用高斯喷洒技术从稀疏视图中重建表面的框架，在网格重建任务中展现出最先进的表现。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PoRe: Position-Reweighted Visual Token Pruning for Vision Language Models",
        "summary": "Vision-Language Models (VLMs) typically process a significantly larger number\nof visual tokens compared to text tokens due to the inherent redundancy in\nvisual signals. Visual token pruning is a promising direction to reduce the\ncomputational cost of VLMs by eliminating redundant visual tokens. The\ntext-visual attention score is a widely adopted criterion for visual token\npruning as it reflects the relevance of visual tokens to the text input.\nHowever, many sequence models exhibit a recency bias, where tokens appearing\nlater in the sequence exert a disproportionately large influence on the model's\noutput. In VLMs, this bias manifests as inflated attention scores for tokens\ncorresponding to the lower regions of the image, leading to suboptimal pruning\nthat disproportionately retains tokens from the image bottom. In this paper, we\npresent an extremely simple yet effective approach to alleviate the recency\nbias in visual token pruning. We propose a straightforward reweighting\nmechanism that adjusts the attention scores of visual tokens according to their\nspatial positions in the image. Our method, termed Position-reweighted Visual\nToken Pruning, is a plug-and-play solution that can be seamlessly incorporated\ninto existing visual token pruning frameworks without any changes to the model\narchitecture or extra training. Extensive experiments on LVLMs demonstrate that\nour method improves the performance of visual token pruning with minimal\ncomputational overhead.",
        "url": "http://arxiv.org/abs/2508.17807v1",
        "published_date": "2025-08-25T08:56:32+00:00",
        "updated_date": "2025-08-25T08:56:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kai Zhao",
            "Wubang Yuan",
            "Alex Lingyu Hung",
            "Dan Zeng"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper introduces a method called PoRe which helps to improve visual token pruning in vision-language models by alleviating recency bias.",
        "tldr_zh": "本文介绍了一种名为 PoRe 的方法，通过缓解最近偏见来帮助改善视觉语言模型中的视觉标记修剪。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Robust Anomaly Detection in Industrial Environments via Meta-Learning",
        "summary": "Anomaly detection is fundamental for ensuring quality control and operational\nefficiency in industrial environments, yet conventional approaches face\nsignificant challenges when training data contains mislabeled samples-a common\noccurrence in real-world scenarios. This paper presents RAD, a robust anomaly\ndetection framework that integrates Normalizing Flows with Model-Agnostic\nMeta-Learning to address the critical challenge of label noise in industrial\nsettings. Our approach employs a bi-level optimization strategy where\nmeta-learning enables rapid adaptation to varying noise conditions, while\nuncertainty quantification guides adaptive L2 regularization to maintain model\nstability. The framework incorporates multiscale feature processing through\npretrained feature extractors and leverages the precise likelihood estimation\ncapabilities of Normalizing Flows for robust anomaly scoring. Comprehensive\nevaluation on MVTec-AD and KSDD2 datasets demonstrates superior performance,\nachieving I-AUROC scores of 95.4% and 94.6% respectively under clean\nconditions, while maintaining robust detection capabilities above 86.8% and\n92.1% even when 50% of training samples are mislabeled. The results highlight\nRAD's exceptional resilience to noisy training conditions and its ability to\ndetect subtle anomalies across diverse industrial scenarios, making it a\npractical solution for real-world anomaly detection applications where perfect\ndata curation is challenging.",
        "url": "http://arxiv.org/abs/2508.17789v1",
        "published_date": "2025-08-25T08:35:28+00:00",
        "updated_date": "2025-08-25T08:35:28+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Muhammad Aqeel",
            "Shakiba Sharifi",
            "Marco Cristani",
            "Francesco Setti"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces RAD, a robust anomaly detection framework using Normalizing Flows and Meta-Learning to handle label noise in industrial environments.",
        "tldr_zh": "本文介绍了RAD，一个使用正规化流和元学习的强大异常检测框架，用于处理工业环境中的标签噪声。",
        "relevance_score": 1,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "CEIDM: A Controlled Entity and Interaction Diffusion Model for Enhanced Text-to-Image Generation",
        "summary": "In Text-to-Image (T2I) generation, the complexity of entities and their\nintricate interactions pose a significant challenge for T2I method based on\ndiffusion model: how to effectively control entity and their interactions to\nproduce high-quality images. To address this, we propose CEIDM, a image\ngeneration method based on diffusion model with dual controls for entity and\ninteraction. First, we propose an entity interactive relationships mining\napproach based on Large Language Models (LLMs), extracting reasonable and rich\nimplicit interactive relationships through chain of thought to guide diffusion\nmodels to generate high-quality images that are closer to realistic logic and\nhave more reasonable interactive relationships. Furthermore, We propose an\ninteractive action clustering and offset method to cluster and offset the\ninteractive action features contained in each text prompts. By constructing\nglobal and local bidirectional offsets, we enhance semantic understanding and\ndetail supplementation of original actions, making the model's understanding of\nthe concept of interactive \"actions\" more accurate and generating images with\nmore accurate interactive actions. Finally, we design an entity control network\nwhich generates masks with entity semantic guidance, then leveraging\nmulti-scale convolutional network to enhance entity feature and dynamic network\nto fuse feature. It effectively controls entities and significantly improves\nimage quality. Experiments show that the proposed CEIDM method is better than\nthe most representative existing methods in both entity control and their\ninteraction control.",
        "url": "http://arxiv.org/abs/2508.17760v1",
        "published_date": "2025-08-25T07:58:57+00:00",
        "updated_date": "2025-08-25T07:58:57+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Mingyue Yang",
            "Dianxi Shi",
            "Jialu Zhou",
            "Xinyu Wei",
            "Leqian Li",
            "Shaowu Yang",
            "Chunping Qiu"
        ],
        "ai_categories": [
            "Transformer",
            "Image Generation",
            "Diffusion"
        ],
        "tldr": "The paper proposes a method called CEIDM for text-to-image generation, which controls entities and interactions to improve image quality, surpassing existing methods.",
        "tldr_zh": "本文提出一种名为CEIDM的文本到图像生成方法，通过控制实体和交互来提高图像质量，超越现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DroneKey: Drone 3D Pose Estimation in Image Sequences using Gated Key-representation and Pose-adaptive Learning",
        "summary": "Estimating the 3D pose of a drone is important for anti-drone systems, but\nexisting methods struggle with the unique challenges of drone keypoint\ndetection. Drone propellers serve as keypoints but are difficult to detect due\nto their high visual similarity and diversity of poses. To address these\nchallenges, we propose DroneKey, a framework that combines a 2D keypoint\ndetector and a 3D pose estimator specifically designed for drones. In the\nkeypoint detection stage, we extract two key-representations (intermediate and\ncompact) from each transformer encoder layer and optimally combine them using a\ngated sum. We also introduce a pose-adaptive Mahalanobis distance in the loss\nfunction to ensure stable keypoint predictions across extreme poses. We built\nnew datasets of drone 2D keypoints and 3D pose to train and evaluate our\nmethod, which have been publicly released. Experiments show that our method\nachieves an AP of 99.68% (OKS) in keypoint detection, outperforming existing\nmethods. Ablation studies confirm that the pose-adaptive Mahalanobis loss\nfunction improves keypoint prediction stability and accuracy. Additionally,\nimprovements in the encoder design enable real-time processing at 44 FPS. For\n3D pose estimation, our method achieved an MAE-angle of 10.62{\\deg}, an RMSE of\n0.221m, and an MAE-absolute of 0.076m, demonstrating high accuracy and\nreliability. The code and dataset are available at\nhttps://github.com/kkanuseobin/DroneKey.",
        "url": "http://arxiv.org/abs/2508.17746v1",
        "published_date": "2025-08-25T07:40:31+00:00",
        "updated_date": "2025-08-25T07:40:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Seo-Bin Hwang",
            "Yeong-Jun Cho"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "DroneKey proposes a framework for 3D pose estimation of drones using key-representations and pose-adaptive learning, achieving high accuracy and real-time processing.",
        "tldr_zh": "DroneKey提出了一个框架，使用关键表示和姿态自适应学习来对无人机进行3D姿态估计，实现高精度和实时处理。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "F2RVLM: Boosting Fine-grained Fragment Retrieval for Multi-Modal Long-form Dialogue with Vision Language Model",
        "summary": "Traditional dialogue retrieval aims to select the most appropriate utterance\nor image from recent dialogue history. However, they often fail to meet users'\nactual needs for revisiting semantically coherent content scattered across\nlong-form conversations. To fill this gap, we define the Fine-grained Fragment\nRetrieval (FFR) task, requiring models to locate query-relevant fragments,\ncomprising both utterances and images, from multimodal long-form dialogues. As\na foundation for FFR, we construct MLDR, the longest-turn multimodal dialogue\nretrieval dataset to date, averaging 25.45 turns per dialogue, with each\nnaturally spanning three distinct topics. To evaluate generalization in\nreal-world scenarios, we curate and annotate a WeChat-based test set comprising\nreal-world multimodal dialogues with an average of 75.38 turns. Building on\nthese resources, we explore existing generation-based Vision-Language Models\n(VLMs) on FFR and observe that they often retrieve incoherent utterance-image\nfragments. While optimized for generating responses from visual-textual inputs,\nthese models lack explicit supervision to ensure semantic coherence within\nretrieved fragments. To this end, we propose F2RVLM, a generative retrieval\nmodel trained in a two-stage paradigm: (1) supervised fine-tuning to inject\nfragment-level retrieval knowledge, and (2) GRPO-based reinforcement learning\nwith multi-objective rewards promoting semantic precision, relevance, and\ncontextual coherence. To handle varying intra-fragment complexity, from locally\ndense to sparsely distributed, we introduce difficulty-aware curriculum\nsampling that ranks training instances by model-predicted difficulty and\ngradually exposes the model to harder samples. This boosts reasoning ability in\nlong, multi-turn contexts. F2RVLM outperforms popular VLMs in both in-domain\nand real-domain settings, demonstrating superior retrieval performance.",
        "url": "http://arxiv.org/abs/2508.17714v1",
        "published_date": "2025-08-25T06:42:47+00:00",
        "updated_date": "2025-08-25T06:42:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hanbo Bi",
            "Zhiqiang Yuan",
            "Zexi Jia",
            "Jiapei Zhang",
            "Chongyang Li",
            "Peixiang Luo",
            "Ying Deng",
            "Xiaoyue Duan",
            "Jinchao Zhang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces F2RVLM, a model for fine-grained fragment retrieval in multi-modal long-form dialogues, outperforming existing models in both in-domain and real-domain settings.",
        "tldr_zh": "本文介绍了F2RVLM，一种用于多模式长篇对话中的细粒度片段检索的模型，在领域内和真实领域设置中表现出色。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "NGD: Neural Gradient Based Deformation for Monocular Garment Reconstruction",
        "summary": "Dynamic garment reconstruction from monocular video is an important yet\nchallenging task due to the complex dynamics and unconstrained nature of the\ngarments. Recent advancements in neural rendering have enabled high-quality\ngeometric reconstruction with image/video supervision. However, implicit\nrepresentation methods that use volume rendering often provide smooth geometry\nand fail to model high-frequency details. While template reconstruction methods\nmodel explicit geometry, they use vertex displacement for deformation, which\nresults in artifacts. Addressing these limitations, we propose NGD, a Neural\nGradient-based Deformation method to reconstruct dynamically evolving textured\ngarments from monocular videos. Additionally, we propose a novel adaptive\nremeshing strategy for modelling dynamically evolving surfaces like wrinkles\nand pleats of the skirt, leading to high-quality reconstruction. Finally, we\nlearn dynamic texture maps to capture per-frame lighting and shadow effects. We\nprovide extensive qualitative and quantitative evaluations to demonstrate\nsignificant improvements over existing SOTA methods and provide high-quality\ngarment reconstructions.",
        "url": "http://arxiv.org/abs/2508.17712v1",
        "published_date": "2025-08-25T06:40:57+00:00",
        "updated_date": "2025-08-25T06:40:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Soham Dasgupta",
            "Shanthika Naik",
            "Preet Savalia",
            "Sujay Kumar Ingle",
            "Avinash Sharma"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces NGD, a method for reconstructing textured garments from monocular videos with high-quality results.",
        "tldr_zh": "本文介绍了NGD，一种用于从单眼视频中重建纹理服装的方法，结果具有高质量。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CATformer: Contrastive Adversarial Transformer for Image Super-Resolution",
        "summary": "Super-resolution remains a promising technique to enhance the quality of\nlow-resolution images. This study introduces CATformer (Contrastive Adversarial\nTransformer), a novel neural network integrating diffusion-inspired feature\nrefinement with adversarial and contrastive learning. CATformer employs a\ndual-branch architecture combining a primary diffusion-inspired transformer,\nwhich progressively refines latent representations, with an auxiliary\ntransformer branch designed to enhance robustness to noise through learned\nlatent contrasts. These complementary representations are fused and decoded\nusing deep Residual-in-Residual Dense Blocks for enhanced reconstruction\nquality. Extensive experiments on benchmark datasets demonstrate that CATformer\noutperforms recent transformer-based and diffusion-inspired methods both in\nefficiency and visual image quality. This work bridges the performance gap\namong transformer-, diffusion-, and GAN-based methods, laying a foundation for\npractical applications of diffusion-inspired transformers in super-resolution.",
        "url": "http://arxiv.org/abs/2508.17708v1",
        "published_date": "2025-08-25T06:30:18+00:00",
        "updated_date": "2025-08-25T06:30:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qinyi Tian",
            "Spence Cox",
            "Laura E. Dalton"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "GAN",
            "Dataset"
        ],
        "tldr": "CATformer is a novel neural network for image super-resolution that combines diffusion-inspired feature refinement with adversarial and contrastive learning, outperforming recent methods in both efficiency and visual quality.",
        "tldr_zh": "CATformer是一种新型的神经网络，用于图像超分辨率，结合了受扩散启发的特征细化和对抗性对比学习，优于最近的方法在效率和视觉质量方面。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Language-Guided Temporal Token Pruning for Efficient VideoLLM Processing",
        "summary": "Vision Language Models (VLMs) struggle with long-form videos due to the\nquadratic complexity of attention mechanisms. We propose Language-Guided\nTemporal Token Pruning (LGTTP), which leverages temporal cues from queries to\nadaptively prune video tokens, preserving contextual continuity while reducing\ncomputational overhead. Unlike uniform pruning or keyframe selection, LGTTP\nretains higher token density in temporally relevant segments. Our\nmodel-agnostic framework integrates with TimeChat and LLaVA-Video, achieving a\n65% reduction in computation while preserving 97-99% of the original\nperformance. On QVHighlights, LGTTP improves HIT@1 by +9.5%, and on\nCharades-STA, it retains 99.6% of R@1. It excels on queries with explicit\ntemporal markers and remains effective across general video understanding\ntasks.",
        "url": "http://arxiv.org/abs/2508.17686v1",
        "published_date": "2025-08-25T05:51:21+00:00",
        "updated_date": "2025-08-25T05:51:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yogesh Kumar"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces Language-Guided Temporal Token Pruning (LGTTP) to efficiently process long-form videos while maintaining performance. It achieves a 65% reduction in computation while preserving 97-99% of the original performance.",
        "tldr_zh": "该论文引入了语言引导的时间标记修剪(LGTTP)来高效处理长形视频，同时保持性能。它实现了计算减少65%，同时保留了原始性能的97-99%。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Robustness Feature Adapter for Efficient Adversarial Training",
        "summary": "Adversarial training (AT) with projected gradient descent is the most popular\nmethod to improve model robustness under adversarial attacks. However,\ncomputational overheads become prohibitively large when AT is applied to large\nbackbone models. AT is also known to have the issue of robust overfitting. This\npaper contributes to solving both problems simultaneously towards building more\ntrustworthy foundation models. In particular, we propose a new adapter-based\napproach for efficient AT directly in the feature space. We show that the\nproposed adapter-based approach can improve the inner-loop convergence quality\nby eliminating robust overfitting. As a result, it significantly increases\ncomputational efficiency and improves model accuracy by generalizing\nadversarial robustness to unseen attacks. We demonstrate the effectiveness of\nthe new adapter-based approach in different backbone architectures and in AT at\nscale.",
        "url": "http://arxiv.org/abs/2508.17680v1",
        "published_date": "2025-08-25T05:23:50+00:00",
        "updated_date": "2025-08-25T05:23:50+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "I.2.6"
        ],
        "authors": [
            "Quanwei Wu",
            "Jun Guo",
            "Wei Wang",
            "Yi Wang"
        ],
        "ai_categories": [
            "GAN",
            "AIGC"
        ],
        "tldr": "The paper proposes a new adapter-based approach for efficient adversarial training to improve model robustness and reduce overfitting, leading to increased computational efficiency and improved accuracy.",
        "tldr_zh": "本文提出了一种新的基于适配器的方法，用于提高模型的鲁棒性和降低过拟合，从而提高计算效率和准确性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Hierarchical Vision-Language Learning for Medical Out-of-Distribution Detection",
        "summary": "In trustworthy medical diagnosis systems, integrating out-of-distribution\n(OOD) detection aims to identify unknown diseases in samples, thereby\nmitigating the risk of misdiagnosis. In this study, we propose a novel OOD\ndetection framework based on vision-language models (VLMs), which integrates\nhierarchical visual information to cope with challenging unknown diseases that\nresemble known diseases. Specifically, a cross-scale visual fusion strategy is\nproposed to couple visual embeddings from multiple scales. This enriches the\ndetailed representation of medical images and thus improves the discrimination\nof unknown diseases. Moreover, a cross-scale hard pseudo-OOD sample generation\nstrategy is proposed to benefit OOD detection maximally. Experimental\nevaluations on three public medical datasets support that the proposed\nframework achieves superior OOD detection performance compared to existing\nmethods. The source code is available at https://openi.pcl.ac.cn/OpenMedIA/HVL.",
        "url": "http://arxiv.org/abs/2508.17667v1",
        "published_date": "2025-08-25T04:55:27+00:00",
        "updated_date": "2025-08-25T04:55:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Runhe Lai",
            "Xinhua Lu",
            "Kanghao Chen",
            "Qichao Chen",
            "Wei-Shi Zheng",
            "Ruixuan Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a vision-language framework for detecting unknown diseases in medical images, achieving superior performance compared to existing methods.",
        "tldr_zh": "该论文提出了一个用于检测医学图像中未知疾病的视觉-语言框架，表现优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FloraSyntropy-Net: Scalable Deep Learning with Novel FloraSyntropy Archive for Large-Scale Plant Disease Diagnosis",
        "summary": "Early diagnosis of plant diseases is critical for global food safety, yet\nmost AI solutions lack the generalization required for real-world agricultural\ndiversity. These models are typically constrained to specific species, failing\nto perform accurately across the broad spectrum of cultivated plants. To\naddress this gap, we first introduce the FloraSyntropy Archive, a large-scale\ndataset of 178,922 images across 35 plant species, annotated with 97 distinct\ndisease classes. We establish a benchmark by evaluating numerous existing\nmodels on this archive, revealing a significant performance gap. We then\npropose FloraSyntropy-Net, a novel federated learning framework (FL) that\nintegrates a Memetic Algorithm (MAO) for optimal base model selection\n(DenseNet201), a novel Deep Block for enhanced feature representation, and a\nclient-cloning strategy for scalable, privacy-preserving training.\nFloraSyntropy-Net achieves a state-of-the-art accuracy of 96.38% on the\nFloraSyntropy benchmark. Crucially, to validate its generalization capability,\nwe test the model on the unrelated multiclass Pest dataset, where it\ndemonstrates exceptional adaptability, achieving 99.84% accuracy. This work\nprovides not only a valuable new resource but also a robust and highly\ngeneralizable framework that advances the field towards practical, large-scale\nagricultural AI applications.",
        "url": "http://arxiv.org/abs/2508.17653v1",
        "published_date": "2025-08-25T04:30:21+00:00",
        "updated_date": "2025-08-25T04:30:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Saif Ur Rehman Khan",
            "Muhammad Nabeel Asim",
            "Sebastian Vollmer",
            "Andreas Dengel"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC",
            "Transformer"
        ],
        "tldr": "Introduces FloraSyntropy-Net for plant disease diagnosis, achieves high accuracy and generalization across various plant species.",
        "tldr_zh": "引入了FloraSyntropy-Net用于植物疾病诊断，在各种植物种类上取得了高准确度和泛化能力。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and Manipulation",
        "summary": "Event cameras offer microsecond latency, high dynamic range, and low power\nconsumption, making them ideal for real-time robotic perception under\nchallenging conditions such as motion blur, occlusion, and illumination\nchanges. However, despite their advantages, synthetic event-based vision\nremains largely unexplored in mainstream robotics simulators. This lack of\nsimulation setup hinders the evaluation of event-driven approaches for robotic\nmanipulation and navigation tasks. This work presents an open-source,\nuser-friendly v2e robotics operating system (ROS) package for Gazebo simulation\nthat enables seamless event stream generation from RGB camera feeds. The\npackage is used to investigate event-based robotic policies (ERP) for real-time\nnavigation and manipulation. Two representative scenarios are evaluated: (1)\nobject following with a mobile robot and (2) object detection and grasping with\na robotic manipulator. Transformer-based ERPs are trained by behavior cloning\nand compared to RGB-based counterparts under various operating conditions.\nExperimental results show that event-guided policies consistently deliver\ncompetitive advantages. The results highlight the potential of event-driven\nperception to improve real-time robotic navigation and manipulation, providing\na foundation for broader integration of event cameras into robotic policy\nlearning. The GitHub repo for the dataset and code:\nhttps://eventbasedvision.github.io/SEBVS/",
        "url": "http://arxiv.org/abs/2508.17643v1",
        "published_date": "2025-08-25T04:14:04+00:00",
        "updated_date": "2025-08-25T04:14:04+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Krishna Vinod",
            "Prithvi Jai Ramesh",
            "Pavan Kumar B N",
            "Bharatesh Chakravarthi"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper presents a synthetic event-based visual servoing system for robot navigation and manipulation, leveraging the advantages of event cameras for real-time perception in challenging conditions.",
        "tldr_zh": "该论文提出了一种用于机器人导航和操作的综合事件驱动视觉伺服系统，利用事件摄像头在挑战性条件下的实时感知优势。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dynamic Embedding of Hierarchical Visual Features for Efficient Vision-Language Fine-Tuning",
        "summary": "Large Vision-Language Models (LVLMs) commonly follow a paradigm that projects\nvisual features and then concatenates them with text tokens to form a unified\nsequence input for Large Language Models (LLMs). However, this paradigm leads\nto a significant increase in the length of the input sequence, resulting in\nsubstantial computational overhead. Existing methods attempt to fuse visual\ninformation into the intermediate layers of LLMs, which alleviate the sequence\nlength issue but often neglect the hierarchical semantic representations within\nthe model and the fine-grained visual information available in the shallower\nvisual encoding layers. To address this limitation, we propose DEHVF, an\nefficient vision-language fine-tuning method based on dynamic embedding and\nfusion of hierarchical visual features. Its core lies in leveraging the\ninherent hierarchical representation characteristics of visual encoders and\nlanguage models. Through a lightweight hierarchical visual fuser, it\ndynamically selects and fuses hierarchical features corresponding to semantic\ngranularity based on the internal representations of each layer in LLMs. The\nfused layer-related visual features are then projected and aligned before being\ndirectly embedded into the Feed-Forward Network (FFN) of the corresponding\nlayer in LLMs. This approach not only avoids sequence expansion but also\ndynamically fuses multi-layer visual information. By fine-tuning only a small\nnumber of parameters, DEHVF achieves precise alignment and complementarity of\ncross-modal information at the same semantic granularity. We conducted\nexperiments across various VL benchmarks, including visual question answering\non ScienceQA and image captioning on COCO Captions. The results demonstrate\nthat DEHVF achieves higher accuracy than existing parameter-efficient\nfine-tuning (PEFT) baselines while maintaining efficient training and\ninference.",
        "url": "http://arxiv.org/abs/2508.17638v1",
        "published_date": "2025-08-25T03:57:46+00:00",
        "updated_date": "2025-08-25T03:57:46+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Xinyu Wei",
            "Guoli Yang",
            "Jialu Zhou",
            "Mingyue Yang",
            "Leqian Li",
            "Kedi Zhang",
            "Chunping Qiu"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes a method, DEHVF, for efficient vision-language fine-tuning by dynamically embedding hierarchical visual features, achieving higher accuracy than existing methods on various benchmarks.",
        "tldr_zh": "本文提出了一种通过动态嵌入分层视觉特征实现高效视觉语言微调的方法DEHVF，在各种基准测试中实现比现有方法更高的精度。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "JCo-MVTON: Jointly Controllable Multi-Modal Diffusion Transformer for Mask-Free Virtual Try-on",
        "summary": "Virtual try-on systems have long been hindered by heavy reliance on human\nbody masks, limited fine-grained control over garment attributes, and poor\ngeneralization to real-world, in-the-wild scenarios. In this paper, we propose\nJCo-MVTON (Jointly Controllable Multi-Modal Diffusion Transformer for Mask-Free\nVirtual Try-On), a novel framework that overcomes these limitations by\nintegrating diffusion-based image generation with multi-modal conditional\nfusion. Built upon a Multi-Modal Diffusion Transformer (MM-DiT) backbone, our\napproach directly incorporates diverse control signals -- such as the reference\nperson image and the target garment image -- into the denoising process through\ndedicated conditional pathways that fuse features within the self-attention\nlayers. This fusion is further enhanced with refined positional encodings and\nattention masks, enabling precise spatial alignment and improved garment-person\nintegration. To address data scarcity and quality, we introduce a bidirectional\ngeneration strategy for dataset construction: one pipeline uses a mask-based\nmodel to generate realistic reference images, while a symmetric ``Try-Off''\nmodel, trained in a self-supervised manner, recovers the corresponding garment\nimages. The synthesized dataset undergoes rigorous manual curation, allowing\niterative improvement in visual fidelity and diversity. Experiments demonstrate\nthat JCo-MVTON achieves state-of-the-art performance on public benchmarks\nincluding DressCode, significantly outperforming existing methods in both\nquantitative metrics and human evaluations. Moreover, it shows strong\ngeneralization in real-world applications, surpassing commercial systems.",
        "url": "http://arxiv.org/abs/2508.17614v1",
        "published_date": "2025-08-25T02:43:57+00:00",
        "updated_date": "2025-08-25T02:43:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aowen Wang",
            "Wei Li",
            "Hao Luo",
            "Mengxing Ao",
            "Chenyu Zhu",
            "Xinyang Li",
            "Fan Wang"
        ],
        "ai_categories": [
            "Multimodality",
            "Diffusion",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a framework called JCo-MVTON for virtual try-on without the need for body masks, allowing more control over garment attributes and better generalization to real-world scenarios.",
        "tldr_zh": "本文提出了一种名为JCo-MVTON的框架，用于虚拟试穿，无需人体面具，可更好地控制服装属性，并更好地适用于实际场景。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "GWM: Towards Scalable Gaussian World Models for Robotic Manipulation",
        "summary": "Training robot policies within a learned world model is trending due to the\ninefficiency of real-world interactions. The established image-based world\nmodels and policies have shown prior success, but lack robust geometric\ninformation that requires consistent spatial and physical understanding of the\nthree-dimensional world, even pre-trained on internet-scale video sources. To\nthis end, we propose a novel branch of world model named Gaussian World Model\n(GWM) for robotic manipulation, which reconstructs the future state by\ninferring the propagation of Gaussian primitives under the effect of robot\nactions. At its core is a latent Diffusion Transformer (DiT) combined with a 3D\nvariational autoencoder, enabling fine-grained scene-level future state\nreconstruction with Gaussian Splatting. GWM can not only enhance the visual\nrepresentation for imitation learning agent by self-supervised future\nprediction training, but can serve as a neural simulator that supports\nmodel-based reinforcement learning. Both simulated and real-world experiments\ndepict that GWM can precisely predict future scenes conditioned on diverse\nrobot actions, and can be further utilized to train policies that outperform\nthe state-of-the-art by impressive margins, showcasing the initial data scaling\npotential of 3D world model.",
        "url": "http://arxiv.org/abs/2508.17600v1",
        "published_date": "2025-08-25T02:01:09+00:00",
        "updated_date": "2025-08-25T02:01:09+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Guanxing Lu",
            "Baoxiong Jia",
            "Puhao Li",
            "Yixin Chen",
            "Ziwei Wang",
            "Yansong Tang",
            "Siyuan Huang"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "A new Gaussian World Model (GWM) is proposed for robotic manipulation, combining a Diffusion Transformer and 3D variational autoencoder for future state reconstruction, showing promising results in improving imitation learning agents and supporting model-based reinforcement learning.",
        "tldr_zh": "提出一种新的高斯世界模型（GWM）用于机器人操作，结合扩散变换器和3D变分自动编码器进行未来状态重建，在改进模仿学习代理和支持基于模型的强化学习方面取得了有希望的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TinyGiantVLM: A Lightweight Vision-Language Architecture for Spatial Reasoning under Resource Constraints",
        "summary": "Reasoning about fine-grained spatial relationships in warehouse-scale\nenvironments poses a significant challenge for existing vision-language models\n(VLMs), which often struggle to comprehend 3D layouts, object arrangements, and\nmultimodal cues in real-world industrial settings. In this paper, we present\nTinyGiantVLM, a lightweight and modular two-stage framework designed for\nphysical spatial reasoning, distinguishing itself from traditional geographic\nreasoning in complex logistics scenes. Our approach encodes both global and\nregion-level features from RGB and depth modalities using pretrained visual\nbackbones. To effectively handle the complexity of high-modality inputs and\ndiverse question types, we incorporate a Mixture-of-Experts (MoE) fusion\nmodule, which dynamically combines spatial representations to support\ndownstream reasoning tasks and improve convergence. Training is conducted in a\ntwo-phase strategy: the first phase focuses on generating free-form answers to\nenhance spatial reasoning ability, while the second phase uses normalized\nanswers for evaluation. Evaluated on Track 3 of the AI City Challenge 2025, our\n64M-parameter base model achieved 5th place on the leaderboard with a score of\n66.8861, demonstrating strong performance in bridging visual perception and\nspatial understanding in industrial environments. We further present an\n80M-parameter variant with expanded MoE capacity, which demonstrates improved\nperformance on spatial reasoning tasks.",
        "url": "http://arxiv.org/abs/2508.17595v1",
        "published_date": "2025-08-25T01:36:22+00:00",
        "updated_date": "2025-08-25T01:36:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Vinh-Thuan Ly",
            "Hoang M. Truong",
            "Xuan-Huong Nguyen"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper presents TinyGiantVLM, a lightweight vision-language model for spatial reasoning in industrial settings, achieving strong performance on spatial reasoning tasks.",
        "tldr_zh": "本文展示了TinyGiantVLM，这是一个轻量级的用于工业环境中空间推理的视觉语言模型，在空间推理任务上表现出色。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "HERO: Hierarchical Extrapolation and Refresh for Efficient World Models",
        "summary": "Generation-driven world models create immersive virtual environments but\nsuffer slow inference due to the iterative nature of diffusion models. While\nrecent advances have improved diffusion model efficiency, directly applying\nthese techniques to world models introduces limitations such as quality\ndegradation. In this paper, we present HERO, a training-free hierarchical\nacceleration framework tailored for efficient world models. Owing to the\nmulti-modal nature of world models, we identify a feature coupling phenomenon,\nwherein shallow layers exhibit high temporal variability, while deeper layers\nyield more stable feature representations. Motivated by this, HERO adopts\nhierarchical strategies to accelerate inference: (i) In shallow layers, a\npatch-wise refresh mechanism efficiently selects tokens for recomputation. With\npatch-wise sampling and frequency-aware tracking, it avoids extra metric\ncomputation and remain compatible with FlashAttention. (ii) In deeper layers, a\nlinear extrapolation scheme directly estimates intermediate features. This\ncompletely bypasses the computations in attention modules and feed-forward\nnetworks. Our experiments show that HERO achieves a 1.73$\\times$ speedup with\nminimal quality degradation, significantly outperforming existing diffusion\nacceleration methods.",
        "url": "http://arxiv.org/abs/2508.17588v1",
        "published_date": "2025-08-25T01:22:15+00:00",
        "updated_date": "2025-08-25T01:22:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Quanjian Song",
            "Xinyu Wang",
            "Donghao Zhou",
            "Jingyu Lin",
            "Cunjian Chen",
            "Yue Ma",
            "Xiu Li"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper presents HERO, a hierarchical acceleration framework for efficient world models, achieving a 1.73x speedup with minimal quality degradation.",
        "tldr_zh": "本文介绍了HERO，一种用于高效世界模型的分层加速框架，实现了1.73倍的加速，质量下降最小。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DinoTwins: Combining DINO and Barlow Twins for Robust, Label-Efficient Vision Transformers",
        "summary": "Training AI models to understand images without costly labeled data remains a\nchallenge. We combine two techniques--DINO (teacher-student learning) and\nBarlow Twins (redundancy reduction)--to create a model that learns better with\nfewer labels and less compute. While both DINO and Barlow Twins have\nindependently demonstrated strong performance in self-supervised learning, each\ncomes with limitations--DINO may be sensitive to certain augmentations, and\nBarlow Twins often requires batch sizes too large to fit on consumer hardware.\nBy combining the redundancy-reduction objective of Barlow Twins with the\nself-distillation strategy of DINO, we aim to leverage their complementary\nstrengths. We train a hybrid model on the MS COCO dataset using only 10\\% of\nlabeled data for linear probing, and evaluate its performance against\nstandalone DINO and Barlow Twins implementations. Preliminary results show that\nthe combined approach achieves comparable loss and classification accuracy to\nDINO while maintaining strong feature representations. Attention visualizations\nfurther suggest improved semantic segmentation capability in the hybrid model.\nThis combined method offers a scalable, label-efficient alternative for\ntraining ViTs in resource-constrained environments.",
        "url": "http://arxiv.org/abs/2508.17509v1",
        "published_date": "2025-08-24T20:18:05+00:00",
        "updated_date": "2025-08-24T20:18:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Michael Podsiadly",
            "Brendon K Lay"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper combines DINO and Barlow Twins to create a model that learns better with fewer labels and less compute for vision transformers.",
        "tldr_zh": "该论文结合了DINO和Barlow Twins，为视觉转换器创建了一个模型，在使用更少的标签和计算资源时能够更好地学习。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Social-MAE: A Transformer-Based Multimodal Autoencoder for Face and Voice",
        "summary": "Human social behaviors are inherently multimodal necessitating the\ndevelopment of powerful audiovisual models for their perception. In this paper,\nwe present Social-MAE, our pre-trained audiovisual Masked Autoencoder based on\nan extended version of Contrastive Audio-Visual Masked Auto-Encoder (CAV-MAE),\nwhich is pre-trained on audiovisual social data. Specifically, we modify\nCAV-MAE to receive a larger number of frames as input and pre-train it on a\nlarge dataset of human social interaction (VoxCeleb2) in a self-supervised\nmanner. We demonstrate the effectiveness of this model by finetuning and\nevaluating the model on different social and affective downstream tasks,\nnamely, emotion recognition, laughter detection and apparent personality\nestimation. The model achieves state-of-the-art results on multimodal emotion\nrecognition and laughter recognition and competitive results for apparent\npersonality estimation, demonstrating the effectiveness of in-domain\nself-supervised pre-training. Code and model weight are available here\nhttps://github.com/HuBohy/SocialMAE.",
        "url": "http://arxiv.org/abs/2508.17502v1",
        "published_date": "2025-08-24T19:49:48+00:00",
        "updated_date": "2025-08-24T19:49:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hugo Bohy",
            "Minh Tran",
            "Kevin El Haddad",
            "Thierry Dutoit",
            "Mohammad Soleymani"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces Social-MAE, a Transformer-based Multimodal Autoencoder for face and voice, achieving state-of-the-art results in emotion and laughter recognition tasks.",
        "tldr_zh": "本文介绍了Social-MAE，一种基于Transformer的多模态自编码器，在情绪和笑声识别任务中取得了最新成果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Optimizing Multi-Modal Trackers via Sensitivity-aware Regularized Tuning",
        "summary": "This paper tackles the critical challenge of optimizing multi-modal trackers\nby effectively adapting the pre-trained models for RGB data. Existing\nfine-tuning paradigms oscillate between excessive freedom and over-restriction,\nboth leading to a suboptimal plasticity-stability trade-off. To mitigate this\ndilemma, we propose a novel sensitivity-aware regularized tuning framework,\nwhich delicately refines the learning process by incorporating intrinsic\nparameter sensitivities. Through a comprehensive investigation from pre-trained\nto multi-modal contexts, we identify that parameters sensitive to pivotal\nfoundational patterns and cross-domain shifts are primary drivers of this\nissue. Specifically, we first analyze the tangent space of pre-trained weights\nto measure and orient prior sensitivities, dedicated to preserving\ngeneralization. Then, we further explore transfer sensitivities during the\ntuning phase, emphasizing adaptability and stability. By incorporating these\nsensitivities as regularization terms, our method significantly enhances the\ntransferability across modalities. Extensive experiments showcase the superior\nperformance of the proposed method, surpassing current state-of-the-art\ntechniques across various multi-modal tracking. The source code and models will\nbe publicly available at https://github.com/zhiwen-xdu/SRTrack.",
        "url": "http://arxiv.org/abs/2508.17488v1",
        "published_date": "2025-08-24T18:42:47+00:00",
        "updated_date": "2025-08-24T18:42:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhiwen Chen",
            "Jinjian Wu",
            "Zhiyu Zhu",
            "Yifan Zhang",
            "Guangming Shi",
            "Junhui Hou"
        ],
        "ai_categories": [
            "Multimodality"
        ],
        "tldr": "The paper proposes a sensitivity-aware regularized tuning framework to optimize multi-modal trackers, enhancing transferability across modalities.",
        "tldr_zh": "本文提出了一种敏感性感知的正则化调优框架，用于优化多模态跟踪器，增强模态之间的可转移性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GraphMMP: A Graph Neural Network Model with Mutual Information and Global Fusion for Multimodal Medical Prognosis",
        "summary": "In the field of multimodal medical data analysis, leveraging diverse types of\ndata and understanding their hidden relationships continues to be a research\nfocus. The main challenges lie in effectively modeling the complex interactions\nbetween heterogeneous data modalities with distinct characteristics while\ncapturing both local and global dependencies across modalities. To address\nthese challenges, this paper presents a two-stage multimodal prognosis model,\nGraphMMP, which is based on graph neural networks. The proposed model\nconstructs feature graphs using mutual information and features a global fusion\nmodule built on Mamba, which significantly boosts prognosis performance.\nEmpirical results show that GraphMMP surpasses existing methods on datasets\nrelated to liver prognosis and the METABRIC study, demonstrating its\neffectiveness in multimodal medical prognosis tasks.",
        "url": "http://arxiv.org/abs/2508.17478v1",
        "published_date": "2025-08-24T18:06:20+00:00",
        "updated_date": "2025-08-24T18:06:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xuhao Shan",
            "Ruiquan Ge",
            "Jikui Liu",
            "Linglong Wu",
            "Chi Zhang",
            "Siqi Liu",
            "Wenjian Qin",
            "Wenwen Min",
            "Ahmed Elazab",
            "Changmiao Wang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper presents GraphMMP, a graph neural network model for multimodal medical prognosis that leverages mutual information and global fusion to improve performance.",
        "tldr_zh": "该论文提出了GraphMMP，一种基于图神经网络的多模态医学预后模型，利用相互信息和全局融合来提高性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Emerging Semantic Segmentation from Positive and Negative Coarse Label Learning",
        "summary": "Large annotated datasets are vital for training segmentation models, but\npixel-level labeling is time-consuming, error-prone, and often requires scarce\nexpert annotators, especially in medical imaging. In contrast, coarse\nannotations are quicker, cheaper, and easier to produce, even by non-experts.\nIn this paper, we propose to use coarse drawings from both positive (target)\nand negative (background) classes in the image, even with noisy pixels, to\ntrain a convolutional neural network (CNN) for semantic segmentation. We\npresent a method for learning the true segmentation label distributions from\npurely noisy coarse annotations using two coupled CNNs. The separation of the\ntwo CNNs is achieved by high fidelity with the characters of the noisy training\nannotations. We propose to add a complementary label learning that encourages\nestimating negative label distribution. To illustrate the properties of our\nmethod, we first use a toy segmentation dataset based on MNIST. We then present\nthe quantitative results of experiments using publicly available datasets:\nCityscapes dataset for multi-class segmentation, and retinal images for medical\napplications. In all experiments, our method outperforms state-of-the-art\nmethods, particularly in the cases where the ratio of coarse annotations is\nsmall compared to the given dense annotations.",
        "url": "http://arxiv.org/abs/2508.18186v1",
        "published_date": "2025-08-25T16:38:51+00:00",
        "updated_date": "2025-08-25T16:38:51+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Le Zhang",
            "Fuping Wu",
            "Arun Thirunavukarasu",
            "Kevin Bronik",
            "Thomas Nichols",
            "Bartlomiej W. Papiez"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper proposes a method for semantic segmentation using coarse annotations from positive and negative classes, outperforming existing methods on various datasets.",
        "tldr_zh": "本文提出了一种利用正负类别的粗糙注释进行语义分割的方法，在各种数据集上表现优于现有方法。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 7.5
    },
    {
        "title": "Annotation-Free Open-Vocabulary Segmentation for Remote-Sensing Images",
        "summary": "Semantic segmentation of remote sensing (RS) images is pivotal for\ncomprehensive Earth observation, but the demand for interpreting new object\ncategories, coupled with the high expense of manual annotation, poses\nsignificant challenges. Although open-vocabulary semantic segmentation (OVSS)\noffers a promising solution, existing frameworks designed for natural images\nare insufficient for the unique complexities of RS data. They struggle with\nvast scale variations and fine-grained details, and their adaptation often\nrelies on extensive, costly annotations. To address this critical gap, this\npaper introduces SegEarth-OV, the first framework for annotation-free\nopen-vocabulary segmentation of RS images. Specifically, we propose SimFeatUp,\na universal upsampler that robustly restores high-resolution spatial details\nfrom coarse features, correcting distorted target shapes without any\ntask-specific post-training. We also present a simple yet effective Global Bias\nAlleviation operation to subtract the inherent global context from patch\nfeatures, significantly enhancing local semantic fidelity. These components\nempower SegEarth-OV to effectively harness the rich semantics of pre-trained\nVLMs, making OVSS possible in optical RS contexts. Furthermore, to extend the\nframework's universality to other challenging RS modalities like SAR images,\nwhere large-scale VLMs are unavailable and expensive to create, we introduce\nAlignEarth, which is a distillation-based strategy and can efficiently transfer\nsemantic knowledge from an optical VLM encoder to an SAR encoder, bypassing the\nneed to build SAR foundation models from scratch and enabling universal OVSS\nacross diverse sensor types. Extensive experiments on both optical and SAR\ndatasets validate that SegEarth-OV can achieve dramatic improvements over the\nSOTA methods, establishing a robust foundation for annotation-free and\nopen-world Earth observation.",
        "url": "http://arxiv.org/abs/2508.18067v1",
        "published_date": "2025-08-25T14:22:57+00:00",
        "updated_date": "2025-08-25T14:22:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaiyu Li",
            "Xiangyong Cao",
            "Ruixun Liu",
            "Shihong Wang",
            "Zixuan Jiang",
            "Zhi Wang",
            "Deyu Meng"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces SegEarth-OV, a framework for annotation-free open-vocabulary segmentation of remote sensing images using innovative techniques like SimFeatUp and Global Bias Alleviation. It achieves significant improvements over existing methods on both optical and SAR datasets.",
        "tldr_zh": "该论文介绍了SegEarth-OV，这是一个使用SimFeatUp和Global Bias Alleviation等创新技术进行无标注开放词汇分割遥感图像的框架。它在光学和SAR数据集上明显优于现有方法。",
        "relevance_score": 2,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration",
        "summary": "Autonomous planetary exploration missions are critically dependent on\nreal-time, accurate environmental perception for navigation and hazard\navoidance. However, deploying deep learning models on the resource-constrained\ncomputational hardware of planetary exploration platforms remains a significant\nchallenge. This paper introduces the Adaptive Quantized Planetary Crater\nDetection System (AQ-PCDSys), a novel framework specifically engineered for\nreal-time, onboard deployment in the computationally constrained environments\nof space exploration missions. AQ-PCDSys synergistically integrates a Quantized\nNeural Network (QNN) architecture, trained using Quantization-Aware Training\n(QAT), with an Adaptive Multi-Sensor Fusion (AMF) module. The QNN architecture\nsignificantly optimizes model size and inference latency suitable for real-time\nonboard deployment in space exploration missions, while preserving high\naccuracy. The AMF module intelligently fuses data from Optical Imagery (OI) and\nDigital Elevation Models (DEMs) at the feature level, utilizing an Adaptive\nWeighting Mechanism (AWM) to dynamically prioritize the most relevant and\nreliable sensor modality based on planetary ambient conditions. This approach\nenhances detection robustness across diverse planetary landscapes. Paired with\nMulti-Scale Detection Heads specifically designed for robust and efficient\ndetection of craters across a wide range of sizes, AQ-PCDSys provides a\ncomputationally efficient, reliable and accurate solution for planetary crater\ndetection, a critical capability for enabling the next generation of autonomous\nplanetary landing, navigation, and scientific exploration.",
        "url": "http://arxiv.org/abs/2508.18025v1",
        "published_date": "2025-08-25T13:44:00+00:00",
        "updated_date": "2025-08-25T13:44:00+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.ET",
            "cs.SY",
            "eess.SY",
            "68T07(2020), 68T45(2020), 68T10(2020), 90C90(2020)",
            "I.2.10; I.2.6; I.2.9; J.2"
        ],
        "authors": [
            "Aditri Paul",
            "Archan Paul"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "AQ-PCDSys is a novel system for real-time, accurate crater detection in space exploration missions, combining a Quantized Neural Network and Adaptive Multi-Sensor Fusion modules.",
        "tldr_zh": "AQ-PCDSys是一个新颖的系统，用于在空间探索任务中实现实时、准确的环境感知，结合了量化神经网络和自适应多传感器融合模块。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Fence off Anomaly Interference: Cross-Domain Distillation for Fully Unsupervised Anomaly Detection",
        "summary": "Fully Unsupervised Anomaly Detection (FUAD) is a practical extension of\nUnsupervised Anomaly Detection (UAD), aiming to detect anomalies without any\nlabels even when the training set may contain anomalous samples. To achieve\nFUAD, we pioneer the introduction of Knowledge Distillation (KD) paradigm based\non teacher-student framework into the FUAD setting. However, due to the\npresence of anomalies in the training data, traditional KD methods risk\nenabling the student to learn the teacher's representation of anomalies under\nFUAD setting, thereby resulting in poor anomaly detection performance. To\naddress this issue, we propose a novel Cross-Domain Distillation (CDD)\nframework based on the widely studied reverse distillation (RD) paradigm.\nSpecifically, we design a Domain-Specific Training, which divides the training\nset into multiple domains with lower anomaly ratios and train a domain-specific\nstudent for each. Cross-Domain Knowledge Aggregation is then performed, where\npseudo-normal features generated by domain-specific students collaboratively\nguide a global student to learn generalized normal representations across all\nsamples. Experimental results on noisy versions of the MVTec AD and VisA\ndatasets demonstrate that our method achieves significant performance\nimprovements over the baseline, validating its effectiveness under FUAD\nsetting.",
        "url": "http://arxiv.org/abs/2508.18007v1",
        "published_date": "2025-08-25T13:15:28+00:00",
        "updated_date": "2025-08-25T13:15:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinyue Liu",
            "Jianyuan Wang",
            "Biao Leng",
            "Shuo Zhang"
        ],
        "ai_categories": [
            "AIGC",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a Cross-Domain Distillation framework to improve Fully Unsupervised Anomaly Detection, achieving significant performance improvements over baseline methods on various datasets.",
        "tldr_zh": "该论文引入了跨领域蒸馏框架，改善了完全无监督异常检测，在各种数据集上实现了明显的性能提升。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "See What You Need: Query-Aware Visual Intelligence through Reasoning-Perception Loops",
        "summary": "Human video comprehension demonstrates dynamic coordination between reasoning\nand visual attention, adaptively focusing on query-relevant details. However,\ncurrent long-form video question answering systems employ rigid pipelines that\ndecouple reasoning from perception, leading to either information loss through\npremature visual abstraction or computational inefficiency through exhaustive\nprocessing. The core limitation lies in the inability to adapt visual\nextraction to specific reasoning requirements, different queries demand\nfundamentally different visual evidence from the same video content. In this\nwork, we present CAVIA, a training-free framework that revolutionizes video\nunderstanding through reasoning, perception coordination. Unlike conventional\napproaches where visual processing operates independently of reasoning, CAVIA\ncreates a closed-loop system where reasoning continuously guides visual\nextraction based on identified information gaps. CAVIA introduces three\ninnovations: (1) hierarchical reasoning, guided localization to precise frames;\n(2) cross-modal semantic bridging for targeted extraction; (3)\nconfidence-driven iterative synthesis. CAVIA achieves state-of-the-art\nperformance on challenging benchmarks: EgoSchema (65.7%, +5.3%), NExT-QA\n(76.1%, +2.6%), and IntentQA (73.8%, +6.9%), demonstrating that dynamic\nreasoning-perception coordination provides a scalable paradigm for video\nunderstanding.",
        "url": "http://arxiv.org/abs/2508.17932v1",
        "published_date": "2025-08-25T12:00:12+00:00",
        "updated_date": "2025-08-25T12:00:12+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "68T45, 68T05",
            "H.5.1; I.2.10; I.4.8; I.5.4"
        ],
        "authors": [
            "Zixuan Dong",
            "Baoyun Peng",
            "Yufei Wang",
            "Lin Liu",
            "Xinxin Dong",
            "Yunlong Cao",
            "Xiaodong Wang"
        ],
        "ai_categories": [
            "LoRA"
        ],
        "tldr": "The paper introduces CAVIA, a framework that improves video understanding through dynamic reasoning-perception coordination, achieving state-of-the-art performance on challenging benchmarks.",
        "tldr_zh": "本次论文介绍了CAVIA框架，通过动态推理-感知协调改善视频理解，在具有挑战性的基准测试上实现了最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images",
        "summary": "Depth estimation is a foundational component for 3D reconstruction in\nminimally invasive endoscopic surgeries. However, existing monocular depth\nestimation techniques often exhibit limited performance to the varying\nillumination and complex textures of the surgical environment. While powerful\nvisual foundation models offer a promising solution, their training on natural\nimages leads to significant domain adaptability limitations and semantic\nperception deficiencies when applied to endoscopy. In this study, we introduce\nEndoUFM, an unsupervised monocular depth estimation framework that innovatively\nintegrating dual foundation models for surgical scenes, which enhance the depth\nestimation performance by leveraging the powerful pre-learned priors. The\nframework features a novel adaptive fine-tuning strategy that incorporates\nRandom Vector Low-Rank Adaptation (RVLoRA) to enhance model adaptability, and a\nResidual block based on Depthwise Separable Convolution (Res-DSC) to improve\nthe capture of fine-grained local features. Furthermore, we design a\nmask-guided smoothness loss to enforce depth consistency within anatomical\ntissue structures. Extensive experiments on the SCARED, Hamlyn, SERV-CT, and\nEndoNeRF datasets confirm that our method achieves state-of-the-art performance\nwhile maintaining an efficient model size. This work contributes to augmenting\nsurgeons' spatial perception during minimally invasive procedures, thereby\nenhancing surgical precision and safety, with crucial implications for\naugmented reality and navigation systems.",
        "url": "http://arxiv.org/abs/2508.17916v1",
        "published_date": "2025-08-25T11:33:05+00:00",
        "updated_date": "2025-08-25T11:33:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinning Yao",
            "Bo Liu",
            "Bojian Li",
            "Jingjing Wang",
            "Jinghua Yue",
            "Fugen Zhou"
        ],
        "ai_categories": [
            "LoRA",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces EndoUFM, a framework for improving depth estimation in endoscopic images by integrating dual foundation models and innovative techniques. It enhances spatial perception during surgical procedures and has implications for augmented reality and navigation systems.",
        "tldr_zh": "本文介绍了EndoUFM框架，通过整合双基础模型和创新技术来改善内窥镜图像的深度估计。它增强了外科手术过程中的空间感知，并对增强现实和导航系统具有重要意义。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "Rethinking the Detail-Preserved Completion of Complex Tubular Structures based on Point Cloud: a Dataset and a Benchmark",
        "summary": "Complex tubular structures are essential in medical imaging and\ncomputer-assisted diagnosis, where their integrity enhances anatomical\nvisualization and lesion detection. However, existing segmentation algorithms\nstruggle with structural discontinuities, particularly in severe clinical cases\nsuch as coronary artery stenosis and vessel occlusions, which leads to\nundesired discontinuity and compromising downstream diagnostic accuracy.\nTherefore, it is imperative to reconnect discontinuous structures to ensure\ntheir completeness. In this study, we explore the tubular structure completion\nbased on point cloud for the first time and establish a Point Cloud-based\nCoronary Artery Completion (PC-CAC) dataset, which is derived from real\nclinical data. This dataset provides a novel benchmark for tubular structure\ncompletion. Additionally, we propose TSRNet, a Tubular Structure Reconnection\nNetwork that integrates a detail-preservated feature extractor, a multiple\ndense refinement strategy, and a global-to-local loss function to ensure\naccurate reconnection while maintaining structural integrity. Comprehensive\nexperiments on our PC-CAC and two additional public datasets (PC-ImageCAS and\nPC-PTR) demonstrate that our method consistently outperforms state-of-the-art\napproaches across multiple evaluation metrics, setting a new benchmark for\npoint cloud-based tubular structure reconstruction. Our benchmark is available\nat https://github.com/YaoleiQi/PCCAC.",
        "url": "http://arxiv.org/abs/2508.17658v1",
        "published_date": "2025-08-25T04:39:15+00:00",
        "updated_date": "2025-08-25T04:39:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yaolei Qi",
            "Yikai Yang",
            "Wenbo Peng",
            "Shumei Miao",
            "Yutao Hu",
            "Guanyu Yang"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a method for completing complex tubular structures using point clouds, with a focus on coronary artery reconstruction, outperforming existing approaches.",
        "tldr_zh": "本文介绍了一种使用点云完成复杂管状结构的方法，重点关注冠状动脉重建，在效果上超越了现有方法。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "BirdRecorder's AI on Sky: Safeguarding birds of prey by detection and classification of tiny objects around wind turbines",
        "summary": "The urgent need for renewable energy expansion, particularly wind power, is\nhindered by conflicts with wildlife conservation. To address this, we developed\nBirdRecorder, an advanced AI-based anti-collision system to protect endangered\nbirds, especially the red kite (Milvus milvus). Integrating robotics,\ntelemetry, and high-performance AI algorithms, BirdRecorder aims to detect,\ntrack, and classify avian species within a range of 800 m to minimize\nbird-turbine collisions.\n  BirdRecorder integrates advanced AI methods with optimized hardware and\nsoftware architectures to enable real-time image processing. Leveraging Single\nShot Detector (SSD) for detection, combined with specialized hardware\nacceleration and tracking algorithms, our system achieves high detection\nprecision while maintaining the speed necessary for real-time decision-making.\nBy combining these components, BirdRecorder outperforms existing approaches in\nboth accuracy and efficiency.\n  In this paper, we summarize results on field tests and performance of the\nBirdRecorder system. By bridging the gap between renewable energy expansion and\nwildlife conservation, BirdRecorder contributes to a more sustainable\ncoexistence of technology and nature.",
        "url": "http://arxiv.org/abs/2508.18136v1",
        "published_date": "2025-08-25T15:41:36+00:00",
        "updated_date": "2025-08-25T15:41:36+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.RO",
            "cs.SY",
            "eess.SY"
        ],
        "authors": [
            "Nico Klar",
            "Nizam Gifary",
            "Felix P. G. Ziegler",
            "Frank Sehnke",
            "Anton Kaifel",
            "Eric Price",
            "Aamir Ahmad"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "BirdRecorder is an AI-based system designed to protect endangered birds by detecting and classifying avian species near wind turbines to minimize collisions.",
        "tldr_zh": "BirdRecorder是一种基于人工智能的系统，旨在通过检测和分类风力涡轮机附近的鸟类物种，从而最小化碰撞。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets",
        "summary": "Recently, detection of label errors and improvement of label quality in\ndatasets for supervised learning tasks has become an increasingly important\ngoal in both research and industry. The consequences of incorrectly annotated\ndata include reduced model performance, biased benchmark results, and lower\noverall accuracy. Current state-of-the-art label error detection methods often\nfocus on a single computer vision task and, consequently, a specific type of\ndataset, containing, for example, either bounding boxes or pixel-wise\nannotations. Furthermore, previous methods are not learning-based. In this\nwork, we overcome this research gap. We present a unified method for detecting\nlabel errors in object detection, semantic segmentation, and instance\nsegmentation datasets. In a nutshell, our approach - learning to detect label\nerrors by making them - works as follows: we inject different kinds of label\nerrors into the ground truth. Then, the detection of label errors, across all\nmentioned primary tasks, is framed as an instance segmentation problem based on\na composite input. In our experiments, we compare the label error detection\nperformance of our method with various baselines and state-of-the-art\napproaches of each task's domain on simulated label errors across multiple\ntasks, datasets, and base models. This is complemented by a generalization\nstudy on real-world label errors. Additionally, we release 459 real label\nerrors identified in the Cityscapes dataset and provide a benchmark for real\nlabel error detection in Cityscapes.",
        "url": "http://arxiv.org/abs/2508.17930v1",
        "published_date": "2025-08-25T11:59:56+00:00",
        "updated_date": "2025-08-25T11:59:56+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Sarina Penquitt",
            "Tobias Riedlinger",
            "Timo Heller",
            "Markus Reischl",
            "Matthias Rottmann"
        ],
        "ai_categories": [
            "Dataset",
            "GAN",
            "Transformer"
        ],
        "tldr": "The paper presents a method for detecting label errors in segmentation and object detection datasets by injecting errors into ground truth and treating error detection as an instance segmentation problem.",
        "tldr_zh": "本文提出了一种在分割和目标检测数据集中检测标签错误的方法，通过在基本真相中注入错误并将错误检测视为实例分割问题。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "ISALux: Illumination and Segmentation Aware Transformer Employing Mixture of Experts for Low Light Image Enhancement",
        "summary": "We introduce ISALux, a novel transformer-based approach for Low-Light Image\nEnhancement (LLIE) that seamlessly integrates illumination and semantic priors.\nOur architecture includes an original self-attention block, Hybrid Illumination\nand Semantics-Aware Multi-Headed Self- Attention (HISA-MSA), which integrates\nillumination and semantic segmentation maps for en- hanced feature extraction.\nISALux employs two self-attention modules to independently process illumination\nand semantic features, selectively enriching each other to regulate luminance\nand high- light structural variations in real-world scenarios. A Mixture of\nExperts (MoE)-based Feed-Forward Network (FFN) enhances contextual learning,\nwith a gating mechanism conditionally activating the top K experts for\nspecialized processing. To address overfitting in LLIE methods caused by\ndistinct light patterns in benchmarking datasets, we enhance the HISA-MSA\nmodule with low-rank matrix adaptations (LoRA). Extensive qualitative and\nquantitative evaluations across multiple specialized datasets demonstrate that\nISALux is competitive with state-of-the-art (SOTA) methods. Addition- ally, an\nablation study highlights the contribution of each component in the proposed\nmodel. Code will be released upon publication.",
        "url": "http://arxiv.org/abs/2508.17885v1",
        "published_date": "2025-08-25T10:47:18+00:00",
        "updated_date": "2025-08-25T10:47:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Raul Balmez",
            "Alexandru Brateanu",
            "Ciprian Orhei",
            "Codruta Ancuti",
            "Cosmin Ancuti"
        ],
        "ai_categories": [
            "Transformer",
            "LoRA",
            "Dataset"
        ],
        "tldr": "ISALux is a transformer-based approach for Low-Light Image Enhancement, integrating illumination and semantic priors using a Mixture of Experts for contextual learning.",
        "tldr_zh": "ISALux是一种基于变压器的低光图像增强方法，利用专家混合体进行语境学习，整合了照明和语义先验知识。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 7.25
    },
    {
        "title": "Box-Level Class-Balanced Sampling for Active Object Detection",
        "summary": "Training deep object detectors demands expensive bounding box annotation.\nActive learning (AL) is a promising technique to alleviate the annotation\nburden. Performing AL at box-level for object detection, i.e., selecting the\nmost informative boxes to label and supplementing the sparsely-labelled image\nwith pseudo labels, has been shown to be more cost-effective than selecting and\nlabelling the entire image. In box-level AL for object detection, we observe\nthat models at early stage can only perform well on majority classes, making\nthe pseudo labels severely class-imbalanced. We propose a class-balanced\nsampling strategy to select more objects from minority classes for labelling,\nso as to make the final training data, \\ie, ground truth labels obtained by AL\nand pseudo labels, more class-balanced to train a better model. We also propose\na task-aware soft pseudo labelling strategy to increase the accuracy of pseudo\nlabels. We evaluate our method on public benchmarking datasets and show that\nour method achieves state-of-the-art performance.",
        "url": "http://arxiv.org/abs/2508.17849v1",
        "published_date": "2025-08-25T09:57:22+00:00",
        "updated_date": "2025-08-25T09:57:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingyi Liao",
            "Xun Xu",
            "Chuan-Sheng Foo",
            "Lile Cai"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper proposes a class-balanced sampling strategy for box-level active learning in object detection, aiming to improve model training by addressing class imbalance and increasing the accuracy of pseudo labels.",
        "tldr_zh": "本文提出了一种针对目标检测中的盒级主动学习的类平衡抽样策略，旨在通过解决类别不平衡问题和提高伪标签准确性来改善模型训练。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7.25
    },
    {
        "title": "Alternating Training-based Label Smoothing Enhances Prompt Generalization",
        "summary": "Recent advances in pre-trained vision-language models have demonstrated\nremarkable zero-shot generalization capabilities. To further enhance these\nmodels' adaptability to various downstream tasks, prompt tuning has emerged as\na parameter-efficient fine-tuning method. However, despite its efficiency, the\ngeneralization ability of prompt remains limited. In contrast, label smoothing\n(LS) has been widely recognized as an effective regularization technique that\nprevents models from becoming over-confident and improves their generalization.\nThis inspires us to explore the integration of LS with prompt tuning. However,\nwe have observed that the vanilla LS even weakens the generalization ability of\nprompt tuning. To address this issue, we propose the Alternating Training-based\nLabel Smoothing (ATLaS) method, which alternately trains with standard one-hot\nlabels and soft labels generated by LS to supervise the prompt tuning.\nMoreover, we introduce two types of efficient offline soft labels, including\nClass-wise Soft Labels (CSL) and Instance-wise Soft Labels (ISL), to provide\ninter-class or instance-class relationships for prompt tuning. The theoretical\nproperties of the proposed ATLaS method are analyzed. Extensive experiments\ndemonstrate that the proposed ATLaS method, combined with CSL and ISL,\nconsistently enhances the generalization performance of prompt tuning.\nMoreover, the proposed ATLaS method exhibits high compatibility with prevalent\nprompt tuning methods, enabling seamless integration into existing methods.",
        "url": "http://arxiv.org/abs/2508.17846v1",
        "published_date": "2025-08-25T09:54:37+00:00",
        "updated_date": "2025-08-25T09:54:37+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yang Chen",
            "Yanbin Wei",
            "Ke Jin",
            "Yi Kong",
            "James Kwok",
            "Yu Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper introduces the Alternating Training-based Label Smoothing (ATLaS) method to enhance prompt tuning's generalization performance, which consistently improves the results in various downstream tasks by combining Class-wise Soft Labels (CSL) and Instance-wise Soft Labels (ISL).",
        "tldr_zh": "本文介绍了交替训练的标签平滑（ATLaS）方法，通过结合类别软标签（CSL）和实例软标签（ISL）来提高提示调整的泛化性能，从而在各种下游任务中持续改善结果。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.25
    },
    {
        "title": "Few-shot Human Action Anomaly Detection via a Unified Contrastive Learning Framework",
        "summary": "Human Action Anomaly Detection (HAAD) aims to identify anomalous actions\ngiven only normal action data during training. Existing methods typically\nfollow a one-model-per-category paradigm, requiring separate training for each\naction category and a large number of normal samples. These constraints hinder\nscalability and limit applicability in real-world scenarios, where data is\noften scarce or novel categories frequently appear. To address these\nlimitations, we propose a unified framework for HAAD that is compatible with\nfew-shot scenarios. Our method constructs a category-agnostic representation\nspace via contrastive learning, enabling AD by comparing test samples with a\ngiven small set of normal examples (referred to as the support set). To improve\ninter-category generalization and intra-category robustness, we introduce a\ngenerative motion augmentation strategy harnessing a diffusion-based foundation\nmodel for creating diverse and realistic training samples. Notably, to the best\nof our knowledge, our work is the first to introduce such a strategy\nspecifically tailored to enhance contrastive learning for action AD. Extensive\nexperiments on the HumanAct12 dataset demonstrate the state-of-the-art\neffectiveness of our approach under both seen and unseen category settings,\nregarding training efficiency and model scalability for few-shot HAAD.",
        "url": "http://arxiv.org/abs/2508.17726v1",
        "published_date": "2025-08-25T07:07:35+00:00",
        "updated_date": "2025-08-25T07:07:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Koichiro Kamide",
            "Shunsuke Sakai",
            "Shun Maeda",
            "Chunzhi Gu",
            "Chao Zhang"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper proposes a unified framework for few-shot Human Action Anomaly Detection using contrastive learning and motion augmentation, showing state-of-the-art effectiveness on the HumanAct12 dataset.",
        "tldr_zh": "本文提出了一种统一的框架，利用对比学习和运动增强来进行少样本人类行为异常检测，在HumanAct12数据集上展示出了最先进的有效性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Benchmarking Class Activation Map Methods for Explainable Brain Hemorrhage Classification on Hemorica Dataset",
        "summary": "Explainable Artificial Intelligence (XAI) has become an essential component\nof medical imaging research, aiming to increase transparency and clinical trust\nin deep learning models. This study investigates brain hemorrhage diagnosis\nwith a focus on explainability through Class Activation Mapping (CAM)\ntechniques. A pipeline was developed to extract pixellevel segmentation and\ndetection annotations from classification models using nine state-of-the-art\nCAM algorithms, applied across multiple network stages, and quantitatively\nevaluated on the Hemorica dataset, which uniquely provides both slice-level\nlabels and high-quality segmentation masks. Metrics including Dice, IoU, and\npixel-wise overlap were employed to benchmark CAM variants. Results show that\nthe strongest localization performance occurred at stage 5 of EfficientNetV2S,\nwith HiResCAM yielding the highest bounding-box alignment and AblationCAM\nachieving the best pixel-level Dice (0.57) and IoU (0.40), representing strong\naccuracy given that models were trained solely for classification without\nsegmentation supervision. To the best of current knowledge, this is among the f\nirst works to quantitatively compare CAM methods for brain hemorrhage\ndetection, establishing a reproducible benchmark and underscoring the potential\nof XAI-driven pipelines for clinically meaningful AI-assisted diagnosis.",
        "url": "http://arxiv.org/abs/2508.17699v1",
        "published_date": "2025-08-25T06:16:32+00:00",
        "updated_date": "2025-08-25T06:16:32+00:00",
        "categories": [
            "cs.CV",
            "q-bio.QM"
        ],
        "authors": [
            "Z. Rafati",
            "M. Hoseyni",
            "J. Khoramdel",
            "A. Nikoofard"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper explores explainable artificial intelligence (XAI) techniques for brain hemorrhage diagnosis, comparing different Class Activation Map (CAM) methods on the Hemorica dataset",
        "tldr_zh": "本文探讨了关于脑出血诊断的可解释人工智能（XAI）技术，在Hemorica数据集上比较不同的类激活地图（CAM）方法",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Improving Interpretability in Alzheimer's Prediction via Joint Learning of ADAS-Cog Scores",
        "summary": "Accurate prediction of clinical scores is critical for early detection and\nprognosis of Alzheimers disease (AD). While existing approaches primarily focus\non forecasting the ADAS-Cog global score, they often overlook the predictive\nvalue of its sub-scores (13 items), which capture domain-specific cognitive\ndecline. In this study, we propose a multi task learning (MTL) framework that\njointly predicts the global ADAS-Cog score and its sub-scores (13 items) at\nMonth 24 using baseline MRI and longitudinal clinical scores from baseline and\nMonth 6. The main goal is to examine how each sub scores particularly those\nassociated with MRI features contribute to the prediction of the global score,\nan aspect largely neglected in prior MTL studies. We employ Vision Transformer\n(ViT) and Swin Transformer architectures to extract imaging features, which are\nfused with longitudinal clinical inputs to model cognitive progression. Our\nresults show that incorporating sub-score learning improves global score\nprediction. Subscore level analysis reveals that a small subset especially Q1\n(Word Recall), Q4 (Delayed Recall), and Q8 (Word Recognition) consistently\ndominates the predicted global score. However, some of these influential\nsub-scores exhibit high prediction errors, pointing to model instability.\nFurther analysis suggests that this is caused by clinical feature dominance,\nwhere the model prioritizes easily predictable clinical scores over more\ncomplex MRI derived features. These findings emphasize the need for improved\nmultimodal fusion and adaptive loss weighting to achieve more balanced\nlearning. Our study demonstrates the value of sub score informed modeling and\nprovides insights into building more interpretable and clinically robust AD\nprediction frameworks. (Github repo provided)",
        "url": "http://arxiv.org/abs/2508.17619v1",
        "published_date": "2025-08-25T02:56:11+00:00",
        "updated_date": "2025-08-25T02:56:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nur Amirah Abd Hamid",
            "Mohd Shahrizal Rusli",
            "Muhammad Thaqif Iman Mohd Taufek",
            "Mohd Ibrahim Shapiai",
            "Daphne Teck Ching Lai"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes a multi-task learning framework to predict ADAS-Cog scores in Alzheimer's disease using MRI and clinical data, focusing on sub-scores to improve global score prediction and interpretability.",
        "tldr_zh": "本文提出了一个多任务学习框架，利用MRI和临床数据预测阿尔茨海默病中的ADAS-Cog分数，重点关注子分数以改进全局分数的预测和可解释性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "IDU: Incremental Dynamic Update of Existing 3D Virtual Environments with New Imagery Data",
        "summary": "For simulation and training purposes, military organizations have made\nsubstantial investments in developing high-resolution 3D virtual environments\nthrough extensive imaging and 3D scanning. However, the dynamic nature of\nbattlefield conditions-where objects may appear or vanish over time-makes\nfrequent full-scale updates both time-consuming and costly. In response, we\nintroduce the Incremental Dynamic Update (IDU) pipeline, which efficiently\nupdates existing 3D reconstructions, such as 3D Gaussian Splatting (3DGS), with\nonly a small set of newly acquired images. Our approach starts with camera pose\nestimation to align new images with the existing 3D model, followed by change\ndetection to pinpoint modifications in the scene. A 3D generative AI model is\nthen used to create high-quality 3D assets of the new elements, which are\nseamlessly integrated into the existing 3D model. The IDU pipeline incorporates\nhuman guidance to ensure high accuracy in object identification and placement,\nwith each update focusing on a single new object at a time. Experimental\nresults confirm that our proposed IDU pipeline significantly reduces update\ntime and labor, offering a cost-effective and targeted solution for maintaining\nup-to-date 3D models in rapidly evolving military scenarios.",
        "url": "http://arxiv.org/abs/2508.17579v1",
        "published_date": "2025-08-25T01:00:35+00:00",
        "updated_date": "2025-08-25T01:00:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Meida Chen",
            "Luis Leal",
            "Yue Hu",
            "Rong Liu",
            "Butian Xiong",
            "Andrew Feng",
            "Jiuyi Xu",
            "Yangming Shi"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces the Incremental Dynamic Update (IDU) pipeline to efficiently update existing 3D reconstructions with new imagery data, focusing on military scenarios.",
        "tldr_zh": "该论文引入了增量动态更新（IDU）管道，以有效更新现有的3D重建与新图像数据，重点关注军事场景。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "GSVisLoc: Generalizable Visual Localization for Gaussian Splatting Scene Representations",
        "summary": "We introduce GSVisLoc, a visual localization method designed for 3D Gaussian\nSplatting (3DGS) scene representations. Given a 3DGS model of a scene and a\nquery image, our goal is to estimate the camera's position and orientation. We\naccomplish this by robustly matching scene features to image features. Scene\nfeatures are produced by downsampling and encoding the 3D Gaussians while image\nfeatures are obtained by encoding image patches. Our algorithm proceeds in\nthree steps, starting with coarse matching, then fine matching, and finally by\napplying pose refinement for an accurate final estimate. Importantly, our\nmethod leverages the explicit 3DGS scene representation for visual localization\nwithout requiring modifications, retraining, or additional reference images. We\nevaluate GSVisLoc on both indoor and outdoor scenes, demonstrating competitive\nlocalization performance on standard benchmarks while outperforming existing\n3DGS-based baselines. Moreover, our approach generalizes effectively to novel\nscenes without additional training.",
        "url": "http://arxiv.org/abs/2508.18242v1",
        "published_date": "2025-08-25T17:31:57+00:00",
        "updated_date": "2025-08-25T17:31:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fadi Khatib",
            "Dror Moran",
            "Guy Trostianetsky",
            "Yoni Kasten",
            "Meirav Galun",
            "Ronen Basri"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "GSVisLoc is a method for visual localization using 3D Gaussian Splatting scene representations, achieving competitive performance without the need for modifications or additional training.",
        "tldr_zh": "GSVisLoc是一种利用3D高斯分布场景表示进行视觉定位的方法，实现了竞争性能而无需修改或额外训练。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Few-shot Unknown Class Discovery of Hyperspectral Images with Prototype Learning and Clustering",
        "summary": "Open-set few-shot hyperspectral image (HSI) classification aims to classify\nimage pixels by using few labeled pixels per class, where the pixels to be\nclassified may be not all from the classes that have been seen. To address the\nopen-set HSI classification challenge, current methods focus mainly on\ndistinguishing the unknown class samples from the known class samples and\nrejecting them to increase the accuracy of identifying known class samples.\nThey fails to further identify or discovery the unknow classes among the\nsamples. This paper proposes a prototype learning and clustering method for\ndiscoverying unknown classes in HSIs under the few-shot environment. Using few\nlabeled samples, it strives to develop the ability of infering the prototypes\nof unknown classes while distinguishing unknown classes from known classes.\nOnce the unknown class samples are rejected by the learned known class\nclassifier, the proposed method can further cluster the unknown class samples\ninto different classes according to their distance to the inferred unknown\nclass prototypes. Compared to existing state-of-the-art methods, extensive\nexperiments on four benchmark HSI datasets demonstrate that our proposed method\nexhibits competitive performance in open-set few-shot HSI classification tasks.\nAll the codes are available at \\href{https://github.com/KOBEN-ff/OpenFUCD-main}\n{https://github.com/KOBEN-ff/OpenFUCD-main}",
        "url": "http://arxiv.org/abs/2508.18075v1",
        "published_date": "2025-08-25T14:40:06+00:00",
        "updated_date": "2025-08-25T14:40:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chun Liu",
            "Chen Zhang",
            "Zhuo Li",
            "Zheng Li",
            "Wei Yang"
        ],
        "ai_categories": [
            "AIGC"
        ],
        "tldr": "This paper introduces a method for discovering unknown classes in hyperspectral images with few labeled samples through prototype learning and clustering.",
        "tldr_zh": "本文介绍了一种通过原型学习和聚类在高光谱图像中发现未知类别的方法。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Beam Geometry and Input Dimensionality: Impact on Sparse-Sampling Artifact Correction for Clinical CT with U-Nets",
        "summary": "This study aims to investigate the effect of various beam geometries and\ndimensions of input data on the sparse-sampling streak artifact correction task\nwith U-Nets for clinical CT scans as a means of incorporating the volumetric\ncontext into artifact reduction tasks to improve model performance. A total of\n22 subjects were retrospectively selected (01.2016-12.2018) from the Technical\nUniversity of Munich's research hospital, TUM Klinikum rechts der Isar.\nSparsely-sampled CT volumes were simulated with the Astra toolbox for parallel,\nfan, and cone beam geometries. 2048 views were taken as full-view scans. 2D and\n3D U-Nets were trained and validated on 14, and tested on 8 subjects,\nrespectively. For the dimensionality study, in addition to the 512x512 2D CT\nimages, the CT scans were further pre-processed to generate a so-called '2.5D',\nand 3D data: Each CT volume was divided into 64x64x64 voxel blocks. The 3D data\nrefers to individual 64-voxel blocks. An axial, coronal, and sagittal cut\nthrough the center of each block resulted in three 64x64 2D patches that were\nrearranged as a single 64x64x3 image, proposed as 2.5D data. Model performance\nwas assessed with the mean squared error (MSE) and structural similarity index\nmeasure (SSIM). For all geometries, the 2D U-Net trained on axial 2D slices\nresults in the best MSE and SSIM values, outperforming the 2.5D and 3D input\ndata dimensions.",
        "url": "http://arxiv.org/abs/2508.17961v1",
        "published_date": "2025-08-25T12:21:49+00:00",
        "updated_date": "2025-08-25T12:21:49+00:00",
        "categories": [
            "cs.CV",
            "physics.med-ph"
        ],
        "authors": [
            "Tina Dorosti",
            "Johannes Thalhammer",
            "Sebastian Peterhansl",
            "Daniela Pfeiffer",
            "Franz Pfeiffer",
            "Florian Schaff"
        ],
        "ai_categories": [
            "Dataset",
            "CT Scans",
            "GAN"
        ],
        "tldr": "The study explores the impact of beam geometries and input data dimensions on artifact correction in clinical CT scans using U-Nets.",
        "tldr_zh": "该研究探讨了光束几何和输入数据维度对使用U-Nets进行临床CT扫描中伪影校正的影响。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "A Contrastive Learning-Guided Confident Meta-learning for Zero Shot Anomaly Detection",
        "summary": "Industrial and medical anomaly detection faces critical challenges from data\nscarcity and prohibitive annotation costs, particularly in evolving\nmanufacturing and healthcare settings. To address this, we propose CoZAD, a\nnovel zero-shot anomaly detection framework that integrates soft confident\nlearning with meta-learning and contrastive feature representation. Unlike\ntraditional confident learning that discards uncertain samples, our method\nassigns confidence-based weights to all training data, preserving boundary\ninformation while emphasizing prototypical normal patterns. The framework\nquantifies data uncertainty through IQR-based thresholding and model\nuncertainty via covariance based regularization within a Model-Agnostic\nMeta-Learning. Contrastive learning creates discriminative feature spaces where\nnormal patterns form compact clusters, enabling rapid domain adaptation.\nComprehensive evaluation across 10 datasets spanning industrial and medical\ndomains demonstrates state-of-the-art performance, outperforming existing\nmethods on 6 out of 7 industrial benchmarks with notable improvements on\ntexture-rich datasets (99.2% I-AUROC on DTD-Synthetic, 97.2% on BTAD) and\npixellevel localization (96.3% P-AUROC on MVTec-AD). The framework eliminates\ndependence on vision-language alignments or model ensembles, making it valuable\nfor resourceconstrained environments requiring rapid deployment.",
        "url": "http://arxiv.org/abs/2508.17827v1",
        "published_date": "2025-08-25T09:27:31+00:00",
        "updated_date": "2025-08-25T09:27:31+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Muhammad Aqeel",
            "Danijel Skocaj",
            "Marco Cristani",
            "Francesco Setti"
        ],
        "ai_categories": [
            "AIGC",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces CoZAD, a novel framework for zero-shot anomaly detection using confident learning, meta-learning, and contrastive feature representation, achieving state-of-the-art performance on industrial and medical datasets.",
        "tldr_zh": "本文介绍了CoZAD，这是一个利用自信学习、元学习和对比特征表示进行零样本异常检测的新框架，在工业和医疗数据集上取得了最先进的性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Citizen Centered Climate Intelligence: Operationalizing Open Tree Data for Urban Cooling and Eco-Routing in Indian Cities",
        "summary": "Urban climate resilience requires more than high-resolution data; it demands\nsystems that embed data collection, interpretation, and action within the daily\nlives of citizens. This chapter presents a scalable, citizen-centric framework\nthat reimagines environmental infrastructure through participatory sensing,\nopen analytics, and prescriptive urban planning tools. Applied in Pune, India,\nthe framework comprises three interlinked modules: (1) a smartphone-based\nmeasurement toolkit enhanced by AI segmentation to extract tree height, canopy\ndiameter, and trunk girth; (2) a percentile-based model using satellite-derived\nLand Surface Temperature to calculate localized cooling through two new\nmetrics, Cooling Efficacy and Ambient Heat Relief; and (3) an eco-routing\nengine that guides mobility using a Static Environmental Quality score, based\non tree density, species diversity, and cumulative carbon sequestration.\nTogether, these modules form a closed feedback loop where citizens generate\nactionable data and benefit from personalized, sustainable interventions. This\nframework transforms open data from a passive repository into an active\nplatform for shared governance and environmental equity. In the face of growing\necological inequality and data centralization, this chapter presents a\nreplicable model for citizen-driven urban intelligence, reframing planning as a\nco-produced, climate-resilient, and radically local practice.",
        "url": "http://arxiv.org/abs/2508.17648v1",
        "published_date": "2025-08-25T04:22:32+00:00",
        "updated_date": "2025-08-25T04:22:32+00:00",
        "categories": [
            "cs.CY",
            "cs.CV",
            "cs.LG",
            "H.2.8; J.1"
        ],
        "authors": [
            "Kaushik Ravi",
            "Andreas Brück"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a citizen-centered framework using open tree data for urban cooling and eco-routing in Indian cities, involving participatory sensing and AI.",
        "tldr_zh": "本文介绍了一种以公民为中心的框架，利用开放的树木数据来在印度城市中进行城市降温和生态路线规划，涉及参与式感知和人工智能。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Wound3DAssist: A Practical Framework for 3D Wound Assessment",
        "summary": "Managing chronic wounds remains a major healthcare challenge, with clinical\nassessment often relying on subjective and time-consuming manual documentation\nmethods. Although 2D digital videometry frameworks aided the measurement\nprocess, these approaches struggle with perspective distortion, a limited field\nof view, and an inability to capture wound depth, especially in anatomically\ncomplex or curved regions. To overcome these limitations, we present\nWound3DAssist, a practical framework for 3D wound assessment using monocular\nconsumer-grade videos. Our framework generates accurate 3D models from short\nhandheld smartphone video recordings, enabling non-contact, automatic\nmeasurements that are view-independent and robust to camera motion. We\nintegrate 3D reconstruction, wound segmentation, tissue classification, and\nperiwound analysis into a modular workflow. We evaluate Wound3DAssist across\ndigital models with known geometry, silicone phantoms, and real patients.\nResults show that the framework supports high-quality wound bed visualization,\nmillimeter-level accuracy, and reliable tissue composition analysis. Full\nassessments are completed in under 20 minutes, demonstrating feasibility for\nreal-world clinical use.",
        "url": "http://arxiv.org/abs/2508.17635v1",
        "published_date": "2025-08-25T03:50:04+00:00",
        "updated_date": "2025-08-25T03:50:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Remi Chierchia",
            "Rodrigo Santa Cruz",
            "Léo Lebrat",
            "Yulia Arzhaeva",
            "Mohammad Ali Armin",
            "Jeremy Oorloff",
            "Chuong Nguyen",
            "Olivier Salvado",
            "Clinton Fookes",
            "David Ahmedt-Aristizabal"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "Wound3DAssist is a framework for 3D wound assessment using smartphone videos, providing accurate models and tissue analysis in under 20 minutes.",
        "tldr_zh": "Wound3DAssist是一个利用智能手机视频进行3D伤口评估的框架，在不到20分钟内提供准确的模型和组织分析。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "A Weighted Vision Transformer-Based Multi-Task Learning Framework for Predicting ADAS-Cog Scores",
        "summary": "Prognostic modeling is essential for forecasting future clinical scores and\nenabling early detection of Alzheimers disease (AD). While most existing\nmethods focus on predicting the ADAS-Cog global score, they often overlook the\npredictive value of its 13 sub-scores, which reflect distinct cognitive\ndomains. Some sub-scores may exert greater influence on determining global\nscores. Assigning higher loss weights to these clinically meaningful sub-scores\ncan guide the model to focus on more relevant cognitive domains, enhancing both\npredictive accuracy and interpretability. In this study, we propose a weighted\nVision Transformer (ViT)-based multi-task learning (MTL) framework to jointly\npredict the ADAS-Cog global score using baseline MRI scans and its 13\nsub-scores at Month 24. Our framework integrates ViT as a feature extractor and\nsystematically investigates the impact of sub-score-specific loss weighting on\nmodel performance. Results show that our proposed weighting strategies are\ngroup-dependent: strong weighting improves performance for MCI subjects with\nmore heterogeneous MRI patterns, while moderate weighting is more effective for\nCN subjects with lower variability. Our findings suggest that uniform weighting\nunderutilizes key sub-scores and limits generalization. The proposed framework\noffers a flexible, interpretable approach to AD prognosis using end-to-end\nMRI-based learning. (Github repo link will be provided after review)",
        "url": "http://arxiv.org/abs/2508.17613v1",
        "published_date": "2025-08-25T02:43:48+00:00",
        "updated_date": "2025-08-25T02:43:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nur Amirah Abd Hamid",
            "Mohd Ibrahim Shapiai",
            "Daphne Teck Ching Lai"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a weighted Vision Transformer-based framework for predicting AD clinical scores using MRI scans, showing the importance of considering sub-scores for accurate predictions.",
        "tldr_zh": "该论文介绍了一种基于加权视觉变换器的框架，使用MRI扫描预测AD临床分数，显示了考虑子分数以获得准确预测的重要性。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MetaGen: A DSL, Database, and Benchmark for VLM-Assisted Metamaterial Generation",
        "summary": "Metamaterials are micro-architected structures whose geometry imparts highly\ntunable-often counter-intuitive-bulk properties. Yet their design is difficult\nbecause of geometric complexity and a non-trivial mapping from architecture to\nbehaviour. We address these challenges with three complementary contributions.\n(i) MetaDSL: a compact, semantically rich domain-specific language that\ncaptures diverse metamaterial designs in a form that is both human-readable and\nmachine-parsable. (ii) MetaDB: a curated repository of more than 150,000\nparameterized MetaDSL programs together with their\nderivatives-three-dimensional geometry, multi-view renderings, and simulated\nelastic properties. (iii) MetaBench: benchmark suites that test three core\ncapabilities of vision-language metamaterial assistants-structure\nreconstruction, property-driven inverse design, and performance prediction. We\nestablish baselines by fine-tuning state-of-the-art vision-language models and\ndeploy an omni-model within an interactive, CAD-like interface. Case studies\nshow that our framework provides a strong first step toward integrated design\nand understanding of structure-representation-property relationships.",
        "url": "http://arxiv.org/abs/2508.17568v1",
        "published_date": "2025-08-25T00:36:07+00:00",
        "updated_date": "2025-08-25T00:36:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CE",
            "cs.LG",
            "cs.PL"
        ],
        "authors": [
            "Liane Makatura",
            "Benjamin Jones",
            "Siyuan Bian",
            "Wojciech Matusik"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC",
            "Transformer"
        ],
        "tldr": "The paper introduces MetaGen, consisting of a DSL, database, and benchmark for assisting in generating metamaterials with complex structures and properties.",
        "tldr_zh": "该论文介绍了MetaGen，其中包括用于辅助生成复杂结构和性质的变形材料的DSL、数据库和基准。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Towards Optimal Convolutional Transfer Learning Architectures for Breast Lesion Classification and ACL Tear Detection",
        "summary": "Modern computer vision models have proven to be highly useful for medical\nimaging classification and segmentation tasks, but the scarcity of medical\nimaging data often limits the efficacy of models trained from scratch. Transfer\nlearning has emerged as a pivotal solution to this, enabling the fine-tuning of\nhigh-performance models on small data. Mei et al. (2022) found that\npre-training CNNs on a large dataset of radiologist-labeled images\n(RadImageNet) enhanced model performance on downstream tasks compared to\nImageNet pretraining. The present work extends Mei et al. (2022) by conducting\na comprehensive investigation to determine optimal CNN architectures for breast\nlesion malignancy detection and ACL tear detection, as well as performing\nstatistical analysis to compare the effect of RadImageNet and ImageNet\npre-training on downstream model performance. Our findings suggest that\n1-dimensional convolutional classifiers with skip connections, ResNet50\npre-trained backbones, and partial backbone unfreezing yields optimal\ndownstream medical classification performance. Our best models achieve AUCs of\n0.9969 for ACL tear detection and 0.9641 for breast nodule malignancy\ndetection, competitive with the results reported by Mei et al. (2022) and\nsurpassing other previous works. We do not find evidence confirming RadImageNet\npre-training to provide superior downstream performance for ACL tear and breast\nlesion classification tasks.",
        "url": "http://arxiv.org/abs/2508.17567v1",
        "published_date": "2025-08-25T00:33:43+00:00",
        "updated_date": "2025-08-25T00:33:43+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "68T45"
        ],
        "authors": [
            "Daniel Frees",
            "Moritz Bolling",
            "Aditri Bhagirath"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper explores the optimal CNN architectures for breast lesion classification and ACL tear detection using transfer learning, achieving high performance.",
        "tldr_zh": "本文探讨了使用迁移学习进行乳腺病变分类和ACL撕裂检测的最佳CNN架构，取得了较高的性能。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "A holistic perception system of internal and external monitoring for ground autonomous vehicles: AutoTRUST paradigm",
        "summary": "This paper introduces a holistic perception system for internal and external\nmonitoring of autonomous vehicles, with the aim of demonstrating a novel\nAI-leveraged self-adaptive framework of advanced vehicle technologies and\nsolutions that optimize perception and experience on-board. Internal monitoring\nsystem relies on a multi-camera setup designed for predicting and identifying\ndriver and occupant behavior through facial recognition, exploiting in addition\na large language model as virtual assistant. Moreover, the in-cabin monitoring\nsystem includes AI-empowered smart sensors that measure air-quality and perform\nthermal comfort analysis for efficient on and off-boarding. On the other hand,\nexternal monitoring system perceives the surrounding environment of vehicle,\nthrough a LiDAR-based cost-efficient semantic segmentation approach, that\nperforms highly accurate and efficient super-resolution on low-quality raw 3D\npoint clouds. The holistic perception framework is developed in the context of\nEU's Horizon Europe programm AutoTRUST, and has been integrated and deployed on\na real electric vehicle provided by ALKE. Experimental validation and\nevaluation at the integration site of Joint Research Centre at Ispra, Italy,\nhighlights increased performance and efficiency of the modular blocks of the\nproposed perception architecture.",
        "url": "http://arxiv.org/abs/2508.17969v1",
        "published_date": "2025-08-25T12:32:13+00:00",
        "updated_date": "2025-08-25T12:32:13+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Alexandros Gkillas",
            "Christos Anagnostopoulos",
            "Nikos Piperigkos",
            "Dimitris Tsiktsiris",
            "Theofilos Christodoulou",
            "Theofanis Siamatras",
            "Dimitrios Triantafyllou",
            "Christos Basdekis",
            "Theoktisti Marinopoulou",
            "Panagiotis Lepentsiotis",
            "Elefterios Blitsis",
            "Aggeliki Zacharaki",
            "Nearchos Stylianidis",
            "Leonidas Katelaris",
            "Lamberto Salvan",
            "Aris S. Lalos",
            "Christos Laoudias",
            "Antonios Lalas",
            "Konstantinos Votis"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset",
            "Transformer"
        ],
        "tldr": "This paper presents a holistic perception system for monitoring both internal and external aspects of autonomous vehicles, utilizing AI technologies to enhance the overall experience onboard.",
        "tldr_zh": "本文介绍了一种全面的感知系统，用于监测自动驾驶车辆的内部和外部方面，利用人工智能技术来增强车内的整体体验。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "From Global to Local: Social Bias Transfer in CLIP",
        "summary": "The recycling of contrastive language-image pre-trained (CLIP) models as\nbackbones for a large number of downstream tasks calls for a thorough analysis\nof their transferability implications, especially their well-documented\nreproduction of social biases and human stereotypes. How do such biases,\nlearned during pre-training, propagate to downstream applications like visual\nquestion answering or image captioning? Do they transfer at all?\n  We investigate this phenomenon, referred to as bias transfer in prior\nliterature, through a comprehensive empirical analysis. Firstly, we examine how\npre-training bias varies between global and local views of data, finding that\nbias measurement is highly dependent on the subset of data on which it is\ncomputed. Secondly, we analyze correlations between biases in the pre-trained\nmodels and the downstream tasks across varying levels of pre-training bias,\nfinding difficulty in discovering consistent trends in bias transfer. Finally,\nwe explore why this inconsistency occurs, showing that under the current\nparadigm, representation spaces of different pre-trained CLIPs tend to converge\nwhen adapted for downstream tasks. We hope this work offers valuable insights\ninto bias behavior and informs future research to promote better bias\nmitigation practices.",
        "url": "http://arxiv.org/abs/2508.17750v1",
        "published_date": "2025-08-25T07:44:03+00:00",
        "updated_date": "2025-08-25T07:44:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ryan Ramos",
            "Yusuke Hirota",
            "Yuta Nakashima",
            "Noa Garcia"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper investigates how social biases in pre-trained models like CLIP affect downstream tasks, finding that bias measurement varies based on data subset and showing challenges in consistent bias transfer.",
        "tldr_zh": "本文研究了预训练模型（如CLIP）中的社会偏见如何影响下游任务，发现偏见度量因数据子集不同而异，并展示了一致性偏见转移方面的挑战。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "HyTver: A Novel Loss Function for Longitudinal Multiple Sclerosis Lesion Segmentation",
        "summary": "Longitudinal Multiple Sclerosis Lesion Segmentation is a particularly\nchallenging problem that involves both input and output imbalance in the data\nand segmentation. Therefore in order to develop models that are practical, one\nof the solutions is to develop better loss functions. Most models naively use\neither Dice loss or Cross-Entropy loss or their combination without too much\nconsideration. However, one must select an appropriate loss function as the\nimbalance can be mitigated by selecting a proper loss function. In order to\nsolve the imbalance problem, multiple loss functions were proposed that claimed\nto solve it. They come with problems of their own which include being too\ncomputationally complex due to hyperparameters as exponents or having\ndetrimental performance in metrics other than region-based ones. We propose a\nnovel hybrid loss called HyTver that achieves good segmentation performance\nwhile maintaining performance in other metrics. We achieve a Dice score of\n0.659 while also ensuring that the distance-based metrics are comparable to\nother popular functions. In addition, we also evaluate the stability of the\nloss functions when used on a pre- trained model and perform extensive\ncomparisons with other popular loss functions",
        "url": "http://arxiv.org/abs/2508.17639v1",
        "published_date": "2025-08-25T04:01:28+00:00",
        "updated_date": "2025-08-25T04:01:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dayan Perera",
            "Ting Fung Fung",
            "Vishnu Monn"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper introduces a novel hybrid loss function called HyTver for Longitudinal Multiple Sclerosis Lesion Segmentation, achieving good segmentation performance and maintaining performance in other metrics compared to popular loss functions.",
        "tldr_zh": "本文介绍了一种名为HyTver的新型混合损失函数，用于长期多发性硬化症病变分割，在与流行损失函数相比，实现了良好的分割性能并保持了其他指标的表现。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Few-Shot Pattern Detection via Template Matching and Regression",
        "summary": "We address the problem of few-shot pattern detection, which aims to detect\nall instances of a given pattern, typically represented by a few exemplars,\nfrom an input image. Although similar problems have been studied in few-shot\nobject counting and detection (FSCD), previous methods and their benchmarks\nhave narrowed patterns of interest to object categories and often fail to\nlocalize non-object patterns. In this work, we propose a simple yet effective\ndetector based on template matching and regression, dubbed TMR. While previous\nFSCD methods typically represent target exemplars as spatially collapsed\nprototypes and lose structural information, we revisit classic template\nmatching and regression. It effectively preserves and leverages the spatial\nlayout of exemplars through a minimalistic structure with a small number of\nlearnable convolutional or projection layers on top of a frozen backbone We\nalso introduce a new dataset, dubbed RPINE, which covers a wider range of\npatterns than existing object-centric datasets. Our method outperforms the\nstate-of-the-art methods on the three benchmarks, RPINE, FSCD-147, and\nFSCD-LVIS, and demonstrates strong generalization in cross-dataset evaluation.",
        "url": "http://arxiv.org/abs/2508.17636v1",
        "published_date": "2025-08-25T03:52:42+00:00",
        "updated_date": "2025-08-25T03:52:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Eunchan Jo",
            "Dahyun Kang",
            "Sanghyun Kim",
            "Yunseon Choi",
            "Minsu Cho"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a new method, TMR, for few-shot pattern detection using template matching and regression. It outperforms existing methods on multiple benchmarks.",
        "tldr_zh": "本文介绍了一种新的方法TMR，通过模板匹配和回归进行少样本模式检测。在多个基准测试中胜过现有方法。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Finding Outliers in a Haystack: Anomaly Detection for Large Pointcloud Scenes",
        "summary": "LiDAR scanning in outdoor scenes acquires accurate distance measurements over\nwide areas, producing large-scale point clouds. Application examples for this\ndata include robotics, automotive vehicles, and land surveillance. During such\napplications, outlier objects from outside the training data will inevitably\nappear. Our research contributes a novel approach to open-set segmentation,\nleveraging the learnings of object defect-detection research. We also draw on\nthe Mamba architecture's strong performance in utilising long-range\ndependencies and scalability to large data. Combining both, we create a\nreconstruction based approach for the task of outdoor scene open-set\nsegmentation. We show that our approach improves performance not only when\napplied to our our own open-set segmentation method, but also when applied to\nexisting methods. Furthermore we contribute a Mamba based architecture which is\ncompetitive with existing voxel-convolution based methods on challenging,\nlarge-scale pointclouds.",
        "url": "http://arxiv.org/abs/2508.17634v1",
        "published_date": "2025-08-25T03:47:33+00:00",
        "updated_date": "2025-08-25T03:47:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ryan Faulkner",
            "Ian Reid",
            "Simon Ratcliffe",
            "Tat-Jun Chin"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper presents a novel approach for outlier detection in large-scale point cloud scenes using a reconstruction-based method.",
        "tldr_zh": "该论文提出了一种新颖的基于重建的方法，用于大规模点云场景中的异常检测。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "HotSpotter - Patterned Species Instance Recognition",
        "summary": "We present HotSpotter, a fast, accurate algorithm for identifying individual\nanimals against a labeled database. It is not species specific and has been\napplied to Grevy's and plains zebras, giraffes, leopards, and lionfish. We\ndescribe two approaches, both based on extracting and matching keypoints or\n\"hotspots\". The first tests each new query image sequentially against each\ndatabase image, generating a score for each database image in isolation, and\nranking the results. The second, building on recent techniques for instance\nrecognition, matches the query image against the database using a fast nearest\nneighbor search. It uses a competitive scoring mechanism derived from the Local\nNaive Bayes Nearest Neighbor algorithm recently proposed for category\nrecognition. We demonstrate results on databases of more than 1000 images,\nproducing more accurate matches than published methods and matching each query\nimage in just a few seconds.",
        "url": "http://arxiv.org/abs/2508.17605v1",
        "published_date": "2025-08-25T02:18:52+00:00",
        "updated_date": "2025-08-25T02:18:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jonathan P. Crall",
            "Charles V. Stewart",
            "Tanya Y. Berger-Wolf",
            "Daniel I. Rubenstein",
            "Siva R. Sundaresan"
        ],
        "ai_categories": [
            "Dataset",
            "LoRA"
        ],
        "tldr": "HotSpotter is a fast and accurate algorithm for identifying individual animals in images against a labeled database using keypoints matching techniques.",
        "tldr_zh": "HotSpotter是一种快速准确的算法，使用关键点匹配技术在图像中识别个体动物并与标记数据库进行比对。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Sketchpose: Learning to Segment Cells with Partial Annotations",
        "summary": "The most popular networks used for cell segmentation (e.g. Cellpose,\nStardist, HoverNet,...) rely on a prediction of a distance map. It yields\nunprecedented accuracy but hinges on fully annotated datasets. This is a\nserious limitation to generate training sets and perform transfer learning. In\nthis paper, we propose a method that still relies on the distance map and\nhandles partially annotated objects. We evaluate the performance of the\nproposed approach in the contexts of frugal learning, transfer learning and\nregular learning on regular databases. Our experiments show that it can lead to\nsubstantial savings in time and resources without sacrificing segmentation\nquality. The proposed algorithm is embedded in a user-friendly Napari plugin.",
        "url": "http://arxiv.org/abs/2508.17798v1",
        "published_date": "2025-08-25T08:43:32+00:00",
        "updated_date": "2025-08-25T08:43:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Clément Cazorla",
            "Nathanaël Munier",
            "Renaud Morin",
            "Pierre Weiss"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a method for cell segmentation that can handle partially annotated objects, leading to time and resource savings without sacrificing quality.",
        "tldr_zh": "本文介绍了一种能够处理部分标注对象的细胞分割方法，可以在不损失质量的前提下节约时间和资源。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.25
    },
    {
        "title": "Towards Trustworthy Breast Tumor Segmentation in Ultrasound using Monte Carlo Dropout and Deep Ensembles for Epistemic Uncertainty Estimation",
        "summary": "Automated segmentation of BUS images is important for precise lesion\ndelineation and tumor characterization, but is challenged by inherent artifacts\nand dataset inconsistencies. In this work, we evaluate the use of a modified\nResidual Encoder U-Net for breast ultrasound segmentation, with a focus on\nuncertainty quantification. We identify and correct for data duplication in the\nBUSI dataset, and use a deduplicated subset for more reliable estimates of\ngeneralization performance. Epistemic uncertainty is quantified using Monte\nCarlo dropout, deep ensembles, and their combination. Models are benchmarked on\nboth in-distribution and out-of-distribution datasets to demonstrate how they\ngeneralize to unseen cross-domain data. Our approach achieves state-of-the-art\nsegmentation accuracy on the Breast-Lesion-USG dataset with in-distribution\nvalidation, and provides calibrated uncertainty estimates that effectively\nsignal regions of low model confidence. Performance declines and increased\nuncertainty observed in out-of-distribution evaluation highlight the persistent\nchallenge of domain shift in medical imaging, and the importance of integrated\nuncertainty modeling for trustworthy clinical deployment. \\footnote{Code\navailable at: https://github.com/toufiqmusah/nn-uncertainty.git}",
        "url": "http://arxiv.org/abs/2508.17768v1",
        "published_date": "2025-08-25T08:06:07+00:00",
        "updated_date": "2025-08-25T08:06:07+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Toufiq Musah",
            "Chinasa Kalaiwo",
            "Maimoona Akram",
            "Ubaida Napari Abdulai",
            "Maruf Adewole",
            "Farouk Dako",
            "Adaobi Chiazor Emegoakor",
            "Udunna C. Anazodo",
            "Prince Ebenezer Adjei",
            "Confidence Raymond"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper explores the use of deep learning models for breast tumor segmentation in ultrasound images, focusing on uncertainty quantification and generalization to new data domains.",
        "tldr_zh": "本文探讨了在超声波图像中使用深度学习模型进行乳腺肿瘤分割，重点关注不确定性量化和对新数据领域的泛化。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.25
    },
    {
        "title": "Segmentation and Classification of Pap Smear Images for Cervical Cancer Detection Using Deep Learning",
        "summary": "Cervical cancer remains a significant global health concern and a leading\ncause of cancer-related deaths among women. Early detection through Pap smear\ntests is essential to reduce mortality rates; however, the manual examination\nis time consuming and prone to human error. This study proposes a deep learning\nframework that integrates U-Net for segmentation and a classification model to\nenhance diagnostic performance. The Herlev Pap Smear Dataset, a publicly\navailable cervical cell dataset, was utilized for training and evaluation. The\nimpact of segmentation on classification performance was evaluated by comparing\nthe model trained on segmented images and another trained on non-segmented\nimages. Experimental results showed that the use of segmented images marginally\nimproved the model performance on precision (about 0.41 percent higher) and\nF1-score (about 1.30 percent higher), which suggests a slightly more balanced\nclassification performance. While segmentation helps in feature extraction, the\nresults showed that its impact on classification performance appears to be\nlimited. The proposed framework offers a supplemental tool for clinical\napplications, which may aid pathologists in early diagnosis.",
        "url": "http://arxiv.org/abs/2508.17728v1",
        "published_date": "2025-08-25T07:11:16+00:00",
        "updated_date": "2025-08-25T07:11:16+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Nisreen Albzour",
            "Sarah S. Lam"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "A deep learning framework was proposed for segmenting and classifying Pap smear images for cervical cancer detection, showing marginal improvement in classification performance with segmentation.",
        "tldr_zh": "提出了一个深度学习框架，用于对宫颈癌检测的子宫颈抹片图像进行分割和分类，展示出了在分割过程中略微提高分类性能的效果。",
        "relevance_score": 4,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6.25
    },
    {
        "title": "M^3-GloDets: Multi-Region and Multi-Scale Analysis of Fine-Grained Diseased Glomerular Detection",
        "summary": "Accurate detection of diseased glomeruli is fundamental to progress in renal\npathology and underpins the delivery of reliable clinical diagnoses. Although\nrecent advances in computer vision have produced increasingly sophisticated\ndetection algorithms, the majority of research efforts have focused on normal\nglomeruli or instances of global sclerosis, leaving the wider spectrum of\ndiseased glomerular subtypes comparatively understudied. This disparity is not\nwithout consequence; the nuanced and highly variable morphological\ncharacteristics that define these disease variants frequently elude even the\nmost advanced computational models. Moreover, ongoing debate surrounds the\nchoice of optimal imaging magnifications and region-of-view dimensions for\nfine-grained glomerular analysis, adding further complexity to the pursuit of\naccurate classification and robust segmentation.\n  To bridge these gaps, we present M^3-GloDet, a systematic framework designed\nto enable thorough evaluation of detection models across a broad continuum of\nregions, scales, and classes. Within this framework, we evaluate both\nlong-standing benchmark architectures and recently introduced state-of-the-art\nmodels that have achieved notable performance, using an experimental design\nthat reflects the diversity of region-of-interest sizes and imaging resolutions\nencountered in routine digital renal pathology. As the results, we found that\nintermediate patch sizes offered the best balance between context and\nefficiency. Additionally, moderate magnifications enhanced generalization by\nreducing overfitting. Through systematic comparison of these approaches on a\nmulti-class diseased glomerular dataset, our aim is to advance the\nunderstanding of model strengths and limitations, and to offer actionable\ninsights for the refinement of automated detection strategies and clinical\nworkflows in the digital pathology domain.",
        "url": "http://arxiv.org/abs/2508.17666v1",
        "published_date": "2025-08-25T04:52:34+00:00",
        "updated_date": "2025-08-25T04:52:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianyu Shi",
            "Xinzi He",
            "Kenji Ikemura",
            "Mert R. Sabuncu",
            "Yihe Yang",
            "Ruining Deng"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper presents M^3-GloDets, a framework for detecting diseased glomeruli in renal pathology using multi-region and multi-scale analysis.",
        "tldr_zh": "该论文提出了M^3-GloDets，这是一个用于检测肾脏病理中患病糖球的框架，采用多区域和多尺度分析。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.25
    },
    {
        "title": "Minimal Solvers for Full DoF Motion Estimation from Asynchronous Tracks",
        "summary": "We address the problem of estimating both translational and angular velocity\nof a camera from asynchronous point tracks, a formulation relevant to rolling\nshutter and event cameras. Since the original problem is non-polynomial, we\npropose a polynomial approximation, classify the resulting minimal problems,\nand determine their algebraic degrees. Furthermore, we develop minimal solvers\nfor several problems with low degrees and evaluate them on synthetic and real\ndatasets. The code will be made publicly available.",
        "url": "http://arxiv.org/abs/2508.17537v1",
        "published_date": "2025-08-24T22:17:00+00:00",
        "updated_date": "2025-08-24T22:17:00+00:00",
        "categories": [
            "cs.CV",
            "68T45",
            "I.4.5"
        ],
        "authors": [
            "Petr Hruby",
            "Marc Pollefeys"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper addresses the problem of estimating camera motion from asynchronous point tracks, proposing minimal solvers for various scenarios and providing evaluation on synthetic and real datasets.",
        "tldr_zh": "本文解决了从异步点轨迹估计摄像机运动的问题，提出了各种情况的最小解算器，并在合成和实际数据集上进行了评估。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.25
    },
    {
        "title": "CMFDNet: Cross-Mamba and Feature Discovery Network for Polyp Segmentation",
        "summary": "Automated colonic polyp segmentation is crucial for assisting doctors in\nscreening of precancerous polyps and diagnosis of colorectal neoplasms.\nAlthough existing methods have achieved promising results, polyp segmentation\nremains hindered by the following limitations,including: (1) significant\nvariation in polyp shapes and sizes, (2) indistinct boundaries between polyps\nand adjacent tissues, and (3) small-sized polyps are easily overlooked during\nthe segmentation process. Driven by these practical difficulties, an innovative\narchitecture, CMFDNet, is proposed with the CMD module, MSA module, and FD\nmodule. The CMD module, serving as an innovative decoder, introduces a\ncross-scanning method to reduce blurry boundaries. The MSA module adopts a\nmulti-branch parallel structure to enhance the recognition ability for polyps\nwith diverse geometries and scale distributions. The FD module establishes\ndependencies among all decoder features to alleviate the under-detection of\npolyps with small-scale features. Experimental results show that CMFDNet\noutperforms six SOTA methods used for comparison, especially on ETIS and\nColonDB datasets, where mDice scores exceed the best SOTA method by 1.83% and\n1.55%, respectively.",
        "url": "http://arxiv.org/abs/2508.17729v1",
        "published_date": "2025-08-25T07:12:00+00:00",
        "updated_date": "2025-08-25T07:12:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Feng Jiang",
            "Zongfei Zhang",
            "Xin Xu"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces CMFDNet, an innovative architecture for colonic polyp segmentation, outperforming existing methods on specific datasets.",
        "tldr_zh": "本文介绍了CMFDNet，一种用于结肠息肉分割的创新架构，在特定数据集上优于现有方法。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]