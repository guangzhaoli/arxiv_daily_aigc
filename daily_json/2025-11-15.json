[
    {
        "title": "Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling",
        "summary": "Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.",
        "url": "http://arxiv.org/abs/2511.10648v1",
        "published_date": "2025-11-13T18:59:57+00:00",
        "updated_date": "2025-11-13T18:59:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahao Wang",
            "Weiye Xu",
            "Aijun Yang",
            "Wengang Zhou",
            "Lewei Lu",
            "Houqiang Li",
            "Xiaohua Wang",
            "Jinguo Zhu"
        ],
        "ai_categories": []
    },
    {
        "title": "Depth Anything 3: Recovering the Visual Space from Any Views",
        "summary": "We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.",
        "url": "http://arxiv.org/abs/2511.10647v1",
        "published_date": "2025-11-13T18:59:53+00:00",
        "updated_date": "2025-11-13T18:59:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haotong Lin",
            "Sili Chen",
            "Junhao Liew",
            "Donny Y. Chen",
            "Zhenyu Li",
            "Guang Shi",
            "Jiashi Feng",
            "Bingyi Kang"
        ],
        "ai_categories": []
    },
    {
        "title": "One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models",
        "summary": "Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding. We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE decoding step. LUA integrates as a drop-in component, requiring no modifications to the base model or additional diffusion stages, and enables high-resolution synthesis through a single feed-forward pass in latent space. A shared Swin-style backbone with scale-specific pixel-shuffle heads supports 2x and 4x factors and remains compatible with image-space SR baselines, achieving comparable perceptual quality with nearly 3x lower decoding and upscaling time (adding only +0.42 s for 1024 px generation from 512 px, compared to 1.87 s for pixel-space SR using the same SwinIR architecture). Furthermore, LUA shows strong generalization across the latent spaces of different VAEs, making it easy to deploy without retraining from scratch for each new decoder. Extensive experiments demonstrate that LUA closely matches the fidelity of native high-resolution generation while offering a practical and efficient path to scalable, high-fidelity image synthesis in modern diffusion pipelines.",
        "url": "http://arxiv.org/abs/2511.10629v1",
        "published_date": "2025-11-13T18:54:18+00:00",
        "updated_date": "2025-11-13T18:54:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aleksandr Razin",
            "Danil Kazantsev",
            "Ilya Makarov"
        ],
        "ai_categories": []
    },
    {
        "title": "Querying Labeled Time Series Data with Scenario Programs",
        "summary": "Simulation-based testing has become a crucial complement to road testing for ensuring the safety of cyber physical systems (CPS). As a result, significant research efforts have been directed toward identifying failure scenarios within simulation environments. However, a critical question remains. Are the AV failure scenarios discovered in simulation reproducible on actual systems in the real world? The sim-to-real gap caused by differences between simulated and real sensor data means that failure scenarios identified in simulation might either be artifacts of synthetic sensor data or actual issues that also occur with real sensor data. To address this, an effective approach to validating simulated failure scenarios is to locate occurrences of these scenarios within real-world datasets and verify whether the failure persists on the datasets. To this end, we introduce a formal definition of how labeled time series sensor data can match an abstract scenario, represented as a scenario program using the Scenic probabilistic programming language. We present a querying algorithm that, given a scenario program and a labeled dataset, identifies the subset of data that matches the specified scenario. Our experiment shows that our algorithm is more accurate and orders of magnitude faster in querying scenarios than the state-of-the-art commercial vision large language models, and can scale with the duration of queried time series data.",
        "url": "http://arxiv.org/abs/2511.10627v1",
        "published_date": "2025-11-13T18:52:27+00:00",
        "updated_date": "2025-11-13T18:52:27+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.FL",
            "cs.LG"
        ],
        "authors": [
            "Edward Kim",
            "Devan Shanker",
            "Varun Bharadwaj",
            "Hongbeen Park",
            "Jinkyu Kim",
            "Hazem Torfah",
            "Daniel J Fremont",
            "Sanjit A Seshia"
        ],
        "ai_categories": []
    },
    {
        "title": "Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals",
        "summary": "Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.",
        "url": "http://arxiv.org/abs/2511.10615v1",
        "published_date": "2025-11-13T18:45:39+00:00",
        "updated_date": "2025-11-13T18:45:39+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Shruti Singh Baghel",
            "Yash Pratap Singh Rathore",
            "Sushovan Jena",
            "Anurag Pradhan",
            "Amit Shukla",
            "Arnav Bhavsar",
            "Pawan Goyal"
        ],
        "ai_categories": []
    },
    {
        "title": "Multitask GLocal OBIA-Mamba for Sentinel-2 Landcover Mapping",
        "summary": "Although Sentinel-2 based land use and land cover (LULC) classification is critical for various environmental monitoring applications, it is a very difficult task due to some key data challenges (e.g., spatial heterogeneity, context information, signature ambiguity). This paper presents a novel Multitask Glocal OBIA-Mamba (MSOM) for enhanced Sentinel-2 classification with the following contributions. First, an object-based image analysis (OBIA) Mamba model (OBIA-Mamba) is designed to reduce redundant computation without compromising fine-grained details by using superpixels as Mamba tokens. Second, a global-local (GLocal) dual-branch convolutional neural network (CNN)-mamba architecture is designed to jointly model local spatial detail and global contextual information. Third, a multitask optimization framework is designed to employ dual loss functions to balance local precision with global consistency. The proposed approach is tested on Sentinel-2 imagery in Alberta, Canada, in comparison with several advanced classification approaches, and the results demonstrate that the proposed approach achieves higher classification accuracy and finer details that the other state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2511.10604v1",
        "published_date": "2025-11-13T18:40:22+00:00",
        "updated_date": "2025-11-13T18:40:22+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Zack Dewis",
            "Yimin Zhu",
            "Zhengsen Xu",
            "Mabel Heffring",
            "Saeid Taleghanidoozdoozan",
            "Kaylee Xiao",
            "Motasem Alkayid",
            "Lincoln Linlin Xu"
        ],
        "ai_categories": []
    },
    {
        "title": "From 2D to 3D Without Extra Baggage: Data-Efficient Cancer Detection in Digital Breast Tomosynthesis",
        "summary": "Digital Breast Tomosynthesis (DBT) enhances finding visibility for breast cancer detection by providing volumetric information that reduces the impact of overlapping tissues; however, limited annotated data has constrained the development of deep learning models for DBT. To address data scarcity, existing methods attempt to reuse 2D full-field digital mammography (FFDM) models by either flattening DBT volumes or processing slices individually, thus discarding volumetric information. Alternatively, 3D reasoning approaches introduce complex architectures that require more DBT training data. Tackling these drawbacks, we propose M&M-3D, an architecture that enables learnable 3D reasoning while remaining parameter-free relative to its FFDM counterpart, M&M. M&M-3D constructs malignancy-guided 3D features, and 3D reasoning is learned through repeatedly mixing these 3D features with slice-level information. This is achieved by modifying operations in M&M without adding parameters, thus enabling direct weight transfer from FFDM. Extensive experiments show that M&M-3D surpasses 2D projection and 3D slice-based methods by 11-54% for localization and 3-10% for classification. Additionally, M&M-3D outperforms complex 3D reasoning variants by 20-47% for localization and 2-10% for classification in the low-data regime, while matching their performance in high-data regime. On the popular BCS-DBT benchmark, M&M-3D outperforms previous top baseline by 4% for classification and 10% for localization.",
        "url": "http://arxiv.org/abs/2511.10597v1",
        "published_date": "2025-11-13T18:35:45+00:00",
        "updated_date": "2025-11-13T18:35:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yen Nhi Truong Vu",
            "Dan Guo",
            "Sripad Joshi",
            "Harshit Kumar",
            "Jason Su",
            "Thomas Paul Matthews"
        ],
        "ai_categories": []
    },
    {
        "title": "Impact of Layer Norm on Memorization and Generalization in Transformers",
        "summary": "Layer Normalization (LayerNorm) is one of the fundamental components in transformers that stabilizes training and improves optimization. In recent times, Pre-LayerNorm transformers have become the preferred choice over Post-LayerNorm transformers due to their stable gradient flow. However, the impact of LayerNorm on learning and memorization across these architectures remains unclear. In this work, we investigate how LayerNorm influences memorization and learning for Pre- and Post-LayerNorm transformers. We identify that LayerNorm serves as a key factor for stable learning in Pre-LayerNorm transformers, while in Post-LayerNorm transformers, it impacts memorization. Our analysis reveals that eliminating LayerNorm parameters in Pre-LayerNorm models exacerbates memorization and destabilizes learning, while in Post-LayerNorm models, it effectively mitigates memorization by restoring genuine labels. We further precisely identify that early layers LayerNorm are the most critical over middle/later layers and their influence varies across Pre and Post LayerNorm models. We have validated it through 13 models across 6 Vision and Language datasets. These insights shed new light on the role of LayerNorm in shaping memorization and learning in transformers.",
        "url": "http://arxiv.org/abs/2511.10566v1",
        "published_date": "2025-11-13T18:07:07+00:00",
        "updated_date": "2025-11-13T18:07:07+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Rishi Singhal",
            "Jung-Eun Kim"
        ],
        "ai_categories": []
    }
]