[
    {
        "title": "One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum Unsupervised Anomaly Detection",
        "summary": "Unsupervised anomaly detection (UAD) has evolved from building specialized\nsingle-class models to unified multi-class models, yet existing multi-class\nmodels significantly underperform the most advanced one-for-one counterparts.\nMoreover, the field has fragmented into specialized methods tailored to\nspecific scenarios (multi-class, 3D, few-shot, etc.), creating deployment\nbarriers and highlighting the need for a unified solution. In this paper, we\npresent Dinomaly2, the first unified framework for full-spectrum image UAD,\nwhich bridges the performance gap in multi-class models while seamlessly\nextending across diverse data modalities and task settings. Guided by the \"less\nis more\" philosophy, we demonstrate that the orchestration of five simple\nelement achieves superior performance in a standard reconstruction-based\nframework. This methodological minimalism enables natural extension across\ndiverse tasks without modification, establishing that simplicity is the\nfoundation of true universality. Extensive experiments on 12 UAD benchmarks\ndemonstrate Dinomaly2's full-spectrum superiority across multiple modalities\n(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,\ninference-unified multi-class, few-shot) and application domains (industrial,\nbiological, outdoor). For example, our multi-class model achieves unprecedented\n99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For\nmulti-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art\nperformance with minimum adaptations. Moreover, using only 8 normal examples\nper class, our method surpasses previous full-shot models, achieving 98.7% and\n97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,\ncomputational scalability, and universal applicability positions Dinomaly2 as a\nunified solution for the full spectrum of real-world anomaly detection\napplications.",
        "url": "http://arxiv.org/abs/2510.17611v1",
        "published_date": "2025-10-20T14:57:52+00:00",
        "updated_date": "2025-10-20T14:57:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jia Guo",
            "Shuai Lu",
            "Lei Fan",
            "Zelin Li",
            "Donglin Di",
            "Yang Song",
            "Weihang Zhang",
            "Wenbing Zhu",
            "Hong Yan",
            "Fang Chen",
            "Huiqi Li",
            "Hongen Liao"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces Dinomaly2, a unified framework for full-spectrum unsupervised anomaly detection across diverse data modalities and task settings, achieving superior performance with minimalistic design.",
        "tldr_zh": "本文介绍了Dinomaly2，这是一个统一的框架，用于全谱无监督异常检测，跨越了不同的数据模态和任务设置，以简约设计实现了更好的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 10,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "ConsistEdit: Highly Consistent and Precise Training-free Visual Editing",
        "summary": "Recent advances in training-free attention control methods have enabled\nflexible and efficient text-guided editing capabilities for existing generation\nmodels. However, current approaches struggle to simultaneously deliver strong\nediting strength while preserving consistency with the source. This limitation\nbecomes particularly critical in multi-round and video editing, where visual\nerrors can accumulate over time. Moreover, most existing methods enforce global\nconsistency, which limits their ability to modify individual attributes such as\ntexture while preserving others, thereby hindering fine-grained editing.\nRecently, the architectural shift from U-Net to MM-DiT has brought significant\nimprovements in generative performance and introduced a novel mechanism for\nintegrating text and vision modalities. These advancements pave the way for\novercoming challenges that previous methods failed to resolve. Through an\nin-depth analysis of MM-DiT, we identify three key insights into its attention\nmechanisms. Building on these, we propose ConsistEdit, a novel attention\ncontrol method specifically tailored for MM-DiT. ConsistEdit incorporates\nvision-only attention control, mask-guided pre-attention fusion, and\ndifferentiated manipulation of the query, key, and value tokens to produce\nconsistent, prompt-aligned edits. Extensive experiments demonstrate that\nConsistEdit achieves state-of-the-art performance across a wide range of image\nand video editing tasks, including both structure-consistent and\nstructure-inconsistent scenarios. Unlike prior methods, it is the first\napproach to perform editing across all inference steps and attention layers\nwithout handcraft, significantly enhancing reliability and consistency, which\nenables robust multi-round and multi-region editing. Furthermore, it supports\nprogressive adjustment of structural consistency, enabling finer control.",
        "url": "http://arxiv.org/abs/2510.17803v1",
        "published_date": "2025-10-20T17:59:52+00:00",
        "updated_date": "2025-10-20T17:59:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zixin Yin",
            "Ling-Hao Chen",
            "Lionel Ni",
            "Xili Dai"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "GAN"
        ],
        "tldr": "The paper introduces ConsistEdit, a novel attention control method tailored for MM-DiT models, achieving state-of-the-art performance in image and video editing tasks.",
        "tldr_zh": "本文介绍了ConsistEdit，一种专为MM-DiT模型量身定制的注意力控制方法，在图像和视频编辑任务中取得了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.75
    },
    {
        "title": "VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models",
        "summary": "Vision-Language Models (VLMs) extend large language models with visual\nreasoning, but their multimodal design also introduces new, underexplored\nvulnerabilities. Existing multimodal red-teaming methods largely rely on\nbrittle templates, focus on single-attack settings, and expose only a narrow\nsubset of vulnerabilities. To address these limitations, we introduce VERA-V, a\nvariational inference framework that recasts multimodal jailbreak discovery as\nlearning a joint posterior distribution over paired text-image prompts. This\nprobabilistic view enables the generation of stealthy, coupled adversarial\ninputs that bypass model guardrails. We train a lightweight attacker to\napproximate the posterior, allowing efficient sampling of diverse jailbreaks\nand providing distributional insights into vulnerabilities. VERA-V further\nintegrates three complementary strategies: (i) typography-based text prompts\nthat embed harmful cues, (ii) diffusion-based image synthesis that introduces\nadversarial signals, and (iii) structured distractors to fragment VLM\nattention. Experiments on HarmBench and HADES benchmarks show that VERA-V\nconsistently outperforms state-of-the-art baselines on both open-source and\nfrontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the\nbest baseline on GPT-4o.",
        "url": "http://arxiv.org/abs/2510.17759v1",
        "published_date": "2025-10-20T17:12:10+00:00",
        "updated_date": "2025-10-20T17:12:10+00:00",
        "categories": [
            "cs.CR",
            "cs.CL",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Qilin Liao",
            "Anamika Lochab",
            "Ruqi Zhang"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "GAN"
        ],
        "tldr": "VERA-V is a variational inference framework that discovers vulnerabilities in vision-language models by generating adversarial inputs, outperforming existing methods on various benchmarks.",
        "tldr_zh": "VERA-V是一个变分推理框架，通过生成对抗输入来发现视觉-语言模型中的漏洞，在各种基准测试中表现优异。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model",
        "summary": "Foundation models for medical image segmentation have achieved remarkable\nperformance. Adaptive fine-tuning of natural image segmentation foundation\nmodels is crucial for medical image segmentation tasks. However, some\nlimitations exist in existing fine-tuning methods: 1) insufficient\nrepresentation of high-level features and 2) the fine-tuning process disrupts\nthe structural integrity of pretrained weights. Inspired by these critical\nproblems, we propose an intelligent communication mixture-of-experts\nboosted-medical image segmentation foundation model, named IC-MoE, with twofold\nideas: 1) We construct basic experts, semantic experts, and adaptive experts.\nMoreover, we implement a pixel probability adaptive voting strategy, which\nenables expert selection and fusion through label consistency and load\nbalancing. This approach preliminarily enhances the representation capability\nof high-level features while preserving the structural integrity of pretrained\nweights. 2) We propose a semantic-guided contrastive learning method to address\nthe issue of weak supervision in contrastive learning. This method further\nenhances the representation capability of high-level features while preserving\nthe structural integrity of pretrained weights. Extensive experiments across\nthree public medical image segmentation datasets demonstrate that the IC-MoE\noutperforms other SOTA models. Consequently, the proposed IC-MoE effectively\nsupplements foundational medical image segmentation models with high-level\nfeatures and pretrained structural integrity. We also validate the superior\ngeneralizability of the IC-MoE across diverse medical image segmentation\nscenarios.",
        "url": "http://arxiv.org/abs/2510.17684v1",
        "published_date": "2025-10-20T16:00:59+00:00",
        "updated_date": "2025-10-20T16:00:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xinwei Zhang",
            "Hu Chen",
            "Zhe Yuan",
            "Sukun Tian",
            "Peng Feng"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper proposes an intelligent communication mixture-of-experts boosted-medical image segmentation model, IC-MoE, which enhances high-level feature representation while preserving the structural integrity of pretrained weights.",
        "tldr_zh": "本文提出了一种智能通信混合专家增强医学图像分割模型IC-MoE，可以增强高级特征表征，同时保持预训练权重的结构完整性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "Closed-Loop Transfer for Weakly-supervised Affordance Grounding",
        "summary": "Humans can perform previously unexperienced interactions with novel objects\nsimply by observing others engage with them. Weakly-supervised affordance\ngrounding mimics this process by learning to locate object regions that enable\nactions on egocentric images, using exocentric interaction images with\nimage-level annotations. However, extracting affordance knowledge solely from\nexocentric images and transferring it one-way to egocentric images limits the\napplicability of previous works in complex interaction scenarios. Instead, this\nstudy introduces LoopTrans, a novel closed-loop framework that not only\ntransfers knowledge from exocentric to egocentric but also transfers back to\nenhance exocentric knowledge extraction. Within LoopTrans, several innovative\nmechanisms are introduced, including unified cross-modal localization and\ndenoising knowledge distillation, to bridge domain gaps between object-centered\negocentric and interaction-centered exocentric images while enhancing knowledge\ntransfer. Experiments show that LoopTrans achieves consistent improvements\nacross all metrics on image and video benchmarks, even handling challenging\nscenarios where object interaction regions are fully occluded by the human\nbody.",
        "url": "http://arxiv.org/abs/2510.17384v1",
        "published_date": "2025-10-20T10:21:35+00:00",
        "updated_date": "2025-10-20T10:21:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiajin Tang",
            "Zhengxuan Wei",
            "Ge Zheng",
            "Sibei Yang"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper introduces a closed-loop framework, LoopTrans, for transferring affordance knowledge between egocentric and exocentric images to enhance interaction understanding.",
        "tldr_zh": "本文介绍了一种闭环框架LoopTrans，用于在目视和非目视图像之间传输适应性知识，以增强交互理解。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding",
        "summary": "We introduce \\textbf{LongInsightBench}, the first benchmark designed to\nassess models' ability to understand long videos, with a focus on human\nlanguage, viewpoints, actions, and other contextual elements, while integrating\n\\textbf{visual, audio, and text} modalities. Our benchmark excels in three key\nareas: \\textbf{a) Long-Duration, Information-Dense Videos:} We carefully select\napproximately 1,000 videos from open-source datasets FineVideo based on\nduration limit and the information density of both visual and audio modalities,\nfocusing on content like lectures, interviews, and vlogs, which contain rich\nlanguage elements. \\textbf{b) Diverse and Challenging Task Scenarios:} We have\ndesigned six challenging task scenarios, including both Intra-Event and\nInter-Event Tasks. \\textbf{c) Rigorous and Comprehensive Quality Assurance\nPipelines:} We have developed a three-step, semi-automated data quality\nassurance pipeline to ensure the difficulty and validity of the synthesized\nquestions and answer options. Based on LongInsightBench, we designed a series\nof experiments. Experimental results shows that Omni-modal models(OLMs) still\nface challenge in tasks requiring precise temporal localization (T-Loc) and\nlong-range causal inference (CE-Caus). Extended experiments reveal the\ninformation loss and processing bias in multi-modal fusion of OLMs. Our dataset\nand code is available at\nhttps://anonymous.4open.science/r/LongInsightBench-910F/.",
        "url": "http://arxiv.org/abs/2510.17305v1",
        "published_date": "2025-10-20T08:49:10+00:00",
        "updated_date": "2025-10-20T08:49:10+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "ZhaoYang Han",
            "Qihan Lin",
            "Hao Liang",
            "Bowen Chen",
            "Zhou Liu",
            "Wentao Zhang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces LongInsightBench, a new benchmark for evaluating models on understanding long videos with a focus on human-centric elements across visual, audio, and text modalities. It reveals challenges faced by Omni-modal models in tasks requiring precise temporal localization and long-range causal inference.",
        "tldr_zh": "本文介绍了LongInsightBench，这是一个新的基准，用于评估模型对理解长视频的能力，侧重于包含视觉、音频和文本多模态元素的人类中心要素。 它揭示了在需要精确时间定位和长距因果推断任务中，千模态模型所面临的挑战。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models",
        "summary": "Current autonomous driving systems rely on specialized models for perceiving\nand predicting motion, which demonstrate reliable performance in standard\nconditions. However, generalizing cost-effectively to diverse real-world\nscenarios remains a significant challenge. To address this, we propose\nPlug-and-Forecast (PnF), a plug-and-play approach that augments existing motion\nforecasting models with multimodal large language models (MLLMs). PnF builds on\nthe insight that natural language provides a more effective way to describe and\nhandle complex scenarios, enabling quick adaptation to targeted behaviors. We\ndesign prompts to extract structured scene understanding from MLLMs and distill\nthis information into learnable embeddings to augment existing behavior\nprediction models. Our method leverages the zero-shot reasoning capabilities of\nMLLMs to achieve significant improvements in motion prediction performance,\nwhile requiring no fine-tuning -- making it practical to adopt. We validate our\napproach on two state-of-the-art motion forecasting models using the Waymo Open\nMotion Dataset and the nuScenes Dataset, demonstrating consistent performance\nimprovements across both benchmarks.",
        "url": "http://arxiv.org/abs/2510.17274v1",
        "published_date": "2025-10-20T08:01:29+00:00",
        "updated_date": "2025-10-20T08:01:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Katie Luo",
            "Jingwei Ji",
            "Tong He",
            "Runsheng Xu",
            "Yichen Xie",
            "Dragomir Anguelov",
            "Mingxing Tan"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces an approach called Plug-and-Forecast (PnF) that enhances motion forecasting in autonomous driving with multimodal large language models (MLLMs), achieving significant improvements without requiring fine-tuning.",
        "tldr_zh": "该论文介绍了一种名为Plug-and-Forecast (PnF)的方法，利用多模态大型语言模型（MLLMs）增强自动驾驶中的运动预测，实现显著改进而无需微调。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Taming Modality Entanglement in Continual Audio-Visual Segmentation",
        "summary": "Recently, significant progress has been made in multi-modal continual\nlearning, aiming to learn new tasks sequentially in multi-modal settings while\npreserving performance on previously learned ones. However, existing methods\nmainly focus on coarse-grained tasks, with limitations in addressing modality\nentanglement in fine-grained continual learning settings. To bridge this gap,\nwe introduce a novel Continual Audio-Visual Segmentation (CAVS) task, aiming to\ncontinuously segment new classes guided by audio. Through comprehensive\nanalysis, two critical challenges are identified: 1) multi-modal semantic\ndrift, where a sounding objects is labeled as background in sequential tasks;\n2) co-occurrence confusion, where frequent co-occurring classes tend to be\nconfused. In this work, a Collision-based Multi-modal Rehearsal (CMR) framework\nis designed to address these challenges. Specifically, for multi-modal semantic\ndrift, a Multi-modal Sample Selection (MSS) strategy is proposed to select\nsamples with high modal consistency for rehearsal. Meanwhile, for co-occurence\nconfusion, a Collision-based Sample Rehearsal (CSR) mechanism is designed,\nallowing for the increase of rehearsal sample frequency of those confusable\nclasses during training process. Moreover, we construct three audio-visual\nincremental scenarios to verify effectiveness of our method. Comprehensive\nexperiments demonstrate that our method significantly outperforms single-modal\ncontinual learning methods.",
        "url": "http://arxiv.org/abs/2510.17234v1",
        "published_date": "2025-10-20T07:23:36+00:00",
        "updated_date": "2025-10-20T07:23:36+00:00",
        "categories": [
            "cs.MM",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Yuyang Hong",
            "Qi Yang",
            "Tao Zhang",
            "Zili Wang",
            "Zhaojin Fu",
            "Kun Ding",
            "Bin Fan",
            "Shiming Xiang"
        ],
        "ai_categories": [
            "Multimodality",
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel framework, Collision-based Multi-modal Rehearsal, to address challenges in multimodal continual learning for audio-visual segmentation, showing significant performance improvements.",
        "tldr_zh": "本文提出了一种新框架，Collision-based Multi-modal Rehearsal，以解决音视频分割中的多模态连续学习挑战，表现出显著的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Capturing Head Avatar with Hand Contacts from a Monocular Video",
        "summary": "Photorealistic 3D head avatars are vital for telepresence, gaming, and VR.\nHowever, most methods focus solely on facial regions, ignoring natural\nhand-face interactions, such as a hand resting on the chin or fingers gently\ntouching the cheek, which convey cognitive states like pondering. In this work,\nwe present a novel framework that jointly learns detailed head avatars and the\nnon-rigid deformations induced by hand-face interactions.\n  There are two principal challenges in this task. First, naively tracking hand\nand face separately fails to capture their relative poses. To overcome this, we\npropose to combine depth order loss with contact regularization during pose\ntracking, ensuring correct spatial relationships between the face and hand.\nSecond, no publicly available priors exist for hand-induced deformations,\nmaking them non-trivial to learn from monocular videos. To address this, we\nlearn a PCA basis specific to hand-induced facial deformations from a face-hand\ninteraction dataset. This reduces the problem to estimating a compact set of\nPCA parameters rather than a full spatial deformation field. Furthermore,\ninspired by physics-based simulation, we incorporate a contact loss that\nprovides additional supervision, significantly reducing interpenetration\nartifacts and enhancing the physical plausibility of the results.\n  We evaluate our approach on RGB(D) videos captured by an iPhone.\nAdditionally, to better evaluate the reconstructed geometry, we construct a\nsynthetic dataset of avatars with various types of hand interactions. We show\nthat our method can capture better appearance and more accurate deforming\ngeometry of the face than SOTA surface reconstruction methods.",
        "url": "http://arxiv.org/abs/2510.17181v1",
        "published_date": "2025-10-20T05:55:18+00:00",
        "updated_date": "2025-10-20T05:55:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haonan He",
            "Yufeng Zheng",
            "Jie Song"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper presents a novel framework for capturing detailed head avatars and non-rigid deformations induced by hand-face interactions from monocular video, improving appearance and geometry reconstruction compared to existing methods.",
        "tldr_zh": "本文提出了一种新颖的框架，可以从单目视频中捕捉详细的头部头像和手-脸互动引起的非刚性变形，相比现有方法改善了外观和几何重建。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling",
        "summary": "Masked Autoregressive (MAR) models promise better efficiency in visual\ngeneration than autoregressive (AR) models for the ability of parallel\ngeneration, yet their acceleration potential remains constrained by the\nmodeling complexity of spatially correlated visual tokens in a single step. To\naddress this limitation, we introduce Generation then Reconstruction (GtR), a\ntraining-free hierarchical sampling strategy that decomposes generation into\ntwo stages: structure generation establishing global semantic scaffolding,\nfollowed by detail reconstruction efficiently completing remaining tokens.\nAssuming that it is more difficult to create an image from scratch than to\ncomplement images based on a basic image framework, GtR is designed to achieve\nacceleration by computing the reconstruction stage quickly while maintaining\nthe generation quality by computing the generation stage slowly. Moreover,\nobserving that tokens on the details of an image often carry more semantic\ninformation than tokens in the salient regions, we further propose\nFrequency-Weighted Token Selection (FTS) to offer more computation budget to\ntokens on image details, which are localized based on the energy of high\nfrequency information. Extensive experiments on ImageNet class-conditional and\ntext-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining\ncomparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1),\nsubstantially outperforming existing acceleration methods across various model\nscales and generation tasks. Our codes will be released in\nhttps://github.com/feihongyan1/GtR.",
        "url": "http://arxiv.org/abs/2510.17171v1",
        "published_date": "2025-10-20T05:22:10+00:00",
        "updated_date": "2025-10-20T05:22:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Feihong Yan",
            "Peiru Wang",
            "Yao Zhu",
            "Kaiyu Pang",
            "Qingyan Wei",
            "Huiqi Li",
            "Linfeng Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Other"
        ],
        "tldr": "The paper introduces a two-stage sampling strategy called Generation then Reconstruction (GtR) to accelerate Masked Autoregressive (MAR) models for image generation, achieving a 3.72x speedup with comparable quality on ImageNet and text-to-image generation tasks.",
        "tldr_zh": "本文介绍了一种名为Generation then Reconstruction (GtR)的两阶段抽样策略，用于加速Masked Autoregressive (MAR)模型的图像生成，在ImageNet和文本到图像生成任务上实现了3.72倍的加速，并保持了可比较的质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "ImaGGen: Zero-Shot Generation of Co-Speech Semantic Gestures Grounded in Language and Image Input",
        "summary": "Human communication combines speech with expressive nonverbal cues such as\nhand gestures that serve manifold communicative functions. Yet, current\ngenerative gesture generation approaches are restricted to simple, repetitive\nbeat gestures that accompany the rhythm of speaking but do not contribute to\ncommunicating semantic meaning. This paper tackles a core challenge in\nco-speech gesture synthesis: generating iconic or deictic gestures that are\nsemantically coherent with a verbal utterance. Such gestures cannot be derived\nfrom language input alone, which inherently lacks the visual meaning that is\noften carried autonomously by gestures. We therefore introduce a zero-shot\nsystem that generates gestures from a given language input and additionally is\ninformed by imagistic input, without manual annotation or human intervention.\nOur method integrates an image analysis pipeline that extracts key object\nproperties such as shape, symmetry, and alignment, together with a semantic\nmatching module that links these visual details to spoken text. An inverse\nkinematics engine then synthesizes iconic and deictic gestures and combines\nthem with co-generated natural beat gestures for coherent multimodal\ncommunication. A comprehensive user study demonstrates the effectiveness of our\napproach. In scenarios where speech alone was ambiguous, gestures generated by\nour system significantly improved participants' ability to identify object\nproperties, confirming their interpretability and communicative value. While\nchallenges remain in representing complex shapes, our results highlight the\nimportance of context-aware semantic gestures for creating expressive and\ncollaborative virtual agents or avatars, marking a substantial step forward\ntowards efficient and robust, embodied human-agent interaction. More\ninformation and example videos are available here:\nhttps://review-anon-io.github.io/ImaGGen.github.io/",
        "url": "http://arxiv.org/abs/2510.17617v1",
        "published_date": "2025-10-20T15:01:56+00:00",
        "updated_date": "2025-10-20T15:01:56+00:00",
        "categories": [
            "cs.HC",
            "cs.CV"
        ],
        "authors": [
            "Hendric Voss",
            "Stefan Kopp"
        ],
        "ai_categories": [
            "Multimodality",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper presents a zero-shot system for generating semantic gestures grounded in language and image input, improving multimodal communication effectiveness.",
        "tldr_zh": "本文介绍了一种零样本系统，用于生成基于语言和图像输入的语义手势，提高了多模态通信的效果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition Performance without Privacy Compromise",
        "summary": "The deployment of facial recognition systems has created an ethical dilemma:\nachieving high accuracy requires massive datasets of real faces collected\nwithout consent, leading to dataset retractions and potential legal liabilities\nunder regulations like GDPR. While synthetic facial data presents a promising\nprivacy-preserving alternative, the field lacks comprehensive empirical\nevidence of its viability. This study addresses this critical gap through\nextensive evaluation of synthetic facial recognition datasets. We present a\nsystematic literature review identifying 25 synthetic facial recognition\ndatasets (2018-2025), combined with rigorous experimental validation. Our\nmethodology examines seven key requirements for privacy-preserving synthetic\ndata: identity leakage prevention, intra-class variability, identity\nseparability, dataset scale, ethical data sourcing, bias mitigation, and\nbenchmark reliability. Through experiments involving over 10 million synthetic\nsamples, extended by a comparison of results reported on five standard\nbenchmarks, we provide the first comprehensive empirical assessment of\nsynthetic data's capability to replace real datasets. Best-performing synthetic\ndatasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and\n94.91% respectively, surpassing established real datasets including\nCASIA-WebFace (94.70%). While those images remain private, publicly available\nalternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our\nfindings reveal that they ensure proper intra-class variability while\nmaintaining identity separability. Demographic bias analysis shows that, even\nthough synthetic data inherits limited biases, it offers unprecedented control\nfor bias mitigation through generation parameters. These results establish\nsynthetic facial data as a scientifically viable and ethically imperative\nalternative for facial recognition research.",
        "url": "http://arxiv.org/abs/2510.17372v1",
        "published_date": "2025-10-20T10:08:53+00:00",
        "updated_date": "2025-10-20T10:08:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Paweł Borsukiewicz",
            "Fadi Boutros",
            "Iyiola E. Olatunji",
            "Charles Beumier",
            "Wendkûuni C. Ouedraogo",
            "Jacques Klein",
            "Tegawendé F. Bissyandé"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper evaluates synthetic facial recognition datasets as a privacy-preserving alternative to real datasets, showing promising results in terms of accuracy and bias mitigation.",
        "tldr_zh": "本文评估合成人脸识别数据集作为真实数据集的隐私保护替代方案，显示出在准确性和偏差缓解方面的有希望结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "FineVision: Open Data Is All You Need",
        "summary": "The advancement of vision-language models (VLMs) is hampered by a fragmented\nlandscape of inconsistent and contaminated public datasets. We introduce\nFineVision, a meticulously collected, curated, and unified corpus of 24 million\nsamples - the largest open resource of its kind. We unify more than 200 sources\ninto 185 subsets via a semi-automated, human-in-the-loop pipeline: automation\nperforms bulk ingestion and schema mapping, while reviewers audit mappings and\nspot-check outputs to verify faithful consumption of annotations, appropriate\nformatting and diversity, and safety; issues trigger targeted fixes and\nre-runs. The workflow further applies rigorous de-duplication within and across\nsources and decontamination against 66 public benchmarks. FineVision also\nencompasses agentic/GUI tasks with a unified action space; reviewers validate\nschemas and inspect a sample of trajectories to confirm executable fidelity.\nModels trained on FineVision consistently outperform those trained on existing\nopen mixtures across a broad evaluation suite, underscoring the benefits of\nscale, data hygiene, and balanced automation with human oversight. We release\nthe corpus and curation tools to accelerate data-centric VLM research.",
        "url": "http://arxiv.org/abs/2510.17269v1",
        "published_date": "2025-10-20T07:54:46+00:00",
        "updated_date": "2025-10-20T07:54:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Luis Wiedmann",
            "Orr Zohar",
            "Amir Mahla",
            "Xiaohan Wang",
            "Rui Li",
            "Thibaud Frere",
            "Leandro von Werra",
            "Aritra Roy Gosthipaty",
            "Andrés Marafioti"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "FineVision introduces a large, meticulously curated corpus of vision-language data to address issues of inconsistent and contaminated datasets, leading to improved model performance.",
        "tldr_zh": "FineVision引入了一个精心策划的视觉-语言数据库，以解决数据集不一致和污染的问题，从而提高模型性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Video Reasoning without Training",
        "summary": "Video reasoning using Large Multimodal Models (LMMs) relies on costly\nreinforcement learning (RL) and verbose chain-of-thought, resulting in\nsubstantial computational overhead during both training and inference.\nMoreover, the mechanisms that control the thinking process in these reasoning\nmodels are very limited. In this paper, using entropy of the model's output as\na signal, we discover that the high-quality models go through a series of\nmicro-explorations and micro-exploitations which keep the reasoning process\ngrounded (i.e., avoid excessive randomness while the model is exploring or\nthinking through an answer). We further observe that once this \"thinking\"\nprocess is over, more accurate models demonstrate a better convergence by\nreducing the entropy significantly via a final exploitation phase (i.e., a more\ncertain convergence towards a solution trajectory). We then use these novel,\ntheoretically-grounded insights to tune the model's behavior directly at\ninference, without using any RL or supervised fine-tuning. Specifically, during\ninference, our proposed approach called V-Reason (Video-Reason) adapts the\nvalue cache of the LMM via a few optimization steps on a small, trainable\ncontroller using an entropy-based objective, i.e., no supervision from any\ndataset or RL is necessary. This tuning improves the model's micro-exploration\nand exploitation behavior during inference. Our experiments show that our\nproposed method achieves significant improvements over the base\ninstruction-tuned models across several video reasoning datasets, narrowing the\ngap with RL-trained models to within 0.6% average accuracy without any\ntraining, while offering massive efficiency benefits: output tokens are reduced\nby 58.6% compared to the RL model.",
        "url": "http://arxiv.org/abs/2510.17045v1",
        "published_date": "2025-10-19T23:17:13+00:00",
        "updated_date": "2025-10-19T23:17:13+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Deepak Sridhar",
            "Kartikeya Bhardwaj",
            "Jeya Pradha Jeyaraj",
            "Nuno Vasconcelos",
            "Ankita Nayak",
            "Harris Teague"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a method called V-Reason that improves video reasoning models without the need for reinforcement learning. It achieves significant improvements in accuracy and efficiency compared to traditional RL-trained models.",
        "tldr_zh": "这篇论文提出了一种名为V-Reason的方法，可以改进视频推理模型，而无需使用强化学习。与传统的RL模型相比，它在准确性和效率上取得了显著的改进。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Glyph: Scaling Context Windows via Visual-Text Compression",
        "summary": "Large language models (LLMs) increasingly rely on long-context modeling for\ntasks such as document understanding, code analysis, and multi-step reasoning.\nHowever, scaling context windows to the million-token level brings prohibitive\ncomputational and memory costs, limiting the practicality of long-context LLMs.\nIn this work, we take a different perspective-visual context scaling-to tackle\nthis challenge. Instead of extending token-based sequences, we propose Glyph, a\nframework that renders long texts into images and processes them with\nvision-language models (VLMs). This approach substantially compresses textual\ninput while preserving semantic information, and we further design an\nLLM-driven genetic search to identify optimal visual rendering configurations\nfor balancing accuracy and compression. Through extensive experiments, we\ndemonstrate that our method achieves 3-4x token compression while maintaining\naccuracy comparable to leading LLMs such as Qwen3-8B on various long-context\nbenchmarks. This compression also leads to around 4x faster prefilling and\ndecoding, and approximately 2x faster SFT training. Furthermore, under extreme\ncompression, a 128K-context VLM could scale to handle 1M-token-level text\ntasks. In addition, the rendered text data benefits real-world multimodal\ntasks, such as document understanding. Our code and model are released at\nhttps://github.com/thu-coai/Glyph.",
        "url": "http://arxiv.org/abs/2510.17800v1",
        "published_date": "2025-10-20T17:58:56+00:00",
        "updated_date": "2025-10-20T17:58:56+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Jiale Cheng",
            "Yusen Liu",
            "Xinyu Zhang",
            "Yulin Fei",
            "Wenyi Hong",
            "Ruiliang Lyu",
            "Weihan Wang",
            "Zhe Su",
            "Xiaotao Gu",
            "Xiao Liu",
            "Yushi Bai",
            "Jie Tang",
            "Hongning Wang",
            "Minlie Huang"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces Glyph, a framework that renders long texts into images to compress textual input while preserving semantic information. It achieves significant token compression while maintaining accuracy comparable to leading language models on various tasks.",
        "tldr_zh": "该论文介绍了Glyph，一种将长文本渲染成图像以压缩文本输入同时保留语义信息的框架。它在保持准确性的同时实现了显著的令牌压缩。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference",
        "summary": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and\ntextual reasoning, powering applications across high-resolution image\nunderstanding, long-video analysis, and multi-turn conversation. However, their\nscalability remains limited by the growing number of visual tokens that\ndominate inference latency. We present SparseVILA, a new paradigm for efficient\nVLM inference that decouples visual sparsity across the prefilling and decoding\nstages. SparseVILA distributes sparsity across stages by pruning redundant\nvisual tokens during prefill and retrieving only query-relevant tokens during\ndecoding. This decoupled design matches leading prefill pruning methods while\npreserving multi-turn fidelity by retaining most of the visual cache so that\nquery-aware tokens can be retrieved at each conversation round. Built on an\nAWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster\nprefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end\nspeedup on long-context video tasks -- while improving accuracy on\ndocument-understanding and reasoning tasks. By decoupling query-agnostic\npruning and query-aware retrieval, SparseVILA establishes a new direction for\nefficient multimodal inference, offering a training-free, architecture-agnostic\nframework for accelerating large VLMs without sacrificing capability.",
        "url": "http://arxiv.org/abs/2510.17777v1",
        "published_date": "2025-10-20T17:35:47+00:00",
        "updated_date": "2025-10-20T17:35:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Samir Khaki",
            "Junxian Guo",
            "Jiaming Tang",
            "Shang Yang",
            "Yukang Chen",
            "Konstantinos N. Plataniotis",
            "Yao Lu",
            "Song Han",
            "Zhijian Liu"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality"
        ],
        "tldr": "SparseVILA presents a new method for efficient VLM inference by decoupling visual sparsity in prefilling and decoding stages, achieving faster inference while maintaining accuracy.",
        "tldr_zh": "SparseVILA提出了一种新的方法，通过在预填充和解码阶段解耦视觉稀疏性，实现更快的推断速度，同时保持准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Explainable Skin Cancer Classification: A Dual-Network Attention Model with Lesion Segmentation and Clinical Metadata Fusion",
        "summary": "Skin cancer is a life-threatening disease where early detection significantly\nimproves patient outcomes. Automated diagnosis from dermoscopic images is\nchallenging due to high intra-class variability and subtle inter-class\ndifferences. Many deep learning models operate as \"black boxes,\" limiting\nclinical trust. In this work, we propose a dual-encoder attention-based\nframework that leverages both segmented lesions and clinical metadata to\nenhance skin lesion classification in terms of both accuracy and\ninterpretability. A novel Deep-UNet architecture with Dual Attention Gates\n(DAG) and Atrous Spatial Pyramid Pooling (ASPP) is first employed to segment\nlesions. The classification stage uses two DenseNet201 encoders-one on the\noriginal image and another on the segmented lesion whose features are fused via\nmulti-head cross-attention. This dual-input design guides the model to focus on\nsalient pathological regions. In addition, a transformer-based module\nincorporates patient metadata (age, sex, lesion site) into the prediction. We\nevaluate our approach on the HAM10000 dataset and the ISIC 2018 and 2019\nchallenges. The proposed method achieves state-of-the-art segmentation\nperformance and significantly improves classification accuracy and average AUC\ncompared to baseline models. To validate our model's reliability, we use\nGradient-weighted Class Activation Mapping (Grad-CAM) to generate heatmaps.\nThese visualizations confirm that our model's predictions are based on the\nlesion area, unlike models that rely on spurious background features. These\nresults demonstrate that integrating precise lesion segmentation and clinical\ndata with attention-based fusion leads to a more accurate and interpretable\nskin cancer classification model.",
        "url": "http://arxiv.org/abs/2510.17773v1",
        "published_date": "2025-10-20T17:33:51+00:00",
        "updated_date": "2025-10-20T17:33:51+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Md. Enamul Atiq",
            "Shaikh Anowarul Fattah"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper proposes a dual-network attention model that combines lesion segmentation and clinical metadata for skin cancer classification, achieving state-of-the-art results in accuracy and interpretability.",
        "tldr_zh": "本文提出了一种双网络注意力模型，结合病变分割和临床元数据用于皮肤癌分类，实现了在准确性和可解释性方面的最新成果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs",
        "summary": "Vision-Language Models (VLMs) achieve strong results on multimodal tasks such\nas visual question answering, yet they can still fail even when the correct\nvisual evidence is present. In this work, we systematically investigate whether\nthese failures arise from not perceiving the evidence or from not leveraging it\neffectively. By examining layer-wise attention dynamics, we find that shallow\nlayers focus primarily on text, while deeper layers sparsely but reliably\nattend to localized evidence regions. Surprisingly, VLMs often perceive the\nvisual evidence when outputting incorrect answers, a phenomenon we term\n``seeing but not believing'' that widely exists in major VLM families. Building\non this, we introduce an inference-time intervention that highlights deep-layer\nevidence regions through selective attention-based masking. It requires no\ntraining and consistently improves accuracy across multiple families, including\nLLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable\nevidence internally but under-utilize it, making such signals explicit can\nbridge the gap between perception and reasoning, advancing the diagnostic\nunderstanding and reliability of VLMs.",
        "url": "http://arxiv.org/abs/2510.17771v1",
        "published_date": "2025-10-20T17:31:09+00:00",
        "updated_date": "2025-10-20T17:31:09+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Zhining Liu",
            "Ziyi Chen",
            "Hui Liu",
            "Chen Luo",
            "Xianfeng Tang",
            "Suhang Wang",
            "Joy Zeng",
            "Zhenwei Dai",
            "Zhan Shi",
            "Tianxin Wei",
            "Benoit Dumoulin",
            "Hanghang Tong"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper investigates why Vision-Language Models fail to use visual evidence effectively, proposing an intervention that highlights evidence regions to improve accuracy.",
        "tldr_zh": "本文研究了为什么视觉-语言模型在使用视觉证据时失败，并提出了一个突出证据区域的干预措施来提高准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition",
        "summary": "We address multi-reference visual place recognition (VPR), where reference\nsets captured under varying conditions are used to improve localisation\nperformance. While deep learning with large-scale training improves robustness,\nincreasing data diversity and model complexity incur extensive computational\ncost during training and deployment. Descriptor-level fusion via voting or\naggregation avoids training, but often targets multi-sensor setups or relies on\nheuristics with limited gains under appearance and viewpoint change. We propose\na training-free, descriptor-agnostic approach that jointly models places using\nmultiple reference descriptors via matrix decomposition into basis\nrepresentations, enabling projection-based residual matching. We also introduce\nSotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance\ndata, our method improves Recall@1 by up to ~18% over single-reference and\noutperforms multi-reference baselines across appearance and viewpoint changes,\nwith gains of ~5% on unstructured data, demonstrating strong generalisation\nwhile remaining lightweight.",
        "url": "http://arxiv.org/abs/2510.17739v1",
        "published_date": "2025-10-20T16:50:03+00:00",
        "updated_date": "2025-10-20T16:50:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Timur Ismagilov",
            "Shakaiba Majeed",
            "Michael Milford",
            "Tan Viet Tuyen Nguyen",
            "Sarvapali D. Ramchurn",
            "Shoaib Ehsan"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes a training-free, descriptor-agnostic approach for multi-reference visual place recognition using matrix factorization, achieving significant improvements in accuracy and generalization.",
        "tldr_zh": "本文提出了一种无需训练的，描述符无关的方法，通过矩阵分解实现多参考视觉地点识别，取得了显著的准确性和泛化性改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Can Image-To-Video Models Simulate Pedestrian Dynamics?",
        "summary": "Recent high-performing image-to-video (I2V) models based on variants of the\ndiffusion transformer (DiT) have displayed remarkable inherent world-modeling\ncapabilities by virtue of training on large scale video datasets. We\ninvestigate whether these models can generate realistic pedestrian movement\npatterns in crowded public scenes. Our framework conditions I2V models on\nkeyframes extracted from pedestrian trajectory benchmarks, then evaluates their\ntrajectory prediction performance using quantitative measures of pedestrian\ndynamics.",
        "url": "http://arxiv.org/abs/2510.17731v1",
        "published_date": "2025-10-20T16:44:40+00:00",
        "updated_date": "2025-10-20T16:44:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aaron Appelle",
            "Jerome P. Lynch"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper investigates the ability of image-to-video models to simulate realistic pedestrian movement patterns in crowded public scenes.",
        "tldr_zh": "本文调查了图像到视频模型在拥挤公共场景中模拟真实行人运动模式的能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues",
        "summary": "The recent development of Multimodal Large Language Models (MLLMs) has\nsignificantly advanced AI's ability to understand visual modalities. However,\nexisting evaluation benchmarks remain limited to single-turn question\nanswering, overlooking the complexity of multi-turn dialogues in real-world\nscenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video\nunderstanding benchmark for evaluating MLLMs in multi-turn dialogues.\nSpecifically, our MT-Video-Bench mainly assesses six core competencies that\nfocus on perceptivity and interactivity, encompassing 987 meticulously curated\nmulti-turn dialogues from diverse domains. These capabilities are rigorously\naligned with real-world applications, such as interactive sports analysis and\nmulti-turn video-based intelligent tutoring. With MT-Video-Bench, we\nextensively evaluate various state-of-the-art open-source and closed-source\nMLLMs, revealing their significant performance discrepancies and limitations in\nhandling multi-turn video dialogues. The benchmark will be publicly available\nto foster future research.",
        "url": "http://arxiv.org/abs/2510.17722v1",
        "published_date": "2025-10-20T16:38:40+00:00",
        "updated_date": "2025-10-20T16:38:40+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yaning Pan",
            "Zekun Wang",
            "Qianqian Xie",
            "Yongqian Wen",
            "Yuanxing Zhang",
            "Guohui Zhang",
            "Haoxuan Hu",
            "Zhiyu Pan",
            "Yibing Huang",
            "Zhidong Gan",
            "Yonghong Lin",
            "An Ping",
            "Tianhao Peng",
            "Jiaheng Liu"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "MT-Video-Bench is a new benchmark for evaluating Multimodal Large Language Models in multi-turn dialogues, focusing on perceptivity and interactivity in real-world applications.",
        "tldr_zh": "MT-Video-Bench是一个新的基准测试，用于评估多模态大型语言模型在多轮对话中的表现，着重于在现实应用中的感知和互动。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Automatic Classification of Circulating Blood Cell Clusters based on Multi-channel Flow Cytometry Imaging",
        "summary": "Circulating blood cell clusters (CCCs) containing red blood cells (RBCs),\nwhite blood cells(WBCs), and platelets are significant biomarkers linked to\nconditions like thrombosis, infection, and inflammation. Flow cytometry, paired\nwith fluorescence staining, is commonly used to analyze these cell clusters,\nrevealing cell morphology and protein profiles. While computational approaches\nbased on machine learning have advanced the automatic analysis of single-cell\nflow cytometry images, there is a lack of effort to build tools to\nautomatically analyze images containing CCCs. Unlike single cells, cell\nclusters often exhibit irregular shapes and sizes. In addition, these cell\nclusters often consist of heterogeneous cell types, which require multi-channel\nstaining to identify the specific cell types within the clusters. This study\nintroduces a new computational framework for analyzing CCC images and\nidentifying cell types within clusters. Our framework uses a two-step analysis\nstrategy. First, it categorizes images into cell cluster and non-cluster groups\nby fine-tuning the You Only Look Once(YOLOv11) model, which outperforms\ntraditional convolutional neural networks (CNNs), Vision Transformers (ViT).\nThen, it identifies cell types by overlaying cluster contours with regions from\nmulti-channel fluorescence stains, enhancing accuracy despite cell debris and\nstaining artifacts. This approach achieved over 95% accuracy in both cluster\nclassification and phenotype identification. In summary, our automated\nframework effectively analyzes CCC images from flow cytometry, leveraging both\nbright-field and fluorescence data. Initially tested on blood cells, it holds\npotential for broader applications, such as analyzing immune and tumor cell\nclusters, supporting cellular research across various diseases.",
        "url": "http://arxiv.org/abs/2510.17716v1",
        "published_date": "2025-10-20T16:32:23+00:00",
        "updated_date": "2025-10-20T16:32:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Suqiang Ma",
            "Subhadeep Sengupta",
            "Yao Lee",
            "Beikang Gu",
            "Xianyan Chen",
            "Xianqiao Wang",
            "Yang Liu",
            "Mengjia Xu",
            "Galit H. Frydman",
            "He Li"
        ],
        "ai_categories": [
            "Multimodality",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a new computational framework for automatic classification of circulating blood cell clusters based on multi-channel flow cytometry imaging, achieving high accuracy in cluster classification and cell type identification.",
        "tldr_zh": "该论文介绍了一种基于多通道流式细胞术成像的循环血细胞簇自动分类的新计算框架，在群集分类和细胞类型识别方面取得高准确度。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Improving Cross-Patient Generalization in Parkinson's Disease Detection through Chunk-Based Analysis of Hand-Drawn Patterns",
        "summary": "Parkinson's disease (PD) is a neurodegenerative disease affecting about 1% of\npeople over the age of 60, causing motor impairments that impede hand\ncoordination activities such as writing and drawing. Many approaches have tried\nto support early detection of Parkinson's disease based on hand-drawn images;\nhowever, we identified two major limitations in the related works: (1) the lack\nof sufficient datasets, (2) the robustness when dealing with unseen patient\ndata. In this paper, we propose a new approach to detect Parkinson's disease\nthat consists of two stages: The first stage classifies based on their drawing\ntype(circle, meander, spiral), and the second stage extracts the required\nfeatures from the images and detects Parkinson's disease. We overcame the\nprevious two limitations by applying a chunking strategy where we divide each\nimage into 2x2 chunks. Each chunk is processed separately when extracting\nfeatures and recognizing Parkinson's disease indicators. To make the final\nclassification, an ensemble method is used to merge the decisions made from\neach chunk. Our evaluation shows that our proposed approach outperforms the top\nperforming state-of-the-art approaches, in particular on unseen patients. On\nthe NewHandPD dataset our approach, it achieved 97.08% accuracy for seen\npatients and 94.91% for unseen patients, our proposed approach maintained a gap\nof only 2.17 percentage points, compared to the 4.76-point drop observed in\nprior work.",
        "url": "http://arxiv.org/abs/2510.17703v1",
        "published_date": "2025-10-20T16:18:36+00:00",
        "updated_date": "2025-10-20T16:18:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mhd Adnan Albani",
            "Riad Sonbol"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper presents a new approach for detecting Parkinson's disease through hand-drawn patterns by classifying drawing types and using a chunking strategy to improve generalization across patients.",
        "tldr_zh": "该论文通过对手绘图案进行分类和使用分块策略来改善跨患者的泛化, 提出了一种新的方法来检测帕金森病。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver",
        "summary": "While diffusion models achieve state-of-the-art generation quality, they\nstill suffer from computationally expensive sampling. Recent works address this\nissue with gradient-based optimization methods that distill a few-step ODE\ndiffusion solver from the full sampling process, reducing the number of\nfunction evaluations from dozens to just a few. However, these approaches often\nrely on intricate training techniques and do not explicitly focus on preserving\nfine-grained details. In this paper, we introduce the Generalized Solver: a\nsimple parameterization of the ODE sampler that does not require additional\ntraining tricks and improves quality over existing approaches. We further\ncombine the original distillation loss with adversarial training, which\nmitigates artifacts and enhances detail fidelity. We call the resulting method\nthe Generalized Adversarial Solver and demonstrate its superior performance\ncompared to existing solver training methods under similar resource\nconstraints. Code is available at https://github.com/3145tttt/GAS.",
        "url": "http://arxiv.org/abs/2510.17699v1",
        "published_date": "2025-10-20T16:14:38+00:00",
        "updated_date": "2025-10-20T16:14:38+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Aleksandr Oganov",
            "Ilya Bykov",
            "Eva Neudachina",
            "Mishan Aliev",
            "Alexander Tolmachev",
            "Alexander Sidorov",
            "Aleksandr Zuev",
            "Andrey Okhotin",
            "Denis Rakitin",
            "Aibek Alanov"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN"
        ],
        "tldr": "This paper introduces the Generalized Adversarial Solver (GAS) to improve the discretization of Diffusion ODEs, enhancing detail fidelity and performance compared to existing methods.",
        "tldr_zh": "本文介绍了广义对抗求解器（GAS），用于改进扩散ODEs的离散化，提高了细节保真度和性能，相比现有方法有所提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning",
        "summary": "Text-to-image person retrieval (TIPR) aims to identify the target person\nusing textual descriptions, facing challenge in modality heterogeneity. Prior\nworks have attempted to address it by developing cross-modal global or local\nalignment strategies. However, global methods typically overlook fine-grained\ncross-modal differences, whereas local methods require prior information to\nexplore explicit part alignments. Additionally, current methods are\nEnglish-centric, restricting their application in multilingual contexts. To\nalleviate these issues, we pioneer a multilingual TIPR task by developing a\nmultilingual TIPR benchmark, for which we leverage large language models for\ninitial translations and refine them by integrating domain-specific knowledge.\nCorrespondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation\nReasoning and Aligning framework to learn alignment across languages and\nmodalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module\nenables bidirectional prediction of masked image and text, implicitly enhancing\nthe modeling of local relations across languages and modalities, a\nmulti-dimensional global alignment module is integrated to bridge the modality\nheterogeneity. The proposed method achieves new state-of-the-art results on all\nmultilingual TIPR datasets. Data and code are presented in\nhttps://github.com/Flame-Chasers/Bi-IRRA.",
        "url": "http://arxiv.org/abs/2510.17685v1",
        "published_date": "2025-10-20T16:01:11+00:00",
        "updated_date": "2025-10-20T16:01:11+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Min Cao",
            "Xinyu Zhou",
            "Ding Jiang",
            "Bo Du",
            "Mang Ye",
            "Min Zhang"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a multilingual Text-to-Image Person Retrieval framework that leverages large language models and domain-specific knowledge to improve alignment across languages and modalities. It achieves state-of-the-art results on multilingual datasets.",
        "tldr_zh": "本文介绍了一种多语言文本到图像人物检索框架，利用大型语言模型和领域特定知识，改善了跨语言和模态之间的对齐。在多语言数据集上取得了最新的成果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PICABench: How Far Are We from Physically Realistic Image Editing?",
        "summary": "Image editing has achieved remarkable progress recently. Modern editing\nmodels could already follow complex instructions to manipulate the original\ncontent. However, beyond completing the editing instructions, the accompanying\nphysical effects are the key to the generation realism. For example, removing\nan object should also remove its shadow, reflections, and interactions with\nnearby objects. Unfortunately, existing models and benchmarks mainly focus on\ninstruction completion but overlook these physical effects. So, at this moment,\nhow far are we from physically realistic image editing? To answer this, we\nintroduce PICABench, which systematically evaluates physical realism across\neight sub-dimension (spanning optics, mechanics, and state transitions) for\nmost of the common editing operations (add, remove, attribute change, etc). We\nfurther propose the PICAEval, a reliable evaluation protocol that uses\nVLM-as-a-judge with per-case, region-level human annotations and questions.\nBeyond benchmarking, we also explore effective solutions by learning physics\nfrom videos and construct a training dataset PICA-100K. After evaluating most\nof the mainstream models, we observe that physical realism remains a\nchallenging problem with large rooms to explore. We hope that our benchmark and\nproposed solutions can serve as a foundation for future work moving from naive\ncontent editing toward physically consistent realism.",
        "url": "http://arxiv.org/abs/2510.17681v1",
        "published_date": "2025-10-20T15:53:57+00:00",
        "updated_date": "2025-10-20T15:53:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuandong Pu",
            "Le Zhuo",
            "Songhao Han",
            "Jinbo Xing",
            "Kaiwen Zhu",
            "Shuo Cao",
            "Bin Fu",
            "Si Liu",
            "Hongsheng Li",
            "Yu Qiao",
            "Wenlong Zhang",
            "Xi Chen",
            "Yihao Liu"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces PICABench to evaluate physical realism in image editing and proposes solutions to improve realism by incorporating physics from videos and constructing a training dataset.",
        "tldr_zh": "该论文介绍了PICABench用于评估图像编辑中的物理现实性，并提出通过从视频中学习物理知识和构建训练数据集来改善真实感的解决方案。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CaMiT: A Time-Aware Car Model Dataset for Classification and Generation",
        "summary": "AI systems must adapt to evolving visual environments, especially in domains\nwhere object appearances change over time. We introduce Car Models in Time\n(CaMiT), a fine-grained dataset capturing the temporal evolution of car models,\na representative class of technological artifacts. CaMiT includes 787K labeled\nsamples of 190 car models (2007-2023) and 5.1M unlabeled samples (2005-2023),\nsupporting both supervised and self-supervised learning. Static pretraining on\nin-domain data achieves competitive performance with large-scale generalist\nmodels while being more resource-efficient, yet accuracy declines when models\nare tested across years. To address this, we propose a time-incremental\nclassification setting, a realistic continual learning scenario with emerging,\nevolving, and disappearing classes. We evaluate two strategies:\ntime-incremental pretraining, which updates the backbone, and time-incremental\nclassifier learning, which updates only the final layer, both improving\ntemporal robustness. Finally, we explore time-aware image generation that\nleverages temporal metadata during training, yielding more realistic outputs.\nCaMiT offers a rich benchmark for studying temporal adaptation in fine-grained\nvisual recognition and generation.",
        "url": "http://arxiv.org/abs/2510.17626v1",
        "published_date": "2025-10-20T15:11:05+00:00",
        "updated_date": "2025-10-20T15:11:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Frédéric LIN",
            "Biruk Abere Ambaw",
            "Adrian Popescu",
            "Hejer Ammar",
            "Romaric Audigier",
            "Hervé Le Borgne"
        ],
        "ai_categories": [
            "Dataset",
            "AIGC"
        ],
        "tldr": "The paper introduces CaMiT, a dataset for studying the temporal evolution of car models and proposes time-incremental classification and generation techniques for improved performance.",
        "tldr_zh": "本文介绍了CaMiT，一个用于研究汽车模型的时间演化的数据集，并提出了时间增量分类和生成技术以提高性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling",
        "summary": "3D generation from natural language offers significant potential to reduce\nexpert manual modeling efforts and enhance accessibility to 3D assets. However,\nexisting methods often yield unstructured meshes and exhibit poor\ninteractivity, making them impractical for artistic workflows. To address these\nlimitations, we represent 3D assets as shape programs and introduce ShapeCraft,\na novel multi-agent framework for text-to-3D generation. At its core, we\npropose a Graph-based Procedural Shape (GPS) representation that decomposes\ncomplex natural language into a structured graph of sub-tasks, thereby\nfacilitating accurate LLM comprehension and interpretation of spatial\nrelationships and semantic shape details. Specifically, LLM agents\nhierarchically parse user input to initialize GPS, then iteratively refine\nprocedural modeling and painting to produce structured, textured, and\ninteractive 3D assets. Qualitative and quantitative experiments demonstrate\nShapeCraft's superior performance in generating geometrically accurate and\nsemantically rich 3D assets compared to existing LLM-based agents. We further\nshow the versatility of ShapeCraft through examples of animated and\nuser-customized editing, highlighting its potential for broader interactive\napplications.",
        "url": "http://arxiv.org/abs/2510.17603v1",
        "published_date": "2025-10-20T14:51:14+00:00",
        "updated_date": "2025-10-20T14:51:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuyuan Zhang",
            "Chenhan Jiang",
            "Zuoou Li",
            "Jiankang Deng"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces ShapeCraft, a multi-agent framework for generating structured, textured, and interactive 3D models from natural language. It outperforms existing methods in creating geometrically accurate and semantically rich assets.",
        "tldr_zh": "本文介绍了ShapeCraft，这是一个用于从自然语言生成结构化、纹理化和交互式3D模型的多智能体框架。它在创建几何精确和语义丰富的资产方面优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning",
        "summary": "Misinformation spreads across web platforms through billions of daily\nmultimodal posts that combine text and images, overwhelming manual\nfact-checking capacity. Supervised detection models require domain-specific\ntraining data and fail to generalize across diverse manipulation tactics. We\npresent MIRAGE, an inference-time, model-pluggable agentic framework that\ndecomposes multimodal verification into four sequential modules: visual\nveracity assessment detects AI-generated images, cross-modal consistency\nanalysis identifies out-of-context repurposing, retrieval-augmented factual\nchecking grounds claims in web evidence through iterative question generation,\nand a calibrated judgment module integrates all signals. MIRAGE orchestrates\nvision-language model reasoning with targeted web retrieval, outputs structured\nand citation-linked rationales. On MMFakeBench validation set (1,000 samples),\nMIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming\nthe strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65\npoints while maintaining 34.3% false positive rate versus 97.3% for a\njudge-only baseline. Test set results (5,000 samples) confirm generalization\nwith 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification\ncontributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97\npoints. Our results demonstrate that decomposed agentic reasoning with web\nretrieval can match supervised detector performance without domain-specific\ntraining, enabling misinformation detection across modalities where labeled\ndata remains scarce.",
        "url": "http://arxiv.org/abs/2510.17590v1",
        "published_date": "2025-10-20T14:40:26+00:00",
        "updated_date": "2025-10-20T14:40:26+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.CY",
            "cs.LG",
            "I.2.7; H.3.3; I.4.9"
        ],
        "authors": [
            "Mir Nafis Sharear Shopnil",
            "Sharad Duwal",
            "Abhishek Tyagi",
            "Adiba Mahbub Proma"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "MIRAGE is a framework for detecting misinformation in multimodal content using a combination of visual verification, cross-modal analysis, retrieval-based fact-checking, and judgment modules.",
        "tldr_zh": "MIRAGE 是一个用于检测多模态内容中的虚假信息的框架，结合了视觉验证、跨模态分析、基于检索的事实核实和判断模块。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Expose Camouflage in the Water: Underwater Camouflaged Instance Segmentation and Dataset",
        "summary": "With the development of underwater exploration and marine protection,\nunderwater vision tasks are widespread. Due to the degraded underwater\nenvironment, characterized by color distortion, low contrast, and blurring,\ncamouflaged instance segmentation (CIS) faces greater challenges in accurately\nsegmenting objects that blend closely with their surroundings. Traditional\ncamouflaged instance segmentation methods, trained on terrestrial-dominated\ndatasets with limited underwater samples, may exhibit inadequate performance in\nunderwater scenes. To address these issues, we introduce the first underwater\ncamouflaged instance segmentation (UCIS) dataset, abbreviated as UCIS4K, which\ncomprises 3,953 images of camouflaged marine organisms with instance-level\nannotations. In addition, we propose an Underwater Camouflaged Instance\nSegmentation network based on Segment Anything Model (UCIS-SAM). Our UCIS-SAM\nincludes three key modules. First, the Channel Balance Optimization Module\n(CBOM) enhances channel characteristics to improve underwater feature learning,\neffectively addressing the model's limited understanding of underwater\nenvironments. Second, the Frequency Domain True Integration Module (FDTIM) is\nproposed to emphasize intrinsic object features and reduce interference from\ncamouflage patterns, enhancing the segmentation performance of camouflaged\nobjects blending with their surroundings. Finally, the Multi-scale Feature\nFrequency Aggregation Module (MFFAM) is designed to strengthen the boundaries\nof low-contrast camouflaged instances across multiple frequency bands,\nimproving the model's ability to achieve more precise segmentation of\ncamouflaged objects. Extensive experiments on the proposed UCIS4K and public\nbenchmarks show that our UCIS-SAM outperforms state-of-the-art approaches.",
        "url": "http://arxiv.org/abs/2510.17585v1",
        "published_date": "2025-10-20T14:34:51+00:00",
        "updated_date": "2025-10-20T14:34:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chuhong Wang",
            "Hua Li",
            "Chongyi Li",
            "Huazhong Liu",
            "Xiongxin Tang",
            "Sam Kwong"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces the first underwater camouflaged instance segmentation dataset and a new network to improve segmentation performance in challenging underwater environments.",
        "tldr_zh": "本文介绍了第一个水下伪装实例分割数据集和一种新的网络，以提高在具有挑战性的水下环境中的分割性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception",
        "summary": "Recent 3D feed-forward models, such as the Visual Geometry Grounded\nTransformer (VGGT), have shown strong capability in inferring 3D attributes of\nstatic scenes. However, since they are typically trained on static datasets,\nthese models often struggle in real-world scenarios involving complex dynamic\nelements, such as moving humans or deformable objects like umbrellas. To\naddress this limitation, we introduce PAGE-4D, a feedforward model that extends\nVGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and\npoint cloud reconstruction -- all without post-processing. A central challenge\nin multi-task 4D reconstruction is the inherent conflict between tasks:\naccurate camera pose estimation requires suppressing dynamic regions, while\ngeometry reconstruction requires modeling them. To resolve this tension, we\npropose a dynamics-aware aggregator that disentangles static and dynamic\ninformation by predicting a dynamics-aware mask -- suppressing motion cues for\npose estimation while amplifying them for geometry reconstruction. Extensive\nexperiments show that PAGE-4D consistently outperforms the original VGGT in\ndynamic scenarios, achieving superior results in camera pose estimation,\nmonocular and video depth estimation, and dense point map reconstruction.",
        "url": "http://arxiv.org/abs/2510.17568v1",
        "published_date": "2025-10-20T14:17:16+00:00",
        "updated_date": "2025-10-20T14:17:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaichen Zhou",
            "Yuhan Wang",
            "Grace Chen",
            "Xinhai Chang",
            "Gaspard Beaudouin",
            "Fangneng Zhan",
            "Paul Pu Liang",
            "Mengyu Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "This paper introduces PAGE-4D, a model that extends VGGT to dynamic scenes for camera pose estimation, depth prediction, and point cloud reconstruction in 4D perception.",
        "tldr_zh": "本文介绍了PAGE-4D模型，将VGGT扩展到动态场景，用于4D感知中的摄像机姿势估计、深度预测和点云重建。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for Longitudinal MRI Segmentation",
        "summary": "Active Surveillance (AS) is a treatment option for managing low and\nintermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while\nmonitoring disease progression through serial MRI and clinical follow-up.\nAccurate prostate segmentation is an important preliminary step for automating\nthis process, enabling automated detection and diagnosis of PCa. However,\nexisting deep-learning segmentation models are often trained on\nsingle-time-point and expertly annotated datasets, making them unsuitable for\nlongitudinal AS analysis, where multiple time points and a scarcity of expert\nlabels hinder their effective fine-tuning. To address these challenges, we\npropose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation\narchitecture that computes the segmentation for time point t by leveraging the\nMRI and the corresponding segmentation mask from the previous time point. We\nintroduce two new components: (i) a Mamba-enhanced Cross-Attention Module,\nwhich integrates the Mamba block into cross attention to efficiently capture\ntemporal evolution and long-range spatial dependencies, and (ii) a Shape\nExtractor Module that encodes the previous segmentation mask into a latent\nanatomical representation for refined zone delination. Moreover, we introduce a\nsemi-supervised self-training strategy that leverages pseudo-labels generated\nfrom a pre-trained nnU-Net, enabling effective learning without expert\nannotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results\nshowed that it significantly outperforms state-of-the-art U-Net and\nTransformer-based models, achieving superior prostate zone segmentation even\nwhen trained on limited and noisy data.",
        "url": "http://arxiv.org/abs/2510.17529v1",
        "published_date": "2025-10-20T13:32:42+00:00",
        "updated_date": "2025-10-20T13:32:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Yovin Yahathugoda",
            "Davide Prezzi",
            "Piyalitt Ittichaiwong",
            "Vicky Goh",
            "Sebastien Ourselin",
            "Michela Antonelli"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "MambaX-Net is a novel deep learning model for automated prostate segmentation in longitudinal MRI analysis, outperforming existing models by leveraging previous segmentation masks and a semi-supervised self-training strategy.",
        "tldr_zh": "MambaX-Net是一个新颖的深度学习模型，用于在长期MRI分析中自动化的前列腺分割，通过利用先前的分割掩模和半监督自训练策略，优于现有模型。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models",
        "summary": "In recent years, large-scale generative models for visual content\n(\\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable\nprogress. However, training large-scale video generation models remains\nparticularly challenging and resource-intensive due to cross-modal text-video\nalignment, the long sequences involved, and the complex spatiotemporal\ndependencies. To address these challenges, we present a training framework that\noptimizes four pillars: (i) data processing, (ii) model architecture, (iii)\ntraining strategy, and (iv) infrastructure for large-scale video generation\nmodels. These optimizations delivered significant efficiency gains and\nperformance improvements across all stages of data preprocessing, video\ncompression, parameter scaling, curriculum-based pretraining, and\nalignment-focused post-training. Our resulting model, MUG-V 10B, matches recent\nstate-of-the-art video generators overall and, on e-commerce-oriented video\ngeneration tasks, surpasses leading open-source baselines in human evaluations.\nMore importantly, we open-source the complete stack, including model weights,\nMegatron-Core-based large-scale training code, and inference pipelines for\nvideo generation and enhancement. To our knowledge, this is the first public\nrelease of large-scale video generation training code that exploits\nMegatron-Core to achieve high training efficiency and near-linear multi-node\nscaling, details are available in\n\\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.",
        "url": "http://arxiv.org/abs/2510.17519v1",
        "published_date": "2025-10-20T13:20:37+00:00",
        "updated_date": "2025-10-20T13:20:37+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yongshun Zhang",
            "Zhongyi Fan",
            "Yonghang Zhang",
            "Zhangzikang Li",
            "Weifeng Chen",
            "Zhongwei Feng",
            "Chaoyue Wang",
            "Peng Hou",
            "Anxiang Zeng"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Transformer",
            "Other"
        ],
        "tldr": "The paper introduces a high-efficiency training pipeline for large video generation models, achieving significant gains in efficiency and performance. The resulting model, MUG-V 10B, outperforms leading baselines on e-commerce video generation tasks and is open-sourced with training code and inference pipelines.",
        "tldr_zh": "该论文介绍了一种针对大规模视频生成模型的高效训练流程，取得了显著的效率和性能提升。最终的模型MUG-V 10B在电子商务视频生成任务上优于主流基线，并向公众开放了训练代码和推断流程。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization",
        "summary": "With the rapid proliferation of video content across social media,\nsurveillance, and education platforms, efficiently summarizing long videos into\nconcise yet semantically faithful surrogates has become increasingly vital.\nExisting supervised methods achieve strong in-domain accuracy by learning from\ndense annotations but suffer from high labeling costs and limited cross-dataset\ngeneralization, while unsupervised approaches, though label-free, often fail to\ncapture high-level human semantics and fine-grained narrative cues. More\nrecently, zero-shot prompting pipelines have leveraged large language models\n(LLMs) for training-free video summarization, yet remain highly sensitive to\nhandcrafted prompt templates and dataset-specific score normalization. To\novercome these limitations, we introduce a rubric-guided, pseudo-labeled\nprompting framework that transforms a small subset of ground-truth annotations\ninto high-confidence pseudo labels, which are aggregated into structured,\ndataset-adaptive scoring rubrics guiding interpretable scene evaluation. During\ninference, first and last segments are scored based solely on their\ndescriptions, whereas intermediate ones incorporate brief contextual summaries\nof adjacent scenes to assess narrative progression and redundancy. This\ncontextual prompting enables the LLM to balance local salience and global\ncoherence without parameter tuning. On SumMe and TVSum, our method achieves F1\nscores of \\textbf{57.58} and \\textbf{63.05}, surpassing unsupervised and prior\nzero-shot baselines while approaching supervised performance. The results\ndemonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based\nscoring and establishes a general, interpretable zero-shot paradigm for video\nsummarization.",
        "url": "http://arxiv.org/abs/2510.17501v1",
        "published_date": "2025-10-20T12:54:32+00:00",
        "updated_date": "2025-10-20T12:54:32+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuanli Wu",
            "Long Zhang",
            "Yue Du",
            "Bin Li"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel approach for zero-shot video summarization using rubric-guided pseudo labeling, achieving competitive results compared to supervised and unsupervised methods.",
        "tldr_zh": "本文介绍了一种新颖的零样本视频摘要方法，使用基于评分标准的伪标记，在与监督和无监督方法相比取得了竞争性结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Rethinking Nighttime Image Deraining via Learnable Color Space Transformation",
        "summary": "Compared to daytime image deraining, nighttime image deraining poses\nsignificant challenges due to inherent complexities of nighttime scenarios and\nthe lack of high-quality datasets that accurately represent the coupling effect\nbetween rain and illumination. In this paper, we rethink the task of nighttime\nimage deraining and contribute a new high-quality benchmark, HQ-NightRain,\nwhich offers higher harmony and realism compared to existing datasets. In\naddition, we develop an effective Color Space Transformation Network (CST-Net)\nfor better removing complex rain from nighttime scenes. Specifically, we\npropose a learnable color space converter (CSC) to better facilitate rain\nremoval in the Y channel, as nighttime rain is more pronounced in the Y channel\ncompared to the RGB color space. To capture illumination information for\nguiding nighttime deraining, implicit illumination guidance is introduced\nenabling the learned features to improve the model's robustness in complex\nscenarios. Extensive experiments show the value of our dataset and the\neffectiveness of our method. The source code and datasets are available at\nhttps://github.com/guanqiyuan/CST-Net.",
        "url": "http://arxiv.org/abs/2510.17440v1",
        "published_date": "2025-10-20T11:28:43+00:00",
        "updated_date": "2025-10-20T11:28:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qiyuan Guan",
            "Xiang Chen",
            "Guiyue Jin",
            "Jiyu Jin",
            "Shumin Fan",
            "Tianyu Song",
            "Jinshan Pan"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces HQ-NightRain dataset and CST-Net for nighttime image deraining, focusing on rain removal in the Y channel and utilizing implicit illumination guidance for better results.",
        "tldr_zh": "本文介绍了HQ-NightRain数据集和CST-Net用于夜间图像去雨，重点是在Y通道中去除雨，并利用隐含的光照指导取得更好的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors",
        "summary": "Existing vision-language-action (VLA) models act in 3D real-world but are\ntypically built on 2D encoders, leaving a spatial reasoning gap that limits\ngeneralization and adaptability. Recent 3D integration techniques for VLAs\neither require specialized sensors and transfer poorly across modalities, or\ninject weak cues that lack geometry and degrade vision-language alignment. In\nthis work, we introduce FALCON (From Spatial to Action), a novel paradigm that\ninjects rich 3D spatial tokens into the action head. FALCON leverages spatial\nfoundation models to deliver strong geometric priors from RGB alone, and\nincludes an Embodied Spatial Model that can optionally fuse depth, or pose for\nhigher fidelity when available, without retraining or architectural changes. To\npreserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced\nAction Head rather than being concatenated into the vision-language backbone.\nThese designs enable FALCON to address limitations in spatial representation,\nmodality transferability, and alignment. In comprehensive evaluations across\nthree simulation benchmarks and eleven real-world tasks, our proposed FALCON\nachieves state-of-the-art performance, consistently surpasses competitive\nbaselines, and remains robust under clutter, spatial-prompt conditioning, and\nvariations in object scale and height.",
        "url": "http://arxiv.org/abs/2510.17439v1",
        "published_date": "2025-10-20T11:26:45+00:00",
        "updated_date": "2025-10-20T11:26:45+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Zhengshen Zhang",
            "Hao Li",
            "Yalun Dai",
            "Zhengbang Zhu",
            "Lei Zhou",
            "Chenchen Liu",
            "Dong Wang",
            "Francis E. H. Tay",
            "Sijin Chen",
            "Ziwei Liu",
            "Yuxiao Liu",
            "Xinghang Li",
            "Pan Zhou"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper introduces a new model, FALCON, that enhances vision-language-action models by incorporating 3D spatial tokens for improved spatial reasoning and alignment in various tasks.",
        "tldr_zh": "本文介绍了一种新的模型FALCON，通过引入3D空间标记，增强了视觉-语言-动作模型在各种任务中的空间推理和对齐能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DeepDetect: Learning All-in-One Dense Keypoints",
        "summary": "Keypoint detection is the foundation of many computer vision tasks, including\nimage registration, structure-from motion, 3D reconstruction, visual odometry,\nand SLAM. Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning\nbased methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong\nperformance yet suffer from key limitations: sensitivity to photometric\nchanges, low keypoint density and repeatability, limited adaptability to\nchallenging scenes, and lack of semantic understanding, often failing to\nprioritize visually important regions. We present DeepDetect, an intelligent,\nall-in-one, dense keypoint detector that unifies the strengths of classical\ndetectors using deep learning. Firstly, we create ground-truth masks by fusing\noutputs of 7 keypoint and 2 edge detectors, extracting diverse visual cues from\ncorners and blobs to prominent edges and textures in the images. Afterwards, a\nlightweight and efficient model: ESPNet, is trained using these masks as\nlabels, enabling DeepDetect to focus semantically on images while producing\nhighly dense keypoints, that are adaptable to diverse and visually degraded\nconditions. Evaluations on the Oxford Affine Covariant Regions dataset\ndemonstrate that DeepDetect surpasses other detectors in keypoint density,\nrepeatability, and the number of correct matches, achieving maximum values of\n0.5143 (average keypoint density), 0.9582 (average repeatability), and 59,003\n(correct matches).",
        "url": "http://arxiv.org/abs/2510.17422v1",
        "published_date": "2025-10-20T11:09:03+00:00",
        "updated_date": "2025-10-20T11:09:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shaharyar Ahmed Khan Tareen",
            "Filza Khan Tareen"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "DeepDetect is an all-in-one dense keypoint detector using deep learning to improve keypoint detection performance in computer vision tasks.",
        "tldr_zh": "DeepDetect是一个全能密集关键点检测器，利用深度学习改善计算机视觉任务中的关键点检测性能。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "MILES: Modality-Informed Learning Rate Scheduler for Balancing Multimodal Learning",
        "summary": "The aim of multimodal neural networks is to combine diverse data sources,\nreferred to as modalities, to achieve enhanced performance compared to relying\non a single modality. However, training of multimodal networks is typically\nhindered by modality overfitting, where the network relies excessively on one\nof the available modalities. This often yields sub-optimal performance,\nhindering the potential of multimodal learning and resulting in marginal\nimprovements relative to unimodal models. In this work, we present the\nModality-Informed Learning ratE Scheduler (MILES) for training multimodal joint\nfusion models in a balanced manner. MILES leverages the differences in\nmodality-wise conditional utilization rates during training to effectively\nbalance multimodal learning. The learning rate is dynamically adjusted during\ntraining to balance the speed of learning from each modality by the multimodal\nmodel, aiming for enhanced performance in both multimodal and unimodal\npredictions. We extensively evaluate MILES on four multimodal joint fusion\ntasks and compare its performance to seven state-of-the-art baselines. Our\nresults show that MILES outperforms all baselines across all tasks and fusion\nmethods considered in our study, effectively balancing modality usage during\ntraining. This results in improved multimodal performance and stronger modality\nencoders, which can be leveraged when dealing with unimodal samples or absent\nmodalities. Overall, our work highlights the impact of balancing multimodal\nlearning on improving model performance.",
        "url": "http://arxiv.org/abs/2510.17394v1",
        "published_date": "2025-10-20T10:34:59+00:00",
        "updated_date": "2025-10-20T10:34:59+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Alejandro Guerra-Manzanares",
            "Farah E. Shamout"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer"
        ],
        "tldr": "The paper introduces MILES, a learning rate scheduler for training multimodal joint fusion models in a balanced manner to enhance performance in both multimodal and unimodal predictions.",
        "tldr_zh": "本论文介绍了MILES，一种学习率调度器，用于以平衡的方式训练多模式联合融合模型，以提高多模式和单模式预测性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Latent Spaces Beyond Synthesis: From GANs to Diffusion Models",
        "summary": "This paper examines the evolving nature of internal representations in\ngenerative visual models, focusing on the conceptual and technical shift from\nGANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi's\naccount of synthesis as the amalgamation of distributed representations, we\npropose a distinction between \"synthesis in a strict sense\", where a compact\nlatent space wholly determines the generative process, and \"synthesis in a\nbroad sense,\" which characterizes models whose representational labor is\ndistributed across layers. Through close readings of model architectures and a\ntargeted experimental setup that intervenes in layerwise representations, we\nshow how diffusion models fragment the burden of representation and thereby\nchallenge assumptions of unified internal space. By situating these findings\nwithin media theoretical frameworks and critically engaging with metaphors such\nas the latent space and the Platonic Representation Hypothesis, we argue for a\nreorientation of how generative AI is understood: not as a direct synthesis of\ncontent, but as an emergent configuration of specialized processes.",
        "url": "http://arxiv.org/abs/2510.17383v1",
        "published_date": "2025-10-20T10:20:42+00:00",
        "updated_date": "2025-10-20T10:20:42+00:00",
        "categories": [
            "cs.LG",
            "cs.CV",
            "cs.CY"
        ],
        "authors": [
            "Ludovica Schaerf"
        ],
        "ai_categories": [
            "GAN",
            "Diffusion",
            "Other"
        ],
        "tldr": "The paper discusses the shift from GANs and VAEs to diffusion models in generative visual models, challenging the concept of a unified internal space.",
        "tldr_zh": "本文讨论了在生成视觉模型中从GAN和VAE到扩散模型的转变，挑战了统一内部空间的概念。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception",
        "summary": "Deploying real-time spatial perception on edge devices requires efficient\nmulti-task models that leverage complementary task information while minimizing\ncomputational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novel\nmulti-task learning framework designed for semantic segmentation and depth,\nedge, and surface normal estimation from a single monocular image. Unlike\nconventional approaches that rely on independent single-task models or shared\nencoder-decoder architectures, M2H introduces a Window-Based Cross-Task\nAttention Module that enables structured feature exchange while preserving\ntask-specific details, improving prediction consistency across tasks. Built on\na lightweight ViT-based DINOv2 backbone, M2H is optimized for real-time\ndeployment and serves as the foundation for monocular spatial perception\nsystems supporting 3D scene graph construction in dynamic environments.\nComprehensive evaluations show that M2H outperforms state-of-the-art multi-task\nmodels on NYUDv2, surpasses single-task depth and semantic baselines on\nHypersim, and achieves superior performance on the Cityscapes dataset, all\nwhile maintaining computational efficiency on laptop hardware. Beyond\nbenchmarks, M2H is validated on real-world data, demonstrating its practicality\nin spatial perception tasks.",
        "url": "http://arxiv.org/abs/2510.17363v1",
        "published_date": "2025-10-20T10:03:31+00:00",
        "updated_date": "2025-10-20T10:03:31+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "U. V. B. L Udugama",
            "George Vosselman",
            "Francesco Nex"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces M2H, a multi-task learning framework for spatial perception from monocular images, outperforming state-of-the-art models on various datasets while maintaining computational efficiency.",
        "tldr_zh": "本文介绍了M2H，一种多任务学习框架，用于从单目图像进行空间感知，它在各种数据集上表现优异，同时保持了计算效率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Exploring The Missing Semantics In Event Modality",
        "summary": "Event cameras offer distinct advantages such as low latency, high dynamic\nrange, and efficient motion capture. However, event-to-video reconstruction\n(E2V), a fundamental event-based vision task, remains challenging, particularly\nfor reconstructing and recovering semantic information. This is primarily due\nto the nature of the event camera, as it only captures intensity changes,\nignoring static objects and backgrounds, resulting in a lack of semantic\ninformation in captured event modality. Further, semantic information plays a\ncrucial role in video and frame reconstruction, yet is often overlooked by\nexisting E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V\nframework that explores the missing visual semantic knowledge in event modality\nand leverages it to enhance event-to-video reconstruction. Specifically,\nSemantic-E2VID introduces a cross-modal feature alignment (CFA) module to\ntransfer the robust visual semantics from a frame-based vision foundation\nmodel, the Segment Anything Model (SAM), to the event encoder, while aligning\nthe high-level features from distinct modalities. To better utilize the learned\nsemantic feature, we further propose a semantic-aware feature fusion (SFF)\nblock to integrate learned semantics in frame modality to form event\nrepresentations with rich semantics that can be decoded by the event decoder.\nFurther, to facilitate the reconstruction of semantic information, we propose a\nnovel Semantic Perceptual E2V Supervision that helps the model to reconstruct\nsemantic details by leveraging SAM-generated categorical labels. Extensive\nexperiments demonstrate that Semantic-E2VID significantly enhances frame\nquality, outperforming state-of-the-art E2V methods across multiple benchmarks.\nThe sample code is included in the supplementary material.",
        "url": "http://arxiv.org/abs/2510.17347v1",
        "published_date": "2025-10-20T09:45:13+00:00",
        "updated_date": "2025-10-20T09:45:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingqian Wu",
            "Shengpeng Xu",
            "Yunbo Jia",
            "Edmund Y. Lam"
        ],
        "ai_categories": [
            "Multimodality"
        ],
        "tldr": "The paper introduces Semantic-E2VID, a framework that enhances event-to-video reconstruction by incorporating missing visual semantic knowledge from event modality, outperforming existing methods.",
        "tldr_zh": "本文引入了Semantic-E2VID框架，通过从事件模态中整合缺失的视觉语义知识，提升了事件到视频的重建，优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA",
        "summary": "Image Quality Assessment (IQA) has progressed from scalar quality prediction\nto more interpretable, human-aligned evaluation paradigms. In this work, we\naddress the emerging challenge of detailed and explainable IQA by proposing\niDETEX-a unified multimodal large language model (MLLM) capable of\nsimultaneously performing three key tasks: quality grounding, perception, and\ndescription. To facilitate efficient and generalizable training across these\nheterogeneous subtasks, we design a suite of task-specific offline augmentation\nmodules and a data mixing strategy. These are further complemented by online\nenhancement strategies to fully exploit multi-sourced supervision. We validate\nour approach on the large-scale ViDA-UGC benchmark, where iDETEX achieves\nstate-of-the-art performance across all subtasks. Our model ranks first in the\nICCV MIPI 2025 Detailed Image Quality Assessment Challenge, demonstrating its\neffectiveness and robustness in delivering accurate and interpretable quality\nassessments.",
        "url": "http://arxiv.org/abs/2510.17332v1",
        "published_date": "2025-10-20T09:26:12+00:00",
        "updated_date": "2025-10-20T09:26:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhaoran Zhao",
            "Xinli Yue",
            "Jianhui Sun",
            "Yuhao Xie",
            "Tao Shao",
            "Liangchao Yao",
            "Fan Xia",
            "Yuetang Deng"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "iDETEX is a multimodal large language model designed for detailed and explainable image quality assessment, achieving state-of-the-art performance.",
        "tldr_zh": "iDETEX是一个面向详细和可解释的图像质量评估的多模态大型语言模型，实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SG-CLDFF: A Novel Framework for Automated White Blood Cell Classification and Segmentation",
        "summary": "Accurate segmentation and classification of white blood cells (WBCs) in\nmicroscopic images are essential for diagnosis and monitoring of many\nhematological disorders, yet remain challenging due to staining variability,\ncomplex backgrounds, and class imbalance. In this paper, we introduce a novel\nSaliency-Guided Cross-Layer Deep Feature Fusion framework (SG-CLDFF) that\ntightly integrates saliency-driven preprocessing with multi-scale deep feature\naggregation to improve both robustness and interpretability for WBC analysis.\nSG-CLDFF first computes saliency priors to highlight candidate WBC regions and\nguide subsequent feature extraction. A lightweight hybrid backbone\n(EfficientSwin-style) produces multi-resolution representations, which are\nfused by a ResNeXt-CC-inspired cross-layer fusion module to preserve\ncomplementary information from shallow and deep layers. The network is trained\nin a multi-task setup with concurrent segmentation and cell-type classification\nheads, using class-aware weighted losses and saliency-alignment regularization\nto mitigate imbalance and suppress background activation. Interpretability is\nenforced through Grad-CAM visualizations and saliency consistency checks,\nallowing model decisions to be inspected at the regional level. We validate the\nframework on standard public benchmarks (BCCD, LISC, ALL-IDB), reporting\nconsistent gains in IoU, F1, and classification accuracy compared to strong CNN\nand transformer baselines. An ablation study also demonstrates the individual\ncontributions of saliency preprocessing and cross-layer fusion. SG-CLDFF offers\na practical and explainable path toward more reliable automated WBC analysis in\nclinical workflows.",
        "url": "http://arxiv.org/abs/2510.17278v1",
        "published_date": "2025-10-20T08:07:39+00:00",
        "updated_date": "2025-10-20T08:07:39+00:00",
        "categories": [
            "cs.CV",
            "68T07, 92C55",
            "I.4.6; I.2.6"
        ],
        "authors": [
            "Mehdi Zekriyapanah Gashti",
            "Mostafa Mohammadpour",
            "Ghasem Farjamnia"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel framework (SG-CLDFF) for automated white blood cell classification and segmentation, achieving improved robustness and interpretability through saliency-driven preprocessing and cross-layer fusion.",
        "tldr_zh": "本文介绍了一种新颖的框架（SG-CLDFF）用于自动白细胞分类和分割，通过显著性驱动的预处理和跨层融合实现了改进的稳健性和解释性。",
        "relevance_score": 1,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Fair and Interpretable Deepfake Detection in Videos",
        "summary": "Existing deepfake detection methods often exhibit bias, lack transparency,\nand fail to capture temporal information, leading to biased decisions and\nunreliable results across different demographic groups. In this paper, we\npropose a fairness-aware deepfake detection framework that integrates temporal\nfeature learning and demographic-aware data augmentation to enhance fairness\nand interpretability. Our method leverages sequence-based clustering for\ntemporal modeling of deepfake videos and concept extraction to improve\ndetection reliability while also facilitating interpretable decisions for\nnon-expert users. Additionally, we introduce a demography-aware data\naugmentation method that balances underrepresented groups and applies\nfrequency-domain transformations to preserve deepfake artifacts, thereby\nmitigating bias and improving generalization. Extensive experiments on\nFaceForensics++, DFD, Celeb-DF, and DFDC datasets using state-of-the-art (SoTA)\narchitectures (Xception, ResNet) demonstrate the efficacy of the proposed\nmethod in obtaining the best tradeoff between fairness and accuracy when\ncompared to SoTA.",
        "url": "http://arxiv.org/abs/2510.17264v1",
        "published_date": "2025-10-20T07:50:22+00:00",
        "updated_date": "2025-10-20T07:50:22+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Akihito Yoshii",
            "Ryosuke Sonoda",
            "Ramya Srinivasan"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a fairness-aware deepfake detection framework that integrates temporal feature learning and demographic-aware data augmentation to enhance fairness and interpretability.",
        "tldr_zh": "本文提出一种关注公平性的深度伪造检测框架，整合了时间特征学习和人口统计信息感知数据增强以提高公平性和可解释性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models",
        "summary": "Recent advances in video diffusion models have significantly enhanced\ntext-to-video generation, particularly through alignment tuning using reward\nmodels trained on human preferences. While these methods improve visual\nquality, they can unintentionally encode and amplify social biases. To\nsystematically trace how such biases evolve throughout the alignment pipeline,\nwe introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating\nsocial representation in video generation. Grounded in established social bias\ntaxonomies, VideoBiasEval employs an event-based prompting strategy to\ndisentangle semantic content (actions and contexts) from actor attributes\n(gender and ethnicity). It further introduces multi-granular metrics to\nevaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity,\n(3) distributional shifts in social attributes across model variants, and (4)\nthe temporal persistence of bias within videos. Using this framework, we\nconduct the first end-to-end analysis connecting biases in human preference\ndatasets, their amplification in reward models, and their propagation through\nalignment-tuned video diffusion models. Our results reveal that alignment\ntuning not only strengthens representational biases but also makes them\ntemporally stable, producing smoother yet more stereotyped portrayals. These\nfindings highlight the need for bias-aware evaluation and mitigation throughout\nthe alignment process to ensure fair and socially responsible video generation.",
        "url": "http://arxiv.org/abs/2510.17247v1",
        "published_date": "2025-10-20T07:37:43+00:00",
        "updated_date": "2025-10-20T07:37:43+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Zefan Cai",
            "Haoyi Qiu",
            "Haozhe Zhao",
            "Ke Wan",
            "Jiachen Li",
            "Jiuxiang Gu",
            "Wen Xiao",
            "Nanyun Peng",
            "Junjie Hu"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset"
        ],
        "tldr": "The paper introduces a comprehensive framework to evaluate social biases in video generation models and highlights the need for bias-aware evaluation and mitigation in the alignment process.",
        "tldr_zh": "本文介绍了一个评估视频生成模型中社会偏见的全面框架，并强调在对齐过程中需要进行偏见感知评估和缓解。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "$\\mathcal{V}isi\\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs",
        "summary": "Multimodal Large Language Models (MLLMs) have achieved strong performance\nacross vision-language tasks, but suffer from significant computational\noverhead due to the quadratic growth of attention computations with the number\nof multimodal tokens. Though efforts have been made to prune tokens in MLLMs,\n\\textit{they lack a fundamental understanding of how MLLMs process and fuse\nmultimodal information.} Through systematic analysis, we uncover a\n\\textbf{three-stage} cross-modal interaction process: (1) Shallow layers\nrecognize task intent, with visual tokens acting as passive attention sinks;\n(2) Cross-modal fusion occurs abruptly in middle layers, driven by a few\ncritical visual tokens; (3) Deep layers discard vision tokens, focusing solely\non linguistic refinement. Based on these findings, we propose\n\\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\\% of\nvision-related attention computations and 53.9\\% of FLOPs on LLaVA-v1.5 7B. It\nsignificantly outperforms existing token pruning methods and generalizes across\ndiverse MLLMs. Beyond pruning, our insights further provide actionable\nguidelines for training efficient MLLMs by aligning model architecture with its\nintrinsic layer-wise processing dynamics. Our code is available at:\nhttps://github.com/EIT-NLP/VisiPruner.",
        "url": "http://arxiv.org/abs/2510.17205v1",
        "published_date": "2025-10-20T06:40:17+00:00",
        "updated_date": "2025-10-20T06:40:17+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Yingqi Fan",
            "Anhao Zhao",
            "Jinlan Fu",
            "Junlong Tong",
            "Hui Su",
            "Yijie Pan",
            "Wei Zhang",
            "Xiaoyu Shen"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer"
        ],
        "tldr": "The paper proposes VisiPruner, a training-free pruning framework for reducing vision-related attention computations in multimodal language models, showing significant performance improvement.",
        "tldr_zh": "本文提出了VisiPruner，一个用于减少多模态语言模型中与视觉相关的注意力计算的训练无关的修剪框架，显示出显著的性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Optimizing DINOv2 with Registers for Face Anti-Spoofing",
        "summary": "Face recognition systems are designed to be robust against variations in head\npose, illumination, and image blur during capture. However, malicious actors\ncan exploit these systems by presenting a face photo of a registered user,\npotentially bypassing the authentication process. Such spoofing attacks must be\ndetected prior to face recognition. In this paper, we propose a DINOv2-based\nspoofing attack detection method to discern minute differences between live and\nspoofed face images. Specifically, we employ DINOv2 with registers to extract\ngeneralizable features and to suppress perturbations in the attention\nmechanism, which enables focused attention on essential and minute features. We\ndemonstrate the effectiveness of the proposed method through experiments\nconducted on the dataset provided by ``The 6th Face Anti-Spoofing Workshop:\nUnified Physical-Digital Attacks Detection@ICCV2025'' and SiW dataset.",
        "url": "http://arxiv.org/abs/2510.17201v1",
        "published_date": "2025-10-20T06:27:02+00:00",
        "updated_date": "2025-10-20T06:27:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mika Feng",
            "Pierre Gallin-Martel",
            "Koichi Ito",
            "Takafumi Aoki"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes a face anti-spoofing method using DINOv2 with registers to detect spoofing attacks by focusing on minute differences between live and spoofed face images.",
        "tldr_zh": "本文提出了一种利用携带寄存器的DINOv2方法来检测伪造攻击的人脸反欺骗方法，重点关注活体和伪造人脸图像之间的微小差异。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Round Outcome Prediction in VALORANT Using Tactical Features from Video Analysis",
        "summary": "Recently, research on predicting match outcomes in esports has been actively\nconducted, but much of it is based on match log data and statistical\ninformation. This research targets the FPS game VALORANT, which requires\ncomplex strategies, and aims to build a round outcome prediction model by\nanalyzing minimap information in match footage. Specifically, based on the\nvideo recognition model TimeSformer, we attempt to improve prediction accuracy\nby incorporating detailed tactical features extracted from minimap information,\nsuch as character position information and other in-game events. This paper\nreports preliminary results showing that a model trained on a dataset augmented\nwith such tactical event labels achieved approximately 81% prediction accuracy,\nespecially from the middle phases of a round onward, significantly\noutperforming a model trained on a dataset with the minimap information itself.\nThis suggests that leveraging tactical features from match footage is highly\neffective for predicting round outcomes in VALORANT.",
        "url": "http://arxiv.org/abs/2510.17199v1",
        "published_date": "2025-10-20T06:23:36+00:00",
        "updated_date": "2025-10-20T06:23:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Nirai Hayakawa",
            "Kazumasa Shimari",
            "Kazuma Yamasaki",
            "Hirotatsu Hoshikawa",
            "Rikuto Tsuchida",
            "Kenichi Matsumoto"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper discusses predicting match outcomes in VALORANT using tactical features from video analysis, achieving 81% accuracy by leveraging detailed minimap information.",
        "tldr_zh": "本文讨论了使用视频分析的战术特征来预测VALORANT比赛结果，通过利用详细的迷你地图信息，实现了81%的准确率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From Pixels to People: Satellite-Based Mapping and Quantification of Riverbank Erosion and Lost Villages in Bangladesh",
        "summary": "The great rivers of Bangladesh, arteries of commerce and sustenance, are also\nagents of relentless destruction. Each year, they swallow whole villages and\nvast tracts of farmland, erasing communities from the map and displacing\nthousands of families. To track this slow-motion catastrophe has, until now,\nbeen a Herculean task for human analysts. Here we show how a powerful\ngeneral-purpose vision model, the Segment Anything Model (SAM), can be adapted\nto this task with remarkable precision. To do this, we assembled a new dataset\n- a digital chronicle of loss compiled from historical Google Earth imagery of\nBangladesh's most vulnerable regions, including Mokterer Char Union, Kedarpur\nUnion, Balchipara village, and Chowhali Upazila, from 2003 to 2025. Crucially,\nthis dataset is the first to include manually annotated data on the settlements\nthat have vanished beneath the water. Our method first uses a simple\ncolor-channel analysis to provide a rough segmentation of land and water, and\nthen fine-tunes SAM's mask decoder to recognize the subtle signatures of\nriverbank erosion. The resulting model demonstrates a keen eye for this\ndestructive process, achieving a mean Intersection over Union of 86.30% and a\nDice score of 92.60% - a performance that significantly surpasses traditional\nmethods and off-the-shelf deep learning models. This work delivers three key\ncontributions: the first annotated dataset of disappeared settlements in\nBangladesh due to river erosion; a specialized AI model fine-tuned for this\ncritical task; and a method for quantifying land loss with compelling visual\nevidence. Together, these tools provide a powerful new lens through which\npolicymakers and disaster management agencies can monitor erosion, anticipate\nits trajectory, and ultimately protect the vulnerable communities in its path.",
        "url": "http://arxiv.org/abs/2510.17198v1",
        "published_date": "2025-10-20T06:20:59+00:00",
        "updated_date": "2025-10-20T06:20:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "M Saifuzzaman Rafat",
            "Mohd Ruhul Ameen",
            "Akif Islam",
            "Abu Saleh Musa Miah",
            "Jungpil Shin"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper presents a method using satellite imagery and AI to track riverbank erosion and lost villages in Bangladesh with high precision.",
        "tldr_zh": "该论文提出了一种利用卫星图像和人工智能跟踪孟加拉国河岸侵蚀和消失村庄的方法，具有高精度。",
        "relevance_score": 1,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models",
        "summary": "As the capabilities of Vision-Language Models (VLMs) advance, they can\nprocess increasingly large inputs, which, unlike in LLMs, generates significant\nvisual token redundancy and leads to prohibitive inference costs. While many\nmethods aim to reduce these costs by pruning visual tokens, existing\napproaches, whether based on attention or diversity, typically neglect the\nguidance of the text prompt and thus fail to prioritize task relevance. In this\nwork, we propose a novel, zero-shot method that reframes the problem by\nintroducing a prompt-aware perspective, explicitly modeling visual token\npruning as a balance between task relevance and information diversity. Our\nhierarchical approach first selects a core set of task-relevant visual tokens\nand then supplements them with diversity tokens to preserve broader context.\nExperiments across multiple models and benchmarks show that our method achieves\nperformance that matches or surpasses the state-of-the-art with only minimal\naccuracy loss, even when pruning up to 90\\% of the tokens. Furthermore, these\ngains are accompanied by significant reductions in GPU memory footprint and\ninference latency.",
        "url": "http://arxiv.org/abs/2510.17197v1",
        "published_date": "2025-10-20T06:18:47+00:00",
        "updated_date": "2025-10-20T06:18:47+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Pu Zhang",
            "Yuwei Li",
            "Xingyuan Xian",
            "Guoming Tang"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper introduces a zero-shot method for pruning visual tokens in Vision-Language Models by considering task relevance and information diversity, achieving performance gains with minimal accuracy loss and reduced GPU memory usage.",
        "tldr_zh": "本文引入了一种零-shot 方法，通过考虑任务相关性和信息多样性，对视觉-语言模型中的视觉标记进行修剪，达到了性能提升，减少 GPU 内存使用的目的。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation",
        "summary": "Articulated objects, such as laptops and drawers, exhibit significant\nchallenges for 3D reconstruction and pose estimation due to their multi-part\ngeometries and variable joint configurations, which introduce structural\ndiversity across different states. To address these challenges, we propose\nKineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object\nShape Reconstruction and Generation, a unified framework for reconstructing\ndiverse articulated instances and pose estimation from single view input.\nSpecifically, we first encode complete geometry (SDFs), joint angles, and part\nsegmentation into a structured latent space via a novel Kinematic-Aware VAE\n(KA-VAE). In addition, we employ two conditional diffusion models: one for\nregressing global pose (SE(3)) and joint parameters, and another for generating\nthe kinematic-aware latent code from partial observations. Finally, we produce\nan iterative optimization module that bidirectionally refines reconstruction\naccuracy and kinematic parameters via Chamfer-distance minimization while\npreserving articulation constraints. Experimental results on synthetic,\nsemi-synthetic, and real-world datasets demonstrate the effectiveness of our\napproach in accurately reconstructing articulated objects and estimating their\nkinematic properties.",
        "url": "http://arxiv.org/abs/2510.17137v1",
        "published_date": "2025-10-20T04:15:40+00:00",
        "updated_date": "2025-10-20T04:15:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "WenBo Xu",
            "Liu Liu",
            "Li Zhang",
            "Ran Zhang",
            "Hao Wu",
            "Dan Guo",
            "Meng Wang"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces KineDiff3D, a framework for reconstructing articulated objects and estimating their kinematic properties using a novel Kinematic-Aware VAE and conditional diffusion models.",
        "tldr_zh": "该论文介绍了KineDiff3D，这是一个利用新颖的Kinematic-Aware VAE和条件扩散模型重建关节对象并估计其运动特性的框架。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection",
        "summary": "Recent advancements have explored text-to-image diffusion models for\nsynthesizing out-of-distribution (OOD) samples, substantially enhancing the\nperformance of OOD detection. However, existing approaches typically rely on\nperturbing text-conditioned embeddings, resulting in semantic instability and\ninsufficient shift diversity, which limit generalization to realistic OOD. To\naddress these challenges, we propose GOOD, a novel and flexible framework that\ndirectly guides diffusion sampling trajectories towards OOD regions using\noff-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level\nguidance: (1) Image-level guidance based on the gradient of log partition to\nreduce input likelihood, drives samples toward low-density regions in pixel\nspace. (2) Feature-level guidance, derived from k-NN distance in the\nclassifier's latent space, promotes sampling in feature-sparse regions. Hence,\nthis dual-guidance design enables more controllable and diverse OOD sample\ngeneration. Additionally, we introduce a unified OOD score that adaptively\ncombines image and feature discrepancies, enhancing detection robustness. We\nperform thorough quantitative and qualitative analyses to evaluate the\neffectiveness of GOOD, demonstrating that training with samples generated by\nGOOD can notably enhance OOD detection performance.",
        "url": "http://arxiv.org/abs/2510.17131v1",
        "published_date": "2025-10-20T03:58:46+00:00",
        "updated_date": "2025-10-20T03:58:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xin Gao",
            "Jiyao Liu",
            "Guanghao Li",
            "Yueming Lyu",
            "Jianxiong Gao",
            "Weichen Yu",
            "Ningsheng Xu",
            "Liang Wang",
            "Caifeng Shan",
            "Ziwei Liu",
            "Chenyang Si"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN"
        ],
        "tldr": "GOOD is a novel framework for generating diverse out-of-distribution samples using guided diffusion sampling, improving OOD detection performance.",
        "tldr_zh": "GOOD是一个新颖的框架，使用引导扩散抽样生成多样化的分布外样本，提高了分布外检测性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Matricial Free Energy as a Gaussianizing Regularizer: Enhancing Autoencoders for Gaussian Code Generation",
        "summary": "We introduce a novel regularization scheme for autoencoders based on\nmatricial free energy. Our approach defines a differentiable loss function in\nterms of the singular values of the code matrix (code dimension x batch size).\nFrom the standpoint of free probability an d random matrix theory, this loss\nachieves its minimum when the singular value distribution of the code matrix\ncoincides with that of an appropriately sculpted random metric with i.i.d.\nGaussian entries. Empirical simulations demonstrate that minimizing the\nnegative matricial free energy through standard stochastic gradient-based\ntraining yields Gaussian-like codes that generalize across training and test\nsets. Building on this foundation, we propose a matricidal free energy\nmaximizing autoencoder that reliably produces Gaussian codes and show its\napplication to underdetermined inverse problems.",
        "url": "http://arxiv.org/abs/2510.17120v1",
        "published_date": "2025-10-20T03:19:44+00:00",
        "updated_date": "2025-10-20T03:19:44+00:00",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "authors": [
            "Rishi Sonthalia",
            "Raj Rao Nadakuditi"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Other"
        ],
        "tldr": "The paper introduces a novel regularization scheme for autoencoders using matricial free energy to generate Gaussian-like codes for underdetermined inverse problems.",
        "tldr_zh": "该论文引入了一种利用矩阵自由能生成类似高斯分布编码的自动编码器的新型正规化方案，用于不定反问题。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Boosting Fidelity for Pre-Trained-Diffusion-Based Low-Light Image Enhancement via Condition Refinement",
        "summary": "Diffusion-based methods, leveraging pre-trained large models like Stable\nDiffusion via ControlNet, have achieved remarkable performance in several\nlow-level vision tasks. However, Pre-Trained Diffusion-Based (PTDB) methods\noften sacrifice content fidelity to attain higher perceptual realism. This\nissue is exacerbated in low-light scenarios, where severely degraded\ninformation caused by the darkness limits effective control. We identify two\nprimary causes of fidelity loss: the absence of suitable conditional latent\nmodeling and the lack of bidirectional interaction between the conditional\nlatent and noisy latent in the diffusion process. To address this, we propose a\nnovel optimization strategy for conditioning in pre-trained diffusion models,\nenhancing fidelity while preserving realism and aesthetics. Our method\nintroduces a mechanism to recover spatial details lost during VAE encoding,\ni.e., a latent refinement pipeline incorporating generative priors.\nAdditionally, the refined latent condition interacts dynamically with the noisy\nlatent, leading to improved restoration performance. Our approach is\nplug-and-play, seamlessly integrating into existing diffusion networks to\nprovide more effective control. Extensive experiments demonstrate significant\nfidelity improvements in PTDB methods.",
        "url": "http://arxiv.org/abs/2510.17105v1",
        "published_date": "2025-10-20T02:40:06+00:00",
        "updated_date": "2025-10-20T02:40:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaogang Xu",
            "Jian Wang",
            "Yunfan Lu",
            "Ruihang Chu",
            "Ruixing Wang",
            "Jiafei Wu",
            "Bei Yu",
            "Liang Lin"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN"
        ],
        "tldr": "The paper proposes a novel optimization strategy for improving the fidelity of low-light image enhancement using pre-trained diffusion models.",
        "tldr_zh": "本文提出了一种新颖的优化策略，以提高使用预训练扩散模型进行低光图像增强的保真度。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Shape-aware Inertial Poser: Motion Tracking for Humans with Diverse Shapes Using Sparse Inertial Sensors",
        "summary": "Human motion capture with sparse inertial sensors has gained significant\nattention recently. However, existing methods almost exclusively rely on a\ntemplate adult body shape to model the training data, which poses challenges\nwhen generalizing to individuals with largely different body shapes (such as a\nchild). This is primarily due to the variation in IMU-measured acceleration\ncaused by changes in body shape. To fill this gap, we propose Shape-aware\nInertial Poser (SAIP), the first solution considering body shape differences in\nsparse inertial-based motion capture. Specifically, we decompose the sensor\nmeasurements related to shape and pose in order to effectively model their\njoint correlations. Firstly, we train a regression model to transfer the\nIMU-measured accelerations of a real body to match the template adult body\nmodel, compensating for the shape-related sensor measurements. Then, we can\neasily follow the state-of-the-art methods to estimate the full body motions of\nthe template-shaped body. Finally, we utilize a second regression model to map\nthe joint velocities back to the real body, combined with a shape-aware\nphysical optimization strategy to calculate global motions on the subject.\nFurthermore, our method relies on body shape awareness, introducing the first\ninertial shape estimation scheme. This is accomplished by modeling the\nshape-conditioned IMU-pose correlation using an MLP-based network. To validate\nthe effectiveness of SAIP, we also present the first IMU motion capture dataset\ncontaining individuals of different body sizes. This dataset features 10\nchildren and 10 adults, with heights ranging from 110 cm to 190 cm, and a total\nof 400 minutes of paired IMU-Motion samples. Extensive experimental results\ndemonstrate that SAIP can effectively handle motion capture tasks for diverse\nbody shapes. The code and dataset are available at\nhttps://github.com/yinlu5942/SAIP.",
        "url": "http://arxiv.org/abs/2510.17101v1",
        "published_date": "2025-10-20T02:20:31+00:00",
        "updated_date": "2025-10-20T02:20:31+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Lu Yin",
            "Ziying Shi",
            "Yinghao Wu",
            "Xinyu Yi",
            "Feng Xu",
            "Shihui Guo"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper introduces a shape-aware inertial poser for motion tracking of humans with diverse body shapes using sparse inertial sensors.",
        "tldr_zh": "该论文介绍了一种形状感知惯性姿态估计器，用于利用稀疏惯性传感器跟踪具有不同体形的人体运动。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards a Generalizable Fusion Architecture for Multimodal Object Detection",
        "summary": "Multimodal object detection improves robustness in chal- lenging conditions\nby leveraging complementary cues from multiple sensor modalities. We introduce\nFiltered Multi- Modal Cross Attention Fusion (FMCAF), a preprocess- ing\narchitecture designed to enhance the fusion of RGB and infrared (IR) inputs.\nFMCAF combines a frequency- domain filtering block (Freq-Filter) to suppress\nredun- dant spectral features with a cross-attention-based fusion module (MCAF)\nto improve intermodal feature sharing. Unlike approaches tailored to specific\ndatasets, FMCAF aims for generalizability, improving performance across\ndifferent multimodal challenges without requiring dataset- specific tuning. On\nLLVIP (low-light pedestrian detec- tion) and VEDAI (aerial vehicle detection),\nFMCAF outper- forms traditional fusion (concatenation), achieving +13.9% mAP@50\non VEDAI and +1.1% on LLVIP. These results support the potential of FMCAF as a\nflexible foundation for robust multimodal fusion in future detection pipelines.",
        "url": "http://arxiv.org/abs/2510.17078v1",
        "published_date": "2025-10-20T01:19:54+00:00",
        "updated_date": "2025-10-20T01:19:54+00:00",
        "categories": [
            "cs.CV",
            "I.2.10; I.4.8"
        ],
        "authors": [
            "Jad Berjawi",
            "Yoann Dupas",
            "Christophe C'erin"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer"
        ],
        "tldr": "The paper introduces FMCAF, a fusion architecture for multimodal object detection that aims for generalizability and improves performance across different challenges without dataset-specific tuning.",
        "tldr_zh": "本文介绍了FMCAF，这是一个用于多模态目标检测的融合架构，旨在提高性能并在不需要特定数据集调整的情况下通过不同挑战。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Conditional Synthetic Live and Spoof Fingerprint Generation",
        "summary": "Large fingerprint datasets, while important for training and evaluation, are\ntime-consuming and expensive to collect and require strict privacy measures.\nResearchers are exploring the use of synthetic fingerprint data to address\nthese issues. This paper presents a novel approach for generating synthetic\nfingerprint images (both spoof and live), addressing concerns related to\nprivacy, cost, and accessibility in biometric data collection. Our approach\nutilizes conditional StyleGAN2-ADA and StyleGAN3 architectures to produce\nhigh-resolution synthetic live fingerprints, conditioned on specific finger\nidentities (thumb through little finger). Additionally, we employ CycleGANs to\ntranslate these into realistic spoof fingerprints, simulating a variety of\npresentation attack materials (e.g., EcoFlex, Play-Doh). These synthetic spoof\nfingerprints are crucial for developing robust spoof detection systems. Through\nthese generative models, we created two synthetic datasets (DB2 and DB3), each\ncontaining 1,500 fingerprint images of all ten fingers with multiple\nimpressions per finger, and including corresponding spoofs in eight material\ntypes. The results indicate robust performance: our StyleGAN3 model achieves a\nFr\\'echet Inception Distance (FID) as low as 5, and the generated fingerprints\nachieve a True Accept Rate of 99.47% at a 0.01% False Accept Rate. The\nStyleGAN2-ADA model achieved a TAR of 98.67% at the same 0.01% FAR. We assess\nfingerprint quality using standard metrics (NFIQ2, MINDTCT), and notably,\nmatching experiments confirm strong privacy preservation, with no significant\nevidence of identity leakage, confirming the strong privacy-preserving\nproperties of our synthetic datasets.",
        "url": "http://arxiv.org/abs/2510.17035v1",
        "published_date": "2025-10-19T22:44:21+00:00",
        "updated_date": "2025-10-19T22:44:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Syed Konain Abbas",
            "Sandip Purnapatra",
            "M. G. Sarwar Murshed",
            "Conor Miller-Lynch",
            "Lambert Igene",
            "Soumyabrata Dey",
            "Stephanie Schuckers",
            "Faraz Hussain"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper introduces a novel method for generating synthetic fingerprint images, both live and spoof, using generative models. The results show high performance in terms of quality and privacy preservation.",
        "tldr_zh": "该论文介绍了一种使用生成模型生成合成指纹图像的新方法，包括真实和伪造指纹。结果显示出高质量和保护隐私的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding",
        "summary": "Multimodal 3D grounding has garnered considerable interest in Vision-Language\nModels (VLMs) \\cite{yin2025spatial} for advancing spatial reasoning in complex\nenvironments. However, these models suffer from a severe \"2D semantic bias\"\nthat arises from over-reliance on 2D image features for coarse localization,\nlargely disregarding 3D geometric inputs and resulting in suboptimal fusion\nperformance. In this paper, we propose a novel training framework called\nWhat-Where Representation Re-Forming (W2R2) to tackle this issue via\ndisentangled representation learning and targeted shortcut suppression. Our\napproach fundamentally reshapes the model's internal space by designating 2D\nfeatures as semantic beacons for \"What\" identification and 3D features as\nspatial anchors for \"Where\" localization, enabling precise 3D grounding without\nmodifying inference architecture. Key components include a dual-objective loss\nfunction with an Alignment Loss that supervises fused predictions using adapted\ncross-entropy for multimodal synergy, and a Pseudo-Label Loss that penalizes\noverly effective 2D-dominant pseudo-outputs via a margin-based mechanism.\nExperiments conducted on ScanRefer and ScanQA demonstrate the effectiveness of\nW2R2, with significant gains in localization accuracy and robustness,\nparticularly in cluttered outdoor scenes.",
        "url": "http://arxiv.org/abs/2510.17034v1",
        "published_date": "2025-10-19T22:40:18+00:00",
        "updated_date": "2025-10-19T22:40:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yutong Zhong"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a novel training framework to improve 3D grounding in Vision-Language Models by separating 2D and 3D features for better localization accuracy in cluttered outdoor scenes.",
        "tldr_zh": "本文提出了一种新的训练框架，通过将2D和3D特征分开，以提高视觉-语言模型中的3D定位准确性，特别是在混乱的室外场景中。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enrich and Detect: Video Temporal Grounding with Multimodal LLMs",
        "summary": "We introduce ED-VTG, a method for fine-grained video temporal grounding\nutilizing multi-modal large language models. Our approach harnesses the\ncapabilities of multimodal LLMs to jointly process text and video, in order to\neffectively localize natural language queries in videos through a two-stage\nprocess. Rather than being directly grounded, language queries are initially\ntransformed into enriched sentences that incorporate missing details and cues\nto aid in grounding. In the second stage, these enriched queries are grounded,\nusing a lightweight decoder, which specializes at predicting accurate\nboundaries conditioned on contextualized representations of the enriched\nqueries. To mitigate noise and reduce the impact of hallucinations, our model\nis trained with a multiple-instance-learning objective that dynamically selects\nthe optimal version of the query for each training sample. We demonstrate\nstate-of-the-art results across various benchmarks in temporal video grounding\nand paragraph grounding settings. Experiments reveal that our method\nsignificantly outperforms all previously proposed LLM-based temporal grounding\napproaches and is either superior or comparable to specialized models, while\nmaintaining a clear advantage against them in zero-shot evaluation scenarios.",
        "url": "http://arxiv.org/abs/2510.17023v1",
        "published_date": "2025-10-19T22:12:45+00:00",
        "updated_date": "2025-10-19T22:12:45+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Shraman Pramanick",
            "Effrosyni Mavroudi",
            "Yale Song",
            "Rama Chellappa",
            "Lorenzo Torresani",
            "Triantafyllos Afouras"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a method for video temporal grounding using multimodal large language models, achieving state-of-the-art results in various benchmarks.",
        "tldr_zh": "本文介绍了一种利用多模大语言模型进行视频时间定位的方法，在各种基准测试中取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "An empirical study of the effect of video encoders on Temporal Video Grounding",
        "summary": "Temporal video grounding is a fundamental task in computer vision, aiming to\nlocalize a natural language query in a long, untrimmed video. It has a key role\nin the scientific community, in part due to the large amount of video generated\nevery day. Although we find extensive work in this task, we note that research\nremains focused on a small selection of video representations, which may lead\nto architectural overfitting in the long run. To address this issue, we propose\nan empirical study to investigate the impact of different video features on a\nclassical architecture. We extract features for three well-known benchmarks,\nCharades-STA, ActivityNet-Captions and YouCookII, using video encoders based on\nCNNs, temporal reasoning and transformers. Our results show significant\ndifferences in the performance of our model by simply changing the video\nencoder, while also revealing clear patterns and errors derived from the use of\ncertain features, ultimately indicating potential feature complementarity.",
        "url": "http://arxiv.org/abs/2510.17007v1",
        "published_date": "2025-10-19T21:10:43+00:00",
        "updated_date": "2025-10-19T21:10:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ignacio M. De la Jara",
            "Cristian Rodriguez-Opazo",
            "Edison Marrese-Taylor",
            "Felipe Bravo-Marquez"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper investigates the effect of different video encoders on temporal video grounding, showing significant performance differences and potential feature complementarity.",
        "tldr_zh": "本文研究不同视频编码器对时间视频定位的影响，展示了显著的性能差异和潜在的特征互补性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Training-free Online Video Step Grounding",
        "summary": "Given a task and a set of steps composing it, Video Step Grounding (VSG) aims\nto detect which steps are performed in a video. Standard approaches for this\ntask require a labeled training set (e.g., with step-level annotations or\nnarrations), which may be costly to collect. Moreover, they process the full\nvideo offline, limiting their applications for scenarios requiring online\ndecisions. Thus, in this work, we explore how to perform VSG online and without\ntraining. We achieve this by exploiting the zero-shot capabilities of recent\nLarge Multimodal Models (LMMs). In particular, we use LMMs to predict the step\nassociated with a restricted set of frames, without access to the whole video.\nWe show that this online strategy without task-specific tuning outperforms\noffline and training-based models. Motivated by this finding, we develop\nBayesian Grounding with Large Multimodal Models (BaGLM), further injecting\nknowledge of past frames into the LMM-based predictions. BaGLM exploits\nBayesian filtering principles, modeling step transitions via (i) a dependency\nmatrix extracted through large language models and (ii) an estimation of step\nprogress. Experiments on three datasets show superior performance of BaGLM over\nstate-of-the-art training-based offline methods.",
        "url": "http://arxiv.org/abs/2510.16989v1",
        "published_date": "2025-10-19T20:11:52+00:00",
        "updated_date": "2025-10-19T20:11:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Luca Zanella",
            "Massimiliano Mancini",
            "Yiming Wang",
            "Alessio Tonioni",
            "Elisa Ricci"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a training-free online video step grounding method that outperforms traditional offline and training-based models by utilizing large multimodal models with zero-shot capabilities.",
        "tldr_zh": "该论文介绍了一种无需训练的在线视频步骤定位方法，通过利用具有零射击能力的大型多模态模型，优于传统的脱机和基于训练的模型。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "One-step Diffusion Models with Bregman Density Ratio Matching",
        "summary": "Diffusion and flow models achieve high generative quality but remain\ncomputationally expensive due to slow multi-step sampling. Distillation methods\naccelerate them by training fast student generators, yet most existing\nobjectives lack a unified theoretical foundation. In this work, we propose\nDi-Bregman, a compact framework that formulates diffusion distillation as\nBregman divergence-based density-ratio matching. This convex-analytic view\nconnects several existing objectives through a common lens. Experiments on\nCIFAR-10 and text-to-image generation demonstrate that Di-Bregman achieves\nimproved one-step FID over reverse-KL distillation and maintains high visual\nfidelity compared to the teacher model. Our results highlight Bregman\ndensity-ratio matching as a practical and theoretically-grounded route toward\nefficient one-step diffusion generation.",
        "url": "http://arxiv.org/abs/2510.16983v1",
        "published_date": "2025-10-19T20:00:54+00:00",
        "updated_date": "2025-10-19T20:00:54+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Yuanzhi Zhu",
            "Eleftherios Tsonis",
            "Lucas Degeorge",
            "Vicky Kalogeiton"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper proposes Di-Bregman, a framework for diffusion distillation using Bregman divergence-based density-ratio matching, achieving efficient one-step diffusion generation with improved FID on CIFAR-10 and text-to-image generation.",
        "tldr_zh": "本文提出了Di-Bregman，一种使用Bregman差异基于密度比匹配的扩散精馏框架，实现了在CIFAR-10和文本到图像生成上改进的FID的高效单步扩散生成。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis",
        "summary": "Recent advancements in artificial intelligence (AI), particularly foundation\nmodels (FMs), have revolutionized medical image analysis, demonstrating strong\nzero- and few-shot performance across diverse medical imaging tasks, from\nsegmentation to report generation. Unlike traditional task-specific AI models,\nFMs leverage large corpora of labeled and unlabeled multimodal datasets to\nlearn generalized representations that can be adapted to various downstream\nclinical applications with minimal fine-tuning. However, despite the rapid\nproliferation of FM research in medical imaging, the field remains fragmented,\nlacking a unified synthesis that systematically maps the evolution of\narchitectures, training paradigms, and clinical applications across modalities.\nTo address this gap, this review article provides a comprehensive and\nstructured analysis of FMs in medical image analysis. We systematically\ncategorize studies into vision-only and vision-language FMs based on their\narchitectural foundations, training strategies, and downstream clinical tasks.\nAdditionally, a quantitative meta-analysis of the studies was conducted to\ncharacterize temporal trends in dataset utilization and application domains. We\nalso critically discuss persistent challenges, including domain adaptation,\nefficient fine-tuning, computational constraints, and interpretability along\nwith emerging solutions such as federated learning, knowledge distillation, and\nadvanced prompting. Finally, we identify key future research directions aimed\nat enhancing the robustness, explainability, and clinical integration of FMs,\nthereby accelerating their translation into real-world medical practice.",
        "url": "http://arxiv.org/abs/2510.16973v1",
        "published_date": "2025-10-19T19:19:23+00:00",
        "updated_date": "2025-10-19T19:19:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "physics.med-ph"
        ],
        "authors": [
            "Praveenbalaji Rajendran",
            "Mojtaba Safari",
            "Wenfeng He",
            "Mingzhe Hu",
            "Shansong Wang",
            "Jun Zhou",
            "Xiaofeng Yang"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper reviews foundation models in medical image analysis, highlighting their strong performance and broad adaptability across clinical applications.",
        "tldr_zh": "这篇文章回顾了医学图像分析中的基础模型，强调它们在临床应用中的强大性能和广泛适应性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Conveying Meaning through Gestures: An Investigation into Semantic Co-Speech Gesture Generation",
        "summary": "This study explores two frameworks for co-speech gesture generation, AQ-GT\nand its semantically-augmented variant AQ-GT-a, to evaluate their ability to\nconvey meaning through gestures and how humans perceive the resulting\nmovements. Using sentences from the SAGA spatial communication corpus,\ncontextually similar sentences, and novel movement-focused sentences, we\nconducted a user-centered evaluation of concept recognition and human-likeness.\nResults revealed a nuanced relationship between semantic annotations and\nperformance. The original AQ-GT framework, lacking explicit semantic input, was\nsurprisingly more effective at conveying concepts within its training domain.\nConversely, the AQ-GT-a framework demonstrated better generalization,\nparticularly for representing shape and size in novel contexts. While\nparticipants rated gestures from AQ-GT-a as more expressive and helpful, they\ndid not perceive them as more human-like. These findings suggest that explicit\nsemantic enrichment does not guarantee improved gesture generation and that its\neffectiveness is highly dependent on the context, indicating a potential\ntrade-off between specialization and generalization.",
        "url": "http://arxiv.org/abs/2510.17599v1",
        "published_date": "2025-10-20T14:47:56+00:00",
        "updated_date": "2025-10-20T14:47:56+00:00",
        "categories": [
            "cs.HC",
            "cs.CV"
        ],
        "authors": [
            "Hendric Voss",
            "Lisa Michelle Bohnenkamp",
            "Stefan Kopp"
        ],
        "ai_categories": [
            "Multimodality"
        ],
        "tldr": "The study investigates two frameworks for gesture generation in conveying meaning, revealing a nuanced relationship between semantic annotations and performance.",
        "tldr_zh": "这项研究探讨了两种手势生成框架在传达含义方面的能力，揭示了语义注释和性能之间微妙的关系。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.75
    },
    {
        "title": "Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats",
        "summary": "Commercial plant phenotyping systems using fixed cameras cannot perceive many\nplant details due to leaf occlusion. In this paper, we present Botany-Bot, a\nsystem for building detailed \"annotated digital twins\" of living plants using\ntwo stereo cameras, a digital turntable inside a lightbox, an industrial robot\narm, and 3D segmentated Gaussian Splat models. We also present robot algorithms\nfor manipulating leaves to take high-resolution indexable images of occluded\ndetails such as stem buds and the underside/topside of leaves. Results from\nexperiments suggest that Botany-Bot can segment leaves with 90.8% accuracy,\ndetect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and\ntake detailed overside/underside images with 77.3% accuracy. Code, videos, and\ndatasets are available at https://berkeleyautomation.github.io/Botany-Bot/.",
        "url": "http://arxiv.org/abs/2510.17783v1",
        "published_date": "2025-10-20T17:42:20+00:00",
        "updated_date": "2025-10-20T17:42:20+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Simeon Adebola",
            "Chung Min Kim",
            "Justin Kerr",
            "Shuangyu Xie",
            "Prithvi Akella",
            "Jose Luis Susa Rincon",
            "Eugen Solowjow",
            "Ken Goldberg"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "Botany-Bot presents a system for detailed monitoring of occluded plant structures using digital twins and Gaussian Splat models.",
        "tldr_zh": "Botany-Bot通过数字孪生和高斯斑点模型，提供了一种监测植物结构的系统。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Signature Forgery Detection: Improving Cross-Dataset Generalization",
        "summary": "Automated signature verification is a critical biometric technique used in\nbanking, identity authentication, and legal documentation. Despite the notable\nprogress achieved by deep learning methods, most approaches in offline\nsignature verification still struggle to generalize across datasets, as\nvariations in handwriting styles and acquisition protocols often degrade\nperformance. This study investigates feature learning strategies for signature\nforgery detection, focusing on improving cross-dataset generalization -- that\nis, model robustness when trained on one dataset and tested on another. Using\nthree public benchmarks -- CEDAR, ICDAR, and GPDS Synthetic -- two experimental\npipelines were developed: one based on raw signature images and another\nemploying a preprocessing method referred to as shell preprocessing. Several\nbehavioral patterns were identified and analyzed; however, no definitive\nsuperiority between the two approaches was established. The results show that\nthe raw-image model achieved higher performance across benchmarks, while the\nshell-based model demonstrated promising potential for future refinement toward\nrobust, cross-domain signature verification.",
        "url": "http://arxiv.org/abs/2510.17724v1",
        "published_date": "2025-10-20T16:42:21+00:00",
        "updated_date": "2025-10-20T16:42:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Matheus Ramos Parracho"
        ],
        "ai_categories": [
            "Dataset",
            "GAN"
        ],
        "tldr": "The paper explores feature learning strategies for signature forgery detection to improve cross-dataset generalization, showing promising results for future refinement in signature verification.",
        "tldr_zh": "本文探讨了用于签名伪造检测的特征学习策略，以改善跨数据集泛化，显示出未来在签名验证方面有潜在改进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7.5
    },
    {
        "title": "Towards 3D Objectness Learning in an Open World",
        "summary": "Recent advancements in 3D object detection and novel category detection have\nmade significant progress, yet research on learning generalized 3D objectness\nremains insufficient. In this paper, we delve into learning open-world 3D\nobjectness, which focuses on detecting all objects in a 3D scene, including\nnovel objects unseen during training. Traditional closed-set 3D detectors\nstruggle to generalize to open-world scenarios, while directly incorporating 3D\nopen-vocabulary models for open-world ability struggles with vocabulary\nexpansion and semantic overlap. To achieve generalized 3D object discovery, We\npropose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect\nany objects within 3D scenes without relying on hand-crafted text prompts. We\nintroduce the strong generalization and zero-shot capabilities of 2D foundation\nmodels, utilizing both 2D semantic priors and 3D geometric priors for\nclass-agnostic proposals to broaden 3D object discovery. Then, by integrating\ncomplementary information from point cloud and RGB image in the cross-modal\nmixture of experts, OP3Det dynamically routes uni-modal and multi-modal\nfeatures to learn generalized 3D objectness. Extensive experiments demonstrate\nthe extraordinary performance of OP3Det, which significantly surpasses existing\nopen-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement\ncompared to closed-world 3D detectors.",
        "url": "http://arxiv.org/abs/2510.17686v1",
        "published_date": "2025-10-20T16:01:20+00:00",
        "updated_date": "2025-10-20T16:01:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Taichi Liu",
            "Zhenyu Wang",
            "Ruofeng Liu",
            "Guang Wang",
            "Desheng Zhang"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces OP3Det, a novel 3D object detector that can detect any objects in a 3D scene, including novel objects unseen during training, without relying on hand-crafted text prompts. It outperforms existing open-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement compared to closed-world 3D detectors.",
        "tldr_zh": "本文介绍了OP3Det，一种新颖的3D物体检测器，可以在3D场景中检测任何物体，包括训练时未见过的新物体，而无需依赖手工文本提示。它的性能优于现有的开放世界3D检测器高达16.0％的绝对退化率，和關閉性3D探測器相比提高了13.5％。",
        "relevance_score": 3,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection",
        "summary": "Road crack detection is essential for intelligent infrastructure maintenance\nin smart cities. To reduce reliance on costly pixel-level annotations, we\npropose WP-CrackNet, an end-to-end weakly-supervised method that trains with\nonly image-level labels for pixel-wise crack detection. WP-CrackNet integrates\nthree components: a classifier generating class activation maps (CAMs), a\nreconstructor measuring feature inferability, and a detector producing\npixel-wise road crack detection results. During training, the classifier and\nreconstructor alternate in adversarial learning to encourage crack CAMs to\ncover complete crack regions, while the detector learns from pseudo labels\nderived from post-processed crack CAMs. This mutual feedback among the three\ncomponents improves learning stability and detection accuracy. To further boost\ndetection performance, we design a path-aware attention module (PAAM) that\nfuses high-level semantics from the classifier with low-level structural cues\nfrom the reconstructor by modeling spatial and channel-wise dependencies.\nAdditionally, a center-enhanced CAM consistency module (CECCM) is proposed to\nrefine crack CAMs using center Gaussian weighting and consistency constraints,\nenabling better pseudo-label generation. We create three image-level datasets\nand extensive experiments show that WP-CrackNet achieves comparable results to\nsupervised methods and outperforms existing weakly-supervised methods,\nsignificantly advancing scalable road inspection. The source code package and\ndatasets are available at https://mias.group/WP-CrackNet/.",
        "url": "http://arxiv.org/abs/2510.17566v1",
        "published_date": "2025-10-20T14:13:26+00:00",
        "updated_date": "2025-10-20T14:13:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nachuan Ma",
            "Zhengfei Song",
            "Qiang Hu",
            "Xiaoyu Tang",
            "Chengxi Zhang",
            "Rui Fan",
            "Lihua Xie"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "WP-CrackNet is a weakly-supervised method for road crack detection using image-level labels, achieving comparable results to supervised methods and outperforming existing weakly-supervised methods.",
        "tldr_zh": "WP-CrackNet是一种使用图像级标签进行道路裂缝检测的弱监督方法，实现了与监督方法相媲美的结果，并超越了现有的弱监督方法。",
        "relevance_score": 2,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "Machine Vision-Based Surgical Lighting System:Design and Implementation",
        "summary": "Effortless and ergonomically designed surgical lighting is critical for\nprecision and safety during procedures. However, traditional systems often rely\non manual adjustments, leading to surgeon fatigue, neck strain, and\ninconsistent illumination due to drift and shadowing. To address these\nchallenges, we propose a novel surgical lighting system that leverages the\nYOLOv11 object detection algorithm to identify a blue marker placed above the\ntarget surgical site. A high-power LED light source is then directed to the\nidentified location using two servomotors equipped with tilt-pan brackets. The\nYOLO model achieves 96.7% mAP@50 on the validation set consisting of annotated\nimages simulating surgical scenes with the blue spherical marker. By automating\nthe lighting process, this machine vision-based solution reduces physical\nstrain on surgeons, improves consistency in illumination, and supports improved\nsurgical outcomes.",
        "url": "http://arxiv.org/abs/2510.17287v1",
        "published_date": "2025-10-20T08:22:45+00:00",
        "updated_date": "2025-10-20T08:22:45+00:00",
        "categories": [
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Amir Gharghabi",
            "Mahdi Hakiminezhad",
            "Maryam Shafaei",
            "Shaghayegh Gharghabi"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces a machine vision-based surgical lighting system that automates the lighting process to reduce physical strain on surgeons and improve consistency in illumination during procedures.",
        "tldr_zh": "本文介绍了一种基于机器视觉的手术照明系统，可以自动化照明过程，减轻外科医生的身体负担，提高手术过程中照明的一致性。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery",
        "summary": "Generalized Category Discovery (GCD) aims to classify test-time samples into\neither seen categories** -- available during training -- or novel ones, without\nrelying on label supervision. Most existing GCD methods assume simultaneous\naccess to labeled and unlabeled data during training and arising from the same\ndomain, limiting applicability in open-world scenarios involving distribution\nshifts. Domain Generalization with GCD (DG-GCD) lifts this constraint by\nrequiring models to generalize to unseen domains containing novel categories,\nwithout accessing targetdomain data during training. The only prior DG-GCD\nmethod, DG2CD-Net, relies on episodic training with multiple synthetic domains\nand task vector aggregation, incurring high computational cost and error\naccumulation. We propose HIDISC, a hyperbolic representation learning framework\nthat achieves domain and category-level generalization without episodic\nsimulation. To expose the model to minimal but diverse domain variations, we\naugment the source domain using GPT-guided diffusion, avoiding overfitting\nwhile maintaining efficiency. To structure the representation space, we\nintroduce Tangent CutMix, a curvature-aware interpolation that synthesizes\npseudo-novel samples in tangent space, preserving manifold consistency. A\nunified loss -- combining penalized Busemann alignment, hybrid hyperbolic\ncontrastive regularization, and adaptive outlier repulsion -- **facilitates\ncompact, semantically structured embeddings. A learnable curvature parameter\nfurther adapts the geometry to dataset complexity. HIDISC achieves\nstate-of-the-art results on PACS , Office-Home , and DomainNet, consistently\noutperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.",
        "url": "http://arxiv.org/abs/2510.17188v1",
        "published_date": "2025-10-20T06:08:33+00:00",
        "updated_date": "2025-10-20T06:08:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Vaibhav Rathore",
            "Divyam Gupta",
            "Biplab Banerjee"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a hyperbolic framework called HIDISC for domain generalization with generalized category discovery, achieving state-of-the-art results by structuring the representation space without episodic simulation.",
        "tldr_zh": "本文提出了一种名为HIDISC的双曲框架，用于领域泛化与广义类别发现，通过在不进行情节式模拟的情况下构建表示空间，实现了最先进的结果。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7.5
    },
    {
        "title": "GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image",
        "summary": "Generating editable, parametric CAD models from a single image holds great\npotential to lower the barriers of industrial concept design. However, current\nmulti-modal large language models (MLLMs) still struggle with accurately\ninferring 3D geometry from 2D images due to limited spatial reasoning\ncapabilities. We address this limitation by introducing GACO-CAD, a novel\ntwo-stage post-training framework. It is designed to achieve a joint objective:\nsimultaneously improving the geometric accuracy of the generated CAD models and\nencouraging the use of more concise modeling procedures. First, during\nsupervised fine-tuning, we leverage depth and surface normal maps as dense\ngeometric priors, combining them with the RGB image to form a multi-channel\ninput. In the context of single-view reconstruction, these priors provide\ncomplementary spatial cues that help the MLLM more reliably recover 3D geometry\nfrom 2D observations. Second, during reinforcement learning, we introduce a\ngroup length reward that, while preserving high geometric fidelity, promotes\nthe generation of more compact and less redundant parametric modeling\nsequences. A simple dynamic weighting strategy is adopted to stabilize\ntraining. Experiments on the DeepCAD and Fusion360 datasets show that GACO-CAD\nachieves state-of-the-art performance under the same MLLM backbone,\nconsistently outperforming existing methods in terms of code validity,\ngeometric accuracy, and modeling conciseness.",
        "url": "http://arxiv.org/abs/2510.17157v1",
        "published_date": "2025-10-20T04:57:20+00:00",
        "updated_date": "2025-10-20T04:57:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yinghui Wang",
            "Xinyu Zhang",
            "Peng Du"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces GACO-CAD, a two-stage framework for generating editable CAD models from a single image, outperforming existing methods in terms of code validity, geometric accuracy, and modeling conciseness.",
        "tldr_zh": "本文介绍了GACO-CAD，一种从单个图像生成可编辑CAD模型的两阶段框架，在代码有效性，几何精度和建模简洁性方面优于现有方法。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "How Universal Are SAM2 Features?",
        "summary": "The trade-off between general-purpose foundation vision models and their\nspecialized counterparts is critical for efficient feature coding design and is\nnot yet fully understood. We investigate this trade-off by comparing the\nfeature versatility of the general-purpose Hiera encoder against the\nsegmentation-specialized Segment Anything Model 2 (SAM2). Using a lightweight,\ntrainable neck to probe the adaptability of their frozen features, we quantify\nthe information-theoretic cost of specialization. Our results reveal that while\nSAM2's specialization is highly effective for spatially-related tasks like\ndepth estimation, it comes at a cost. The specialized SAM2 encoder\nunderperforms its generalist predecessor, Hiera, on conceptually distant tasks\nsuch as pose estimation and image captioning, demonstrating a measurable loss\nof broader semantic information. A novel cross-neck analysis on SAM2 reveals\nthat each level of adaptation creates a further representational bottleneck.\nOur analysis illuminates these trade-offs in feature universality, providing a\nquantitative foundation for designing efficient feature coding and adaptation\nstrategies for diverse downstream applications.",
        "url": "http://arxiv.org/abs/2510.17051v1",
        "published_date": "2025-10-19T23:31:37+00:00",
        "updated_date": "2025-10-19T23:31:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Masoud Khairi Atani",
            "Alon Harell",
            "Hyomin Choi",
            "Runyu Yang",
            "Fabien Racape",
            "Ivan V. Bajic"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper compares the feature versatility of a general-purpose vision model with a segmentation-specialized model, showing trade-offs in performance for different tasks.",
        "tldr_zh": "本文比较了通用视觉模型和分割专用模型的特征多功能性，展示了在不同任务中性能的权衡。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7.5
    },
    {
        "title": "DINO-CVA: A Multimodal Goal-Conditioned Vision-to-Action Model for Autonomous Catheter Navigation",
        "summary": "Cardiac catheterization remains a cornerstone of minimally invasive\ninterventions, yet it continues to rely heavily on manual operation. Despite\nadvances in robotic platforms, existing systems are predominantly follow-leader\nin nature, requiring continuous physician input and lacking intelligent\nautonomy. This dependency contributes to operator fatigue, more radiation\nexposure, and variability in procedural outcomes. This work moves towards\nautonomous catheter navigation by introducing DINO-CVA, a multimodal\ngoal-conditioned behavior cloning framework. The proposed model fuses visual\nobservations and joystick kinematics into a joint embedding space, enabling\npolicies that are both vision-aware and kinematic-aware. Actions are predicted\nautoregressively from expert demonstrations, with goal conditioning guiding\nnavigation toward specified destinations. A robotic experimental setup with a\nsynthetic vascular phantom was designed to collect multimodal datasets and\nevaluate performance. Results show that DINO-CVA achieves high accuracy in\npredicting actions, matching the performance of a kinematics-only baseline\nwhile additionally grounding predictions in the anatomical environment. These\nfindings establish the feasibility of multimodal, goal-conditioned\narchitectures for catheter navigation, representing an important step toward\nreducing operator dependency and improving the reliability of catheterbased\ntherapies.",
        "url": "http://arxiv.org/abs/2510.17038v1",
        "published_date": "2025-10-19T22:59:32+00:00",
        "updated_date": "2025-10-19T22:59:32+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Pedram Fekri",
            "Majid Roshanfar",
            "Samuel Barbeau",
            "Seyedfarzad Famouri",
            "Thomas Looi",
            "Dale Podolsky",
            "Mehrdad Zadeh",
            "Javad Dargahi"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces DINO-CVA, a multimodal goal-conditioned model for autonomous catheter navigation, showing high accuracy in predicting actions and reducing operator dependency.",
        "tldr_zh": "本文介绍了DINO-CVA，这是一个用于自主导管导航的多模态目标条件模型，显示出在预测动作和减少操作者依赖性方面具有高准确性。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data Augmentation for Robust Lung Ultrasound Classification",
        "summary": "Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and\nstructurally normal lungs in lung ultrasound (LUS) videos remains challenging\ndue to the high visual variability of non-cardiogenic inflammatory patterns\n(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This\nheterogeneity complicates automated classification as overlapping B-lines and\npleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive\nCompact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer\nvariant that removes both positional embeddings and the [CLS] token, making it\nfully permutation-invariant and suitable for unordered medical image data. To\nenhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),\nwhich permutes probe-view sequences and frame orders while preserving\nanatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95\ncritically ill patients against nine state-of-the-art baselines. Despite the\nheterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest\nvalidation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)\nand specificity (0.91), while all competing models collapsed to trivial\nclassification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with\n2.5x fewer parameters, supporting real-time clinical deployment. These results\nshow that aligning architectural design with data structure can outperform\nscale in small-data medical imaging.",
        "url": "http://arxiv.org/abs/2510.17650v1",
        "published_date": "2025-10-20T15:26:38+00:00",
        "updated_date": "2025-10-20T15:26:38+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Athanasios Angelakis",
            "Amne Mousa",
            "Micah L. A. Heldeweg",
            "Laurens A. Biesheuvel",
            "Mark A. Haaksma",
            "Jasper M. Smit",
            "Pieter R. Tuinman",
            "Paul W. G. Elbers"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "ZACH-ViT is a Zero-Token Vision Transformer designed for lung ultrasound classification, achieving high performance and faster training than existing models.",
        "tldr_zh": "ZACH-ViT是一种零令牌视觉Transformer，设计用于肺部超声分类，在速度和性能方面优于现有模型。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Self-supervised Pre-training for Mapping of Archaeological Stone Wall in Historic Landscapes Using High-Resolution DEM Derivatives",
        "summary": "Dry-stone walls hold significant heritage and environmental value. Mapping\nthese structures is essential for ecosystem preservation and wildfire\nmanagement in Australia. Yet, many walls remain unidentified due to their\ninaccessibility and the high cost of manual mapping. Deep learning-based\nsegmentation offers a scalable solution, but two major challenges persist: (1)\nvisual occlusion of low-lying walls by dense vegetation, and (2) limited\nlabeled data for supervised training. We propose DINO-CV, a segmentation\nframework for automatic mapping of low-lying dry-stone walls using\nhigh-resolution Airborne LiDAR-derived digital elevation models (DEMs). DEMs\novercome visual occlusion by capturing terrain structures hidden beneath\nvegetation, enabling analysis of structural rather than spectral cues. DINO-CV\nintroduces a self-supervised cross-view pre-training strategy based on\nknowledge distillation to mitigate data scarcity. It learns invariant visual\nand geometric representations across multiple DEM derivatives, supporting\nvarious vision backbones including ResNet, Wide ResNet, and Vision\nTransformers. Applied to the UNESCO World Heritage cultural landscape of Budj\nBim, Victoria, the method identifies one of Australia's densest collections of\ncolonial dry-stone walls beyond Indigenous heritage contexts. DINO-CV achieves\na mean Intersection over Union (mIoU) of 68.6% on test areas and maintains\n63.8% mIoU when fine-tuned with only 10% labeled data. These results\ndemonstrate the potential of self-supervised learning on high-resolution DEM\nderivatives for automated dry-stone wall mapping in vegetated and heritage-rich\nenvironments with scarce annotations.",
        "url": "http://arxiv.org/abs/2510.17644v1",
        "published_date": "2025-10-20T15:23:05+00:00",
        "updated_date": "2025-10-20T15:23:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zexian Huang",
            "Mashnoon Islam",
            "Brian Armstrong",
            "Kourosh Khoshelham",
            "Martin Tomko"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper proposes a self-supervised pre-training method called DINO-CV for mapping archaeological stone walls using high-resolution DEM derivatives, addressing challenges like visual occlusion and limited labeled data.",
        "tldr_zh": "本文提出了一种名为DINO-CV的自监督预训练方法，使用高分辨率DEM派生物图映射考古石墙，解决了视觉遮挡和有限标记数据等挑战。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "CausalMamba: Scalable Conditional State Space Models for Neural Causal Inference",
        "summary": "We introduce CausalMamba, a scalable framework that addresses fundamental\nlimitations in fMRI-based causal inference: the ill-posed nature of inferring\nneural causality from hemodynamically distorted BOLD signals and the\ncomputational intractability of existing methods like Dynamic Causal Modeling\n(DCM). Our approach decomposes this complex inverse problem into two tractable\nstages: BOLD deconvolution to recover latent neural activity, followed by\ncausal graph inference using a novel Conditional Mamba architecture. On\nsimulated data, CausalMamba achieves 37% higher accuracy than DCM. Critically,\nwhen applied to real task fMRI data, our method recovers well-established\nneural pathways with 88% fidelity, whereas conventional approaches fail to\nidentify these canonical circuits in over 99% of subjects. Furthermore, our\nnetwork analysis of working memory data reveals that the brain strategically\nshifts its primary causal hub-recruiting executive or salience networks\ndepending on the stimulus-a sophisticated reconfiguration that remains\nundetected by traditional methods. This work provides neuroscientists with a\npractical tool for large-scale causal inference that captures both fundamental\ncircuit motifs and flexible network dynamics underlying cognitive function.",
        "url": "http://arxiv.org/abs/2510.17318v1",
        "published_date": "2025-10-20T09:04:25+00:00",
        "updated_date": "2025-10-20T09:04:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sangyoon Bae",
            "Jiook Cha"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "CausalMamba is a scalable framework for neural causal inference from fMRI data, outperforming existing methods and revealing new insights on brain dynamics in cognitive function.",
        "tldr_zh": "CausalMamba是一个可伸缩的框架，用于从fMRI数据进行神经因果推断，优于现有方法，并揭示了认知功能中大脑动态的新见解。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "When One Moment Isn't Enough: Multi-Moment Retrieval with Cross-Moment Interactions",
        "summary": "Existing Moment retrieval (MR) methods focus on Single-Moment Retrieval\n(SMR). However, one query can correspond to multiple relevant moments in\nreal-world applications. This makes the existing datasets and methods\ninsufficient for video temporal grounding. By revisiting the gap between\ncurrent MR tasks and real-world applications, we introduce a high-quality\ndatasets called QVHighlights Multi-Moment Dataset (QV-M$^2$), along with new\nevaluation metrics tailored for multi-moment retrieval (MMR). QV-M$^2$ consists\nof 2,212 annotations covering 6,384 video segments. Building on existing\nefforts in MMR, we propose a framework called FlashMMR. Specifically, we\npropose a Multi-moment Post-verification module to refine the moment\nboundaries. We introduce constrained temporal adjustment and subsequently\nleverage a verification module to re-evaluate the candidate segments. Through\nthis sophisticated filtering pipeline, low-confidence proposals are pruned, and\nrobust multi-moment alignment is achieved. We retrain and evaluate 6 existing\nMR methods on QV-M$^2$ and QVHighlights under both SMR and MMR settings.\nResults show that QV-M$^2$ serves as an effective benchmark for training and\nevaluating MMR models, while FlashMMR provides a strong baseline. Specifically,\non QV-M$^2$, it achieves improvements over prior SOTA method by 3.00% on G-mAP,\n2.70% on mAP@3+tgt, and 2.56% on mR@3. The proposed benchmark and method\nestablish a foundation for advancing research in more realistic and challenging\nvideo temporal grounding scenarios. Code is released at\nhttps://github.com/Zhuo-Cao/QV-M2.",
        "url": "http://arxiv.org/abs/2510.17218v1",
        "published_date": "2025-10-20T07:01:16+00:00",
        "updated_date": "2025-10-20T07:01:16+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhuo Cao",
            "Heming Du",
            "Bingqing Zhang",
            "Xin Yu",
            "Xue Li",
            "Sen Wang"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a new dataset and evaluation metrics for multi-moment retrieval in videos, along with a new method called FlashMMR that improves existing results.",
        "tldr_zh": "本文介绍了一个新的数据集和评估指标，用于视频中的多时刻检索，以及一种名为FlashMMR的新方法，可以改善现有结果。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "EndoCIL: A Class-Incremental Learning Framework for Endoscopic Image Classification",
        "summary": "Class-incremental learning (CIL) for endoscopic image analysis is crucial for\nreal-world clinical applications, where diagnostic models should continuously\nadapt to evolving clinical data while retaining performance on previously\nlearned ones. However, existing replay-based CIL methods fail to effectively\nmitigate catastrophic forgetting due to severe domain discrepancies and class\nimbalance inherent in endoscopic imaging. To tackle these challenges, we\npropose EndoCIL, a novel and unified CIL framework specifically tailored for\nendoscopic image diagnosis. EndoCIL incorporates three key components: Maximum\nMean Discrepancy Based Replay (MDBR), employing a distribution-aligned greedy\nstrategy to select diverse and representative exemplars, Prior Regularized\nClass Balanced Loss (PRCBL), designed to alleviate both inter-phase and\nintra-phase class imbalance by integrating prior class distributions and\nbalance weights into the loss function, and Calibration of Fully-Connected\nGradients (CFG), which adjusts the classifier gradients to mitigate bias toward\nnew classes. Extensive experiments conducted on four public endoscopic datasets\ndemonstrate that EndoCIL generally outperforms state-of-the-art CIL methods\nacross varying buffer sizes and evaluation metrics. The proposed framework\neffectively balances stability and plasticity in lifelong endoscopic diagnosis,\nshowing promising potential for clinical scalability and deployment.",
        "url": "http://arxiv.org/abs/2510.17200v1",
        "published_date": "2025-10-20T06:26:54+00:00",
        "updated_date": "2025-10-20T06:26:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bingrong Liu",
            "Jun Shi",
            "Yushan Zheng"
        ],
        "ai_categories": [
            "AIGC",
            "Dataset"
        ],
        "tldr": "EndoCIL is a novel Class-Incremental Learning framework designed for endoscopic image classification, outperforming existing methods in balancing stability and plasticity in lifelong diagnosis.",
        "tldr_zh": "EndoCIL是一种新颖的面向内窥镜图像分类的Class-Incremental Learning框架，表现优异，平衡了终身诊断中的稳定性和可塑性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Benchmarking Out-of-Distribution Detection for Plankton Recognition: A Systematic Evaluation of Advanced Methods in Marine Ecological Monitoring",
        "summary": "Automated plankton recognition models face significant challenges during\nreal-world deployment due to distribution shifts (Out-of-Distribution, OoD)\nbetween training and test data. This stems from plankton's complex\nmorphologies, vast species diversity, and the continuous discovery of novel\nspecies, which leads to unpredictable errors during inference. Despite rapid\nadvancements in OoD detection methods in recent years, the field of plankton\nrecognition still lacks a systematic integration of the latest computer vision\ndevelopments and a unified benchmark for large-scale evaluation. To address\nthis, this paper meticulously designed a series of OoD benchmarks simulating\nvarious distribution shift scenarios based on the DYB-PlanktonNet dataset\n\\cite{875n-f104-21}, and systematically evaluated twenty-two OoD detection\nmethods. Extensive experimental results demonstrate that the ViM\n\\cite{wang2022vim} method significantly outperforms other approaches in our\nconstructed benchmarks, particularly excelling in Far-OoD scenarios with\nsubstantial improvements in key metrics. This comprehensive evaluation not only\nprovides a reliable reference for algorithm selection in automated plankton\nrecognition but also lays a solid foundation for future research in plankton\nOoD detection. To our knowledge, this study marks the first large-scale,\nsystematic evaluation and analysis of Out-of-Distribution data detection\nmethods in plankton recognition. Code is available at\nhttps://github.com/BlackJack0083/PlanktonOoD.",
        "url": "http://arxiv.org/abs/2510.17179v1",
        "published_date": "2025-10-20T05:50:13+00:00",
        "updated_date": "2025-10-20T05:50:13+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yingzi Han",
            "Jiakai He",
            "Chuanlong Xie",
            "Jianping Li"
        ],
        "ai_categories": [
            "Dataset",
            "Other"
        ],
        "tldr": "The paper presents a systematic evaluation of Out-of-Distribution detection methods for plankton recognition, with ViM method showing significant improvement in Far-OoD scenarios.",
        "tldr_zh": "本文对浮游生物识别中的分布外检测方法进行了系统评估，ViM方法在远离训练数据分布的情况下表现出显著改进。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "Click, Predict, Trust: Clinician-in-the-Loop AI Segmentation for Lung Cancer CT-Based Prognosis within the Knowledge-to-Action Framework",
        "summary": "Lung cancer remains the leading cause of cancer mortality, with CT imaging\ncentral to screening, prognosis, and treatment. Manual segmentation is variable\nand time-intensive, while deep learning (DL) offers automation but faces\nbarriers to clinical adoption. Guided by the Knowledge-to-Action framework,\nthis study develops a clinician-in-the-loop DL pipeline to enhance\nreproducibility, prognostic accuracy, and clinical trust. Multi-center CT data\nfrom 999 patients across 12 public datasets were analyzed using five DL models\n(3D Attention U-Net, ResUNet, VNet, ReconNet, SAM-Med3D), benchmarked against\nexpert contours on whole and click-point cropped images. Segmentation\nreproducibility was assessed using 497 PySERA-extracted radiomic features via\nSpearman correlation, ICC, Wilcoxon tests, and MANOVA, while prognostic\nmodeling compared supervised (SL) and semi-supervised learning (SSL) across 38\ndimensionality reduction strategies and 24 classifiers. Six physicians\nqualitatively evaluated masks across seven domains, including clinical\nmeaningfulness, boundary quality, prognostic value, trust, and workflow\nintegration. VNet achieved the best performance (Dice = 0.83, IoU = 0.71),\nradiomic stability (mean correlation = 0.76, ICC = 0.65), and predictive\naccuracy under SSL (accuracy = 0.88, F1 = 0.83). SSL consistently outperformed\nSL across models. Radiologists favored VNet for peritumoral representation and\nsmoother boundaries, preferring AI-generated initial masks for refinement\nrather than replacement. These results demonstrate that integrating VNet with\nSSL yields accurate, reproducible, and clinically trusted CT-based lung cancer\nprognosis, highlighting a feasible path toward physician-centered AI\ntranslation.",
        "url": "http://arxiv.org/abs/2510.17039v1",
        "published_date": "2025-10-19T23:02:43+00:00",
        "updated_date": "2025-10-19T23:02:43+00:00",
        "categories": [
            "cs.CV",
            "F.2.2; I.2.7"
        ],
        "authors": [
            "Mohammad R. Salmanpour",
            "Sonya Falahati",
            "Amir Hossein Pouria",
            "Amin Mousavi",
            "Somayeh Sadat Mehrnia",
            "Morteza Alizadeh",
            "Arman Gorji",
            "Zeinab Farsangi",
            "Alireza Safarian",
            "Mehdi Maghsudi",
            "Carlos Uribe",
            "Arman Rahmim",
            "Ren Yuan"
        ],
        "ai_categories": [
            "AIGC",
            "Multimodality",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a clinician-in-the-loop DL pipeline for lung cancer prognosis using CT images, showcasing improved reproducibility, accuracy, and trust in clinical settings.",
        "tldr_zh": "该论文介绍了一种在临床设置中使用CT图像进行肺癌预后的医生辅助深度学习管道，展示了改进的可重现性，准确性和信任度。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.25
    },
    {
        "title": "UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action",
        "summary": "Multimodal agents for computer use rely exclusively on primitive actions\n(click, type, scroll) that require accurate visual grounding and lengthy\nexecution chains, leading to cascading failures and performance bottlenecks.\nWhile other agents leverage rich programmatic interfaces (APIs, MCP servers,\ntools), computer-use agents (CUAs) remain isolated from these capabilities. We\npresent UltraCUA, a foundation model that bridges this gap through hybrid\naction -- seamlessly integrating GUI primitives with high-level programmatic\ntool calls. To achieve this, our approach comprises four key components: (1) an\nautomated pipeline that scales programmatic tools from software documentation,\nopen-source repositories, and code generation; (2) a synthetic data engine\nproducing over 17,000 verifiable tasks spanning real-world computer-use\nscenarios; (3) a large-scale high-quality hybrid action trajectory collection\nwith both low-level GUI actions and high-level programmatic tool calls; and (4)\na two-stage training pipeline combining supervised fine-tuning with online\nreinforcement learning, enabling strategic alternation between low-level and\nhigh-level actions. Experiments with our 7B and 32B models demonstrate\nsubstantial improvements over state-of-the-art agents. On OSWorld, UltraCUA\nmodels achieve an average 22% relative improvement over base models, while\nbeing 11% faster in terms of steps. Out-of-domain evaluation on\nWindowsAgentArena shows our model reaches 21.7% success rate, outperforming\nbaselines trained on Windows data. The hybrid action mechanism proves critical,\nreducing error propagation while maintaining execution efficiency.",
        "url": "http://arxiv.org/abs/2510.17790v1",
        "published_date": "2025-10-20T17:48:26+00:00",
        "updated_date": "2025-10-20T17:48:26+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Yuhao Yang",
            "Zhen Yang",
            "Zi-Yi Dou",
            "Anh Nguyen",
            "Keen You",
            "Omar Attia",
            "Andrew Szot",
            "Michael Feng",
            "Ram Ramrakhya",
            "Alexander Toshev",
            "Chao Huang",
            "Yinfei Yang",
            "Zhe Gan"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "UltraCUA is a foundation model that integrates GUI actions with programmatic tool calls to improve computer-use agents, showing significant improvements over existing agents.",
        "tldr_zh": "UltraCUA 是一个基础模型，将 GUI 操作与程序工具调用相结合，以改善计算机使用代理，相比现有代理，显示出显著的改进。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Elastic ViTs from Pretrained Models without Retraining",
        "summary": "Vision foundation models achieve remarkable performance but are only\navailable in a limited set of pre-determined sizes, forcing sub-optimal\ndeployment choices under real-world constraints. We introduce SnapViT:\nSingle-shot network approximation for pruned Vision Transformers, a new\npost-pretraining structured pruning method that enables elastic inference\nacross a continuum of compute budgets. Our approach efficiently combines\ngradient information with cross-network structure correlations, approximated\nvia an evolutionary algorithm, does not require labeled data, generalizes to\nmodels without a classification head, and is retraining-free. Experiments on\nDINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over\nstate-of-the-art methods across various sparsities, requiring less than five\nminutes on a single A100 GPU to generate elastic models that can be adjusted to\nany computational budget. Our key contributions include an efficient pruning\nstrategy for pretrained Vision Transformers, a novel evolutionary approximation\nof Hessian off-diagonal structures, and a self-supervised importance scoring\nmechanism that maintains strong performance without requiring retraining or\nlabels. Code and pruned models are available at: https://elastic.ashita.nl/",
        "url": "http://arxiv.org/abs/2510.17700v1",
        "published_date": "2025-10-20T16:15:03+00:00",
        "updated_date": "2025-10-20T16:15:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Walter Simoncini",
            "Michael Dorkenwald",
            "Tijmen Blankevoort",
            "Cees G. M. Snoek",
            "Yuki M. Asano"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces SnapViT, a structured pruning method for Vision Transformers that enables elastic inference across various compute budgets without retraining, achieving superior performance over state-of-the-art methods.",
        "tldr_zh": "该论文介绍了SnapViT，一种用于Vision Transformers的结构化修剪方法，可以在不重新训练的情况下实现不同计算预算下的弹性推理，胜过现有方法。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads",
        "summary": "4D panoptic segmentation in a streaming setting is critical for highly\ndynamic environments, such as evacuating dense crowds and autonomous driving in\ncomplex scenarios, where real-time, fine-grained perception within a\nconstrained time budget is essential. In this paper, we introduce\n4DSegStreamer, a novel framework that employs a Dual-Thread System to\nefficiently process streaming frames. The framework is general and can be\nseamlessly integrated into existing 3D and 4D segmentation methods to enable\nreal-time capability. It also demonstrates superior robustness compared to\nexisting streaming perception approaches, particularly under high FPS\nconditions. The system consists of a predictive thread and an inference thread.\nThe predictive thread leverages historical motion and geometric information to\nextract features and forecast future dynamics. The inference thread ensures\ntimely prediction for incoming frames by aligning with the latest memory and\ncompensating for ego-motion and dynamic object movements. We evaluate\n4DSegStreamer on the indoor HOI4D dataset and the outdoor SemanticKITTI and\nnuScenes datasets. Comprehensive experiments demonstrate the effectiveness of\nour approach, particularly in accurately predicting dynamic objects in complex\nscenes.",
        "url": "http://arxiv.org/abs/2510.17664v1",
        "published_date": "2025-10-20T15:37:49+00:00",
        "updated_date": "2025-10-20T15:37:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ling Liu",
            "Jun Tian",
            "Li Yi"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper introduces 4DSegStreamer, a framework for real-time 4D panoptic segmentation in dynamic environments, showcasing superior robustness and accuracy in predicting dynamic objects.",
        "tldr_zh": "本文介绍了4DSegStreamer，一个用于实时4D全景分割的框架，在动态环境中展示了对预测动态物体的卓越鲁棒性和准确性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Frugal Federated Learning for Violence Detection: A Comparison of LoRA-Tuned VLMs and Personalized CNNs",
        "summary": "We examine frugal federated learning approaches to violence detection by\ncomparing two complementary strategies: (i) zero-shot and federated fine-tuning\nof vision-language models (VLMs), and (ii) personalized training of a compact\n3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter\nCNN3D as representative cases, we evaluate accuracy, calibration, and energy\nusage under realistic non-IID settings.\n  Both approaches exceed 90% accuracy. CNN3D slightly outperforms Low-Rank\nAdaptation(LoRA)-tuned VLMs in ROC AUC and log loss, while using less energy.\nVLMs remain favorable for contextual reasoning and multimodal inference. We\nquantify energy and CO$_2$ emissions across training and inference, and analyze\nsustainability trade-offs for deployment.\n  To our knowledge, this is the first comparative study of LoRA-tuned\nvision-language models and personalized CNNs for federated violence detection,\nwith an emphasis on energy efficiency and environmental metrics.\n  These findings support a hybrid model: lightweight CNNs for routine\nclassification, with selective VLM activation for complex or descriptive\nscenarios. The resulting framework offers a reproducible baseline for\nresponsible, resource-aware AI in video surveillance, with extensions toward\nreal-time, multimodal, and lifecycle-aware systems.",
        "url": "http://arxiv.org/abs/2510.17651v1",
        "published_date": "2025-10-20T15:26:43+00:00",
        "updated_date": "2025-10-20T15:26:43+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Sébastien Thuau",
            "Siba Haidar",
            "Ayush Bajracharya",
            "Rachid Chelouah"
        ],
        "ai_categories": [
            "LoRA",
            "Multimodality",
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper compares frugal federated learning approaches for violence detection, showing that lightweight CNNs perform well for routine classification with selective activation of VLMs for complex scenarios.",
        "tldr_zh": "本文比较了用于暴力检测的节俭联邦学习方法，表明轻量级CNN对于常规分类效果良好，VLMs在复杂场景中的选择性激活表现出色。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Integrating BIM and UAV-based photogrammetry for Automated 3D Structure Model Segmentation",
        "summary": "The advancement of UAV technology has enabled efficient, non-contact\nstructural health monitoring. Combined with photogrammetry, UAVs can capture\nhigh-resolution scans and reconstruct detailed 3D models of infrastructure.\nHowever, a key challenge remains in segmenting specific structural components\nfrom these models-a process traditionally reliant on time-consuming and\nerror-prone manual labeling. To address this issue, we propose a machine\nlearning-based framework for automated segmentation of 3D point clouds. Our\napproach uses the complementary strengths of real-world UAV-scanned point\nclouds and synthetic data generated from Building Information Modeling (BIM) to\novercome the limitations associated with manual labeling. Validation on a\nrailroad track dataset demonstrated high accuracy in identifying and segmenting\nmajor components such as rails and crossties. Moreover, by using smaller-scale\ndatasets supplemented with BIM data, the framework significantly reduced\ntraining time while maintaining reasonable segmentation accuracy. This\nautomated approach improves the precision and efficiency of 3D infrastructure\nmodel segmentation and advances the integration of UAV and BIM technologies in\nstructural health monitoring and infrastructure management.",
        "url": "http://arxiv.org/abs/2510.17609v1",
        "published_date": "2025-10-20T14:54:54+00:00",
        "updated_date": "2025-10-20T14:54:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Siqi Chen",
            "Shanyue Guan"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper presents a machine learning-based framework for automated segmentation of 3D infrastructure models using UAV-scanned point clouds and BIM data, improving efficiency in structural health monitoring and infrastructure management.",
        "tldr_zh": "本文提出了一种基于机器学习的框架，利用无人机扫描的点云和BIM数据对3D基础设施模型进行自动分割，提高了结构健康监测和基础设施管理的效率。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Detecting streaks in smart telescopes images with Deep Learning",
        "summary": "The growing negative impact of the visibility of satellites in the night sky\nis influencing the practice of astronomy and astrophotograph, both at the\namateur and professional levels. The presence of these satellites has the\neffect of introducing streaks into the images captured during astronomical\nobservation, requiring the application of additional post processing to\nmitigate the undesirable impact, whether for data loss or cosmetic reasons. In\nthis paper, we show how we test and adapt various Deep Learning approaches to\ndetect streaks in raw astronomical data captured between March 2022 and\nFebruary 2023 with smart telescopes.",
        "url": "http://arxiv.org/abs/2510.17540v1",
        "published_date": "2025-10-20T13:43:09+00:00",
        "updated_date": "2025-10-20T13:43:09+00:00",
        "categories": [
            "astro-ph.IM",
            "cs.CV"
        ],
        "authors": [
            "Olivier Parisot",
            "Mahmoud Jaziri"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper discusses the use of Deep Learning to detect streaks caused by satellites in smart telescope images.",
        "tldr_zh": "本文讨论了使用深度学习来检测智能望远镜图像中由卫星引起的条纹。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Split-Fuse-Transport: Annotation-Free Saliency via Dual Clustering and Optimal Transport Alignment",
        "summary": "Salient object detection (SOD) aims to segment visually prominent regions in\nimages and serves as a foundational task for various computer vision\napplications. We posit that SOD can now reach near-supervised accuracy without\na single pixel-level label, but only when reliable pseudo-masks are available.\nWe revisit the prototype-based line of work and make two key observations.\nFirst, boundary pixels and interior pixels obey markedly different geometry;\nsecond, the global consistency enforced by optimal transport (OT) is\nunderutilized if prototype quality is weak. To address this, we introduce\nPOTNet, an adaptation of Prototypical Optimal Transport that replaces POT's\nsingle k-means step with an entropy-guided dual-clustering head: high-entropy\npixels are organized by spectral clustering, low-entropy pixels by k-means, and\nthe two prototype sets are subsequently aligned by OT. This\nsplit-fuse-transport design yields sharper, part-aware pseudo-masks in a single\nforward pass, without handcrafted priors. Those masks supervise a standard\nMaskFormer-style encoder-decoder, giving rise to AutoSOD, an end-to-end\nunsupervised SOD pipeline that eliminates SelfMask's offline voting yet\nimproves both accuracy and training efficiency. Extensive experiments on five\nbenchmarks show that AutoSOD outperforms unsupervised methods by up to 26% and\nweakly supervised methods by up to 36% in F-measure, further narrowing the gap\nto fully supervised models.",
        "url": "http://arxiv.org/abs/2510.17484v1",
        "published_date": "2025-10-20T12:27:55+00:00",
        "updated_date": "2025-10-20T12:27:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Muhammad Umer Ramzan",
            "Ali Zia",
            "Abdelwahed Khamis",
            "Noman Ali",
            "Usman Ali",
            "Wei Xiang"
        ],
        "ai_categories": [
            "GAN",
            "Transformer",
            "Other"
        ],
        "tldr": "The paper introduces a novel method for unsupervised salient object detection in images, achieving high accuracy without pixel-level labels.",
        "tldr_zh": "本文介绍了一种新颖的方法，用于在图像中无监督地检测显著对象，无需像素级标签即可实现高准确性。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries",
        "summary": "Semantic occupancy has emerged as a powerful representation in world models\nfor its ability to capture rich spatial semantics. However, most existing\noccupancy world models rely on static and fixed embeddings or grids, which\ninherently limit the flexibility of perception. Moreover, their ``in-place\nclassification\" over grids exhibits a potential misalignment with the dynamic\nand continuous nature of real scenarios.In this paper, we propose SparseWorld,\na novel 4D occupancy world model that is flexible, adaptive, and efficient,\npowered by sparse and dynamic queries. We propose a Range-Adaptive Perception\nmodule, in which learnable queries are modulated by the ego vehicle states and\nenriched with temporal-spatial associations to enable extended-range\nperception. To effectively capture the dynamics of the scene, we design a\nState-Conditioned Forecasting module, which replaces classification-based\nforecasting with regression-guided formulation, precisely aligning the dynamic\nqueries with the continuity of the 4D environment. In addition, We specifically\ndevise a Temporal-Aware Self-Scheduling training strategy to enable smooth and\nefficient training. Extensive experiments demonstrate that SparseWorld achieves\nstate-of-the-art performance across perception, forecasting, and planning\ntasks. Comprehensive visualizations and ablation studies further validate the\nadvantages of SparseWorld in terms of flexibility, adaptability, and\nefficiency. The code is available at https://github.com/MSunDYY/SparseWorld.",
        "url": "http://arxiv.org/abs/2510.17482v1",
        "published_date": "2025-10-20T12:26:25+00:00",
        "updated_date": "2025-10-20T12:26:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Chenxu Dang",
            "Haiyan Liu",
            "Guangjun Bao",
            "Pei An",
            "Xinyue Tang",
            "Jie Ma",
            "Bingchuan Sun",
            "Yan Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces SparseWorld, a 4D occupancy world model that is flexible, adaptive, and efficient, powered by sparse and dynamic queries. It achieves state-of-the-art performance in perception, forecasting, and planning tasks.",
        "tldr_zh": "本文介绍了SparseWorld，这是一个由稀疏和动态查询驱动的灵活、自适应和高效的4D占据世界模型。它在感知、预测和规划任务中取得了最先进的性能。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS",
        "summary": "Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training\nviews, leading to artifacts like blurring in novel view rendering. Prior work\naddresses it either by enhancing the initialization (\\emph{i.e.}, the point\ncloud from Structure-from-Motion (SfM)) or by adding training-time constraints\n(regularization) to the 3DGS optimization. Yet our controlled ablations reveal\nthat initialization is the decisive factor: it determines the attainable\nperformance band in sparse-view 3DGS, while training-time constraints yield\nonly modest within-band improvements at extra cost. Given initialization's\nprimacy, we focus our design there. Although SfM performs poorly under sparse\nviews due to its reliance on feature matching, it still provides reliable seed\npoints. Thus, building on SfM, our effort aims to supplement the regions it\nfails to cover as comprehensively as possible. Specifically, we design: (i)\nfrequency-aware SfM that improves low-texture coverage via low-frequency view\naugmentation and relaxed multi-view correspondences; (ii) 3DGS\nself-initialization that lifts photometric supervision into additional points,\ncompensating SfM-sparse regions with learned Gaussian centers; and (iii)\npoint-cloud regularization that enforces multi-view consistency and uniform\nspatial coverage through simple geometric/visibility priors, yielding a clean\nand reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrate\nconsistent gains in sparse-view settings, establishing our approach as a\nstronger initialization strategy. Code is available at\nhttps://github.com/zss171999645/ItG-GS.",
        "url": "http://arxiv.org/abs/2510.17479v1",
        "published_date": "2025-10-20T12:23:19+00:00",
        "updated_date": "2025-10-20T12:23:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Feng Zhou",
            "Wenkai Guo",
            "Pu Cao",
            "Zhicheng Zhang",
            "Jianqin Yin"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a stronger initialization pipeline for sparse-view 3D Gaussian Splatting (3DGS) to improve rendering quality by focusing on enhancing initialization.",
        "tldr_zh": "本文提出了一种更强的初始化管道，用于改进稀疏视角3D高斯斑点（3DGS）的渲染质量，重点是增强初始化。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Leveraging AV1 motion vectors for Fast and Dense Feature Matching",
        "summary": "We repurpose AV1 motion vectors to produce dense sub-pixel correspondences\nand short tracks filtered by cosine consistency. On short videos, this\ncompressed-domain front end runs comparably to sequential SIFT while using far\nless CPU, and yields denser matches with competitive pairwise geometry. As a\nsmall SfM demo on a 117-frame clip, MV matches register all images and\nreconstruct 0.46-0.62M points at 0.51-0.53,px reprojection error; BA time grows\nwith match density. These results show compressed-domain correspondences are a\npractical, resource-efficient front end with clear paths to scaling in full\npipelines.",
        "url": "http://arxiv.org/abs/2510.17434v1",
        "published_date": "2025-10-20T11:22:52+00:00",
        "updated_date": "2025-10-20T11:22:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Julien Zouein",
            "Hossein Javidnia",
            "François Pitié",
            "Anil Kokaram"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper repurposes AV1 motion vectors for producing dense sub-pixel correspondences and short tracks with competitive performance, showing promising results for scalable pipelines.",
        "tldr_zh": "本文重新利用AV1运动矢量生成密集子像素对应和短轨道，性能竞争激烈，显示出在可扩展管线中的潜在价值。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Facial Expression-based Parkinson's Disease Severity Diagnosis via Feature Fusion and Adaptive Class Balancing",
        "summary": "Parkinson's disease (PD) severity diagnosis is crucial for early detecting\npotential patients and adopting tailored interventions. Diagnosing PD based on\nfacial expression is grounded in PD patients' \"masked face\" symptom and gains\ngrowing interest recently for its convenience and affordability. However,\ncurrent facial expression-based approaches often rely on single type of\nexpression which can lead to misdiagnosis, and ignore the class imbalance\nacross different PD stages which degrades the prediction performance. Moreover,\nmost existing methods focus on binary classification (i.e., PD / non-PD) rather\nthan diagnosing the severity of PD. To address these issues, we propose a new\nfacial expression-based method for PD severity diagnosis which integrates\nmultiple facial expression features through attention-based feature fusion.\nMoreover, we mitigate the class imbalance problem via an adaptive class\nbalancing strategy which dynamically adjusts the contribution of training\nsamples based on their class distribution and classification difficulty.\nExperimental results demonstrate the promising performance of the proposed\nmethod for PD severity diagnosis, as well as the efficacy of attention-based\nfeature fusion and adaptive class balancing.",
        "url": "http://arxiv.org/abs/2510.17373v1",
        "published_date": "2025-10-20T10:09:12+00:00",
        "updated_date": "2025-10-20T10:09:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yintao Zhou",
            "Wei Huang",
            "Zhengyu Li",
            "Jing Huang",
            "Meng Pang"
        ],
        "ai_categories": [
            "GAN",
            "Dataset"
        ],
        "tldr": "This paper introduces a new method for diagnosing Parkinson's disease severity using facial expressions, featuring attention-based feature fusion and adaptive class balancing to improve prediction performance.",
        "tldr_zh": "本文提出一种利用面部表情诊断帕金森病严重程度的新方法，采用基于注意力的特征融合和自适应类平衡以提高预测性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Nearest-Class Mean and Logits Agreement for Wildlife Open-Set Recognition",
        "summary": "Current state-of-the-art Wildlife classification models are trained under the\nclosed world setting. When exposed to unknown classes, they remain\noverconfident in their predictions. Open-set Recognition (OSR) aims to classify\nknown classes while rejecting unknown samples. Several OSR methods have been\nproposed to model the closed-set distribution by observing the feature, logit,\nor softmax probability space. A significant drawback of many existing\napproaches is the requirement to retrain the pre-trained classification model\nwith the OSR-specific strategy. This study contributes a post-processing OSR\nmethod that measures the agreement between the models' features and predicted\nlogits. We propose a probability distribution based on an input's distance to\nits Nearest Class Mean (NCM). The NCM-based distribution is then compared with\nthe softmax probabilities from the logit space to measure agreement between the\nNCM and the classification head. Our proposed strategy ranks within the top\nthree on two evaluated datasets, showing consistent performance across the two\ndatasets. In contrast, current state-of-the-art methods excel on a single\ndataset. We achieve an AUROC of 93.41 and 95.35 for African and Swedish\nanimals. The code can be found\nhttps://github.com/Applied-Representation-Learning-Lab/OSR.",
        "url": "http://arxiv.org/abs/2510.17338v1",
        "published_date": "2025-10-20T09:32:08+00:00",
        "updated_date": "2025-10-20T09:32:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahao Huo",
            "Mufhumudzi Muthivhi",
            "Terence L. van Zyl",
            "Fredrik Gustafsson"
        ],
        "ai_categories": [
            "AIGC"
        ],
        "tldr": "This paper introduces a post-processing method for open-set recognition in wildlife classification by measuring the agreement between features and predicted logits.",
        "tldr_zh": "本文介绍了一种针对野生动物分类中开放集识别的后处理方法，通过测量特征和预测logits之间的一致性来实现。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment",
        "summary": "Conventional end-to-end (E2E) driving models are effective at generating\nphysically plausible trajectories, but often fail to generalize to long-tail\nscenarios due to the lack of essential world knowledge to understand and reason\nabout surrounding environments. In contrast, Vision-Language-Action (VLA)\nmodels leverage world knowledge to handle challenging cases, but their limited\n3D reasoning capability can lead to physically infeasible actions. In this work\nwe introduce DiffVLA++, an enhanced autonomous driving framework that\nexplicitly bridges cognitive reasoning and E2E planning through metric-guided\nalignment. First, we build a VLA module directly generating semantically\ngrounded driving trajectories. Second, we design an E2E module with a dense\ntrajectory vocabulary that ensures physical feasibility. Third, and most\ncritically, we introduce a metric-guided trajectory scorer that guides and\naligns the outputs of the VLA and E2E modules, thereby integrating their\ncomplementary strengths. The experiment on the ICCV 2025 Autonomous Grand\nChallenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.",
        "url": "http://arxiv.org/abs/2510.17148v1",
        "published_date": "2025-10-20T04:49:14+00:00",
        "updated_date": "2025-10-20T04:49:14+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Yu Gao",
            "Yiru Wang",
            "Anqing Jiang",
            "Heng Yuwen",
            "Wang Shuo",
            "Sun Hao",
            "Wang Jijun"
        ],
        "ai_categories": [
            "AIGC",
            "Transformer"
        ],
        "tldr": "DiffVLA++ is an enhanced autonomous driving framework that combines cognitive reasoning and end-to-end planning through metric-guided alignment to tackle long-tail scenarios and ensure physical feasibility of actions.",
        "tldr_zh": "DiffVLA++是一个增强的自主驾驶框架，通过度量引导的对齐，结合认知推理和端到端规划，以解决长尾场景和确保行动的物理可行性。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ProDAT: Progressive Density-Aware Tail-Drop for Point Cloud Coding",
        "summary": "Three-dimensional (3D) point clouds are becoming increasingly vital in\napplications such as autonomous driving, augmented reality, and immersive\ncommunication, demanding real-time processing and low latency. However, their\nlarge data volumes and bandwidth constraints hinder the deployment of\nhigh-quality services in resource-limited environments. Progres- sive coding,\nwhich allows for decoding at varying levels of detail, provides an alternative\nby allowing initial partial decoding with subsequent refinement. Although\nrecent learning-based point cloud geometry coding methods have achieved notable\nsuccess, their fixed latent representation does not support progressive\ndecoding. To bridge this gap, we propose ProDAT, a novel density-aware\ntail-drop mechanism for progressive point cloud coding. By leveraging density\ninformation as a guidance signal, latent features and coordinates are decoded\nadaptively based on their significance, therefore achieving progressive\ndecoding at multiple bitrates using one single model. Experimental results on\nbenchmark datasets show that the proposed ProDAT not only enables progressive\ncoding but also achieves superior coding efficiency compared to\nstate-of-the-art learning-based coding techniques, with over 28.6% BD-rate\nimprovement for PSNR- D2 on SemanticKITTI and over 18.15% for ShapeNet",
        "url": "http://arxiv.org/abs/2510.17068v1",
        "published_date": "2025-10-20T00:50:16+00:00",
        "updated_date": "2025-10-20T00:50:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhe Luo",
            "Wenjing Jia",
            "Stuart Perry"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes ProDAT, a density-aware tail-drop mechanism for progressive point cloud coding, achieving progressive decoding at multiple bitrates using one model and outperforming state-of-the-art techniques.",
        "tldr_zh": "本文提出了ProDAT，一种密度感知的逐步点云编码机制，使用一种模型在多个比特率下实现逐步解码，并优于现有技术。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Person Re-Identification via Generalized Class Prototypes",
        "summary": "Advanced feature extraction methods have significantly contributed to\nenhancing the task of person re-identification. In addition, modifications to\nobjective functions have been developed to further improve performance.\nNonetheless, selecting better class representatives is an underexplored area of\nresearch that can also lead to advancements in re-identification performance.\nAlthough past works have experimented with using the centroid of a gallery\nimage class during training, only a few have investigated alternative\nrepresentations during the retrieval stage. In this paper, we demonstrate that\nthese prior techniques yield suboptimal results in terms of re-identification\nmetrics. To address the re-identification problem, we propose a generalized\nselection method that involves choosing representations that are not limited to\nclass centroids. Our approach strikes a balance between accuracy and mean\naverage precision, leading to improvements beyond the state of the art. For\nexample, the actual number of representations per class can be adjusted to meet\nspecific application requirements. We apply our methodology on top of multiple\nre-identification embeddings, and in all cases it substantially improves upon\ncontemporary results",
        "url": "http://arxiv.org/abs/2510.17043v1",
        "published_date": "2025-10-19T23:16:57+00:00",
        "updated_date": "2025-10-19T23:16:57+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Md Ahmed Al Muzaddid",
            "William J. Beksi"
        ],
        "ai_categories": [
            "AIGC"
        ],
        "tldr": "The paper introduces a new method for person re-identification that improves upon existing techniques by selecting representations beyond class centroids, leading to better performance.",
        "tldr_zh": "本文提出了一种新的人员再识别方法，通过选择超越类质心的表示形式，改进了现有技术，带来更好的性能。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CARE: Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams",
        "summary": "The recognition of Activities of Daily Living (ADLs) from event-triggered\nambient sensors is an essential task in Ambient Assisted Living, yet existing\nmethods remain constrained by representation-level limitations. Sequence-based\napproaches preserve temporal order of sensor activations but are sensitive to\nnoise and lack spatial awareness, while image-based approaches capture global\npatterns and implicit spatial correlations but compress fine-grained temporal\ndynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation)\nfail to enforce alignment between sequence- and image-based representation\nviews, underutilizing their complementary strengths. We propose Contrastive\nAlignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), an\nend-to-end framework that jointly optimizes representation learning via\nSequence-Image Contrastive Alignment (SICA) and classification via\ncross-entropy, ensuring both cross-representation alignment and task-specific\ndiscriminability. CARE integrates (i) time-aware, noise-resilient sequence\nencoding with (ii) spatially-informed and frequency-sensitive image\nrepresentations, and employs (iii) a joint contrastive-classification objective\nfor end-to-end learning of aligned and discriminative embeddings. Evaluated on\nthree CASAS datasets, CARE achieves state-of-the-art performance (89.8% on\nMilan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness to\nsensor malfunctions and layout variability, highlighting its potential for\nreliable ADL recognition in smart homes.",
        "url": "http://arxiv.org/abs/2510.16988v1",
        "published_date": "2025-10-19T20:11:12+00:00",
        "updated_date": "2025-10-19T20:11:12+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Junhao Zhao",
            "Zishuai Liu",
            "Ruili Fang",
            "Jin Lu",
            "Linghan Zhang",
            "Fei Dou"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper proposes a framework called CARE for recognizing Activities of Daily Living from sensor streams, achieving state-of-the-art results and robustness to sensor malfunctions.",
        "tldr_zh": "该论文提出了一种名为CARE的框架，用于从传感器流中识别日常生活活动，取得了最先进的结果，并对传感器故障具有稳健性。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 7
    },
    {
        "title": "Monitoring Horses in Stalls: From Object to Event Detection",
        "summary": "Monitoring the behavior of stalled horses is essential for early detection of\nhealth and welfare issues but remains labor-intensive and time-consuming. In\nthis study, we present a prototype vision-based monitoring system that\nautomates the detection and tracking of horses and people inside stables using\nobject detection and multi-object tracking techniques. The system leverages\nYOLOv11 and BoT-SORT for detection and tracking, while event states are\ninferred based on object trajectories and spatial relations within the stall.\nTo support development, we constructed a custom dataset annotated with\nassistance from foundation models CLIP and GroundingDINO. The system\ndistinguishes between five event types and accounts for the camera's blind\nspots. Qualitative evaluation demonstrated reliable performance for\nhorse-related events, while highlighting limitations in detecting people due to\ndata scarcity. This work provides a foundation for real-time behavioral\nmonitoring in equine facilities, with implications for animal welfare and\nstable management.",
        "url": "http://arxiv.org/abs/2510.17409v1",
        "published_date": "2025-10-20T10:52:42+00:00",
        "updated_date": "2025-10-20T10:52:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dmitrii Galimzianov",
            "Viacheslav Vyshegorodtsev",
            "Ivan Nezhivykh"
        ],
        "ai_categories": [
            "Dataset",
            "Multimodality",
            "Transformer"
        ],
        "tldr": "The paper introduces a vision-based monitoring system for detecting and tracking horses and people in stalls, with a focus on early detection of health and welfare issues.",
        "tldr_zh": "本文介绍了一种基于视觉的监测系统，用于在马厩中检测和跟踪马匹和人员，重点关注早期发现健康和福祉问题。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs",
        "summary": "Video Large Language Models (Video-LLMs) excel at understanding videos\nin-context, provided they have full access to the video when answering queries.\nHowever, these models face challenges in streaming scenarios where hour-long\nvideos must be processed online, and questions need timely responses. In this\nwork, we propose a training-free approach compatible with standard Video-LLMs,\nleveraging three key concepts: 1) LLM-informed selection of visual tokens to\nidentify those that the LLM has attended to and contributed to its\nunderstanding of each short clip. Our attention-based selection allows us to\ndiscard up to ~95% of unimportant visual tokens with minimal performance loss;\n2) Recurrent processing of past selected tokens to generate temporally coherent\nunderstanding of each processed clip; 3) Caption-based question answering for\nlightweight and accurate responses. Our method achieves state-of-the-art\nperformance on streaming video benchmarks, striking a balance between\nefficiency and effectiveness.",
        "url": "http://arxiv.org/abs/2510.17364v1",
        "published_date": "2025-10-20T10:04:49+00:00",
        "updated_date": "2025-10-20T10:04:49+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Vaggelis Dorovatas",
            "Soroush Seifi",
            "Gunshi Gupta",
            "Rahaf Aljundi"
        ],
        "ai_categories": [
            "Transformer",
            "Other"
        ],
        "tldr": "The paper proposes a method for efficient processing of streaming videos using attention-based token selection and recurrent processing, achieving state-of-the-art performance on benchmarks.",
        "tldr_zh": "本文提出了一种使用基于注意力的标记选择和循环处理的方法，以实现对流式视频的高效处理，在基准测试中取得了最先进的性能。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Towards Imperceptible Watermarking Via Environment Illumination for Consumer Cameras",
        "summary": "This paper introduces a method for using LED-based environmental lighting to\nproduce visually imperceptible watermarks for consumer cameras. Our approach\noptimizes an LED light source's spectral profile to be minimally visible to the\nhuman eye while remaining highly detectable by typical consumer cameras. The\nmethod jointly considers the human visual system's sensitivity to visible\nspectra, modern consumer camera sensors' spectral sensitivity, and narrowband\nLEDs' ability to generate broadband spectra perceived as \"white light\"\n(specifically, D65 illumination). To ensure imperceptibility, we employ\nspectral modulation rather than intensity modulation. Unlike conventional\nvisible light communication, our approach enables watermark extraction at\nstandard low frame rates (30-60 fps). While the information transfer rate is\nmodest-embedding 128 bits within a 10-second video clip-this capacity is\nsufficient for essential metadata supporting privacy protection and content\nverification.",
        "url": "http://arxiv.org/abs/2510.17114v1",
        "published_date": "2025-10-20T03:02:32+00:00",
        "updated_date": "2025-10-20T03:02:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hodaka Kawachi",
            "Tomoya Nakamura",
            "Hiroaki Santo",
            "SaiKiran Kumar Tedla",
            "Trevor Dalton Canham",
            "Yasushi Yagi",
            "Michael S. Brown"
        ],
        "ai_categories": [
            "Other"
        ],
        "tldr": "The paper proposes a method using LED lighting to create imperceptible watermarks for consumer cameras, enabling data embedding in videos while maintaining visual quality.",
        "tldr_zh": "本文提出一种使用LED照明创建消费级相机难以察觉水印的方法，实现在视频中嵌入数据而保持视觉质量。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Do Satellite Tasks Need Special Pretraining?",
        "summary": "Foundation models have advanced machine learning across various modalities,\nincluding images. Recently multiple teams trained foundation models specialized\nfor remote sensing applications. This line of research is motivated by the\ndistinct characteristics of remote sensing imagery, specific applications and\ntypes of robustness useful for satellite image analysis. In this work we\nsystematically challenge the idea that specific foundation models are more\nuseful than general-purpose vision foundation models, at least in the small\nscale. First, we design a simple benchmark that measures generalization of\nremote sensing models towards images with lower resolution for two downstream\ntasks. Second, we train iBOT, a self-supervised vision encoder, on MillionAID,\nan ImageNet-scale satellite imagery dataset, with several modifications\nspecific to remote sensing. We show that none of those pretrained models bring\nconsistent improvements upon general-purpose baselines at the ViT-B scale.",
        "url": "http://arxiv.org/abs/2510.17014v1",
        "published_date": "2025-10-19T21:32:01+00:00",
        "updated_date": "2025-10-19T21:32:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ani Vanyan",
            "Alvard Barseghyan",
            "Hakob Tamazyan",
            "Tigran Galstyan",
            "Vahan Huroyan",
            "Naira Hovakimyan",
            "Hrant Khachatrian"
        ],
        "ai_categories": [
            "Dataset",
            "Multimodality"
        ],
        "tldr": "The paper challenges the idea that specialized foundation models are more useful than general-purpose vision models for satellite tasks, showing that pretrained models do not consistently improve performance at a small scale.",
        "tldr_zh": "本文质疑特定基础模型是否比通用视觉模型更适用于卫星任务，表明预训练模型在小规模上并未连续改善表现。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.5
    },
    {
        "title": "Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions",
        "summary": "3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe\nocclusions and optical distortions caused by raindrop contamination on the\ncamera lens, substantially degrading reconstruction quality. Existing\nbenchmarks typically evaluate 3DGS using synthetic raindrop images with known\ncamera poses (constrained images), assuming ideal conditions. However, in\nreal-world scenarios, raindrops often interfere with accurate camera pose\nestimation and point cloud initialization. Moreover, a significant domain gap\nbetween synthetic and real raindrops further impairs generalization. To tackle\nthese issues, we introduce RaindropGS, a comprehensive benchmark designed to\nevaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images\nto clear 3DGS reconstructions. Specifically, the whole benchmark pipeline\nconsists of three parts: data preparation, data processing, and raindrop-aware\n3DGS evaluation, including types of raindrop interference, camera pose\nestimation and point cloud initialization, single image rain removal\ncomparison, and 3D Gaussian training comparison. First, we collect a real-world\nraindrop reconstruction dataset, in which each scene contains three aligned\nimage sets: raindrop-focused, background-focused, and rain-free ground truth,\nenabling a comprehensive evaluation of reconstruction quality under different\nfocus conditions. Through comprehensive experiments and analyses, we reveal\ncritical insights into the performance limitations of existing 3DGS methods on\nunconstrained raindrop images and the varying impact of different pipeline\ncomponents: the impact of camera focus position on 3DGS reconstruction\nperformance, and the interference caused by inaccurate pose and point cloud\ninitialization on reconstruction. These insights establish clear directions for\ndeveloping more robust 3DGS methods under raindrop conditions.",
        "url": "http://arxiv.org/abs/2510.17719v1",
        "published_date": "2025-10-20T16:36:15+00:00",
        "updated_date": "2025-10-20T16:36:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhiqiang Teng",
            "Beibei Lin",
            "Tingting Chen",
            "Zifeng Yuan",
            "Xuanyi Li",
            "Xuanyu Zhang",
            "Shunli Zhang"
        ],
        "ai_categories": [
            "Dataset"
        ],
        "tldr": "The paper introduces RaindropGS, a benchmark for evaluating 3D Gaussian Splatting under real-world raindrop conditions, highlighting the challenges and proposing insights for developing more robust methods.",
        "tldr_zh": "该论文引入了Raindrop GS，这是一个用于评估3D高斯分层技术在真实雨滴条件下的基准，突出挑战并提出发展更强大方法的见解。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.25
    },
    {
        "title": "CharDiff: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration",
        "summary": "The significance of license plate image restoration goes beyond the\npreprocessing stage of License Plate Recognition (LPR) systems, as it also\nserves various purposes, including increasing evidential value, enhancing the\nclarity of visual interface, and facilitating further utilization of license\nplate images. We propose a novel diffusion-based framework with character-level\nguidance, CharDiff, which effectively restores and recognizes severely degraded\nlicense plate images captured under realistic conditions. CharDiff leverages\nfine-grained character-level priors extracted through external segmentation and\nOptical Character Recognition (OCR) modules tailored for low-quality license\nplate images. For precise and focused guidance, CharDiff incorporates a novel\nCharacter-guided Attention through Region-wise Masking (CHARM) module, which\nensures that each character's guidance is restricted to its own region, thereby\navoiding interference with other regions. In experiments, CharDiff\nsignificantly outperformed the baseline restoration models in both restoration\nquality and recognition accuracy, achieving a 28% relative reduction in CER on\nthe Roboflow-LP dataset, compared to the best-performing baseline model. These\nresults indicate that the structured character-guided conditioning effectively\nenhances the robustness of diffusion-based license plate restoration and\nrecognition in practical deployment scenarios.",
        "url": "http://arxiv.org/abs/2510.17330v1",
        "published_date": "2025-10-20T09:23:29+00:00",
        "updated_date": "2025-10-20T09:23:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Gyuhwan Park",
            "Kihyun Na",
            "Injung Kim"
        ],
        "ai_categories": [
            "Diffusion",
            "Dataset",
            "Other"
        ],
        "tldr": "CharDiff is a novel diffusion-based framework for restoring and recognizing license plate images with character-level guidance, outperforming baseline models in both restoration quality and recognition accuracy.",
        "tldr_zh": "CharDiff是一个新颖的基于扩散的框架，通过字符级指导来修复和识别车牌图像，在恢复质量和识别准确性方面优于基准模型。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6.25
    },
    {
        "title": "Investigating Adversarial Robustness against Preprocessing used in Blackbox Face Recognition",
        "summary": "Face Recognition (FR) models have been shown to be vulnerable to adversarial\nexamples that subtly alter benign facial images, exposing blind spots in these\nsystems, as well as protecting user privacy. End-to-end FR systems first obtain\npreprocessed faces from diverse facial imagery prior to computing the\nsimilarity of the deep feature embeddings. Whilst face preprocessing is a\ncritical component of FR systems, and hence adversarial attacks against them,\nwe observe that this preprocessing is often overlooked in blackbox settings.\nOur study seeks to investigate the transferability of several out-of-the-box\nstate-of-the-art adversarial attacks against FR when applied against different\npreprocessing techniques used in a blackbox setting. We observe that the choice\nof face detection model can degrade the attack success rate by up to 78%,\nwhereas choice of interpolation method during downsampling has relatively\nminimal impacts. Furthermore, we find that the requirement for facial\npreprocessing even degrades attack strength in a whitebox setting, due to the\nunintended interaction of produced noise vectors against face detection models.\nBased on these findings, we propose a preprocessing-invariant method using\ninput transformations that improves the transferability of the studied attacks\nby up to 27%. Our findings highlight the importance of preprocessing in FR\nsystems, and the need for its consideration towards improving the adversarial\ngeneralisation of facial adversarial examples.",
        "url": "http://arxiv.org/abs/2510.17169v1",
        "published_date": "2025-10-20T05:19:35+00:00",
        "updated_date": "2025-10-20T05:19:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Roland Croft",
            "Brian Du",
            "Darcy Joseph",
            "Sharath Kumar"
        ],
        "ai_categories": [
            "GAN",
            "AIGC",
            "Dataset"
        ],
        "tldr": "The paper investigates the impact of different preprocessing techniques on the vulnerability of face recognition models to adversarial attacks, proposing a preprocessing-invariant method to improve attack transferability.",
        "tldr_zh": "本文研究了不同预处理技术对人脸识别模型对抗攻击的影响，提出了一种预处理不变方法来提高攻击可迁移性。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6.25
    },
    {
        "title": "A Single Set of Adversarial Clothes Breaks Multiple Defense Methods in the Physical World",
        "summary": "In recent years, adversarial attacks against deep learning-based object\ndetectors in the physical world have attracted much attention. To defend\nagainst these attacks, researchers have proposed various defense methods\nagainst adversarial patches, a typical form of physically-realizable attack.\nHowever, our experiments showed that simply enlarging the patch size could make\nthese defense methods fail. Motivated by this, we evaluated various defense\nmethods against adversarial clothes which have large coverage over the human\nbody. Adversarial clothes provide a good test case for adversarial defense\nagainst patch-based attacks because they not only have large sizes but also\nlook more natural than a large patch on humans. Experiments show that all the\ndefense methods had poor performance against adversarial clothes in both the\ndigital world and the physical world. In addition, we crafted a single set of\nclothes that broke multiple defense methods on Faster R-CNN. The set achieved\nan Attack Success Rate (ASR) of 96.06% against the undefended detector and over\n64.84% ASRs against nine defended models in the physical world, unveiling the\ncommon vulnerability of existing adversarial defense methods against\nadversarial clothes. Code is available at:\nhttps://github.com/weiz0823/adv-clothes-break-multiple-defenses.",
        "url": "http://arxiv.org/abs/2510.17322v1",
        "published_date": "2025-10-20T09:16:25+00:00",
        "updated_date": "2025-10-20T09:16:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wei Zhang",
            "Zhanhao Hu",
            "Xiao Li",
            "Xiaopei Zhu",
            "Xiaolin Hu"
        ],
        "ai_categories": [
            "GAN",
            "Other"
        ],
        "tldr": "The paper explores the vulnerability of defense methods against adversarial attacks using clothes, showing that existing methods are ineffective in both digital and physical worlds.",
        "tldr_zh": "该论文探讨了防御物理世界中的对抗性攻击的方法对衣物产生的漏洞，表明现有方法在数字和物理世界中都无效。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "Exploring Structural Degradation in Dense Representations for Self-supervised Learning",
        "summary": "In this work, we observe a counterintuitive phenomenon in self-supervised\nlearning (SSL): longer training may impair the performance of dense prediction\ntasks (e.g., semantic segmentation). We refer to this phenomenon as\nSelf-supervised Dense Degradation (SDD) and demonstrate its consistent presence\nacross sixteen state-of-the-art SSL methods with various losses, architectures,\nand datasets. When the model performs suboptimally on dense tasks at the end of\ntraining, measuring the performance during training becomes essential. However,\nevaluating dense performance effectively without annotations remains an open\nchallenge. To tackle this issue, we introduce a Dense representation Structure\nEstimator (DSE), composed of a class-relevance measure and an effective\ndimensionality measure. The proposed DSE is both theoretically grounded and\nempirically validated to be closely correlated with the downstream performance.\nBased on this metric, we introduce a straightforward yet effective model\nselection strategy and a DSE-based regularization method. Experiments on\nsixteen SSL methods across four benchmarks confirm that model selection\nimproves mIoU by $3.0\\%$ on average with negligible computational cost.\nAdditionally, DSE regularization consistently mitigates the effects of dense\ndegradation. Code is available at\nhttps://github.com/EldercatSAM/SSL-Degradation.",
        "url": "http://arxiv.org/abs/2510.17299v1",
        "published_date": "2025-10-20T08:40:16+00:00",
        "updated_date": "2025-10-20T08:40:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Siran Dai",
            "Qianqian Xu",
            "Peisong Wen",
            "Yang Liu",
            "Qingming Huang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces a phenomenon of self-supervised dense degradation in AI learning and proposes a method to measure and mitigate this issue.",
        "tldr_zh": "该论文介绍了自监督稠密降解现象，并提出了一种方法来衡量和减轻这一问题。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "GSPlane: Concise and Accurate Planar Reconstruction via Structured Representation",
        "summary": "Planes are fundamental primitives of 3D sences, especially in man-made\nenvironments such as indoor spaces and urban streets. Representing these planes\nin a structured and parameterized format facilitates scene editing and physical\nsimulations in downstream applications. Recently, Gaussian Splatting (GS) has\ndemonstrated remarkable effectiveness in the Novel View Synthesis task, with\nextensions showing great potential in accurate surface reconstruction. However,\neven state-of-the-art GS representations often struggle to reconstruct planar\nregions with sufficient smoothness and precision. To address this issue, we\npropose GSPlane, which recovers accurate geometry and produces clean and\nwell-structured mesh connectivity for plane regions in the reconstructed scene.\nBy leveraging off-the-shelf segmentation and normal prediction models, GSPlane\nextracts robust planar priors to establish structured representations for\nplanar Gaussian coordinates, which help guide the training process by enforcing\ngeometric consistency. To further enhance training robustness, a Dynamic\nGaussian Re-classifier is introduced to adaptively reclassify planar Gaussians\nwith persistently high gradients as non-planar, ensuring more reliable\noptimization. Furthermore, we utilize the optimized planar priors to refine the\nmesh layouts, significantly improving topological structure while reducing the\nnumber of vertices and faces. We also explore applications of the structured\nplanar representation, which enable decoupling and flexible manipulation of\nobjects on supportive planes. Extensive experiments demonstrate that, with no\nsacrifice in rendering quality, the introduction of planar priors significantly\nimproves the geometric accuracy of the extracted meshes across various\nbaselines.",
        "url": "http://arxiv.org/abs/2510.17095v1",
        "published_date": "2025-10-20T01:59:21+00:00",
        "updated_date": "2025-10-20T01:59:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruitong Gan",
            "Junran Peng",
            "Yang Liu",
            "Chuanchen Luo",
            "Qing Li",
            "Zhaoxiang Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces GSPlane, a method for accurate planar reconstruction in 3D scenes, improving geometric accuracy and mesh connectivity for plane regions.",
        "tldr_zh": "本文介绍了GSPlane，一种用于准确重建三维场景中平面的方法，提高了对平面区域的几何精度和网格连接性。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]