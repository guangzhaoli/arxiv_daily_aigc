[
    {
        "title": "E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training",
        "summary": "Self-supervised pre-training has revolutionized foundation models for languages, individual 2D images and videos, but remains largely unexplored for learning 3D-aware representations from multi-view images. In this paper, we present E-RayZer, a self-supervised large 3D Vision model that learns truly 3D-aware representations directly from unlabeled images. Unlike prior self-supervised methods such as RayZer that infer 3D indirectly through latent-space view synthesis, E-RayZer operates directly in 3D space, performing self-supervised 3D reconstruction with Explicit geometry. This formulation eliminates shortcut solutions and yields representations that are geometrically grounded. To ensure convergence and scalability, we introduce a novel fine-grained learning curriculum that organizes training from easy to hard samples and harmonizes heterogeneous data sources in an entirely unsupervised manner. Experiments demonstrate that E-RayZer significantly outperforms RayZer on pose estimation, matches or sometimes surpasses fully supervised reconstruction models such as VGGT. Furthermore, its learned representations outperform leading visual pre-training models (e.g., DINOv3, CroCo v2, VideoMAE V2, and RayZer) when transferring to 3D downstream tasks, establishing E-RayZer as a new paradigm for 3D-aware visual pre-training.",
        "url": "http://arxiv.org/abs/2512.10950v1",
        "published_date": "2025-12-11T18:59:53+00:00",
        "updated_date": "2025-12-11T18:59:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qitao Zhao",
            "Hao Tan",
            "Qianqian Wang",
            "Sai Bi",
            "Kai Zhang",
            "Kalyan Sunkavalli",
            "Shubham Tulsiani",
            "Hanwen Jiang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "E-RayZer is a self-supervised 3D reconstruction model that directly learns 3D-aware representations from unlabeled images, outperforming previous methods and leading visual pre-training models on various tasks.",
        "tldr_zh": "E-RayZer是一种自监督3D重建模型，直接从未标记的图像中学习3D感知表示，在各种任务上优于先前方法和领先的视觉预训练模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8.5
    },
    {
        "title": "AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation",
        "summary": "Recent advances in subject-driven video generation with large diffusion models have enabled personalized content synthesis conditioned on user-provided subjects. However, existing methods lack fine-grained temporal control over subject appearance and disappearance, which are essential for applications such as compositional video synthesis, storyboarding, and controllable animation. We propose AlcheMinT, a unified framework that introduces explicit timestamps conditioning for subject-driven video generation. Our approach introduces a novel positional encoding mechanism that unlocks the encoding of temporal intervals, associated in our case with subject identities, while seamlessly integrating with the pretrained video generation model positional embeddings. Additionally, we incorporate subject-descriptive text tokens to strengthen binding between visual identity and video captions, mitigating ambiguity during generation. Through token-wise concatenation, AlcheMinT avoids any additional cross-attention modules and incurs negligible parameter overhead. We establish a benchmark evaluating multiple subject identity preservation, video fidelity, and temporal adherence. Experimental results demonstrate that AlcheMinT achieves visual quality matching state-of-the-art video personalization methods, while, for the first time, enabling precise temporal control over multi-subject generation within videos. Project page is at https://snap-research.github.io/Video-AlcheMinT",
        "url": "http://arxiv.org/abs/2512.10943v1",
        "published_date": "2025-12-11T18:59:34+00:00",
        "updated_date": "2025-12-11T18:59:34+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sharath Girish",
            "Viacheslav Ivanov",
            "Tsai-Shien Chen",
            "Hao Chen",
            "Aliaksandr Siarohin",
            "Sergey Tulyakov"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "AlcheMinT is a framework for subject-driven video generation that allows fine-grained temporal control and integration of subject-descriptive text tokens for improved video synthesis.",
        "tldr_zh": "AlcheMinT 是一个用于主题驱动视频生成的框架，可以实现细粒度的时间控制，同时整合主题描述性文本令牌以改进视频合成。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language",
        "summary": "We introduce VL-JEPA, a vision-language model built on a Joint Embedding Predictive Architecture (JEPA). Instead of autoregressively generating tokens as in classical VLMs, VL-JEPA predicts continuous embeddings of the target texts. By learning in an abstract representation space, the model focuses on task-relevant semantics while abstracting away surface-level linguistic variability. In a strictly controlled comparison against standard token-space VLM training with the same vision encoder and training data, VL-JEPA achieves stronger performance while having 50% fewer trainable parameters. At inference time, a lightweight text decoder is invoked only when needed to translate VL-JEPA predicted embeddings into text. We show that VL-JEPA natively supports selective decoding that reduces the number of decoding operations by 2.85x while maintaining similar performance compared to non-adaptive uniform decoding. Beyond generation, the VL-JEPA's embedding space naturally supports open-vocabulary classification, text-to-video retrieval, and discriminative VQA without any architecture modification. On eight video classification and eight video retrieval datasets, the average performance VL-JEPA surpasses that of CLIP, SigLIP2, and Perception Encoder. At the same time, the model achieves comparable performance as classical VLMs (InstructBLIP, QwenVL) on four VQA datasets: GQA, TallyQA, POPE and POPEv2, despite only having 1.6B parameters.",
        "url": "http://arxiv.org/abs/2512.10942v1",
        "published_date": "2025-12-11T18:59:22+00:00",
        "updated_date": "2025-12-11T18:59:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Delong Chen",
            "Mustafa Shukor",
            "Theo Moutakanni",
            "Willy Chung",
            "Jade Yu",
            "Tejaswi Kasarla",
            "Allen Bolourchi",
            "Yann LeCun",
            "Pascale Fung"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality",
            "Dataset"
        ],
        "tldr": "VL-JEPA is a vision-language model that predicts continuous embeddings of target texts in an abstract representation space, achieving strong performance with fewer parameters and supporting selective decoding.",
        "tldr_zh": "VL-JEPA是一种视觉语言模型，预测目标文本的连续嵌入在一个抽象表示空间中，以较少的参数实现强大性能，并支持选择性解码。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis",
        "summary": "Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks. Our method separately represents space, time, and view conditions, enabling flexible combinations of these inputs. For example, OmniView can synthesize novel views from static, dynamic, and multiview inputs, extrapolate trajectories forward and backward in time, and create videos from text or image prompts with full camera control. OmniView is competitive with task-specific models across diverse benchmarks and metrics, improving image quality scores among camera-conditioned diffusion models by up to 33\\% in multiview NVS LLFF dataset, 60\\% in dynamic NVS Neural 3D Video benchmark, 20\\% in static camera control on RE-10K, and reducing camera trajectory errors by 4x in text-conditioned video generation. With strong generalizability in one model, OmniView demonstrates the feasibility of a generalist 4D video model. Project page is available at https://snap-research.github.io/OmniView/",
        "url": "http://arxiv.org/abs/2512.10940v1",
        "published_date": "2025-12-11T18:59:05+00:00",
        "updated_date": "2025-12-11T18:59:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xiang Fan",
            "Sharath Girish",
            "Vivek Ramanujan",
            "Chaoyang Wang",
            "Ashkan Mirzaei",
            "Petr Sushko",
            "Aliaksandr Siarohin",
            "Sergey Tulyakov",
            "Ranjay Krishna"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN",
            "Transformer",
            "Dataset"
        ],
        "tldr": "OmniView is a unified framework for 4D view synthesis that can handle a wide variety of tasks such as novel view synthesis, trajectory extrapolation, and video generation from text or image prompts with full camera control.",
        "tldr_zh": "OmniView是一个统一的框架，用于处理多种3D视图合成任务，包括新颖视图合成，轨迹外推，以及基于文本或图像提示生成视频并具有完整的摄像机控制。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization",
        "summary": "Visual concept personalization aims to transfer only specific image attributes, such as identity, expression, lighting, and style, into unseen contexts. However, existing methods rely on holistic embeddings from general-purpose image encoders, which entangle multiple visual factors and make it difficult to isolate a single attribute. This often leads to information leakage and incoherent synthesis. To address this limitation, we introduce Omni-Attribute, the first open-vocabulary image attribute encoder designed to learn high-fidelity, attribute-specific representations. Our approach jointly designs the data and model: (i) we curate semantically linked image pairs annotated with positive and negative attributes to explicitly teach the encoder what to preserve or suppress; and (ii) we adopt a dual-objective training paradigm that balances generative fidelity with contrastive disentanglement. The resulting embeddings prove effective for open-vocabulary attribute retrieval, personalization, and compositional generation, achieving state-of-the-art performance across multiple benchmarks.",
        "url": "http://arxiv.org/abs/2512.10955v1",
        "published_date": "2025-12-11T18:59:56+00:00",
        "updated_date": "2025-12-11T18:59:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tsai-Shien Chen",
            "Aliaksandr Siarohin",
            "Guocheng Gordon Qian",
            "Kuan-Chieh Jackson Wang",
            "Egor Nemchinov",
            "Moayed Haji-Ali",
            "Riza Alp Guler",
            "Willi Menapace",
            "Ivan Skorokhodov",
            "Anil Kag",
            "Jun-Yan Zhu",
            "Sergey Tulyakov"
        ],
        "ai_categories": [
            "Dataset",
            "GAN"
        ],
        "tldr": "The paper introduces Omni-Attribute, an open-vocabulary image attribute encoder aiming to learn attribute-specific representations for visual concept personalization.",
        "tldr_zh": "本文介绍了Omni-Attribute，一个旨在学习属性特定表示的开放词汇图像属性编码器，用于视觉概念个性化。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Group Diffusion: Enhancing Image Generation by Unlocking Cross-Sample Collaboration",
        "summary": "In this work, we explore an untapped signal in diffusion model inference. While all previous methods generate images independently at inference, we instead ask if samples can be generated collaboratively. We propose Group Diffusion, unlocking the attention mechanism to be shared across images, rather than limited to just the patches within an image. This enables images to be jointly denoised at inference time, learning both intra and inter-image correspondence. We observe a clear scaling effect - larger group sizes yield stronger cross-sample attention and better generation quality. Furthermore, we introduce a qualitative measure to capture this behavior and show that its strength closely correlates with FID. Built on standard diffusion transformers, our GroupDiff achieves up to 32.2% FID improvement on ImageNet-256x256. Our work reveals cross-sample inference as an effective, previously unexplored mechanism for generative modeling.",
        "url": "http://arxiv.org/abs/2512.10954v1",
        "published_date": "2025-12-11T18:59:55+00:00",
        "updated_date": "2025-12-11T18:59:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sicheng Mo",
            "Thao Nguyen",
            "Richard Zhang",
            "Nick Kolkin",
            "Siddharth Srinivasan Iyer",
            "Eli Shechtman",
            "Krishna Kumar Singh",
            "Yong Jae Lee",
            "Bolei Zhou",
            "Yuheng Li"
        ],
        "ai_categories": [
            "Diffusion",
            "Transformer",
            "GAN"
        ],
        "tldr": "The paper introduces Group Diffusion, a method that allows images to be generated collaboratively and jointly denoised at inference time, leading to improved generation quality.",
        "tldr_zh": "该论文介绍了Group Diffusion，一种允许图像在推断时协同生成并联合去噪的方法，从而提高生成质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "Any4D: Unified Feed-Forward Metric 4D Reconstruction",
        "summary": "We present Any4D, a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available. One of the key innovations that allows for such a flexible framework is a modular representation of a 4D scene; specifically, per-view 4D predictions are encoded using a variety of egocentric factors (depthmaps and camera intrinsics) represented in local camera coordinates, and allocentric factors (camera extrinsics and scene flow) represented in global world coordinates. We achieve superior performance across diverse setups - both in terms of accuracy (2-3X lower error) and compute efficiency (15X faster), opening avenues for multiple downstream applications.",
        "url": "http://arxiv.org/abs/2512.10935v1",
        "published_date": "2025-12-11T18:57:39+00:00",
        "updated_date": "2025-12-11T18:57:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Jay Karhade",
            "Nikhil Keetha",
            "Yuchen Zhang",
            "Tanisha Gupta",
            "Akash Sharma",
            "Sebastian Scherer",
            "Deva Ramanan"
        ],
        "ai_categories": [
            "Transformer",
            "Multimodality"
        ],
        "tldr": "Any4D is a novel transformer for 4D reconstruction that can handle various modalities and sensors, achieving superior performance and efficiency.",
        "tldr_zh": "Any4D是一种新的转换器，用于4D重建，可以处理各种模态和传感器，实现更优越的性能和效率。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 9,
        "overall_priority_score": 8.25
    },
    {
        "title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
        "summary": "We introduce StereoSpace, a diffusion-based framework for monocular-to-stereo synthesis that models geometry purely through viewpoint conditioning, without explicit depth or warping. A canonical rectified space and the conditioning guide the generator to infer correspondences and fill disocclusions end-to-end. To ensure fair and leakage-free evaluation, we introduce an end-to-end protocol that excludes any ground truth or proxy geometry estimates at test time. The protocol emphasizes metrics reflecting downstream relevance: iSQoE for perceptual comfort and MEt3R for geometric consistency. StereoSpace surpasses other methods from the warp & inpaint, latent-warping, and warped-conditioning categories, achieving sharp parallax and strong robustness on layered and non-Lambertian scenes. This establishes viewpoint-conditioned diffusion as a scalable, depth-free solution for stereo generation.",
        "url": "http://arxiv.org/abs/2512.10959v1",
        "published_date": "2025-12-11T18:59:59+00:00",
        "updated_date": "2025-12-11T18:59:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tjark Behrens",
            "Anton Obukhov",
            "Bingxin Ke",
            "Fabio Tosi",
            "Matteo Poggi",
            "Konrad Schindler"
        ],
        "ai_categories": [
            "Diffusion",
            "GAN"
        ],
        "tldr": "StereoSpace is a depth-free framework for monocular-to-stereo synthesis using viewpoint conditioning and diffusion, achieving sharp parallax and robustness on complex scenes.",
        "tldr_zh": "StereoSpace是一种无深度的框架，通过视点调节和扩散实现单目到立体合成，在复杂场景中实现了清晰的视差和稳健性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World",
        "summary": "Generative world models are reshaping embodied AI, enabling agents to synthesize realistic 4D driving environments that look convincing but often fail physically or behaviorally. Despite rapid progress, the field still lacks a unified way to assess whether generated worlds preserve geometry, obey physics, or support reliable control. We introduce WorldLens, a full-spectrum benchmark evaluating how well a model builds, understands, and behaves within its generated world. It spans five aspects -- Generation, Reconstruction, Action-Following, Downstream Task, and Human Preference -- jointly covering visual realism, geometric consistency, physical plausibility, and functional reliability. Across these dimensions, no existing world model excels universally: those with strong textures often violate physics, while geometry-stable ones lack behavioral fidelity. To align objective metrics with human judgment, we further construct WorldLens-26K, a large-scale dataset of human-annotated videos with numerical scores and textual rationales, and develop WorldLens-Agent, an evaluation model distilled from these annotations to enable scalable, explainable scoring. Together, the benchmark, dataset, and agent form a unified ecosystem for measuring world fidelity -- standardizing how future models are judged not only by how real they look, but by how real they behave.",
        "url": "http://arxiv.org/abs/2512.10958v1",
        "published_date": "2025-12-11T18:59:58+00:00",
        "updated_date": "2025-12-11T18:59:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ao Liang",
            "Lingdong Kong",
            "Tianyi Yan",
            "Hongsi Liu",
            "Wesley Yang",
            "Ziqi Huang",
            "Wei Yin",
            "Jialong Zuo",
            "Yixuan Hu",
            "Dekai Zhu",
            "Dongyue Lu",
            "Youquan Liu",
            "Guangfeng Jiang",
            "Linfeng Li",
            "Xiangtai Li",
            "Long Zhuo",
            "Lai Xing Ng",
            "Benoit R. Cottereau",
            "Changxin Gao",
            "Liang Pan",
            "Wei Tsang Ooi",
            "Ziwei Liu"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper presents WorldLens, a benchmark for evaluating the fidelity of generated driving world models by assessing visual realism, geometric consistency, physical plausibility, and functional reliability.",
        "tldr_zh": "本文介绍了WorldLens，这是一个用于评估生成的驾驶世界模型准确性的基准，评估视觉真实性、几何一致性、物理合理性和功能可靠性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model",
        "summary": "We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.",
        "url": "http://arxiv.org/abs/2512.10957v1",
        "published_date": "2025-12-11T18:59:56+00:00",
        "updated_date": "2025-12-11T18:59:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yukai Shi",
            "Weiyu Li",
            "Zihao Wang",
            "Hongyang Li",
            "Xingyu Chen",
            "Ping Tan",
            "Lei Zhang"
        ],
        "ai_categories": [
            "GAN",
            "Dataset",
            "Other"
        ],
        "tldr": "The paper introduces SceneMaker, a framework for 3D scene generation with improved de-occlusion and pose estimation models, showcasing superior performance on diverse open-set scenes.",
        "tldr_zh": "本文介绍了SceneMaker，这是一个用于3D场景生成的框架，通过改进去除遮挡和姿态估计模型，在多样的开放场景中展现出卓越性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Bidirectional Normalizing Flow: From Data to Noise and Back",
        "summary": "Normalizing Flows (NFs) have been established as a principled framework for generative modeling. Standard NFs consist of a forward process and a reverse process: the forward process maps data to noise, while the reverse process generates samples by inverting it. Typical NF forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. Recent developments in TARFlow and its variants have revitalized NF methods by combining Transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. In this work, we introduce Bidirectional Normalizing Flow ($\\textbf{BiFlow}$), a framework that removes the need for an exact analytic inverse. BiFlow learns a reverse model that approximates the underlying noise-to-data inverse mapping, enabling more flexible loss functions and architectures. Experiments on ImageNet demonstrate that BiFlow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. BiFlow yields state-of-the-art results among NF-based methods and competitive performance among single-evaluation (\"1-NFE\") methods. Following recent encouraging progress on NFs, we hope our work will draw further attention to this classical paradigm.",
        "url": "http://arxiv.org/abs/2512.10953v1",
        "published_date": "2025-12-11T18:59:55+00:00",
        "updated_date": "2025-12-11T18:59:55+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Yiyang Lu",
            "Qiao Sun",
            "Xianbang Wang",
            "Zhicheng Jiang",
            "Hanhong Zhao",
            "Kaiming He"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Diffusion"
        ],
        "tldr": "The paper introduces Bidirectional Normalizing Flow (BiFlow) framework for generative modeling, improving generation quality and sampling speed compared to existing methods.",
        "tldr_zh": "本文介绍了双向归一化流（BiFlow）框架，用于生成建模，改善了与现有方法相比的生成质量和采样速度。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation",
        "summary": "Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.",
        "url": "http://arxiv.org/abs/2512.10949v1",
        "published_date": "2025-12-11T18:59:52+00:00",
        "updated_date": "2025-12-11T18:59:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Yiwen Tang",
            "Zoey Guo",
            "Kaixin Zhu",
            "Ray Zhang",
            "Qizhi Chen",
            "Dongzhi Jiang",
            "Junli Liu",
            "Bohan Zeng",
            "Haoming Song",
            "Delin Qu",
            "Tianyi Bai",
            "Dan Xu",
            "Wentao Zhang",
            "Bin Zhao"
        ],
        "ai_categories": [
            "Transformer",
            "GAN",
            "Dataset"
        ],
        "tldr": "The paper investigates the application of reinforcement learning in text-to-3D generation, addressing challenges in reward design, RL algorithms, benchmarks, and proposing a hierarchical RL paradigm for 3D generation.",
        "tldr_zh": "本文研究了强化学习在文本到3D生成中的应用，解决了奖励设计，RL算法，基准测试等挑战，并提出了用于3D生成的分层RL范例。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ClusIR: Towards Cluster-Guided All-in-One Image Restoration",
        "summary": "All-in-One Image Restoration (AiOIR) aims to recover high-quality images from diverse degradations within a unified framework. However, existing methods often fail to explicitly model degradation types and struggle to adapt their restoration behavior to complex or mixed degradations. To address these issues, we propose ClusIR, a Cluster-Guided Image Restoration framework that explicitly models degradation semantics through learnable clustering and propagates cluster-aware cues across spatial and frequency domains for adaptive restoration. Specifically, ClusIR comprises two key components: a Probabilistic Cluster-Guided Routing Mechanism (PCGRM) and a Degradation-Aware Frequency Modulation Module (DAFMM). The proposed PCGRM disentangles degradation recognition from expert activation, enabling discriminative degradation perception and stable expert routing. Meanwhile, DAFMM leverages the cluster-guided priors to perform adaptive frequency decomposition and targeted modulation, collaboratively refining structural and textural representations for higher restoration fidelity. The cluster-guided synergy seamlessly bridges semantic cues with frequency-domain modulation, empowering ClusIR to attain remarkable restoration results across a wide range of degradations. Extensive experiments on diverse benchmarks validate that ClusIR reaches competitive performance under several scenarios.",
        "url": "http://arxiv.org/abs/2512.10948v1",
        "published_date": "2025-12-11T18:59:47+00:00",
        "updated_date": "2025-12-11T18:59:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shengkai Hu",
            "Jiaqi Ma",
            "Jun Wan",
            "Wenwen Min",
            "Yongcheng Jing",
            "Lefei Zhang",
            "Dacheng Tao"
        ],
        "ai_categories": [
            "Transformer",
            "GAN"
        ],
        "tldr": "ClusIR proposes a Cluster-Guided Image Restoration framework for adaptive restoration of images from diverse degradations, achieving competitive performance.",
        "tldr_zh": "ClusIR提出了一种基于聚类引导的图像恢复框架，可以自适应地从各种退化中恢复图像，达到竞争性表现。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving",
        "summary": "We present Flex, an efficient and effective scene encoder that addresses the computational bottleneck of processing high-volume multi-camera data in end-to-end autonomous driving. Flex employs a small set of learnable scene tokens to jointly encode information from all image tokens across different cameras and timesteps. By design, our approach is geometry-agnostic, learning a compact scene representation directly from data without relying on the explicit 3D inductive biases, such as Bird-Eye-View (BEV), occupancy or tri-plane representations, which are common in prior work. This holistic encoding strategy aggressively compresses the visual input for the downstream Large Language Model (LLM) based policy model. Evaluated on a large-scale proprietary dataset of 20,000 driving hours, our Flex achieves 2.2x greater inference throughput while improving driving performance by a large margin compared to state-of-the-art methods. Furthermore, we show that these compact scene tokens develop an emergent capability for scene decomposition without any explicit supervision. Our findings challenge the prevailing assumption that 3D priors are necessary, demonstrating that a data-driven, joint encoding strategy offers a more scalable, efficient and effective path for future autonomous driving systems.",
        "url": "http://arxiv.org/abs/2512.10947v1",
        "published_date": "2025-12-11T18:59:46+00:00",
        "updated_date": "2025-12-11T18:59:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiawei Yang",
            "Ziyu Chen",
            "Yurong You",
            "Yan Wang",
            "Yiming Li",
            "Yuxiao Chen",
            "Boyi Li",
            "Boris Ivanovic",
            "Marco Pavone",
            "Yue Wang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "The paper introduces Flex, an efficient scene encoder for processing multi-camera data in autonomous driving, achieving better performance without relying on 3D inductive biases.",
        "tldr_zh": "该论文介绍了Flex，一种用于处理自动驾驶中的多摄像头数据的高效场景编码器，实现更好的性能而不依赖于3D归纳偏差。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation",
        "summary": "This paper proposes a large-scale multi-modal dataset for referring motion expression video segmentation, focusing on segmenting and tracking target objects in videos based on language description of objects' motions. Existing referring video segmentation datasets often focus on salient objects and use language expressions rich in static attributes, potentially allowing the target object to be identified in a single frame. Such datasets underemphasize the role of motion in both videos and languages. To explore the feasibility of using motion expressions and motion reasoning clues for pixel-level video understanding, we introduce MeViS, a dataset containing 33,072 human-annotated motion expressions in both text and audio, covering 8,171 objects in 2,006 videos of complex scenarios. We benchmark 15 existing methods across 4 tasks supported by MeViS, including 6 referring video object segmentation (RVOS) methods, 3 audio-guided video object segmentation (AVOS) methods, 2 referring multi-object tracking (RMOT) methods, and 4 video captioning methods for the newly introduced referring motion expression generation (RMEG) task. The results demonstrate weaknesses and limitations of existing methods in addressing motion expression-guided video understanding. We further analyze the challenges and propose an approach LMPM++ for RVOS/AVOS/RMOT that achieves new state-of-the-art results. Our dataset provides a platform that facilitates the development of motion expression-guided video understanding algorithms in complex video scenes. The proposed MeViS dataset and the method's source code are publicly available at https://henghuiding.com/MeViS/",
        "url": "http://arxiv.org/abs/2512.10945v1",
        "published_date": "2025-12-11T18:59:44+00:00",
        "updated_date": "2025-12-11T18:59:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Henghui Ding",
            "Chang Liu",
            "Shuting He",
            "Kaining Ying",
            "Xudong Jiang",
            "Chen Change Loy",
            "Yu-Gang Jiang"
        ],
        "ai_categories": [
            "Dataset",
            "Multimodality"
        ],
        "tldr": "The paper introduces MeViS, a dataset for referring motion expression video segmentation, highlighting the role of motion in video understanding. It evaluates existing methods and proposes an approach that achieves state-of-the-art results.",
        "tldr_zh": "本文介绍了MeViS，一个用于指代运动表达视频分割的数据集，强调了视频理解中运动的重要性。它评估了现有方法并提出了一种取得最先进结果的方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Mull-Tokens: Modality-Agnostic Latent Thinking",
        "summary": "Reasoning goes beyond language; the real world requires reasoning about space, time, affordances, and much more that words alone cannot convey. Existing multimodal models exploring the potential of reasoning with images are brittle and do not scale. They rely on calling specialist tools, costly generation of images, or handcrafted reasoning data to switch between text and image thoughts. Instead, we offer a simpler alternative -- Mull-Tokens -- modality-agnostic latent tokens pre-trained to hold intermediate information in either image or text modalities to let the model think free-form towards the correct answer. We investigate best practices to train Mull-Tokens inspired by latent reasoning frameworks. We first train Mull-Tokens using supervision from interleaved text-image traces, and then fine-tune without any supervision by only using the final answers. Across four challenging spatial reasoning benchmarks involving tasks such as solving puzzles and taking different perspectives, we demonstrate that Mull-Tokens improve upon several baselines utilizing text-only reasoning or interleaved image-text reasoning, achieving a +3% average improvement and up to +16% on a puzzle solving reasoning-heavy split compared to our strongest baseline. Adding to conversations around challenges in grounding textual and visual reasoning, Mull-Tokens offers a simple solution to abstractly think in multiple modalities.",
        "url": "http://arxiv.org/abs/2512.10941v1",
        "published_date": "2025-12-11T18:59:08+00:00",
        "updated_date": "2025-12-11T18:59:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Arijit Ray",
            "Ahmed Abdelkader",
            "Chengzhi Mao",
            "Bryan A. Plummer",
            "Kate Saenko",
            "Ranjay Krishna",
            "Leonidas Guibas",
            "Wen-Sheng Chu"
        ],
        "ai_categories": [
            "Multimodality",
            "Transformer"
        ],
        "tldr": "The paper introduces Mull-Tokens, modality-agnostic latent tokens for free-form thinking in both image and text modalities, improving spatial reasoning tasks.",
        "tldr_zh": "本文介绍了Mull-Tokens，这是一种模态无关的潜在令牌，用于在图像和文本模态中自由形式地思考，从而提高空间推理任务的准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GaussianHeadTalk: Wobble-Free 3D Talking Heads with Audio Driven Gaussian Splatting",
        "summary": "Speech-driven talking heads have recently emerged and enable interactive avatars. However, real-world applications are limited, as current methods achieve high visual fidelity but slow or fast yet temporally unstable. Diffusion methods provide realistic image generation, yet struggle with oneshot settings. Gaussian Splatting approaches are real-time, yet inaccuracies in facial tracking, or inconsistent Gaussian mappings, lead to unstable outputs and video artifacts that are detrimental to realistic use cases. We address this problem by mapping Gaussian Splatting using 3D Morphable Models to generate person-specific avatars. We introduce transformer-based prediction of model parameters, directly from audio, to drive temporal consistency. From monocular video and independent audio speech inputs, our method enables generation of real-time talking head videos where we report competitive quantitative and qualitative performance.",
        "url": "http://arxiv.org/abs/2512.10939v1",
        "published_date": "2025-12-11T18:59:02+00:00",
        "updated_date": "2025-12-11T18:59:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Madhav Agarwal",
            "Mingtian Zhang",
            "Laura Sevilla-Lara",
            "Steven McDonagh"
        ],
        "ai_categories": [
            "Transformer",
            "Diffusion",
            "GAN"
        ],
        "tldr": "The paper introduces a method for generating real-time talking head videos from audio inputs using Gaussian Splatting and 3D Morphable Models, resulting in stable and person-specific avatars.",
        "tldr_zh": "本文介绍了一种使用高斯分布和三维可变模型从音频输入生成实时语音头像视频的方法，产生稳定且具有人物特征的头像。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Stronger Normalization-Free Transformers",
        "summary": "Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce $\\mathrm{Derf}(x) = \\mathrm{erf}(αx + s)$, where $\\mathrm{erf}(x)$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.",
        "url": "http://arxiv.org/abs/2512.10938v1",
        "published_date": "2025-12-11T18:58:49+00:00",
        "updated_date": "2025-12-11T18:58:49+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Mingzhi Chen",
            "Taiming Lu",
            "Jiachen Zhu",
            "Mingjie Sun",
            "Zhuang Liu"
        ],
        "ai_categories": [
            "Transformer"
        ],
        "tldr": "The paper introduces a new normalization-free transformer design called Derf, which outperforms existing methods across various domains without sacrificing simplicity.",
        "tldr_zh": "本文介绍了一种新的无归一化 Transformer 设计 Derf，它在各个领域的性能都超越了现有方法，同时保持了简单性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models",
        "summary": "Early children's developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation. The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversational data that mirror infant experiences. DevCV Toolbox adapts all vision-related measures of the recently released NIH Baby Toolbox into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children's capabilities. Experimental results show that a compact model pretrained from scratch can achieve competitive performance on DevCV Toolbox, outperforming GPT-4o on some tasks. We hope the principled, unified BabyVLM-V2 framework will accelerate research in developmentally plausible pretraining of vision foundation models.",
        "url": "http://arxiv.org/abs/2512.10932v1",
        "published_date": "2025-12-11T18:57:05+00:00",
        "updated_date": "2025-12-11T18:57:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shengao Wang",
            "Wenqi Wang",
            "Zecheng Wang",
            "Max Whitton",
            "Michael Wakeham",
            "Arjun Chandra",
            "Joey Huang",
            "Pengyue Zhu",
            "Helen Chen",
            "David Li",
            "Jeffrey Li",
            "Shawn Li",
            "Andrew Zagula",
            "Amy Zhao",
            "Andrew Zhu",
            "Sayaka Nakamura",
            "Yuki Yamamoto",
            "Jerry Jun Yokono",
            "Aaron Mueller",
            "Bryan A. Plummer",
            "Kate Saenko",
            "Venkatesh Saligrama",
            "Boqing Gong"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset",
            "Transformer"
        ],
        "tldr": "BabyVLM-V2 introduces a developmentally grounded pretraining framework for vision-language modeling using an infant-centric audiovisual corpus and DevCV Toolbox for cognitive evaluation.",
        "tldr_zh": "BabyVLM-V2引入了一种基于发展阶段的预训练框架，利用婴儿中心的视听语料库和DevCV工具箱进行认知评估。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance",
        "summary": "Recent vision-language model (VLM)-based approaches have achieved impressive results on SVG generation. However, because they generate only text and lack visual signals during decoding, they often struggle with complex semantics and fail to produce visually appealing or geometrically coherent SVGs. We introduce DuetSVG, a unified multimodal model that jointly generates image tokens and corresponding SVG tokens in an end-to-end manner. DuetSVG is trained on both image and SVG datasets. At inference, we apply a novel test-time scaling strategy that leverages the model's native visual predictions as guidance to improve SVG decoding quality. Extensive experiments show that our method outperforms existing methods, producing visually faithful, semantically aligned, and syntactically clean SVGs across a wide range of applications.",
        "url": "http://arxiv.org/abs/2512.10894v1",
        "published_date": "2025-12-11T18:23:03+00:00",
        "updated_date": "2025-12-11T18:23:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Peiying Zhang",
            "Nanxuan Zhao",
            "Matthew Fisher",
            "Yiran Xu",
            "Jing Liao",
            "Difan Liu"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "DuetSVG is a unified multimodal model that generates visually appealing and geometrically coherent SVGs by leveraging visual guidance during decoding.",
        "tldr_zh": "DuetSVG是一个统一的多模态模型，通过在解码过程中利用视觉引导，生成视觉吸引人和几何连贯的SVG。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos",
        "summary": "Motion capture now underpins content creation far beyond digital humans, yet most existing pipelines remain species- or template-specific. We formalize this gap as Category-Agnostic Motion Capture (CAMoCap): given a monocular video and an arbitrary rigged 3D asset as a prompt, the goal is to reconstruct a rotation-based animation such as BVH that directly drives the specific asset. We present MoCapAnything, a reference-guided, factorized framework that first predicts 3D joint trajectories and then recovers asset-specific rotations via constraint-aware inverse kinematics. The system contains three learnable modules and a lightweight IK stage: (1) a Reference Prompt Encoder that extracts per-joint queries from the asset's skeleton, mesh, and rendered images; (2) a Video Feature Extractor that computes dense visual descriptors and reconstructs a coarse 4D deforming mesh to bridge the gap between video and joint space; and (3) a Unified Motion Decoder that fuses these cues to produce temporally coherent trajectories. We also curate Truebones Zoo with 1038 motion clips, each providing a standardized skeleton-mesh-render triad. Experiments on both in-domain benchmarks and in-the-wild videos show that MoCapAnything delivers high-quality skeletal animations and exhibits meaningful cross-species retargeting across heterogeneous rigs, enabling scalable, prompt-driven 3D motion capture for arbitrary assets. Project page: https://animotionlab.github.io/MoCapAnything/",
        "url": "http://arxiv.org/abs/2512.10881v1",
        "published_date": "2025-12-11T18:09:48+00:00",
        "updated_date": "2025-12-11T18:09:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kehong Gong",
            "Zhengyu Wen",
            "Weixia He",
            "Mingxi Xu",
            "Qi Wang",
            "Ning Zhang",
            "Zhengyu Li",
            "Dongze Lian",
            "Wei Zhao",
            "Xiaoyu He",
            "Mingyuan Zhang"
        ],
        "ai_categories": [
            "Transformer",
            "Dataset"
        ],
        "tldr": "MoCapAnything presents a new method for 3D motion capture from monocular videos, enabling prompt-driven animation for arbitrary skeletons with cross-species retargeting.",
        "tldr_zh": "MoCapAnything 提出了一种新的方法，通过单眼视频实现3D动作捕捉，实现了跨物种重新定位的动画。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models",
        "summary": "This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose a systematic benchmark framework MiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, a fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/datasets/zongzhao/MiSI-bench.",
        "url": "http://arxiv.org/abs/2512.10867v1",
        "published_date": "2025-12-11T18:00:21+00:00",
        "updated_date": "2025-12-11T18:00:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zongzhao Li",
            "Xiangzhe Kong",
            "Jiahui Su",
            "Zongyang Ma",
            "Mingze Li",
            "Songyou Li",
            "Yuelin Zhang",
            "Yu Rong",
            "Tingyang Xu",
            "Deli Zhao",
            "Wenbing Huang"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces Microscopic Spatial Intelligence (MiSI) and assesses Vision-Language Models (VLMs) for molecular spatial reasoning. VLMs show potential but need domain knowledge for scientific tasks.",
        "tldr_zh": "本文介绍了微观空间情报（MiSI）的概念，并评估了视觉-语言模型（VLMs）在分子空间推理中的应用。VLMs表现出潜力，但在科学任务中需要领域知识。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision",
        "summary": "The success of foundation models in language and vision motivated research in fully end-to-end robot navigation foundation models (NFMs). NFMs directly map monocular visual input to control actions and ignore mid-level vision modules (tracking, depth estimation, etc) entirely. While the assumption that vision capabilities will emerge implicitly is compelling, it requires large amounts of pixel-to-action supervision that are difficult to obtain. The challenge is especially pronounced in dynamic and unstructured settings, where robust navigation requires precise geometric and dynamic understanding, while the depth-scale ambiguity in monocular views further limits accurate spatial reasoning. In this paper, we show that relying on monocular vision and ignoring mid-level vision priors is inefficient.\n  We present StereoWalker, which augments NFMs with stereo inputs and explicit mid-level vision such as depth estimation and dense pixel tracking. Our intuition is straightforward: stereo inputs resolve the depth-scale ambiguity, and modern mid-level vision models provide reliable geometric and motion structure in dynamic scenes. We also curate a large stereo navigation dataset with automatic action annotation from Internet stereo videos to support training of StereoWalker and to facilitate future research. Through our experiments, we find that mid-level vision enables StereoWalker to achieve a comparable performance as the state-of-the-art using only 1.5% of the training data, and surpasses the state-of-the-art using the full data. We also observe that stereo vision yields higher navigation performance than monocular input.",
        "url": "http://arxiv.org/abs/2512.10956v1",
        "published_date": "2025-12-11T18:59:56+00:00",
        "updated_date": "2025-12-11T18:59:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wentao Zhou",
            "Xuweiyi Chen",
            "Vignesh Rajagopal",
            "Jeffrey Chen",
            "Rohan Chandra",
            "Zezhou Cheng"
        ],
        "ai_categories": [
            "Multimodality",
            "Dataset"
        ],
        "tldr": "The paper proposes a model that combines stereo vision and mid-level vision modules to improve robot navigation performance, demonstrating the effectiveness of incorporating these components.",
        "tldr_zh": "本文提出了一种结合立体视觉和中层视觉模块的模型，以提高机器人导航性能，证明了融合这些组件的有效性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos",
        "summary": "Motion understanding is fundamental to physical reasoning, enabling models to infer dynamics and predict future states. However, state-of-the-art models still struggle on recent motion benchmarks, primarily due to the scarcity of large-scale, fine-grained motion datasets. Existing motion datasets are often constructed from costly manual annotation, severely limiting scalability. To address this challenge, we introduce FoundationMotion, a fully automated data curation pipeline that constructs large-scale motion datasets. Our approach first detects and tracks objects in videos to extract their trajectories, then leverages these trajectories and video frames with Large Language Models (LLMs) to generate fine-grained captions and diverse question-answer pairs about motion and spatial reasoning. Using datasets produced by this pipeline, we fine-tune open-source models including NVILA-Video-15B and Qwen2.5-7B, achieving substantial improvements in motion understanding without compromising performance on other tasks. Notably, our models outperform strong closed-source baselines like Gemini-2.5 Flash and large open-source models such as Qwen2.5-VL-72B across diverse motion understanding datasets and benchmarks. FoundationMotion thus provides a scalable solution for curating fine-grained motion datasets that enable effective fine-tuning of diverse models to enhance motion understanding and spatial reasoning capabilities.",
        "url": "http://arxiv.org/abs/2512.10927v1",
        "published_date": "2025-12-11T18:53:15+00:00",
        "updated_date": "2025-12-11T18:53:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yulu Gan",
            "Ligeng Zhu",
            "Dandan Shan",
            "Baifeng Shi",
            "Hongxu Yin",
            "Boris Ivanovic",
            "Song Han",
            "Trevor Darrell",
            "Jitendra Malik",
            "Marco Pavone",
            "Boyi Li"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces FoundationMotion, a system that automatically creates large-scale motion datasets for training AI models in motion understanding and spatial reasoning.",
        "tldr_zh": "本文介绍了FoundationMotion，一个自动创建大规模运动数据集的系统，用于训练人工智能模型进行运动理解和空间推理。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7.5
    },
    {
        "title": "PubTables-v2: A new large-scale dataset for full-page and multi-page table extraction",
        "summary": "Table extraction (TE) is a key challenge in visual document understanding. Traditional approaches detect tables first, then recognize their structure. Recently, interest has surged in developing methods, such as vision-language models (VLMs), that can extract tables directly in their full page or document context. However, progress has been difficult to demonstrate due to a lack of annotated data. To address this, we create a new large-scale dataset, PubTables-v2. PubTables-v2 supports a number of current challenging table extraction tasks. Notably, it is the first large-scale benchmark for multi-page table structure recognition. We demonstrate its usefulness by evaluating domain-specialized VLMs on these tasks and highlighting current progress. Finally, we use PubTables-v2 to create the Page-Object Table Transformer (POTATR), an image-to-graph extension of the Table Transformer to comprehensive page-level TE. Data, code, and trained models will be released.",
        "url": "http://arxiv.org/abs/2512.10888v1",
        "published_date": "2025-12-11T18:19:00+00:00",
        "updated_date": "2025-12-11T18:19:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Brandon Smock",
            "Valerie Faucon-Morin",
            "Max Sokolov",
            "Libin Liang",
            "Tayyibah Khanam",
            "Maury Courtland"
        ],
        "ai_categories": [
            "Dataset",
            "Transformer"
        ],
        "tldr": "The paper introduces a new large-scale dataset for table extraction, particularly focusing on multi-page table structure recognition, with the goal of evaluating vision-language models for this task.",
        "tldr_zh": "该论文介绍了一个新的大规模数据集，用于表格提取，特别关注多页表结构识别，旨在评估视觉-语言模型在这一任务上的应用。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    }
]